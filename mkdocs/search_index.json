{
    "docs": [
        {
            "location": "/",
            "text": "",
            "title": "Home"
        },
        {
            "location": "/SystemProgramming/#Example Markdown/",
            "text": "Example\n\n\nSubsection\n\n\nHere's some code,\n\n\nint main() {\n  int a = 4;\n  a = a + 1;\n  return a;\n}",
            "title": "#Example Markdown"
        },
        {
            "location": "/SystemProgramming/#Example Markdown/#example",
            "text": "",
            "title": "Example"
        },
        {
            "location": "/SystemProgramming/#Example Markdown/#subsection",
            "text": "Here's some code,  int main() {\n  int a = 4;\n  a = a + 1;\n  return a;\n}",
            "title": "Subsection"
        },
        {
            "location": "/SystemProgramming/#Informal-Glossary/",
            "text": "What is the kernel?\n\n\nThe kernel is central part of the operating system that manages processes, resources (including memory) and hardware input-output devices. User programs interact with the kernel by making system calls.\n\n\nLearn more:\n[[http://en.wikipedia.org/wiki/Kernel_%28operating_system%29]]\n\n\nWhat is a process?\n\n\nA process is an instance of a program that is running on a machine. There can be multiple processes of the same program. For example, you and I might both be running 'cat' or 'gnuchess'\n\n\nA process contains the program code and modifiable state information such as variables, signals, open file descriptors for files, network connections and other system resources which are stored inside the process's memory. An operating system also stores meta-information about the process which is used by the system to manage and monitor the process's activity and resource use.\n\n\nLearn more:\n[[http://en.wikipedia.org/wiki/Process_%28computing%29]]\n\n\nWhat is virtual memory?\n\n\nProcesses running on your smart phone and laptop use virtual memory: Each process is isolated from other processes and appears to get complete access to all possible memory addresses! In reality only a small fraction of the process's address space is mapped to physical memory and the actual amount of physical memory allocated to a process can change over time and be paged out to disk, re-mapped and securely shared with other processes. Virtual memory provides significant benefits including strong process isolation (security), resource and performance benefits (simplified and efficient physical memory use) that we will discuss later. \n\n\nLearn more:\n[[http://en.wikipedia.org/wiki/Virtual_memory]]",
            "title": "#Informal Glossary"
        },
        {
            "location": "/SystemProgramming/#Informal-Glossary/#what-is-the-kernel",
            "text": "The kernel is central part of the operating system that manages processes, resources (including memory) and hardware input-output devices. User programs interact with the kernel by making system calls.  Learn more:\n[[http://en.wikipedia.org/wiki/Kernel_%28operating_system%29]]",
            "title": "What is the kernel?"
        },
        {
            "location": "/SystemProgramming/#Informal-Glossary/#what-is-a-process",
            "text": "A process is an instance of a program that is running on a machine. There can be multiple processes of the same program. For example, you and I might both be running 'cat' or 'gnuchess'  A process contains the program code and modifiable state information such as variables, signals, open file descriptors for files, network connections and other system resources which are stored inside the process's memory. An operating system also stores meta-information about the process which is used by the system to manage and monitor the process's activity and resource use.  Learn more:\n[[http://en.wikipedia.org/wiki/Process_%28computing%29]]",
            "title": "What is a process?"
        },
        {
            "location": "/SystemProgramming/#Informal-Glossary/#what-is-virtual-memory",
            "text": "Processes running on your smart phone and laptop use virtual memory: Each process is isolated from other processes and appears to get complete access to all possible memory addresses! In reality only a small fraction of the process's address space is mapped to physical memory and the actual amount of physical memory allocated to a process can change over time and be paged out to disk, re-mapped and securely shared with other processes. Virtual memory provides significant benefits including strong process isolation (security), resource and performance benefits (simplified and efficient physical memory use) that we will discuss later.   Learn more:\n[[http://en.wikipedia.org/wiki/Virtual_memory]]",
            "title": "What is virtual memory?"
        },
        {
            "location": "/SystemProgramming/#Piazza: When And How to Ask For Help/",
            "text": "Purpose\n\n\nTAs and student assistants get a ton of questions. Some are well-researched, and some...are not. This is a handy guide that'll help you move away from the latter and towards the former. (Oh, and did I mention that this is an easy way to score points with your internship managers?)\n\n\nAsk yourself...\n\n\n\n\nAm I running on EWS?\n\n\nDid I check the man pages?\n\n\nHave I searched for similar questions/followups on Piazza?\n\n\nHave I read the MP/DS specification completely?\n\n\nHave I watched all of the videos?\n\n\nDid I Google the error message\n (and a few permutations thereof if necessary)?\n\n\nDid I try commenting out, printlining, and/or stepping through parts of the code bit by bit to find out precisely where the error occurs?\n\n\nDid I commit my code to SVN in case the TAs need more context?\n\n\nDid I include the console/GDB/Valgrind output \nAND\n code surrounding the bug in my Piazza post?\n\n\nHave I fixed other segmentation faults not related to the issue I'm having?\n\n\nAm I following good programming practice? (i.e. encapsulation, functions to limit repetition, etc)",
            "title": "#Piazza: When And How to Ask For Help"
        },
        {
            "location": "/SystemProgramming/#Piazza: When And How to Ask For Help/#purpose",
            "text": "TAs and student assistants get a ton of questions. Some are well-researched, and some...are not. This is a handy guide that'll help you move away from the latter and towards the former. (Oh, and did I mention that this is an easy way to score points with your internship managers?)",
            "title": "Purpose"
        },
        {
            "location": "/SystemProgramming/#Piazza: When And How to Ask For Help/#ask-yourself",
            "text": "Am I running on EWS?  Did I check the man pages?  Have I searched for similar questions/followups on Piazza?  Have I read the MP/DS specification completely?  Have I watched all of the videos?  Did I Google the error message  (and a few permutations thereof if necessary)?  Did I try commenting out, printlining, and/or stepping through parts of the code bit by bit to find out precisely where the error occurs?  Did I commit my code to SVN in case the TAs need more context?  Did I include the console/GDB/Valgrind output  AND  code surrounding the bug in my Piazza post?  Have I fixed other segmentation faults not related to the issue I'm having?  Am I following good programming practice? (i.e. encapsulation, functions to limit repetition, etc)",
            "title": "Ask yourself..."
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/",
            "text": "Want a quick introduction to C?\n\n\n\n\nKeep reading for the quick crash-course to C Programming below\n\n\nThen see the [[C Gotchas wiki page|C Programming, Part 3: Common Gotchas]].\n\n\nAnd learn about [[text I/O|C Programming, Part 2: Text Input And Output]].\n\n\nKick back relax with \nLawrence's intro videos\n (Also there is a virtual machine-in-a-browser you can play with!)\n\n\n\n\nExternal resources\n\n\n\n\nLearn X in Y\n (Highly recommended to skim through!)\n\n\nC for C++/Java Programmers\n\n\nC Tutorial by Brian Kernighan\n\n\nc faq\n\n\nC Bootcamp\n\n\nC/C++ function reference\n\n\ngdb (Gnu debugger) tutorial\n Tip: run gdb with the \"-tui\" command line argument to get a full screen version of the debugger.\n\n\nAdd your favorite resources here\n\n\n\n\nCrash course intro to C\n\n\nWarning new page\n Please fix typos and formatting mistakes for me and add useful links too.*\n\n\nHow do you write a complete hello world program in C?\n\n\n#include <stdio.h>\nint main(void) { \n    printf(\"Hello World\\n\");\n    return 0; \n}\n\n\n\n\nWhy do we use '\n#include <stdio.h>\n'?\n\n\nWe're lazy! We don't want to declare the \nprintf\n function. It's already done for us inside the file '\nstdio.h\n'. The \n#include\n includes the text of the file as part of our file to be compiled.\n\n\nSpecifically, the \n#include\n directive takes the file \nstdio.h\n (which stands for \nst\nan\nd\nard \ni\nnput and \no\nutput) located somewhere in your operating system, copies the text, and substitutes it where the \n#include\n was.\n\n\nHow are C strings represented?\n\n\nThey are represented as characters in memory.  The end of the string includes a NULL (0) byte. So \"ABC\" requires four(4) bytes \n['A','B','C','\\0']\n. The only way to find out the length of a C string is to keep reading memory until you find the NULL byte. C characters are always exactly one byte each.\n\n\nWhen you write a string literal \n\"ABC\"\n in an expression the string literal evaluates to a char pointer (\nchar *\n), which points to the first byte/char of the string.  This means \nptr\n in the example below will hold the memory address of the first character in the string.\n\n\nchar *ptr = \"ABC\"\n\n\n\n\nHow do you declare a pointer?\n\n\nA pointer refers to a memory address. The type of the pointer is useful - it tells the compiler how many bytes need to be read/written. You can declare a pointer as follows.\n\n\nint *ptr1;\nchar *ptr2;\n\n\n\n\nDue to C's grammar, a \nint*\n or any pointer is not actually its own type. You have to precede each pointer variable with an asterisk. As a common gotcha, the following\n\n\nint* ptr3, ptr4;\n\n\n\n\nWill only declare \n*ptr3\n as a pointer. \nptr4\n will actually be a regular int variable. To fix this declaration, keep the \n*\n preceding to the pointer\n\n\nint *ptr3, *ptr4;\n\n\n\n\nHow do you use a pointer to read/write some memory?\n\n\nLet's say that we declare a pointer \nint *ptr\n. For the sake of discussion, let's say that \nptr\n points to memory address \n0x1000\n. If we want to write to a pointer, we can deference and assign \n*ptr\n.\n\n\n*ptr = 0; // Writes some memory.\n\n\n\n\nWhat C will do is take the type of the pointer which is an \nint\n and writes \nsizeof(int)\n bytes from the start of the pointer, meaning that bytes \n0x1000\n, \n0x1004\n, \n0x1008\n, \n0x100a\n will all be zero. The number of bytes written depends on the pointer type. It is the same for all primitive types but structs are a little different.\n\n\nWhat is pointer arithmetic?\n\n\nYou can add an integer to a pointer. However the pointer type is used to determine how much to increment the pointer. For char pointers this is trivial because characters are always one byte:\n\n\nchar *ptr = \"Hello\"; // ptr holds the memory location of 'H'\nptr += 2; //ptr now points to the first'l'\n\n\n\n\nIf an int is 4 bytes then ptr+1 points to 4 bytes after whatever ptr is pointing at.\n\n\nchar *ptr = \"ABCDEFGH\";\nint *bna = (int *) ptr;\nbna +=1; // Would cause iterate by one integer space (i.e 4 bytes on some systems)\nptr = (char *) bna;\nprintf(\"%s\", ptr);\n/* Notice how only 'EFGH' is printed. Why is that? Well as mentioned above, when performing 'bna+=1' we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte)*/\nreturn 0;\n\n\n\n\nBecause pointer arithmetic in C is always automatically scaled by the size of the type that is pointed to, you can't perform pointer arithmetic on void pointers.\n\n\nYou can think of pointer arithmetic in C as essentially doing the following\n\n\nIf I want to do\n\n\nint *ptr1 = ...;\nint *offset = ptr1 + 4;\n\n\n\n\nThink\n\n\nint *ptr1 = ...;\nchar *temp_ptr1 = (char*) ptr1;\nint *offset = (int*)(temp_ptr1 + sizeof(int)*4);\n\n\n\n\nTo get the value. \nEvery time you do pointer arithmetic, take a deep breath and make sure that you are shifting over the number of bytes you think you are shifting over.\n\n\nWhat is a void pointer?\n\n\nA pointer without a type (very similar to a void variable). Void pointers are used when either a datatype you're dealing with is unknown or when you're interfacing C code with other programming languages. You can think of this as a raw pointer, or just a memory address. You cannot directly read or write to it because the void type does not have a size. For Example\n\n\nvoid *give_me_space = malloc(10);\nchar *string = give_me_space;\n\n\n\n\nThis does not require a cast because C automatically promotes \nvoid*\n to its appropriate type.\n\nNote:\n\n\ngcc and clang are not total ISO-C compliant, meaning that they will let you do arithmetic on a void pointer. They will treat it as a char pointer but do not do this because it may not work with all compilers!\n\n\nDoes \nprintf\n call write or does write call \nprintf\n?\n\n\nprintf\n calls \nwrite\n. \nprintf\n includes an internal buffer so, to increase performance \nprintf\n may not call \nwrite\n everytime you call \nprintf\n. \nprintf\n is a C library function. \nwrite\n is a system call and as we know system calls are expensive. On the other hand \nprintf\n uses a buffer which suits our needs better at that point\n\n\nHow do you print out pointer values? integers? strings?\n\n\nUse format specifiers \"%p\" for pointers, \"%d\" for integers and \"%s\" for Strings.\nA full list of all of the format specifiers is found \nhere\n\n\nExample of integer:\n\n\nint num1 = 10;\nprintf(\"%d\", num1); //prints num1\n\n\n\n\nExample of integer pointer:\n\n\nint *ptr = (int *) malloc(sizeof(int));\n*ptr = 10;\nprintf(\"%p\\n\", ptr); //prints the address pointed to by the pointer\nprintf(\"%p\\n\", &ptr); /*prints the address of pointer -- extremely useful\nwhen dealing with double pointers*/\nprintf(\"%d\", *ptr); //prints the integer content of ptr\nreturn 0;\n\n\n\n\nExample of string:\n\n\nchar *str = (char *) malloc(256 * sizeof(char));\nstrcpy(str, \"Hello there!\");\nprintf(\"%p\\n\", str); // print the address in the heap\nprintf(\"%s\", str);\nreturn 0;\n\n\n\n\nStrings as Pointers & Arrays @ BU\n\n\nHow would you make standard out be saved to a file?\n\n\nSimplest way: run your program and use shell redirection\ne.g.\n\n\n./program > output.txt\n\n#To read the contents of the file,\ncat output.txt\n\n\n\n\nMore complicated way: close(1) and then use open to re-open the file descriptor.\nSee [[http://cs-education.github.io/sys/#chapter/0/section/3/activity/0]]\n\n\nWhat's the difference between a pointer and an array? Give an example of something you can do with one but not the other.\n\n\nchar ary[] = \"Hello\";\nchar *ptr = \"Hello\";\n\n\n\n\nExample \n\n\nThe array name points to the first byte of the array. Both \nary\n and \nptr\n can be printed out:\n\n\nchar ary[] = \"Hello\";\nchar *ptr = \"Hello\";\n// Print out address and contents\nprintf(\"%p : %s\\n\", ary, ary);\nprintf(\"%p : %s\\n\", ptr, ptr);\n\n\n\n\nThe array is mutable, so we can change its contents (be careful not to write bytes beyond the end of the array though). Fortunately 'World' is no longer than 'Hello\"\n\n\nIn this case, the char pointer \nptr\n points to some read only memory (where the statically allocated string literal is stored), so we cannot change those contents.\n\n\nstrcpy(ary, \"World\"); // OK\nstrcpy(ptr, \"World\"); // NOT OK - Segmentation fault (crashes)\n\n\n\n\n\nWe can, however, unlike the array, we change \nptr\n to point to another piece of memory,\n\n\nptr = \"World\"; // OK!\nptr = ary; // OK!\nary = (..anything..) ; // WONT COMPILE\n// ary is doomed to always refer to the original array.\nprintf(\"%p : %s\\n\", ptr, ptr);\nstrcpy(ptr, \"World\"); // OK because now ptr is pointing to mutable memory (the array)\n\n\n\n\nWhat to take away from this is that pointers * can point to any type of memory while C arrays [] can only point to memory on the stack. In a more common case, pointers will point to heap memory in which case the memory referred to by the pointer CAN be modified.\n\n\nsizeof()\n returns the number of bytes. So using above code, what is sizeof(ary) and sizeof(ptr)?\n\n\nsizeof(ary)\n: \nary\n is an array. Returns the number of bytes required for the entire array (5 chars + zero byte = 6 bytes)\n\nsizeof(ptr)\n: Same as sizeof(char *). Returns the number bytes required for a pointer (e.g. 4 or 8 for a 32 bit or 64 bit machine)\n\n\nsizeof\n is a special operator. Really it's something the compiler substitutes in before compiling the program because the size of all types is known at compile time. When you have \nsizeof(char*)\n that takes the size of a pointer on your machine (8 bytes for a 64 bit machine and 4 for a 32 bit and so on). When you try \nsizeof(char[])\n, the compiler looks at that and substitutes the number of bytes that the \nentire\n array contains because the total size of the array is known at compile time.\n\n\nchar str1[] = \"will be 11\";\nchar* str2 = \"will be 8\";\nsizeof(str1) //11 because it is an array\nsizeof(str2) //8 because it is a pointer\n\n\n\n\nBe careful, using sizeof for the length of a string!\n\n\nWhich of the following code is incorrect or correct and why?\n\n\nint* f1(int *p) {\n    *p = 42;\n    return p;\n} // This code is correct;\n\n\n\n\nchar* f2() {\n    char p[] = \"Hello\";\n    return p;\n} // Incorrect!\n\n\n\n\nExplanation: An array p is created on the stack for the correct size to hold H,e,l,l,o, and a null byte i.e. (6) bytes. This array is stored on the stack and is invalid after we return from f2.\n\n\nchar* f3() {\n    char *p = \"Hello\";\n    return p;\n} // OK\n\n\n\n\nExplanation: p is a pointer. It holds the address of the string constant. The string constant is unchanged and valid even after f3 returns.\n\n\nchar* f4() {\n    static char p[] = \"Hello\";\n    return p;\n} // OK\n\n\n\n\nExplanation: The array is static meaning it exists for the lifetime of the process (static variables are not on the heap or the stack).\n\n\nHow do you look up information C library calls and system calls?\n\n\nUse the man pages. Note the man pages are organized into sections. Section 2 = System calls. Section 3 = C libraries.\nWeb: Google \"man7 open\"\nshell: man -S2 open  or man -S3 printf\n\n\nHow do you allocate memory on the heap?\n\n\nUse malloc. There's also realloc and calloc.\nTypically used with sizeof. e.g. enough space to hold 10 integers\n\n\nint *space = malloc(sizeof(int) * 10);\n\n\n\n\nWhat's wrong with this string copy code?\n\n\nvoid mystrcpy(char*dest, char* src) { \n  // void means no return value   \n  while( *src ) { dest = src; src ++; dest++; }  \n}\n\n\n\n\nIn the above code it simply changes the dest pointer to point to source string. Also the nuls bytes are not copied. Here's a better version - \n\n\n  while( *src ) { *dest = *src; src ++; dest++; } \n  *dest = *src;\n\n\n\n\nNote it's also usual to see the following kind of implementation, which does everything inside the expression test, including copying the nul byte.\n\n\n  while( (*dest++ = *src++ )) {};\n\n\n\n\nHow do you write a strdup replacement?\n\n\n// Use strlen+1 to find the zero byte... \nchar* mystrdup(char*source) {\n   char *p = (char *) malloc ( strlen(source)+1 );\n   strcpy(p,source);\n   return p;\n}\n\n\n\n\nHow do you unallocate memory on the heap?\n\n\nUse free!\n\n\nint *n = (int *) malloc(sizeof(int));\n*n = 10;\n//Do some work\nfree(n);\n\n\n\n\nWhat is double free error? How can you avoid? What is a dangling pointer? How do you avoid?\n\n\nA double free error is when you accidentally attempt to free the same allocation twice.\n\n\nint *p = malloc(sizeof(int));\nfree(p);\n\n*p = 123; // Oops! - Dangling pointer! Writing to memory we don't own anymore\n\nfree(p); // Oops! - Double free!\n\n\n\n\nThe fix is firstly to write correct programs! Secondly, it's good programming hygiene to reset pointers\nonce the memory has been freed. This ensures the pointer cant be used incorrectly without the program crashing.\n\n\nFix:\n\n\np = NULL; // Now you can't use this pointer by mistake\n\n\n\n\nWhat is an example of buffer overflow?\n\n\nFamous example: Heart Bleed (performed a memcpy into a buffer that was of insufficient size).\nSimple example: implement a strcpy and forget to add one to strlen, when determining the size of the memory required.\n\n\nWhat is 'typedef' and how do you use it?\n\n\nDeclares an alias for a type. Often used with structs to reduce the visual clutter of having to write 'struct' as part of the type.\n\n\ntypedef float real; \nreal gravity = 10;\n// Also typedef gives us an abstraction over the underlying type used. \n// For example in the future we only need to change this typedef if we\n// wanted our physics library to use doubles instead of floats.\n\ntypedef struct link link_t; \n//With structs, include the keyword 'struct' as part of the original types\n\n\n\n\nIn this class, we regularly typedef functions. A typedef for a function can be this for example\n\n\ntypedef int (*comparator)(void*,void*);\n\nint greater_than(void* a, void* b){\n    return a > b;\n}\ncomparator gt = greater_than;\n\n\n\n\nThis declares a function type comparator that accepts two \nvoid*\n params and returns an integer.\n\n\nWow that was a lot of C\n\n\nDon't worry more to come!\n\n\n\n\n\nNext: C Programming, Part 2: Text Input And Output",
            "title": "C Programming, Part 1: Introduction"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#want-a-quick-introduction-to-c",
            "text": "Keep reading for the quick crash-course to C Programming below  Then see the [[C Gotchas wiki page|C Programming, Part 3: Common Gotchas]].  And learn about [[text I/O|C Programming, Part 2: Text Input And Output]].  Kick back relax with  Lawrence's intro videos  (Also there is a virtual machine-in-a-browser you can play with!)",
            "title": "Want a quick introduction to C?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#external-resources",
            "text": "Learn X in Y  (Highly recommended to skim through!)  C for C++/Java Programmers  C Tutorial by Brian Kernighan  c faq  C Bootcamp  C/C++ function reference  gdb (Gnu debugger) tutorial  Tip: run gdb with the \"-tui\" command line argument to get a full screen version of the debugger.  Add your favorite resources here",
            "title": "External resources"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#crash-course-intro-to-c",
            "text": "Warning new page  Please fix typos and formatting mistakes for me and add useful links too.*",
            "title": "Crash course intro to C"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-do-you-write-a-complete-hello-world-program-in-c",
            "text": "#include <stdio.h>\nint main(void) { \n    printf(\"Hello World\\n\");\n    return 0; \n}",
            "title": "How do you write a complete hello world program in C?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#why-do-we-use-include-stdioh",
            "text": "We're lazy! We don't want to declare the  printf  function. It's already done for us inside the file ' stdio.h '. The  #include  includes the text of the file as part of our file to be compiled.  Specifically, the  #include  directive takes the file  stdio.h  (which stands for  st an d ard  i nput and  o utput) located somewhere in your operating system, copies the text, and substitutes it where the  #include  was.",
            "title": "Why do we use '#include &lt;stdio.h&gt;'?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-are-c-strings-represented",
            "text": "They are represented as characters in memory.  The end of the string includes a NULL (0) byte. So \"ABC\" requires four(4) bytes  ['A','B','C','\\0'] . The only way to find out the length of a C string is to keep reading memory until you find the NULL byte. C characters are always exactly one byte each.  When you write a string literal  \"ABC\"  in an expression the string literal evaluates to a char pointer ( char * ), which points to the first byte/char of the string.  This means  ptr  in the example below will hold the memory address of the first character in the string.  char *ptr = \"ABC\"",
            "title": "How are C strings represented?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-do-you-declare-a-pointer",
            "text": "A pointer refers to a memory address. The type of the pointer is useful - it tells the compiler how many bytes need to be read/written. You can declare a pointer as follows.  int *ptr1;\nchar *ptr2;  Due to C's grammar, a  int*  or any pointer is not actually its own type. You have to precede each pointer variable with an asterisk. As a common gotcha, the following  int* ptr3, ptr4;  Will only declare  *ptr3  as a pointer.  ptr4  will actually be a regular int variable. To fix this declaration, keep the  *  preceding to the pointer  int *ptr3, *ptr4;",
            "title": "How do you declare a pointer?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-do-you-use-a-pointer-to-readwrite-some-memory",
            "text": "Let's say that we declare a pointer  int *ptr . For the sake of discussion, let's say that  ptr  points to memory address  0x1000 . If we want to write to a pointer, we can deference and assign  *ptr .  *ptr = 0; // Writes some memory.  What C will do is take the type of the pointer which is an  int  and writes  sizeof(int)  bytes from the start of the pointer, meaning that bytes  0x1000 ,  0x1004 ,  0x1008 ,  0x100a  will all be zero. The number of bytes written depends on the pointer type. It is the same for all primitive types but structs are a little different.",
            "title": "How do you use a pointer to read/write some memory?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#what-is-pointer-arithmetic",
            "text": "You can add an integer to a pointer. However the pointer type is used to determine how much to increment the pointer. For char pointers this is trivial because characters are always one byte:  char *ptr = \"Hello\"; // ptr holds the memory location of 'H'\nptr += 2; //ptr now points to the first'l'  If an int is 4 bytes then ptr+1 points to 4 bytes after whatever ptr is pointing at.  char *ptr = \"ABCDEFGH\";\nint *bna = (int *) ptr;\nbna +=1; // Would cause iterate by one integer space (i.e 4 bytes on some systems)\nptr = (char *) bna;\nprintf(\"%s\", ptr);\n/* Notice how only 'EFGH' is printed. Why is that? Well as mentioned above, when performing 'bna+=1' we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte)*/\nreturn 0;  Because pointer arithmetic in C is always automatically scaled by the size of the type that is pointed to, you can't perform pointer arithmetic on void pointers.  You can think of pointer arithmetic in C as essentially doing the following  If I want to do  int *ptr1 = ...;\nint *offset = ptr1 + 4;  Think  int *ptr1 = ...;\nchar *temp_ptr1 = (char*) ptr1;\nint *offset = (int*)(temp_ptr1 + sizeof(int)*4);  To get the value.  Every time you do pointer arithmetic, take a deep breath and make sure that you are shifting over the number of bytes you think you are shifting over.",
            "title": "What is pointer arithmetic?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#what-is-a-void-pointer",
            "text": "A pointer without a type (very similar to a void variable). Void pointers are used when either a datatype you're dealing with is unknown or when you're interfacing C code with other programming languages. You can think of this as a raw pointer, or just a memory address. You cannot directly read or write to it because the void type does not have a size. For Example  void *give_me_space = malloc(10);\nchar *string = give_me_space;  This does not require a cast because C automatically promotes  void*  to its appropriate type. Note:  gcc and clang are not total ISO-C compliant, meaning that they will let you do arithmetic on a void pointer. They will treat it as a char pointer but do not do this because it may not work with all compilers!",
            "title": "What is a void pointer?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#does-printf-call-write-or-does-write-call-printf",
            "text": "printf  calls  write .  printf  includes an internal buffer so, to increase performance  printf  may not call  write  everytime you call  printf .  printf  is a C library function.  write  is a system call and as we know system calls are expensive. On the other hand  printf  uses a buffer which suits our needs better at that point",
            "title": "Does printf call write or does write call printf?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-do-you-print-out-pointer-values-integers-strings",
            "text": "Use format specifiers \"%p\" for pointers, \"%d\" for integers and \"%s\" for Strings.\nA full list of all of the format specifiers is found  here  Example of integer:  int num1 = 10;\nprintf(\"%d\", num1); //prints num1  Example of integer pointer:  int *ptr = (int *) malloc(sizeof(int));\n*ptr = 10;\nprintf(\"%p\\n\", ptr); //prints the address pointed to by the pointer\nprintf(\"%p\\n\", &ptr); /*prints the address of pointer -- extremely useful\nwhen dealing with double pointers*/\nprintf(\"%d\", *ptr); //prints the integer content of ptr\nreturn 0;  Example of string:  char *str = (char *) malloc(256 * sizeof(char));\nstrcpy(str, \"Hello there!\");\nprintf(\"%p\\n\", str); // print the address in the heap\nprintf(\"%s\", str);\nreturn 0;  Strings as Pointers & Arrays @ BU",
            "title": "How do you print out pointer values? integers? strings?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-would-you-make-standard-out-be-saved-to-a-file",
            "text": "Simplest way: run your program and use shell redirection\ne.g.  ./program > output.txt\n\n#To read the contents of the file,\ncat output.txt  More complicated way: close(1) and then use open to re-open the file descriptor.\nSee [[http://cs-education.github.io/sys/#chapter/0/section/3/activity/0]]",
            "title": "How would you make standard out be saved to a file?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#whats-the-difference-between-a-pointer-and-an-array-give-an-example-of-something-you-can-do-with-one-but-not-the-other",
            "text": "char ary[] = \"Hello\";\nchar *ptr = \"Hello\";  Example   The array name points to the first byte of the array. Both  ary  and  ptr  can be printed out:  char ary[] = \"Hello\";\nchar *ptr = \"Hello\";\n// Print out address and contents\nprintf(\"%p : %s\\n\", ary, ary);\nprintf(\"%p : %s\\n\", ptr, ptr);  The array is mutable, so we can change its contents (be careful not to write bytes beyond the end of the array though). Fortunately 'World' is no longer than 'Hello\"  In this case, the char pointer  ptr  points to some read only memory (where the statically allocated string literal is stored), so we cannot change those contents.  strcpy(ary, \"World\"); // OK\nstrcpy(ptr, \"World\"); // NOT OK - Segmentation fault (crashes)  We can, however, unlike the array, we change  ptr  to point to another piece of memory,  ptr = \"World\"; // OK!\nptr = ary; // OK!\nary = (..anything..) ; // WONT COMPILE\n// ary is doomed to always refer to the original array.\nprintf(\"%p : %s\\n\", ptr, ptr);\nstrcpy(ptr, \"World\"); // OK because now ptr is pointing to mutable memory (the array)  What to take away from this is that pointers * can point to any type of memory while C arrays [] can only point to memory on the stack. In a more common case, pointers will point to heap memory in which case the memory referred to by the pointer CAN be modified.",
            "title": "What's the difference between a pointer and an array? Give an example of something you can do with one but not the other."
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#sizeof-returns-the-number-of-bytes-so-using-above-code-what-is-sizeofary-and-sizeofptr",
            "text": "sizeof(ary) :  ary  is an array. Returns the number of bytes required for the entire array (5 chars + zero byte = 6 bytes) sizeof(ptr) : Same as sizeof(char *). Returns the number bytes required for a pointer (e.g. 4 or 8 for a 32 bit or 64 bit machine)  sizeof  is a special operator. Really it's something the compiler substitutes in before compiling the program because the size of all types is known at compile time. When you have  sizeof(char*)  that takes the size of a pointer on your machine (8 bytes for a 64 bit machine and 4 for a 32 bit and so on). When you try  sizeof(char[]) , the compiler looks at that and substitutes the number of bytes that the  entire  array contains because the total size of the array is known at compile time.  char str1[] = \"will be 11\";\nchar* str2 = \"will be 8\";\nsizeof(str1) //11 because it is an array\nsizeof(str2) //8 because it is a pointer  Be careful, using sizeof for the length of a string!",
            "title": "sizeof() returns the number of bytes. So using above code, what is sizeof(ary) and sizeof(ptr)?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#which-of-the-following-code-is-incorrect-or-correct-and-why",
            "text": "int* f1(int *p) {\n    *p = 42;\n    return p;\n} // This code is correct;  char* f2() {\n    char p[] = \"Hello\";\n    return p;\n} // Incorrect!  Explanation: An array p is created on the stack for the correct size to hold H,e,l,l,o, and a null byte i.e. (6) bytes. This array is stored on the stack and is invalid after we return from f2.  char* f3() {\n    char *p = \"Hello\";\n    return p;\n} // OK  Explanation: p is a pointer. It holds the address of the string constant. The string constant is unchanged and valid even after f3 returns.  char* f4() {\n    static char p[] = \"Hello\";\n    return p;\n} // OK  Explanation: The array is static meaning it exists for the lifetime of the process (static variables are not on the heap or the stack).",
            "title": "Which of the following code is incorrect or correct and why?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-do-you-look-up-information-c-library-calls-and-system-calls",
            "text": "Use the man pages. Note the man pages are organized into sections. Section 2 = System calls. Section 3 = C libraries.\nWeb: Google \"man7 open\"\nshell: man -S2 open  or man -S3 printf",
            "title": "How do you look up information C library calls and system calls?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-do-you-allocate-memory-on-the-heap",
            "text": "Use malloc. There's also realloc and calloc.\nTypically used with sizeof. e.g. enough space to hold 10 integers  int *space = malloc(sizeof(int) * 10);",
            "title": "How do you allocate memory on the heap?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#whats-wrong-with-this-string-copy-code",
            "text": "void mystrcpy(char*dest, char* src) { \n  // void means no return value   \n  while( *src ) { dest = src; src ++; dest++; }  \n}  In the above code it simply changes the dest pointer to point to source string. Also the nuls bytes are not copied. Here's a better version -     while( *src ) { *dest = *src; src ++; dest++; } \n  *dest = *src;  Note it's also usual to see the following kind of implementation, which does everything inside the expression test, including copying the nul byte.    while( (*dest++ = *src++ )) {};",
            "title": "What's wrong with this string copy code?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-do-you-write-a-strdup-replacement",
            "text": "// Use strlen+1 to find the zero byte... \nchar* mystrdup(char*source) {\n   char *p = (char *) malloc ( strlen(source)+1 );\n   strcpy(p,source);\n   return p;\n}",
            "title": "How do you write a strdup replacement?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#how-do-you-unallocate-memory-on-the-heap",
            "text": "Use free!  int *n = (int *) malloc(sizeof(int));\n*n = 10;\n//Do some work\nfree(n);",
            "title": "How do you unallocate memory on the heap?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#what-is-double-free-error-how-can-you-avoid-what-is-a-dangling-pointer-how-do-you-avoid",
            "text": "A double free error is when you accidentally attempt to free the same allocation twice.  int *p = malloc(sizeof(int));\nfree(p);\n\n*p = 123; // Oops! - Dangling pointer! Writing to memory we don't own anymore\n\nfree(p); // Oops! - Double free!  The fix is firstly to write correct programs! Secondly, it's good programming hygiene to reset pointers\nonce the memory has been freed. This ensures the pointer cant be used incorrectly without the program crashing.  Fix:  p = NULL; // Now you can't use this pointer by mistake",
            "title": "What is double free error? How can you avoid? What is a dangling pointer? How do you avoid?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#what-is-an-example-of-buffer-overflow",
            "text": "Famous example: Heart Bleed (performed a memcpy into a buffer that was of insufficient size).\nSimple example: implement a strcpy and forget to add one to strlen, when determining the size of the memory required.",
            "title": "What is an example of buffer overflow?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#what-is-typedef-and-how-do-you-use-it",
            "text": "Declares an alias for a type. Often used with structs to reduce the visual clutter of having to write 'struct' as part of the type.  typedef float real; \nreal gravity = 10;\n// Also typedef gives us an abstraction over the underlying type used. \n// For example in the future we only need to change this typedef if we\n// wanted our physics library to use doubles instead of floats.\n\ntypedef struct link link_t; \n//With structs, include the keyword 'struct' as part of the original types  In this class, we regularly typedef functions. A typedef for a function can be this for example  typedef int (*comparator)(void*,void*);\n\nint greater_than(void* a, void* b){\n    return a > b;\n}\ncomparator gt = greater_than;  This declares a function type comparator that accepts two  void*  params and returns an integer.",
            "title": "What is 'typedef' and how do you use it?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-1:-Introduction/#wow-that-was-a-lot-of-c",
            "text": "Don't worry more to come!   \nNext: C Programming, Part 2: Text Input And Output",
            "title": "Wow that was a lot of C"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/",
            "text": "Printing to Streams\n\n\nHow do I print strings, ints, chars to the standard output stream?\n\n\nUse \nprintf\n. The first parameter is a format string that includes placeholders for the data to be printed. Common format specifiers are \n%s\n treat the argument as a c string pointer, keep printing all characters until the NULL-character is reached; \n%d\n print the argument as an integer; \n%p\n print the argument as a memory address. \n\n\nA simple example is shown below:\n\n\nchar *name = ... ; int score = ...;\nprintf(\"Hello %s, your result is %d\\n\", name, score);\nprintf(\"Debug: The string and int are stored at: %p and %p\\n\", name, &score );\n// name already is a char pointer and points to the start of the array. \n// We need \"&\" to get the address of the int variable\n\n\n\n\nBy default, for performance, \nprintf\n does not actually write anything out (by calling write) until its buffer is full or a newline is printed. \n\n\nHow else can I print strings and single characters?\n\n\nUse \nputs( name );\n and \nputchar( c )\n  where name is a pointer to a C string and c is just a \nchar\n\n\nHow do I print to other file streams?\n\n\nUse \nfprintf( _file_ , \"Hello %s, score: %d\", name, score);\n\nWhere _file_ is either predefined 'stdout' 'stderr' or a FILE pointer that was returned by \nfopen\n or \nfdopen\n\n\nCan I use file descriptors?\n\n\nYes! Just use \ndprintf(int fd, char* format_string, ...);\n Just remember the stream may be buffered, so you will need to assure that the data is written to the file descriptor.\n\n\nHow do I print data into a C string?\n\n\nUse \nsprintf\n or better \nsnprintf\n.\n\n\nchar result[200];\nint len = snprintf(result, sizeof(result), \"%s:%d\", name, score);\n\n\n\n\nsnprintf returns the number of characters written excluding the terminating byte. In the above example this would be a maximum of 199.\n\n\nWhat if I really really want \nprintf\n to call \nwrite\n without a newline?\n\n\nUse \nfflush( FILE* inp )\n. The contents of the file will be written. If I wanted to write \"Hello World\" with no newline, I could write it like this.\n\n\nint main(){\n    fprintf(stdout, \"Hello World\");\n    fflush(stdout);\n    return 0;\n}\n\n\n\n\nHow is \nperror\n helpful?\n\n\nLet's say that you have a function call that just failed (because you checked the man page and it is a failing return code). \nperror(const char* message)\n will print the english version of the error to stderr\n\n\nint main(){\n    int ret = open(\"IDoNotExist.txt\", O_RDONLY);\n    if(ret < 0){\n        perror(\"Opening IDoNotExist:\");\n    }\n    //...\n    return 0;\n}\n\n\n\n\nParsing Input\n\n\nHow do I parse numbers from strings?\n\n\nUse \nlong int strtol(const char *nptr, char **endptr, int base);\n or \nlong long int strtoll(const char *nptr, char **endptr, int base);\n.\n\n\nWhat these functions do is take the pointer to your string \n*nptr\n and a \nbase\n (ie binary, octal, decimal, hexadecimal etc) and an optional pointer \nendptr\n and returns a parsed int.\n\n\nint main(){\n    const char *num = \"1A2436\";\n    char* endptr;\n    long int parsed = strtol(num, &endptr, 16);\n    return 0;\n}\n\n\n\n\nBe careful though! Error handling is kinda tricky because the function won't return an error code. On error, it'll return 0, and you have to manually check errno, but that could lead to trouble.\n\n\nint main(){\n    const char *zero = \"0\";\n    char* endptr;\n    printf(\"Parsing number\"); //printf sets errno\n    long int parsed = strtol(num, &endptr, 16);\n    if(parsed == 0){\n        perror(\"Error: \"); //oops strtol actually worked!\n    }\n    return 0;\n}\n\n\n\n\nHow do I parse input using \nscanf\n into parameters?\n\n\nUse \nscanf\n (or \nfscanf\n or \nsscanf\n) to get input from the default input stream, an arbitrary file stream or a C string respectively.\nIt's a good idea to check the return value to see how many items were parsed.\n\nscanf\n functions require valid pointers. It's a common source of error to pass in an incorrect pointer value. For example,\n\n\nint *data = (int *) malloc(sizeof(int));\nchar *line = \"v 10\";\nchar type;\n// Good practice: Check scanf parsed the line and read two values:\nint ok = 2 == sscanf(line, \"%c %d\", &type, &data); // pointer error\n\n\n\n\nWe wanted to write the character value into c and the integer value into the malloc'd memory.\nHowever we passed the address of the data pointer, not what the pointer is pointing to! So \nsscanf\n will change the pointer itself. i.e. the pointer will now point to address 10 so this code will later fail e.g. when free(data) is called.\n\n\nHow do I stop scanf from causing a buffer overflow?\n\n\nThe following code assumes the scanf won't read more than 10 characters (including the terminating byte) into the buffer.\n\n\nchar buffer[10];\nscanf(\"%s\",buffer);\n\n\n\n\nYou can include an optional integer to specify how many characters EXCLUDING the terminating byte:\n\n\nchar buffer[10];\nscanf(\"%9s\", buffer); // reads upto 9 charactes from input (leave room for the 10th byte to be the terminating byte)\n\n\n\n\nWhy is \ngets\n dangerous? What should I use instead?\n\n\nThe following code is vulnerable to buffer overflow. It assumes or trusts that the input line will be no more than 10 characters, including the terminating byte.\n\n\nchar buf[10];\ngets(buf); // Remember the array name means the first byte of the array\n\n\n\n\ngets\n is deprecated in C99 standard and has been removed from the latest C standard (C11). Programs should use \nfgets\n or \ngetline\n instead. \n\n\nWhere each have the following structure respectively:\n\n\nchar *fgets (char *str, int num, FILE *stream); \n\nssize_t getline(char **lineptr, size_t *n, FILE *stream);\n\n\n\n\nHere's a simple, safe way to read a single line. Lines longer than 9 characters will be truncated:\n\n\nchar buffer[10];\nchar *result = fgets(buffer, sizeof(buffer), stdin);\n\n\n\n\nThe result is NULL if there was an error or the end of the file is reached.\nNote, unlike \ngets\n,  \nfgets\n copies the newline into the buffer, which you may want to discard-\n\n\nif (!result) { return; /* no data - don't read the buffer contents */}\n\nint i = strlen(buffer) - 1;\nif (buffer[i] == '\\n') \n    buffer[i] = '\\0';\n\n\n\n\nHow do I use \ngetline\n?\n\n\nOne of the advantages of \ngetline\n is that will automatically (re-) allocate a buffer on the heap of sufficient size.\n\n\n// ssize_t getline(char **lineptr, size_t *n, FILE *stream);\n\n /* set buffer and size to 0; they will be changed by getline */\nchar *buffer = NULL;\nsize_t size = 0;\n\nssize_t chars = getline(&buffer, &size, stdin);\n\n// Discard newline character if it is present,\nif (chars > 0 && buffer[chars-1] == '\\n') \n    buffer[chars-1] = '\\0';\n\n// Read another line.\n// The existing buffer will be re-used, or, if necessary,\n// It will be `free`'d and a new larger buffer will `malloc`'d\nchars = getline(&buffer, &size, stdin);\n\n// Later... don't forget to free the buffer!\nfree(buffer);\n\n\n\n\n\n\n\nBack: C Programming, Part 1: Introduction\n\n |\n\n\nNext: C Programming, Part 3: Common Gotchas",
            "title": "C Programming, Part 2: Text Input And Output"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#printing-to-streams",
            "text": "",
            "title": "Printing to Streams"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-print-strings-ints-chars-to-the-standard-output-stream",
            "text": "Use  printf . The first parameter is a format string that includes placeholders for the data to be printed. Common format specifiers are  %s  treat the argument as a c string pointer, keep printing all characters until the NULL-character is reached;  %d  print the argument as an integer;  %p  print the argument as a memory address.   A simple example is shown below:  char *name = ... ; int score = ...;\nprintf(\"Hello %s, your result is %d\\n\", name, score);\nprintf(\"Debug: The string and int are stored at: %p and %p\\n\", name, &score );\n// name already is a char pointer and points to the start of the array. \n// We need \"&\" to get the address of the int variable  By default, for performance,  printf  does not actually write anything out (by calling write) until its buffer is full or a newline is printed.",
            "title": "How do I print strings, ints, chars to the standard output stream?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-else-can-i-print-strings-and-single-characters",
            "text": "Use  puts( name );  and  putchar( c )   where name is a pointer to a C string and c is just a  char",
            "title": "How else can I print strings and single characters?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-print-to-other-file-streams",
            "text": "Use  fprintf( _file_ , \"Hello %s, score: %d\", name, score); \nWhere _file_ is either predefined 'stdout' 'stderr' or a FILE pointer that was returned by  fopen  or  fdopen",
            "title": "How do I print to other file streams?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#can-i-use-file-descriptors",
            "text": "Yes! Just use  dprintf(int fd, char* format_string, ...);  Just remember the stream may be buffered, so you will need to assure that the data is written to the file descriptor.",
            "title": "Can I use file descriptors?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-print-data-into-a-c-string",
            "text": "Use  sprintf  or better  snprintf .  char result[200];\nint len = snprintf(result, sizeof(result), \"%s:%d\", name, score);  snprintf returns the number of characters written excluding the terminating byte. In the above example this would be a maximum of 199.",
            "title": "How do I print data into a C string?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#what-if-i-really-really-want-printf-to-call-write-without-a-newline",
            "text": "Use  fflush( FILE* inp ) . The contents of the file will be written. If I wanted to write \"Hello World\" with no newline, I could write it like this.  int main(){\n    fprintf(stdout, \"Hello World\");\n    fflush(stdout);\n    return 0;\n}",
            "title": "What if I really really want printf to call write without a newline?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-is-perror-helpful",
            "text": "Let's say that you have a function call that just failed (because you checked the man page and it is a failing return code).  perror(const char* message)  will print the english version of the error to stderr  int main(){\n    int ret = open(\"IDoNotExist.txt\", O_RDONLY);\n    if(ret < 0){\n        perror(\"Opening IDoNotExist:\");\n    }\n    //...\n    return 0;\n}",
            "title": "How is perror helpful?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#parsing-input",
            "text": "",
            "title": "Parsing Input"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-parse-numbers-from-strings",
            "text": "Use  long int strtol(const char *nptr, char **endptr, int base);  or  long long int strtoll(const char *nptr, char **endptr, int base); .  What these functions do is take the pointer to your string  *nptr  and a  base  (ie binary, octal, decimal, hexadecimal etc) and an optional pointer  endptr  and returns a parsed int.  int main(){\n    const char *num = \"1A2436\";\n    char* endptr;\n    long int parsed = strtol(num, &endptr, 16);\n    return 0;\n}  Be careful though! Error handling is kinda tricky because the function won't return an error code. On error, it'll return 0, and you have to manually check errno, but that could lead to trouble.  int main(){\n    const char *zero = \"0\";\n    char* endptr;\n    printf(\"Parsing number\"); //printf sets errno\n    long int parsed = strtol(num, &endptr, 16);\n    if(parsed == 0){\n        perror(\"Error: \"); //oops strtol actually worked!\n    }\n    return 0;\n}",
            "title": "How do I parse numbers from strings?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-parse-input-using-scanf-into-parameters",
            "text": "Use  scanf  (or  fscanf  or  sscanf ) to get input from the default input stream, an arbitrary file stream or a C string respectively.\nIt's a good idea to check the return value to see how many items were parsed. scanf  functions require valid pointers. It's a common source of error to pass in an incorrect pointer value. For example,  int *data = (int *) malloc(sizeof(int));\nchar *line = \"v 10\";\nchar type;\n// Good practice: Check scanf parsed the line and read two values:\nint ok = 2 == sscanf(line, \"%c %d\", &type, &data); // pointer error  We wanted to write the character value into c and the integer value into the malloc'd memory.\nHowever we passed the address of the data pointer, not what the pointer is pointing to! So  sscanf  will change the pointer itself. i.e. the pointer will now point to address 10 so this code will later fail e.g. when free(data) is called.",
            "title": "How do I parse input using scanf into parameters?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-stop-scanf-from-causing-a-buffer-overflow",
            "text": "The following code assumes the scanf won't read more than 10 characters (including the terminating byte) into the buffer.  char buffer[10];\nscanf(\"%s\",buffer);  You can include an optional integer to specify how many characters EXCLUDING the terminating byte:  char buffer[10];\nscanf(\"%9s\", buffer); // reads upto 9 charactes from input (leave room for the 10th byte to be the terminating byte)",
            "title": "How do I stop scanf from causing a buffer overflow?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#why-is-gets-dangerous-what-should-i-use-instead",
            "text": "The following code is vulnerable to buffer overflow. It assumes or trusts that the input line will be no more than 10 characters, including the terminating byte.  char buf[10];\ngets(buf); // Remember the array name means the first byte of the array  gets  is deprecated in C99 standard and has been removed from the latest C standard (C11). Programs should use  fgets  or  getline  instead.   Where each have the following structure respectively:  char *fgets (char *str, int num, FILE *stream); \n\nssize_t getline(char **lineptr, size_t *n, FILE *stream);  Here's a simple, safe way to read a single line. Lines longer than 9 characters will be truncated:  char buffer[10];\nchar *result = fgets(buffer, sizeof(buffer), stdin);  The result is NULL if there was an error or the end of the file is reached.\nNote, unlike  gets ,   fgets  copies the newline into the buffer, which you may want to discard-  if (!result) { return; /* no data - don't read the buffer contents */}\n\nint i = strlen(buffer) - 1;\nif (buffer[i] == '\\n') \n    buffer[i] = '\\0';",
            "title": "Why is gets dangerous? What should I use instead?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-use-getline",
            "text": "One of the advantages of  getline  is that will automatically (re-) allocate a buffer on the heap of sufficient size.  // ssize_t getline(char **lineptr, size_t *n, FILE *stream);\n\n /* set buffer and size to 0; they will be changed by getline */\nchar *buffer = NULL;\nsize_t size = 0;\n\nssize_t chars = getline(&buffer, &size, stdin);\n\n// Discard newline character if it is present,\nif (chars > 0 && buffer[chars-1] == '\\n') \n    buffer[chars-1] = '\\0';\n\n// Read another line.\n// The existing buffer will be re-used, or, if necessary,\n// It will be `free`'d and a new larger buffer will `malloc`'d\nchars = getline(&buffer, &size, stdin);\n\n// Later... don't forget to free the buffer!\nfree(buffer);   \nBack: C Programming, Part 1: Introduction  | \nNext: C Programming, Part 3: Common Gotchas",
            "title": "How do I use getline?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/",
            "text": "What common mistakes do C programmers make?\n\n\nMemory mistakes\n\n\nString constants are constant\n\n\nchar array[] = \"Hi!\"; // array contains a mutable copy \nstrcpy(array, \"OK\");\n\nchar *ptr = \"Can't change me\"; // ptr points to some immutable memory\nstrcpy(ptr, \"Will not work\");\n\n\n\n\nString literals are character arrays stored in the code segment of the program, which is immutable. Two string literals may share the same space in memory. An example follows:\n\n\nchar * str1 = \"Brandon Chong is the best TA\";\nchar * str2 = \"Brandon Chong is the best TA\";\n\n\n\n\nThe strings pointed to by \nstr1\n and \nstr2\n may actually reside in the same location in memory.\n\n\nChar arrays, however, contain the literal value which has been copied from the code segment into either the stack or static memory. These following char arrays do not reside in the same place in memory.\n\n\nchar arr1[] = \"Brandon Chong didn't write this\";\nchar arr2[] = \"Brandon Chong didn't write this\";\n\n\n\n\nBuffer overflow/ underflow\n\n\n#define N (10)\nint i = N, array[N];\nfor( ; i >= 0; i--) array[i] = i;\n\n\n\n\nC does not check that pointers are valid. The above example writes into \narray[10]\n which is outside the array bounds. This can cause memory corruption because that memory location is probably being used for something else.\nIn practice, this can be harder to spot because the overflow/underflow may occur in a library call e.g.\n\n\ngets(array); // Let's hope the input is shorter than my array!\n\n\n\n\nReturning pointers to automatic variables\n\n\nint *f() {\n    int result = 42;\n    static int imok;\n    return &imok; // OK - static variables are not on the stack\n    return &result; // Not OK\n}\n\n\n\n\nAutomatic variables are bound to stack memory only for the lifetime of the function.\nAfter the function returns it is an error to continue to use the memory.\n\n\nInsufficient memory allocation\n\n\nstruct User {\n   char name[100];\n};\ntypedef struct User user_t;\n\nuser_t *user = (user_t *) malloc(sizeof(user));\n\n\n\n\nIn the above example, we needed to allocate enough bytes for the struct. Instead we allocated enough bytes to hold a pointer. Once we start using the user pointer we will corrupt memory. Correct code is shown below.\n\n\nstruct User {\n   char name[100];\n};\ntypedef struct User user_t;\n\nuser_t * user = (user_t *) malloc(sizeof(user_t));\n\n\n\n\nStrings require \nstrlen(s)+1\n bytes\n\n\nEvery string must have a null byte after the last characters. To store the string \n\"Hi\"\n it takes 3 bytes: \n[H] [i] [\\0]\n.\n\n\n  char *strdup(const char *input) {  /* return a copy of 'input' */\n    char *copy;\n    copy = malloc(sizeof(char*));     /* nope! this allocates space for a pointer, not a string */\n    copy = malloc(strlen(input));     /* Almost...but what about the null terminator? */\n    copy = malloc(strlen(input) + 1); /* That's right. */\n    strcpy(copy, input);   /* strcpy will provide the null terminator */\n    return copy;\n}\n\n\n\n\nUsing uninitialized variables\n\n\nint myfunction() {\n  int x;\n  int y = x + 2;\n...\n\n\n\n\nAutomatic variables hold garbage (whatever bit pattern happened to be in memory). It is an error to assume that it will always be initialized to zero.\n\n\nAssuming Uninitialized memory will be zeroed\n\n\nvoid myfunct() {\n   char array[10];\n   char *p = malloc(10);\n\n\n\n\nAutomatic (temporary variables) are not automatically initialized to zero.\nHeap allocations using malloc are not automatically initialized to zero.\n\n\nDouble-free\n\n\n  char *p = malloc(10);\n  free(p);\n//  .. later ...\n  free(p); \n\n\n\n\nIt is an error to free the same block of memory twice.\n\n\nDangling pointers\n\n\n  char *p = malloc(10);\n  strcpy(p, \"Hello\");\n  free(p);\n//  .. later ...\n  strcpy(p,\"World\"); \n\n\n\n\nPointers to freed memory should not be used. A defensive programming practice is to set pointers to null as soon as the memory is freed.\n\n\nIt is a good idea to turn free into the following snippet that automatically sets the freed variable to null right after:(vim - ultisnips)  \n\n\nsnippet free \"free(something)\" b\nfree(${1});\n$1 = NULL;\n${2}\nendsnippet\n\n\n\n\nLogic and Program flow mistakes\n\n\nForgetting break\n\n\nint flag = 1; // Will print all three lines.\nswitch(flag) {\n  case 1: printf(\"I'm printed\\n\");\n  case 2: printf(\"Me too\\n\");\n  case 3: printf(\"Me three\\n\");\n}\n\n\n\n\nCase statements without a break will just continue onto the code of the next case statement. Correct code is show below. The break for the last statements is unnecessary because there are no more cases to be executed after the last one. However if more are added, it can cause some bugs.\n\n\nint flag = 1; // Will print only \"I'm printed\\n\"\nswitch(flag) {\n  case 1: \n    printf(\"I'm printed\\n\");\n    break;\n  case 2: \n    printf(\"Me too\\n\");\n    break;\n  case 3: \n    printf(\"Me three\\n\");\n    break; //unnecessary\n}\n\n\n\n\nEqual vs. equality\n\n\nint answer = 3; // Will print out the answer.\nif (answer = 42) { printf(\"I've solved the answer! It's %d\", answer);}\n\n\n\n\nUndeclared or incorrectly prototyped functions\n\n\ntime_t start = time();\n\n\n\n\nThe system function 'time' actually takes a parameter (a pointer to some memory that can receive the time_t structure). The compiler did not catch this error because the programmer did not provide a valid function prototype by including \ntime.h\n\n\nExtra Semicolons\n\n\nfor(int i = 0; i < 5; i++) ; printf(\"I'm printed once\");\nwhile(x < 10); x++ ; // X is never incremented\n\n\n\n\nHowever, the following code is perfectly OK.\n\n\nfor(int i = 0; i < 5; i++){\n    printf(\"%d\\n\", i);;;;;;;;;;;;;\n}\n\n\n\n\nIt is OK to have this kind of code, because the C language uses semicolons (;) to separate statements. If there is no statement in between semicolons, then there is nothing to do and the compiler moves on to the next statement\n\n\nOther Gotchas\n\n\nPreprocessor\n\n\nWhat is the preprocessor? It is an operation that the compiler performs \nbefore\n actually compiling the program. It is a copy and paste command. Meaning that if I do the following.\n\n\n#define MAX_LENGTH 10\nchar buffer[MAX_LENGTH]\n\n\n\n\nAfter preprocessing, it'll look like this.\n\n\nchar buffer[10]\n\n\n\n\nC Preprocessor macros and side-effects\n\n\n#define min(a,b) ((a)<(b) ? (a) : (b))\nint x = 4;\nif(min(x++, 100)) printf(\"%d is six\", x);\n\n\n\n\nMacros are simple text substitution so the above example expands to \nx++ < 100 ? x++ : 100\n (parenthesis omitted for clarity)\n\n\nC Preprocessor macros and precedence\n\n\n#define min(a,b) a<b ? a : b\nint x = 99;\nint r = 10 + min(99, 100); // r is 100!\n\n\n\n\nMacros are simple text substitution so the above example expands to \n10 + 99 < 100 ? 99 : 100\n\n\nC Preprocessor logical gotcha\n\n\n#define ARRAY_LENGTH(A) (sizeof((A)) / sizeof((A)[0]))\nint static_array[10]; // ARRAY_LENGTH(static_array) = 10\nint* dynamic_array = malloc(10); // ARRAY_LENGTH(dynamic_array) = 2 or 1\n\n\n\n\nWhat is wrong with the macro? Well it works if we have a static array like the first array because sizeof a static array returns the bytes that array takes up, and dividing it by the sizeof(an_element) would give you the number of entries. But if we use a pointer to a piece of memory, taking the sizeof the pointer and dividing it by the size of the first entry won't always give us the size of the array.\n\n\nDoes \nsizeof\n do anything?\n\n\nint a = 0;\nsize_t size = sizeof(a++);\nprintf(\"size: %lu, a: %d\", size, a);\n\n\n\n\nWhat does the code print out?\n\n\nsize: 4, a: 0\n\n\n\n\nBecause sizeof is not actually evaluated at runtime. The compiler assigns the type of all expressions and discards the extra results of the expression.\n\n\n\n\n\nBack: C Programming, Part 2: Text Input And Output\n\n |\n\n\nNext: C Programming, Part 4: Strings and Structs",
            "title": "C Programming, Part 3: Common Gotchas"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#memory-mistakes",
            "text": "",
            "title": "Memory mistakes"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#string-constants-are-constant",
            "text": "char array[] = \"Hi!\"; // array contains a mutable copy \nstrcpy(array, \"OK\");\n\nchar *ptr = \"Can't change me\"; // ptr points to some immutable memory\nstrcpy(ptr, \"Will not work\");  String literals are character arrays stored in the code segment of the program, which is immutable. Two string literals may share the same space in memory. An example follows:  char * str1 = \"Brandon Chong is the best TA\";\nchar * str2 = \"Brandon Chong is the best TA\";  The strings pointed to by  str1  and  str2  may actually reside in the same location in memory.  Char arrays, however, contain the literal value which has been copied from the code segment into either the stack or static memory. These following char arrays do not reside in the same place in memory.  char arr1[] = \"Brandon Chong didn't write this\";\nchar arr2[] = \"Brandon Chong didn't write this\";",
            "title": "String constants are constant"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#buffer-overflow-underflow",
            "text": "#define N (10)\nint i = N, array[N];\nfor( ; i >= 0; i--) array[i] = i;  C does not check that pointers are valid. The above example writes into  array[10]  which is outside the array bounds. This can cause memory corruption because that memory location is probably being used for something else.\nIn practice, this can be harder to spot because the overflow/underflow may occur in a library call e.g.  gets(array); // Let's hope the input is shorter than my array!",
            "title": "Buffer overflow/ underflow"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#returning-pointers-to-automatic-variables",
            "text": "int *f() {\n    int result = 42;\n    static int imok;\n    return &imok; // OK - static variables are not on the stack\n    return &result; // Not OK\n}  Automatic variables are bound to stack memory only for the lifetime of the function.\nAfter the function returns it is an error to continue to use the memory.",
            "title": "Returning pointers to automatic variables"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#insufficient-memory-allocation",
            "text": "struct User {\n   char name[100];\n};\ntypedef struct User user_t;\n\nuser_t *user = (user_t *) malloc(sizeof(user));  In the above example, we needed to allocate enough bytes for the struct. Instead we allocated enough bytes to hold a pointer. Once we start using the user pointer we will corrupt memory. Correct code is shown below.  struct User {\n   char name[100];\n};\ntypedef struct User user_t;\n\nuser_t * user = (user_t *) malloc(sizeof(user_t));",
            "title": "Insufficient memory allocation"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#strings-require-strlens1-bytes",
            "text": "Every string must have a null byte after the last characters. To store the string  \"Hi\"  it takes 3 bytes:  [H] [i] [\\0] .    char *strdup(const char *input) {  /* return a copy of 'input' */\n    char *copy;\n    copy = malloc(sizeof(char*));     /* nope! this allocates space for a pointer, not a string */\n    copy = malloc(strlen(input));     /* Almost...but what about the null terminator? */\n    copy = malloc(strlen(input) + 1); /* That's right. */\n    strcpy(copy, input);   /* strcpy will provide the null terminator */\n    return copy;\n}",
            "title": "Strings require strlen(s)+1 bytes"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#using-uninitialized-variables",
            "text": "int myfunction() {\n  int x;\n  int y = x + 2;\n...  Automatic variables hold garbage (whatever bit pattern happened to be in memory). It is an error to assume that it will always be initialized to zero.",
            "title": "Using uninitialized variables"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#assuming-uninitialized-memory-will-be-zeroed",
            "text": "void myfunct() {\n   char array[10];\n   char *p = malloc(10);  Automatic (temporary variables) are not automatically initialized to zero.\nHeap allocations using malloc are not automatically initialized to zero.",
            "title": "Assuming Uninitialized memory will be zeroed"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#double-free",
            "text": "char *p = malloc(10);\n  free(p);\n//  .. later ...\n  free(p);   It is an error to free the same block of memory twice.",
            "title": "Double-free"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#dangling-pointers",
            "text": "char *p = malloc(10);\n  strcpy(p, \"Hello\");\n  free(p);\n//  .. later ...\n  strcpy(p,\"World\");   Pointers to freed memory should not be used. A defensive programming practice is to set pointers to null as soon as the memory is freed.  It is a good idea to turn free into the following snippet that automatically sets the freed variable to null right after:(vim - ultisnips)    snippet free \"free(something)\" b\nfree(${1});\n$1 = NULL;\n${2}\nendsnippet",
            "title": "Dangling pointers"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#logic-and-program-flow-mistakes",
            "text": "",
            "title": "Logic and Program flow mistakes"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#forgetting-break",
            "text": "int flag = 1; // Will print all three lines.\nswitch(flag) {\n  case 1: printf(\"I'm printed\\n\");\n  case 2: printf(\"Me too\\n\");\n  case 3: printf(\"Me three\\n\");\n}  Case statements without a break will just continue onto the code of the next case statement. Correct code is show below. The break for the last statements is unnecessary because there are no more cases to be executed after the last one. However if more are added, it can cause some bugs.  int flag = 1; // Will print only \"I'm printed\\n\"\nswitch(flag) {\n  case 1: \n    printf(\"I'm printed\\n\");\n    break;\n  case 2: \n    printf(\"Me too\\n\");\n    break;\n  case 3: \n    printf(\"Me three\\n\");\n    break; //unnecessary\n}",
            "title": "Forgetting break"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#equal-vs-equality",
            "text": "int answer = 3; // Will print out the answer.\nif (answer = 42) { printf(\"I've solved the answer! It's %d\", answer);}",
            "title": "Equal vs. equality"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#undeclared-or-incorrectly-prototyped-functions",
            "text": "time_t start = time();  The system function 'time' actually takes a parameter (a pointer to some memory that can receive the time_t structure). The compiler did not catch this error because the programmer did not provide a valid function prototype by including  time.h",
            "title": "Undeclared or incorrectly prototyped functions"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#extra-semicolons",
            "text": "for(int i = 0; i < 5; i++) ; printf(\"I'm printed once\");\nwhile(x < 10); x++ ; // X is never incremented  However, the following code is perfectly OK.  for(int i = 0; i < 5; i++){\n    printf(\"%d\\n\", i);;;;;;;;;;;;;\n}  It is OK to have this kind of code, because the C language uses semicolons (;) to separate statements. If there is no statement in between semicolons, then there is nothing to do and the compiler moves on to the next statement",
            "title": "Extra Semicolons"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#other-gotchas",
            "text": "",
            "title": "Other Gotchas"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#preprocessor",
            "text": "What is the preprocessor? It is an operation that the compiler performs  before  actually compiling the program. It is a copy and paste command. Meaning that if I do the following.  #define MAX_LENGTH 10\nchar buffer[MAX_LENGTH]  After preprocessing, it'll look like this.  char buffer[10]",
            "title": "Preprocessor"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#c-preprocessor-macros-and-side-effects",
            "text": "#define min(a,b) ((a)<(b) ? (a) : (b))\nint x = 4;\nif(min(x++, 100)) printf(\"%d is six\", x);  Macros are simple text substitution so the above example expands to  x++ < 100 ? x++ : 100  (parenthesis omitted for clarity)",
            "title": "C Preprocessor macros and side-effects"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#c-preprocessor-macros-and-precedence",
            "text": "#define min(a,b) a<b ? a : b\nint x = 99;\nint r = 10 + min(99, 100); // r is 100!  Macros are simple text substitution so the above example expands to  10 + 99 < 100 ? 99 : 100",
            "title": "C Preprocessor macros and precedence"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#c-preprocessor-logical-gotcha",
            "text": "#define ARRAY_LENGTH(A) (sizeof((A)) / sizeof((A)[0]))\nint static_array[10]; // ARRAY_LENGTH(static_array) = 10\nint* dynamic_array = malloc(10); // ARRAY_LENGTH(dynamic_array) = 2 or 1  What is wrong with the macro? Well it works if we have a static array like the first array because sizeof a static array returns the bytes that array takes up, and dividing it by the sizeof(an_element) would give you the number of entries. But if we use a pointer to a piece of memory, taking the sizeof the pointer and dividing it by the size of the first entry won't always give us the size of the array.",
            "title": "C Preprocessor logical gotcha"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-3:-Common-Gotchas/#does-sizeof-do-anything",
            "text": "int a = 0;\nsize_t size = sizeof(a++);\nprintf(\"size: %lu, a: %d\", size, a);  What does the code print out?  size: 4, a: 0  Because sizeof is not actually evaluated at runtime. The compiler assigns the type of all expressions and discards the extra results of the expression.   \nBack: C Programming, Part 2: Text Input And Output  | \nNext: C Programming, Part 4: Strings and Structs",
            "title": "Does sizeof do anything?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/",
            "text": "Strings, Structs, and Gotcha's\n\n\nSo what's a string?\n\n\n\n\nIn C we have \nNull Terminated\n strings rather than \nLength Prefixed\n for historical reasons. What that means for your average everyday programming is that you need to remember the null character! A string in C is defined as a bunch of bytes until you reach '\\0' or the Null Byte.\n\n\nTwo places for strings\n\n\nWhenever you define a constant string (ie one in the form \nchar* str = \"constant\"\n) That string is stored in the \ndata\n or \ncode\n segment that is \nread-only\n meaning that any attempt to modify the string will cause a segfault.\n\n\nIf one however \nmalloc\n's space, one can change that string to be whatever they want.\n\n\nMemory Mismanagement\n\n\nOne common gotcha is when you write the following\n\n\nchar* hello_string = malloc(14);\n                       ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\n// hello_string ----> | g | a | r | b | a | g | e | g | a | r | b | a | g | e |\n                       \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\nhello_string = \"Hello Bhuvan!\";\n// (constant string in the text segment)\n// hello_string ----> [ \"H\" , \"e\" , \"l\" , \"l\" , \"o\" , \" \" , \"B\" , \"h\" , \"u\" , \"v\" , \"a\" , \"n\" , \"!\" , \"\\0\" ]\n                       ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\n// memory_leak -----> | g | a | r | b | a | g | e | g | a | r | b | a | g | e |\n                       \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\nhello_string[9] = 't'; //segfault!!\n\n\n\n\nWhat did we do? We allocated space for 14 bytes, reassigned the pointer and successfully segfaulted! Remember to keep track of what your pointers are doing. What you probably wanted to do was use a \nstring.h\n function \nstrcpy\n.\n\n\nstrcpy(hello_string, \"Hello Bhuvan!\");\n\n\n\n\nRemember the NULL byte!\n\n\nForgetting to NULL terminate a string is a big affect on the strings! Bounds checking is important. The heartbleed bug mentioned earlier in the wikibook is partially because of this.\n\n\nWhere can I find an In-Depth and Assignment-Comprehensive explanation of all of these functions?\n\n\nRight Here!\n\n\nString Information/Comparison: \nstrlen\n \nstrcmp\n\n\nint strlen(const char *s)\n returns the length of the string not including the null byte\n\n\nint strcmp(const char *s1, const char *s2)\n returns an integer determining the lexicographic order of the strings. If s1 where to come before s2 in a dictionary, then a -1 is returned. If the two strings are equal, then 0. Else, -1. \n\n\nWith most of these functions, they expect the strings to be readable and not NULL but there is undefined behavior when you pass them NULL.\n\n\nString Alteration: \nstrcpy\n \nstrcat\n \nstrdup\n\n\nchar *strcpy(char *dest, const char *src)\n Copies the string at \nsrc\n to \ndest\n. \nassumes dest has enough space for src\n\n\nchar *strcat(char *dest, const char *src)\n Concatenates the string at \nsrc\n to the end of destination. \nThis function assumes that there is enough space for \nsrc\n at the end of destination including the NULL byte\n\n\nchar *strdup(const char *dest)\n Returns a \nmalloc\n'ed copy of the string.\n\n\nString Search: \nstrchr\n \nstrstr\n\n\nchar *strchr(const char *haystack, int needle)\n Returns a pointer to the first occurrence of \nneedle\n in \nhaystack\n. If none found, \nNULL\n is returned.\n\n\nchar *strchr(const char *haystack, const char *needle)\n Same as above but this time a string!\n\n\nString Tokenize: \nstrtok\n\n\nA dangerous but useful function strtok takes a string and tokenizes it. Meaning that it will transform the strings into separate strings. This function has a lot of specs so please read the man pages a contrived examples is below.\n\n\n#include <stdio.h>\n#include <string.h>\n\nint main(){\n    char* upped = strdup(\"strtok,is,tricky,!!\");\n    char* start = strtok(upped, \",\");\n    do{\n        printf(\"%s\\n\", start);\n    }while((start = strtok(NULL, \",\")));\n    return 0;\n}\n\n\n\n\nOutput\n\n\nstrtok\nis\ntricky\n!!\n\n\n\n\nWhat happens when I change \nupped\n like this?\n\n\nchar* upped = strdup(\"strtok,is,tricky,,,!!\");\n\n\n\n\nMemory Movement: \nmemcpy\n and \nmemmove\n\n\nWhy are \nmemcpy\n and \nmemmove\n both in \n<string.h>\n? Because strings are essentially raw memory with a null byte at the end of them!\n\n\nvoid *memcpy(void *dest, const void *src, size_t n)\n moves \nn\n bytes starting at \nstr\n to \ndest\n. \nBe careful\n There is undefined behavior when the memory regions overlap. This is one of the classic works on my machine examples because many times valgrind won't be able to pick it up because it will look like it works on your machine. When the autograder hits, fail. Consider the safer version which is.\n\n\nvoid *memmove(void *dest, const void *src, size_t n)\n does the same thing as above, but if the memory regions overlap then it is guaranteed that all the bytes will get copied over correctly.\n\n\nSo what's a \nstruct\n?\n\n\n\n\nIn low level terms, a struct is just a piece of contiguous memory, nothing more. Just like an array, a struct has enough space to keep all of its members. But unlike an array, it can store different types. Consider the contact struct declared above\n\n\nstruct contact {\n    char firstname[20];\n    char lastname[20];\n    unsigned int phone;\n};\n\nstruct contact bhuvan;\n\n\n\n\nBrief aside\n\n\n/* a lot of times we will do the following typdef\n so we can just write contact contact1 */\n\ntypedef struct contact contact;\ncontact bhuvan;\n\n/* You can also declare the struct like this to get\n it done in one statement */\ntypedef struct optional_name {\n    ...\n} contact;\n\n\n\n\nIf you compile the code without any optimizations and reordering, you can expect the addresses of each of the variables to look like this.\n\n\n&bhuvan           // 0x100\n&bhuvan.firstname // 0x100 = 0x100+0x00\n&bhuvan.lastname  // 0x114 = 0x100+0x14\n&bhuvan.phone     // 0x128 = 0x100+0x28\n\n\n\n\nBecause all your compiler does is say 'hey reserve this much space, and I will go and calculate the offsets of whatever variables you want to write to'.\n\n\nWhat do these offsets mean?\n\n\nThe offsets are where the variable starts at. The phone variables starts at the \n0x128\nth bytes and continues for sizeof(int) bytes, but not always. \nOffsets don't determine where the variable ends though\n. Consider the following hack that you see in a lot of kernel code.\n\n\n\ntypedef struct {\n    int length;\n    char c_str[0];\n} string;\n\nconst char* to_convert = \"bhuvan\";\nint length = strlen(to_convert);\n\n// Let's convert to a c string\nstring* bhuvan_name;\nbhuvan_name = malloc(sizeof(string) + length+1);\n/*\nCurrently, our memory looks like this with junk in those black spaces\n                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\n bhuvan_name = |   |   |   |   |   |   |   |   |   |   |   |\n                \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\n*/\n\n\nbhuvan_name->length = length;\n/*\nThis writes the following values to the first four bytes\nThe rest is still garbage\n                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\n bhuvan_name = | 0 | 0 | 0 | 6 |   |   |   |   |   |   |   |\n                \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\n*/\n\n\nstrcpy(bhuvan_name->c_str, to_convert);\n/*\nNow our string is filled in correctly at the end of the struct\n\n                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ____\n bhuvan_name = | 0 | 0 | 0 | 6 | b | h | u | v | a | n | \\0 |\n                \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\u203e\n*/\n\nstrcmp(bhuvan_name->c_str, \"bhuvan\") == 0 //The strings are equal!\n\n\n\n\nBut not all structs are perfect\n\n\nStructs may require something called \npadding\n (tutorial). **We do not expect you to pack structs in this course, just know that it is there This is because in the early days (and even now) when you have to an address from memory you have to do it in 32bit or 64bit blocks. This also meant that you could only request addresses that were multiples of that. Meaning that\n\n\nstruct picture{\n    int height;\n    pixel** data;\n    int width;\n    char* enconding;\n}\n// You think picture looks like this\n           height      data         width     encoding\n           ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\npicture = |       |               |       |               |\n           \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\n\n\n\n\nWould conceptually may look like this\n\n\nstruct picture{\n    int height;\n    char slop1[4];\n    pixel** data;\n    int width;\n    char slop2[4];\n    char* enconding;\n}\n           height   slop1       data        width   slop2  encoding\n           ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\npicture = |       |       |               |       |       |               |\n           \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\n\n\n\n\n(This is on a 64bit system)\nThis is not always the case because sometimes your processor supports unaligned accesses. What does this mean? Well there are two options you can set an attribute\n\n\nstruct __attribute__((packed, aligned(4))) picture{\n    int height;\n    pixel** data;\n    int width;\n    char* enconding;\n}\n// Will look like this\n           height       data        width     encoding\n           ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\npicture = |       |               |       |               |\n           \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\n\n\n\n\nBut now every time I want to access \ndata\n or \nencoding\n, I have to do two memory accesses. The other thing you can do is reorder the struct, although this is not always possible\n\n\nstruct picture{\n    int height;\n    int width;\n    pixel** data;\n    char* enconding;\n}\n// You think picture looks like this\n           height   width        data         encoding\n           ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\npicture = |       |       |               |               |\n           \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\n\n\n\n\n\n\n\nBack: C Programming, Part 3: Common Gotchas\n\n |\n\n\nNext: C Programming, Part 5: Debugging",
            "title": "C Programming, Part 4: Strings and Structs"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#strings-structs-and-gotchas",
            "text": "",
            "title": "Strings, Structs, and Gotcha's"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#so-whats-a-string",
            "text": "In C we have  Null Terminated  strings rather than  Length Prefixed  for historical reasons. What that means for your average everyday programming is that you need to remember the null character! A string in C is defined as a bunch of bytes until you reach '\\0' or the Null Byte.",
            "title": "So what's a string?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#two-places-for-strings",
            "text": "Whenever you define a constant string (ie one in the form  char* str = \"constant\" ) That string is stored in the  data  or  code  segment that is  read-only  meaning that any attempt to modify the string will cause a segfault.  If one however  malloc 's space, one can change that string to be whatever they want.",
            "title": "Two places for strings"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#memory-mismanagement",
            "text": "One common gotcha is when you write the following  char* hello_string = malloc(14);\n                       ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\n// hello_string ----> | g | a | r | b | a | g | e | g | a | r | b | a | g | e |\n                       \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\nhello_string = \"Hello Bhuvan!\";\n// (constant string in the text segment)\n// hello_string ----> [ \"H\" , \"e\" , \"l\" , \"l\" , \"o\" , \" \" , \"B\" , \"h\" , \"u\" , \"v\" , \"a\" , \"n\" , \"!\" , \"\\0\" ]\n                       ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\n// memory_leak -----> | g | a | r | b | a | g | e | g | a | r | b | a | g | e |\n                       \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\nhello_string[9] = 't'; //segfault!!  What did we do? We allocated space for 14 bytes, reassigned the pointer and successfully segfaulted! Remember to keep track of what your pointers are doing. What you probably wanted to do was use a  string.h  function  strcpy .  strcpy(hello_string, \"Hello Bhuvan!\");",
            "title": "Memory Mismanagement"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#remember-the-null-byte",
            "text": "Forgetting to NULL terminate a string is a big affect on the strings! Bounds checking is important. The heartbleed bug mentioned earlier in the wikibook is partially because of this.",
            "title": "Remember the NULL byte!"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#where-can-i-find-an-in-depth-and-assignment-comprehensive-explanation-of-all-of-these-functions",
            "text": "Right Here!",
            "title": "Where can I find an In-Depth and Assignment-Comprehensive explanation of all of these functions?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#string-informationcomparison-strlen-strcmp",
            "text": "int strlen(const char *s)  returns the length of the string not including the null byte  int strcmp(const char *s1, const char *s2)  returns an integer determining the lexicographic order of the strings. If s1 where to come before s2 in a dictionary, then a -1 is returned. If the two strings are equal, then 0. Else, -1.   With most of these functions, they expect the strings to be readable and not NULL but there is undefined behavior when you pass them NULL.",
            "title": "String Information/Comparison: strlen strcmp"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#string-alteration-strcpy-strcat-strdup",
            "text": "char *strcpy(char *dest, const char *src)  Copies the string at  src  to  dest .  assumes dest has enough space for src  char *strcat(char *dest, const char *src)  Concatenates the string at  src  to the end of destination.  This function assumes that there is enough space for  src  at the end of destination including the NULL byte  char *strdup(const char *dest)  Returns a  malloc 'ed copy of the string.",
            "title": "String Alteration: strcpy strcat strdup"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#string-search-strchr-strstr",
            "text": "char *strchr(const char *haystack, int needle)  Returns a pointer to the first occurrence of  needle  in  haystack . If none found,  NULL  is returned.  char *strchr(const char *haystack, const char *needle)  Same as above but this time a string!",
            "title": "String Search: strchr strstr"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#string-tokenize-strtok",
            "text": "A dangerous but useful function strtok takes a string and tokenizes it. Meaning that it will transform the strings into separate strings. This function has a lot of specs so please read the man pages a contrived examples is below.  #include <stdio.h>\n#include <string.h>\n\nint main(){\n    char* upped = strdup(\"strtok,is,tricky,!!\");\n    char* start = strtok(upped, \",\");\n    do{\n        printf(\"%s\\n\", start);\n    }while((start = strtok(NULL, \",\")));\n    return 0;\n}  Output  strtok\nis\ntricky\n!!  What happens when I change  upped  like this?  char* upped = strdup(\"strtok,is,tricky,,,!!\");",
            "title": "String Tokenize: strtok"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#memory-movement-memcpy-and-memmove",
            "text": "Why are  memcpy  and  memmove  both in  <string.h> ? Because strings are essentially raw memory with a null byte at the end of them!  void *memcpy(void *dest, const void *src, size_t n)  moves  n  bytes starting at  str  to  dest .  Be careful  There is undefined behavior when the memory regions overlap. This is one of the classic works on my machine examples because many times valgrind won't be able to pick it up because it will look like it works on your machine. When the autograder hits, fail. Consider the safer version which is.  void *memmove(void *dest, const void *src, size_t n)  does the same thing as above, but if the memory regions overlap then it is guaranteed that all the bytes will get copied over correctly.",
            "title": "Memory Movement: memcpy and memmove"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#so-whats-a-struct",
            "text": "In low level terms, a struct is just a piece of contiguous memory, nothing more. Just like an array, a struct has enough space to keep all of its members. But unlike an array, it can store different types. Consider the contact struct declared above  struct contact {\n    char firstname[20];\n    char lastname[20];\n    unsigned int phone;\n};\n\nstruct contact bhuvan;  Brief aside  /* a lot of times we will do the following typdef\n so we can just write contact contact1 */\n\ntypedef struct contact contact;\ncontact bhuvan;\n\n/* You can also declare the struct like this to get\n it done in one statement */\ntypedef struct optional_name {\n    ...\n} contact;  If you compile the code without any optimizations and reordering, you can expect the addresses of each of the variables to look like this.  &bhuvan           // 0x100\n&bhuvan.firstname // 0x100 = 0x100+0x00\n&bhuvan.lastname  // 0x114 = 0x100+0x14\n&bhuvan.phone     // 0x128 = 0x100+0x28  Because all your compiler does is say 'hey reserve this much space, and I will go and calculate the offsets of whatever variables you want to write to'.",
            "title": "So what's a struct?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#what-do-these-offsets-mean",
            "text": "The offsets are where the variable starts at. The phone variables starts at the  0x128 th bytes and continues for sizeof(int) bytes, but not always.  Offsets don't determine where the variable ends though . Consider the following hack that you see in a lot of kernel code.  \ntypedef struct {\n    int length;\n    char c_str[0];\n} string;\n\nconst char* to_convert = \"bhuvan\";\nint length = strlen(to_convert);\n\n// Let's convert to a c string\nstring* bhuvan_name;\nbhuvan_name = malloc(sizeof(string) + length+1);\n/*\nCurrently, our memory looks like this with junk in those black spaces\n                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\n bhuvan_name = |   |   |   |   |   |   |   |   |   |   |   |\n                \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\n*/\n\n\nbhuvan_name->length = length;\n/*\nThis writes the following values to the first four bytes\nThe rest is still garbage\n                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\n bhuvan_name = | 0 | 0 | 0 | 6 |   |   |   |   |   |   |   |\n                \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\n*/\n\n\nstrcpy(bhuvan_name->c_str, to_convert);\n/*\nNow our string is filled in correctly at the end of the struct\n\n                ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ____\n bhuvan_name = | 0 | 0 | 0 | 6 | b | h | u | v | a | n | \\0 |\n                \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e\u203e\n*/\n\nstrcmp(bhuvan_name->c_str, \"bhuvan\") == 0 //The strings are equal!",
            "title": "What do these offsets mean?"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-4:-Strings-and-Structs/#but-not-all-structs-are-perfect",
            "text": "Structs may require something called  padding  (tutorial). **We do not expect you to pack structs in this course, just know that it is there This is because in the early days (and even now) when you have to an address from memory you have to do it in 32bit or 64bit blocks. This also meant that you could only request addresses that were multiples of that. Meaning that  struct picture{\n    int height;\n    pixel** data;\n    int width;\n    char* enconding;\n}\n// You think picture looks like this\n           height      data         width     encoding\n           ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\npicture = |       |               |       |               |\n           \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e  Would conceptually may look like this  struct picture{\n    int height;\n    char slop1[4];\n    pixel** data;\n    int width;\n    char slop2[4];\n    char* enconding;\n}\n           height   slop1       data        width   slop2  encoding\n           ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\npicture = |       |       |               |       |       |               |\n           \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e  (This is on a 64bit system)\nThis is not always the case because sometimes your processor supports unaligned accesses. What does this mean? Well there are two options you can set an attribute  struct __attribute__((packed, aligned(4))) picture{\n    int height;\n    pixel** data;\n    int width;\n    char* enconding;\n}\n// Will look like this\n           height       data        width     encoding\n           ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\npicture = |       |               |       |               |\n           \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e  But now every time I want to access  data  or  encoding , I have to do two memory accesses. The other thing you can do is reorder the struct, although this is not always possible  struct picture{\n    int height;\n    int width;\n    pixel** data;\n    char* enconding;\n}\n// You think picture looks like this\n           height   width        data         encoding\n           ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___ ___\npicture = |       |       |               |               |\n           \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e \u203e\u203e\u203e   \nBack: C Programming, Part 3: Common Gotchas  | \nNext: C Programming, Part 5: Debugging",
            "title": "But not all structs are perfect"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/",
            "text": "The Hitchhiker's Guide to Debugging C Programs\n\n\nThis is going to be a massive guide to helping you debug your C programs. There are different levels that you can check errors and we will be going through most of them. Feel free to add anything that you found helpful in debugging C programs including but not limited to, debugger usage, recognizing common error types, gotchas, and effective googling tips.\n\n\nIn-Code Debugging\n\n\nClean code\n\n\nMake your code modular using helper functions. If there is a repeated task (getting the pointers to contiguous blocks in MP2 for example), make them helper functions. And make sure each function does one thing very well, so that you don't have to debug twice.\n\n\nLet's say that we are doing selection sort by finding the minimum element each iteration like so,\n\n\nvoid selection_sort(int *a, long len){\n     for(long i = len-1; i > 0; --i){\n         long max_index = i;\n         for(long j = len-1; j >= 0; --j){\n             if(a[max_index] < a[j]){\n                  max_index = j;\n             }\n         }\n         int temp = a[i];\n         a[i] = a[max_index];\n         a[max_index] = temp;\n     }\n\n}\n\n\n\n\nMany can see the bug in the code, but it can help to refactor the above method into\n\n\nlong max_index(int *a, long start, long end);\nvoid swap(int *a, long idx1, long idx2);\nvoid selection_sort(int *a, long len);\n\n\n\n\nAnd the error is specifically in one function.\n\n\nIn the end, we are not a class about refactoring/debugging your code -- In fact most systems code is so atrocious that you don't want to read it. But for the sake of debugging, it may benefit you in the long run to adopt some practices.\n\n\nAsserts!\n\n\nUse assertions to make sure your code works up to a certain point -- and importantly, to make sure you don't break it later. For example, if your data structure is a doubly linked list, you can do something like, assert(node->size == node->next->prev->size) to assert that the next node has a pointer to the current node. You can also check the pointer is pointing to an expected range of memory address, not null, ->size is reasonable etc.\nThe NDEBUG macro will disable all assertions, so don't forget to set that once you finish debugging. http://www.cplusplus.com/reference/cassert/assert/\n\n\nA quick example with assert is let's say that I'm writing code using memcpy\n\n\nassert(!(src < dest+n && dest < src+n)); //Checks overlap\nmemcpy(dest, src, n);\n\n\n\n\nThis check can be turned off at compile time, but will save you \ntons\n of trouble debugging!\n\n\nprintfs\n\n\nWhen all else fails, print like crazy! Each of your functions should have an idea of what it is going to do (ie find_min better find the minimum element). You want to test that each of your functions is doing what it set out to do and see exactly where your code breaks. In the case with race conditions, tsan may be able to help, but having each thread print out data at certain times could help you identify the race condition.\n\n\nValgrind\n\n\n(ToDo)\n\n\nTsan\n\n\nThreadSanitizer is a tool from Google, built into clang (and gcc), to help you detect race conditions in your code. For more information about the tool, see the Github wiki.\n\n\nNote that running with tsan will slow your code down a bit.\n\n\n#include <pthread.h>\n#include <stdio.h>\n\nint Global;\n\nvoid *Thread1(void *x) {\n    Global++;\n    return NULL;\n}\n\nint main() {\n    pthread_t t[2];\n    pthread_create(&t[0], NULL, Thread1, NULL);\n    Global = 100;\n    pthread_join(t[0], NULL);\n}\n// compile with gcc -fsanitize=thread -pie -fPIC -ltsan -g simple_race.c\n\n\n\n\nWe can see that there is a race condition on the variable Global. Both the main thread and the thread created with pthread_create will try to changethe value at the same time. But, does ThreadSantizer catch it?\n\n\n$ ./a.out\n==================\nWARNING: ThreadSanitizer: data race (pid=28888)\n  Read of size 4 at 0x7f73ed91c078 by thread T1:\n    #0 Thread1 /home/zmick2/simple_race.c:7 (exe+0x000000000a50)\n    #1  :0 (libtsan.so.0+0x00000001b459)\n\n  Previous write of size 4 at 0x7f73ed91c078 by main thread:\n    #0 main /home/zmick2/simple_race.c:14 (exe+0x000000000ac8)\n\n  Thread T1 (tid=28889, running) created by main thread at:\n    #0  :0 (libtsan.so.0+0x00000001f6ab)\n    #1 main /home/zmick2/simple_race.c:13 (exe+0x000000000ab8)\n\nSUMMARY: ThreadSanitizer: data race /home/zmick2/simple_race.c:7 Thread1\n==================\nThreadSanitizer: reported 1 warnings\n\n\n\n\nIf we compiled with the debug flag, then it would give us the variable name as well.\n\n\nGDB\n\n\nIntroduction: http://www.cs.cmu.edu/~gilpin/tutorial/\n\n\nSetting breakpoints programmatically\n\n\nA very useful trick when debugging complex C programs with GDB is setting breakpoints in the source code.\n\n\nint main() {\n    int val = 1;\n    val = 42;\n    asm(\"int $3\"); // set a breakpoint here\n    val = 7;\n}\n\n\n\n\n$ gcc main.c -g -o main && ./main\n(gdb) r\n[...]\nProgram received signal SIGTRAP, Trace/breakpoint trap.\nmain () at main.c:6\n6       val = 7;\n(gdb) p val\n$1 = 42\n\n\n\n\nChecking memory content\n\n\nhttp://www.delorie.com/gnu/docs/gdb/gdb_56.html\n\n\nFor example,\n\n\nint main() {\n    char bad_string[3] = {'C', 'a', 't'};\n    printf(\"%s\", bad_string);\n}\n\n\n\n\n$ gcc main.c -g -o main && ./main\n$ Cat ZVQ\ufffd\u007f $\n\n\n\n\n(gdb) l\n1   #include <stdio.h>\n2   int main() {\n3       char bad_string[3] = {'C', 'a', 't'};\n4       printf(\"%s\", bad_string);\n5   }\n(gdb) b 4\nBreakpoint 1 at 0x100000f57: file main.c, line 4.\n(gdb) r\n[...]\nBreakpoint 1, main () at main.c:4\n4       printf(\"%s\", bad_string);\n(gdb) x/16xb bad_string\n0x7fff5fbff9cd: 0x63    0x61    0x74    0xe0    0xf9    0xbf    0x5f    0xff\n0x7fff5fbff9d5: 0x7f    0x00    0x00    0xfd    0xb5    0x23    0x89    0xff\n\n(gdb)\n\n\n\n\nHere, by using the \nx\n command with parameters \n16xb\n, we can see that starting at memory address \n0x7fff5fbff9c\n (value of \nbad_string\n), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.\n\n\n0x43 0x61 0x74 0xe0 0xf9 0xbf 0x5f 0xff 0x7f 0x00\n\n\n\n\n\nBack: C Programming, Part 4: Strings and Structs\n\n\n| \n\nBack: C Programming, Review Questions",
            "title": "C Programming, Part 5: Debugging"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#the-hitchhikers-guide-to-debugging-c-programs",
            "text": "This is going to be a massive guide to helping you debug your C programs. There are different levels that you can check errors and we will be going through most of them. Feel free to add anything that you found helpful in debugging C programs including but not limited to, debugger usage, recognizing common error types, gotchas, and effective googling tips.",
            "title": "The Hitchhiker's Guide to Debugging C Programs"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#in-code-debugging",
            "text": "",
            "title": "In-Code Debugging"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#clean-code",
            "text": "Make your code modular using helper functions. If there is a repeated task (getting the pointers to contiguous blocks in MP2 for example), make them helper functions. And make sure each function does one thing very well, so that you don't have to debug twice.  Let's say that we are doing selection sort by finding the minimum element each iteration like so,  void selection_sort(int *a, long len){\n     for(long i = len-1; i > 0; --i){\n         long max_index = i;\n         for(long j = len-1; j >= 0; --j){\n             if(a[max_index] < a[j]){\n                  max_index = j;\n             }\n         }\n         int temp = a[i];\n         a[i] = a[max_index];\n         a[max_index] = temp;\n     }\n\n}  Many can see the bug in the code, but it can help to refactor the above method into  long max_index(int *a, long start, long end);\nvoid swap(int *a, long idx1, long idx2);\nvoid selection_sort(int *a, long len);  And the error is specifically in one function.  In the end, we are not a class about refactoring/debugging your code -- In fact most systems code is so atrocious that you don't want to read it. But for the sake of debugging, it may benefit you in the long run to adopt some practices.",
            "title": "Clean code"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#asserts",
            "text": "Use assertions to make sure your code works up to a certain point -- and importantly, to make sure you don't break it later. For example, if your data structure is a doubly linked list, you can do something like, assert(node->size == node->next->prev->size) to assert that the next node has a pointer to the current node. You can also check the pointer is pointing to an expected range of memory address, not null, ->size is reasonable etc.\nThe NDEBUG macro will disable all assertions, so don't forget to set that once you finish debugging. http://www.cplusplus.com/reference/cassert/assert/  A quick example with assert is let's say that I'm writing code using memcpy  assert(!(src < dest+n && dest < src+n)); //Checks overlap\nmemcpy(dest, src, n);  This check can be turned off at compile time, but will save you  tons  of trouble debugging!",
            "title": "Asserts!"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#printfs",
            "text": "When all else fails, print like crazy! Each of your functions should have an idea of what it is going to do (ie find_min better find the minimum element). You want to test that each of your functions is doing what it set out to do and see exactly where your code breaks. In the case with race conditions, tsan may be able to help, but having each thread print out data at certain times could help you identify the race condition.",
            "title": "printfs"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#valgrind",
            "text": "(ToDo)",
            "title": "Valgrind"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#tsan",
            "text": "ThreadSanitizer is a tool from Google, built into clang (and gcc), to help you detect race conditions in your code. For more information about the tool, see the Github wiki.  Note that running with tsan will slow your code down a bit.  #include <pthread.h>\n#include <stdio.h>\n\nint Global;\n\nvoid *Thread1(void *x) {\n    Global++;\n    return NULL;\n}\n\nint main() {\n    pthread_t t[2];\n    pthread_create(&t[0], NULL, Thread1, NULL);\n    Global = 100;\n    pthread_join(t[0], NULL);\n}\n// compile with gcc -fsanitize=thread -pie -fPIC -ltsan -g simple_race.c  We can see that there is a race condition on the variable Global. Both the main thread and the thread created with pthread_create will try to changethe value at the same time. But, does ThreadSantizer catch it?  $ ./a.out\n==================\nWARNING: ThreadSanitizer: data race (pid=28888)\n  Read of size 4 at 0x7f73ed91c078 by thread T1:\n    #0 Thread1 /home/zmick2/simple_race.c:7 (exe+0x000000000a50)\n    #1  :0 (libtsan.so.0+0x00000001b459)\n\n  Previous write of size 4 at 0x7f73ed91c078 by main thread:\n    #0 main /home/zmick2/simple_race.c:14 (exe+0x000000000ac8)\n\n  Thread T1 (tid=28889, running) created by main thread at:\n    #0  :0 (libtsan.so.0+0x00000001f6ab)\n    #1 main /home/zmick2/simple_race.c:13 (exe+0x000000000ab8)\n\nSUMMARY: ThreadSanitizer: data race /home/zmick2/simple_race.c:7 Thread1\n==================\nThreadSanitizer: reported 1 warnings  If we compiled with the debug flag, then it would give us the variable name as well.",
            "title": "Tsan"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#gdb",
            "text": "Introduction: http://www.cs.cmu.edu/~gilpin/tutorial/",
            "title": "GDB"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#setting-breakpoints-programmatically",
            "text": "A very useful trick when debugging complex C programs with GDB is setting breakpoints in the source code.  int main() {\n    int val = 1;\n    val = 42;\n    asm(\"int $3\"); // set a breakpoint here\n    val = 7;\n}  $ gcc main.c -g -o main && ./main\n(gdb) r\n[...]\nProgram received signal SIGTRAP, Trace/breakpoint trap.\nmain () at main.c:6\n6       val = 7;\n(gdb) p val\n$1 = 42",
            "title": "Setting breakpoints programmatically"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Part-5:-Debugging/#checking-memory-content",
            "text": "http://www.delorie.com/gnu/docs/gdb/gdb_56.html  For example,  int main() {\n    char bad_string[3] = {'C', 'a', 't'};\n    printf(\"%s\", bad_string);\n}  $ gcc main.c -g -o main && ./main\n$ Cat ZVQ\ufffd\u007f $  (gdb) l\n1   #include <stdio.h>\n2   int main() {\n3       char bad_string[3] = {'C', 'a', 't'};\n4       printf(\"%s\", bad_string);\n5   }\n(gdb) b 4\nBreakpoint 1 at 0x100000f57: file main.c, line 4.\n(gdb) r\n[...]\nBreakpoint 1, main () at main.c:4\n4       printf(\"%s\", bad_string);\n(gdb) x/16xb bad_string\n0x7fff5fbff9cd: 0x63    0x61    0x74    0xe0    0xf9    0xbf    0x5f    0xff\n0x7fff5fbff9d5: 0x7f    0x00    0x00    0xfd    0xb5    0x23    0x89    0xff\n\n(gdb)  Here, by using the  x  command with parameters  16xb , we can see that starting at memory address  0x7fff5fbff9c  (value of  bad_string ), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.  0x43 0x61 0x74 0xe0 0xf9 0xbf 0x5f 0xff 0x7f 0x00   \nBack: C Programming, Part 4: Strings and Structs \n|  \nBack: C Programming, Review Questions",
            "title": "Checking memory content"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Review-Questions/",
            "text": "Topics\n\n\n\n\nC Strings representation\n\n\nC Strings as pointers\n\n\nchar p[]vs char* p\n\n\nSimple C string functions (strcmp, strcat, strcpy)\n\n\nsizeof char\n\n\nsizeof x vs x*\n\n\nHeap memory lifetime\n\n\nCalls to heap allocation\n\n\nDeferencing pointers\n\n\nAddress-of operator\n\n\nPointer arithmetic\n\n\nString duplication\n\n\nString truncation\n\n\ndouble-free error\n\n\nString literals\n\n\nPrint formatting.\n\n\nmemory out of bounds errors\n\n\nstatic memory\n\n\nfileio POSIX v C library\n\n\nC io fprintf and printf\n\n\nPOSIX file io (read|write|open)\n\n\nBuffering of stdout\n\n\n\n\nQuestions/Exercises\n\n\n\n\nWhat does the following print out\n\n\n\n\nint main(){\n    fprintf(stderr, \"Hello \");\n    fprintf(stdout, \"It's a small \");\n    fprintf(stderr, \"World\\n\");\n    fprintf(stdout, \"place\\n\");\n    return 0;\n}\n\n\n\n\n\n\nWhat are the differences between the following two declarations? What does \nsizeof\n return for one of them?\n\n\n\n\nchar str1[] = \"bhuvan\";\nchar *str2 = \"another one\";\n\n\n\n\n\n\nWhat is a string in c?\n\n\nCode up a simple \nmy_strcmp\n. How about \nmy_strcat\n, \nmy_strcpy\n, or \nmy_strdup\n? Bonus: Code the functions while only going through the strings \nonce\n.\n\n\nWhat should the following usually return?\n\n\n\n\nint *ptr;\nsizeof(ptr);\nsizeof(*ptr);\n\n\n\n\n\n\nWhat is \nmalloc\n? How is it different than \ncalloc\n. Once memory is \nmalloc\ned how can I use \nrealloc\n?\n\n\nWhat is the \n&\n operator? How about \n*\n?\n\n\nPointer Arithmetic. Assume the following addresses. What are the following shifts?\n\n\n\n\nchar** ptr = malloc(10); //0x100\nptr[0] = malloc(20); //0x200\nptr[1] = malloc(20); //0x300\n\n\n\n\n * `ptr + 2`\n * `ptr + 4`\n * `ptr[0] + 4`\n * `ptr[1] + 2000`\n * `*((int)(ptr + 1)) + 3`\n\n\n\n\n\nHow do we prevent double free errors?\n\n\nWhat is the printf specifier to print a string, \nint\n, or \nchar\n?\n\n\nIs the following code valid? If so, why? Where is \noutput\n located?\n\n\n\n\nchar *foo(int var){\n    static char output[20];\n    snprintf(output, 20, \"%d\", var);\n    return output;\n}\n\n\n\n\n\n\nWrite a function that accepts a string and opens that file prints out the file 40 bytes at a time but every other print reverses the string (try using POSIX API for this).\n\n\nWhat are some differences between the POSIX filedescriptor model and C's \nFILE*\n (ie what function calls are used and which is buffered)? Does POSIX use C's \nFILE*\n internally or vice versa?\n\n\n\n\n\n\n\nBack: C Programming, Part 5: Debugging",
            "title": "C Programming, Review Questions"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Review-Questions/#topics",
            "text": "C Strings representation  C Strings as pointers  char p[]vs char* p  Simple C string functions (strcmp, strcat, strcpy)  sizeof char  sizeof x vs x*  Heap memory lifetime  Calls to heap allocation  Deferencing pointers  Address-of operator  Pointer arithmetic  String duplication  String truncation  double-free error  String literals  Print formatting.  memory out of bounds errors  static memory  fileio POSIX v C library  C io fprintf and printf  POSIX file io (read|write|open)  Buffering of stdout",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/C-Programming,-Review-Questions/#questionsexercises",
            "text": "What does the following print out   int main(){\n    fprintf(stderr, \"Hello \");\n    fprintf(stdout, \"It's a small \");\n    fprintf(stderr, \"World\\n\");\n    fprintf(stdout, \"place\\n\");\n    return 0;\n}   What are the differences between the following two declarations? What does  sizeof  return for one of them?   char str1[] = \"bhuvan\";\nchar *str2 = \"another one\";   What is a string in c?  Code up a simple  my_strcmp . How about  my_strcat ,  my_strcpy , or  my_strdup ? Bonus: Code the functions while only going through the strings  once .  What should the following usually return?   int *ptr;\nsizeof(ptr);\nsizeof(*ptr);   What is  malloc ? How is it different than  calloc . Once memory is  malloc ed how can I use  realloc ?  What is the  &  operator? How about  * ?  Pointer Arithmetic. Assume the following addresses. What are the following shifts?   char** ptr = malloc(10); //0x100\nptr[0] = malloc(20); //0x200\nptr[1] = malloc(20); //0x300   * `ptr + 2`\n * `ptr + 4`\n * `ptr[0] + 4`\n * `ptr[1] + 2000`\n * `*((int)(ptr + 1)) + 3`   How do we prevent double free errors?  What is the printf specifier to print a string,  int , or  char ?  Is the following code valid? If so, why? Where is  output  located?   char *foo(int var){\n    static char output[20];\n    snprintf(output, 20, \"%d\", var);\n    return output;\n}   Write a function that accepts a string and opens that file prints out the file 40 bytes at a time but every other print reverses the string (try using POSIX API for this).  What are some differences between the POSIX filedescriptor model and C's  FILE*  (ie what function calls are used and which is buffered)? Does POSIX use C's  FILE*  internally or vice versa?    \nBack: C Programming, Part 5: Debugging",
            "title": "Questions/Exercises"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/",
            "text": "Warning - question numbers subject to change\n\n\nMemory and Strings\n\n\nQ1.1\n\n\nIn the example below, which variables are guaranteed to print the value of zero?\n\n\nint a;\nstatic int b;\n\nvoid func() {\n   static int c;\n   int d;\n   printf(\"%d %d %d %d\\n\",a,b,c,d);\n}\n\n\n\n\nQ 1.2\n\n\nIn the example below, which variables are guaranteed to print the value of zero?\n\n\nvoid func() {\n   int* ptr1 = malloc( sizeof(int) );\n   int* ptr2 = realloc(NULL, sizeof(int) );\n   int* ptr3 = calloc( 1, sizeof(int) );\n   int* ptr4 = calloc( sizeof(int) , 1);\n\n   printf(\"%d %d %d %d\\n\",*ptr1,*ptr2,*ptr3,*ptr4);\n}\n\n\n\n\nQ 1.3\n\n\nExplain the error in the following attempt to copy a string.\n\n\nchar* copy(char*src) {\n char*result = malloc( strlen(src) ); \n strcpy(result, src); \n return result;\n}\n\n\n\n\nQ 1.4\n\n\nWhy does the following attempt to copy a string sometimes work and sometimes fail?\n\n\nchar* copy(char*src) {\n char*result = malloc( strlen(src) +1 ); \n strcat(result, src); \n return result;\n}\n\n\n\n\nQ 1.4\n\n\nExplain the two errors in the following code that attempts to copy a string.\n\n\nchar* copy(char*src) {\n char result[sizeof(src)]; \n strcpy(result, src); \n return result;\n}\n\n\n\n\nQ 1.5\n\n\nWhich of the following is legal?\n\n\nchar a[] = \"Hello\"; strcpy(a, \"World\");\nchar b[] = \"Hello\"; strcpy(b, \"World12345\", b);\nchar* c = \"Hello\"; strcpy(c, \"World\");\n\n\n\n\nQ 1.6\n\n\nComplete the function pointer typedef to declare a pointer to a function that takes a void\n argument and returns a void\n. Name your type 'pthread_callback'\n\n\ntypedef ______________________;\n\n\n\n\nQ 1.7\n\n\nIn addition to the function arguments what else is stored on a thread's stack?\n\n\nQ 1.8\n\n\nImplement a version of \nchar* strcat(char*dest, const char*src)\n using only \nstrcpy\n  \nstrlen\n and pointer arithmetic\n\n\nchar* mystrcat(char*dest, const char*src) {\n\n  ? Use strcpy strlen here\n\n  return dest;\n}\n\n\n\n\nQ 1.9\n\n\nImplement version of size_t strlen(const char*) using a loop and no function calls.\n\n\nsize_t mystrlen(const char*s) {\n\n}\n\n\n\n\nQ 1.10\n\n\nIdentify the three bugs in the following implementation of \nstrcpy\n.\n\n\nchar* strcpy(const char* dest, const char* src) {\n  while(*src) { *dest++ = *src++; }\n  return dest;\n}\n\n\n\n\nPrinting\n\n\nQ 2.1\n\n\nSpot the two errors!\n\n\nfprintf(\"You scored 100%\");\n\n\n\n\nFormatting and Printing to a file\n\n\nQ 3.1\n\n\nComplete the following code to print to a file. Print the name, a comma and the score to the file 'result.txt'\n\n\nchar* name = .....;\nint score = ......\nFILE *f = fopen(\"result.txt\",_____);\nif(f) {\n    _____\n}\nfclose(f);\n\n\n\n\nPrinting to a string\n\n\nQ 4.1\n\n\nHow would you print the values of variables a,mesg,val and ptr to a string? Print a as an integer, mesg as C string, val as a double val and ptr as a hexadecimal pointer. You may assume the mesg points to a short C string(<50 characters).\nBonus: How would you make this code more robust or able to cope with?\n\n\nchar* toString(int a, char*mesg, double val, void* ptr) {\n   char* result = malloc( strlen(mesg) + 50);\n    _____\n   return result;\n}\n\n\n\n\nInput parsing\n\n\nQ 5.1\n\n\nWhy should you check the return value of sscanf and scanf?\n\n\nQ 5.2\n\n\nWhy is 'gets' dangerous?\n\n\nQ 5.3\n\n\nWrite a complete program that uses \ngetline\n. Ensure your program has no memory leaks.\n\n\nHeap memory\n\n\nWhen would you use calloc not malloc? \nWhen would realloc be useful?\n\n\n(Todo - move this question to another page)\nWhat mistake did the programmer make in the following code? Is it possible to fix it i) using heap memory? ii) using global (static) memory?\n\n\nstatic int id;\n\nchar* next_ticket() {\n  id ++;\n  char result[20];\n  sprintf(result,\"%d\",id);\n  return result;\n}",
            "title": "C Programming: Review Questions"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#warning-question-numbers-subject-to-change",
            "text": "",
            "title": "Warning - question numbers subject to change"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#memory-and-strings",
            "text": "",
            "title": "Memory and Strings"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q11",
            "text": "In the example below, which variables are guaranteed to print the value of zero?  int a;\nstatic int b;\n\nvoid func() {\n   static int c;\n   int d;\n   printf(\"%d %d %d %d\\n\",a,b,c,d);\n}",
            "title": "Q1.1"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-12",
            "text": "In the example below, which variables are guaranteed to print the value of zero?  void func() {\n   int* ptr1 = malloc( sizeof(int) );\n   int* ptr2 = realloc(NULL, sizeof(int) );\n   int* ptr3 = calloc( 1, sizeof(int) );\n   int* ptr4 = calloc( sizeof(int) , 1);\n\n   printf(\"%d %d %d %d\\n\",*ptr1,*ptr2,*ptr3,*ptr4);\n}",
            "title": "Q 1.2"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-13",
            "text": "Explain the error in the following attempt to copy a string.  char* copy(char*src) {\n char*result = malloc( strlen(src) ); \n strcpy(result, src); \n return result;\n}",
            "title": "Q 1.3"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-14",
            "text": "Why does the following attempt to copy a string sometimes work and sometimes fail?  char* copy(char*src) {\n char*result = malloc( strlen(src) +1 ); \n strcat(result, src); \n return result;\n}",
            "title": "Q 1.4"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-14_1",
            "text": "Explain the two errors in the following code that attempts to copy a string.  char* copy(char*src) {\n char result[sizeof(src)]; \n strcpy(result, src); \n return result;\n}",
            "title": "Q 1.4"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-15",
            "text": "Which of the following is legal?  char a[] = \"Hello\"; strcpy(a, \"World\");\nchar b[] = \"Hello\"; strcpy(b, \"World12345\", b);\nchar* c = \"Hello\"; strcpy(c, \"World\");",
            "title": "Q 1.5"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-16",
            "text": "Complete the function pointer typedef to declare a pointer to a function that takes a void  argument and returns a void . Name your type 'pthread_callback'  typedef ______________________;",
            "title": "Q 1.6"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-17",
            "text": "In addition to the function arguments what else is stored on a thread's stack?",
            "title": "Q 1.7"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-18",
            "text": "Implement a version of  char* strcat(char*dest, const char*src)  using only  strcpy    strlen  and pointer arithmetic  char* mystrcat(char*dest, const char*src) {\n\n  ? Use strcpy strlen here\n\n  return dest;\n}",
            "title": "Q 1.8"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-19",
            "text": "Implement version of size_t strlen(const char*) using a loop and no function calls.  size_t mystrlen(const char*s) {\n\n}",
            "title": "Q 1.9"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-110",
            "text": "Identify the three bugs in the following implementation of  strcpy .  char* strcpy(const char* dest, const char* src) {\n  while(*src) { *dest++ = *src++; }\n  return dest;\n}",
            "title": "Q 1.10"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#printing",
            "text": "",
            "title": "Printing"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-21",
            "text": "Spot the two errors!  fprintf(\"You scored 100%\");",
            "title": "Q 2.1"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#formatting-and-printing-to-a-file",
            "text": "",
            "title": "Formatting and Printing to a file"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-31",
            "text": "Complete the following code to print to a file. Print the name, a comma and the score to the file 'result.txt'  char* name = .....;\nint score = ......\nFILE *f = fopen(\"result.txt\",_____);\nif(f) {\n    _____\n}\nfclose(f);",
            "title": "Q 3.1"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#printing-to-a-string",
            "text": "",
            "title": "Printing to a string"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-41",
            "text": "How would you print the values of variables a,mesg,val and ptr to a string? Print a as an integer, mesg as C string, val as a double val and ptr as a hexadecimal pointer. You may assume the mesg points to a short C string(<50 characters).\nBonus: How would you make this code more robust or able to cope with?  char* toString(int a, char*mesg, double val, void* ptr) {\n   char* result = malloc( strlen(mesg) + 50);\n    _____\n   return result;\n}",
            "title": "Q 4.1"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#input-parsing",
            "text": "",
            "title": "Input parsing"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-51",
            "text": "Why should you check the return value of sscanf and scanf?",
            "title": "Q 5.1"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-52",
            "text": "Why is 'gets' dangerous?",
            "title": "Q 5.2"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#q-53",
            "text": "Write a complete program that uses  getline . Ensure your program has no memory leaks.",
            "title": "Q 5.3"
        },
        {
            "location": "/SystemProgramming/C-Programming:-Review-Questions/#heap-memory",
            "text": "When would you use calloc not malloc? \nWhen would realloc be useful?  (Todo - move this question to another page)\nWhat mistake did the programmer make in the following code? Is it possible to fix it i) using heap memory? ii) using global (static) memory?  static int id;\n\nchar* next_ticket() {\n  id ++;\n  char result[20];\n  sprintf(result,\"%d\",id);\n  return result;\n}",
            "title": "Heap memory"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-1:-Resource-Allocation-Graph/",
            "text": "What is a Resource Allocation Graph?\n\n\nA resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. It is very powerful and simple tool to illustrate how interacting  processes can deadlock. If a process is \nusing\n a resource, an arrow is drawn from the resource node to the process node. If a process is \nrequesting\n a resource, an arrow is drawn from the process node to the resource node.\n\n\nIf there is a cycle in the Resource Allocation Graph and each resource in the cycle provides only one instance, then the processes will deadlock. For example, if process 1 holds resource A, process 2 holds resource B and process 1 is waiting for B and process 2 is waiting for A, then process 1 and 2 process will be deadlocked.\n\n\nHere's another example, that shows Processes 1 and 2 acquiring resources 1 and 2 while process 3 is waiting to acquire both resources. In this example there is no deadlock because there is no circular dependency.\n\n\n\n\nDeadlock!\n\n\nA lot of times, we don't know the specific order that a resource may be acquired so we can draw the graph directed.\n\n\n\n\nAs a possibility matrix. Then we can draw arrows and see if there is a directed version that would lead us to a deadlock.\n\n\n\n\nConsider the following resource allocation graph (assume that the processes ask for exclusive access to the file). If you have a bunch of processes running and they request resources and the operating system ends up in this state, you deadlock! You may not see this because the operating system may *\npreempt\n some processes breaking the cycle but there is still a change that your three lonely processes could deadlock. You can also make these kind of graphs with \nmake\n and rule dependencies (with our parmake MP for example).",
            "title": "Deadlock, Part 1: Resource Allocation Graph"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-1:-Resource-Allocation-Graph/#what-is-a-resource-allocation-graph",
            "text": "A resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. It is very powerful and simple tool to illustrate how interacting  processes can deadlock. If a process is  using  a resource, an arrow is drawn from the resource node to the process node. If a process is  requesting  a resource, an arrow is drawn from the process node to the resource node.  If there is a cycle in the Resource Allocation Graph and each resource in the cycle provides only one instance, then the processes will deadlock. For example, if process 1 holds resource A, process 2 holds resource B and process 1 is waiting for B and process 2 is waiting for A, then process 1 and 2 process will be deadlocked.  Here's another example, that shows Processes 1 and 2 acquiring resources 1 and 2 while process 3 is waiting to acquire both resources. In this example there is no deadlock because there is no circular dependency.",
            "title": "What is a Resource Allocation Graph?"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-1:-Resource-Allocation-Graph/#deadlock",
            "text": "A lot of times, we don't know the specific order that a resource may be acquired so we can draw the graph directed.   As a possibility matrix. Then we can draw arrows and see if there is a directed version that would lead us to a deadlock.   Consider the following resource allocation graph (assume that the processes ask for exclusive access to the file). If you have a bunch of processes running and they request resources and the operating system ends up in this state, you deadlock! You may not see this because the operating system may * preempt  some processes breaking the cycle but there is still a change that your three lonely processes could deadlock. You can also make these kind of graphs with  make  and rule dependencies (with our parmake MP for example).",
            "title": "Deadlock!"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-2:-Deadlock-Conditions/",
            "text": "Coffman conditions\n\n\nThere are four \nnecessary\n and \nsufficient\n conditions for deadlock. These are known as the Coffman conditions.\n\n\n\n\nMutual Exclusion\n\n\nCircular Wait\n\n\nHold and Wait\n\n\nNo pre-emption\n\n\n\n\nIf you break any of them, you cannot have deadlock!\n\n\nAll of these conditions are required for deadlock, so let's discuss each one in turn. First the easy ones-\n\n Mutual Exclusion: The resource cannot be shared\n\n Circular Wait: There exists a cycle in the Resource Allocation Graph. There exists a set of processes {P1,P2,...} such that P1 is waiting for resources held by P2, which is waiting for P3,..., which is waiting for P1.\n\n Hold and Wait: A process acquires an incomplete set of resources and holds onto them while waiting for the other resources.\n\n No pre-emption: Once a process has acquired a resource, the resource cannot be taken away from a process and the process will not voluntarily give up a resource.\n\n\nBreaking the Coffman Conditions\n\n\nTwo students need a pen and paper:\n\n The students share a pen and paper. Deadlock is avoided because Mutual Exclusion was not required.\n\n The students both agree to grab the pen before grabbing the paper. Deadlock is avoided because there cannot be a circular wait.\n\n The students grab both the pen and paper in one operation (\"Get both or get none\"). Deadlock is avoided because there is no \nHold and Wait\n\n\n The students are friends and will ask each other to give up a held resource. Deadlock is avoided because pre-emption is allowed.\n\n\nLivelock\n\n\nLivelock is \nnot\n deadlock-\n\n\nConsider the following 'solution'\n* The students will put down one held resource if they are unable to pick up the other resource within 10 seconds. This solution avoids deadlock however it may suffer from livelock.\n\n\nLivelock occurs when a process continues to execute but is unable to make progress.\nIn practice Livelock may occur because the programmer has taken steps to avoid deadlock. In the above example, in a busy system, the student will continually release the first resource because they are never able to obtain the second resource. The system is not deadlock (the student process is still executing) however it's not making any progress either.\n\n\nDeadlock Prevention/Avoidance vs Deadlock Detection\n\n\nDeadlock prevention is making sure that deadlock cannot happen, meaning that you break a coffman condition. This works the best inside a single program and the software engineer making the choice to break a certain coffman condition. Consider the \nBanker's Algorithm\n. It is another algorithm for deadlock avoidance. The whole implementation is outside the scope of this class, just know that there are more generalized algorithms for operating systems.\n\n\nDeadlock detection on the other hand is allowing the system to enter a deadlocked state. After entering, the system uses the information that it has to break deadlock. As an example, consider multiple processes accessing files. The operating system is able to keep track of all of the files/resources through file descriptors at some level (either abstracted through an API or directly). If the operating system detects a directed cycle in the operating system file descriptor table it may break one process' hold (through scheduling for example) and let the system proceed.\n\n\nDining Philosophers\n\n\nThe Dining Philosophers problem is a classic synchronization problem. Imagine I invite N (let's say 5) philosophers to a meal. We will sit them at a table with 5 chopsticks (one between each philosopher). A philosopher alternates between wanting to eat or think. To eat the philosopher must pick up the two chopsticks either side of their position (the original problem required each philosopher to have two forks). However these chopsticks are shared with his neighbor.\n\n\n\n\nIs it possible to design an efficient solution such that all philosophers get to eat? Or, will some philosophers starve, never obtaining a second chopstick? Or will all of them deadlock? For example, imagine each guest picks up the chopstick on their left and then waits for the chopstick on their right to be free. Oops - our philosophers have deadlocked!",
            "title": "Deadlock, Part 2: Deadlock Conditions"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-2:-Deadlock-Conditions/#coffman-conditions",
            "text": "There are four  necessary  and  sufficient  conditions for deadlock. These are known as the Coffman conditions.   Mutual Exclusion  Circular Wait  Hold and Wait  No pre-emption   If you break any of them, you cannot have deadlock!  All of these conditions are required for deadlock, so let's discuss each one in turn. First the easy ones-  Mutual Exclusion: The resource cannot be shared  Circular Wait: There exists a cycle in the Resource Allocation Graph. There exists a set of processes {P1,P2,...} such that P1 is waiting for resources held by P2, which is waiting for P3,..., which is waiting for P1.  Hold and Wait: A process acquires an incomplete set of resources and holds onto them while waiting for the other resources.  No pre-emption: Once a process has acquired a resource, the resource cannot be taken away from a process and the process will not voluntarily give up a resource.",
            "title": "Coffman conditions"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-2:-Deadlock-Conditions/#breaking-the-coffman-conditions",
            "text": "Two students need a pen and paper:  The students share a pen and paper. Deadlock is avoided because Mutual Exclusion was not required.  The students both agree to grab the pen before grabbing the paper. Deadlock is avoided because there cannot be a circular wait.  The students grab both the pen and paper in one operation (\"Get both or get none\"). Deadlock is avoided because there is no  Hold and Wait   The students are friends and will ask each other to give up a held resource. Deadlock is avoided because pre-emption is allowed.",
            "title": "Breaking the Coffman Conditions"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-2:-Deadlock-Conditions/#livelock",
            "text": "Livelock is  not  deadlock-  Consider the following 'solution'\n* The students will put down one held resource if they are unable to pick up the other resource within 10 seconds. This solution avoids deadlock however it may suffer from livelock.  Livelock occurs when a process continues to execute but is unable to make progress.\nIn practice Livelock may occur because the programmer has taken steps to avoid deadlock. In the above example, in a busy system, the student will continually release the first resource because they are never able to obtain the second resource. The system is not deadlock (the student process is still executing) however it's not making any progress either.",
            "title": "Livelock"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-2:-Deadlock-Conditions/#deadlock-preventionavoidance-vs-deadlock-detection",
            "text": "Deadlock prevention is making sure that deadlock cannot happen, meaning that you break a coffman condition. This works the best inside a single program and the software engineer making the choice to break a certain coffman condition. Consider the  Banker's Algorithm . It is another algorithm for deadlock avoidance. The whole implementation is outside the scope of this class, just know that there are more generalized algorithms for operating systems.  Deadlock detection on the other hand is allowing the system to enter a deadlocked state. After entering, the system uses the information that it has to break deadlock. As an example, consider multiple processes accessing files. The operating system is able to keep track of all of the files/resources through file descriptors at some level (either abstracted through an API or directly). If the operating system detects a directed cycle in the operating system file descriptor table it may break one process' hold (through scheduling for example) and let the system proceed.",
            "title": "Deadlock Prevention/Avoidance vs Deadlock Detection"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-2:-Deadlock-Conditions/#dining-philosophers",
            "text": "The Dining Philosophers problem is a classic synchronization problem. Imagine I invite N (let's say 5) philosophers to a meal. We will sit them at a table with 5 chopsticks (one between each philosopher). A philosopher alternates between wanting to eat or think. To eat the philosopher must pick up the two chopsticks either side of their position (the original problem required each philosopher to have two forks). However these chopsticks are shared with his neighbor.   Is it possible to design an efficient solution such that all philosophers get to eat? Or, will some philosophers starve, never obtaining a second chopstick? Or will all of them deadlock? For example, imagine each guest picks up the chopstick on their left and then waits for the chopstick on their right to be free. Oops - our philosophers have deadlocked!",
            "title": "Dining Philosophers"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/",
            "text": "Backstory\n\n\n\n\nSo you have your philosophers sitting around a table all wanting to eat some pasta (or whatever that is) and they are really hungry. Each of the philosophers are essentially the same, meaning that each philosopher has the same instruction set based on the other philosopher ie you can't tell every even philosopher to do one thing and every odd philosopher to do another thing.\n\n\nFailed Solutions\n\n\nLeft-Right Deadlock\n\n\nWhat do we do? Let's try a simple solution\n\n\nvoid* philosopher(void* forks){\n     info phil_info = forks;\n     pthread_mutex_t* left_fork = phil_info->left_fork;\n     pthread_mutex_t* right_fork = phil_info->right_fork;\n     while(phil_info->simulation){\n          pthread_mutex_lock(left_fork);\n          pthread_mutex_lock(right_fork);\n          eat(left_fork, right_fork);\n          pthread_mutex_unlock(left_fork);\n          pthread_mutex_unlock(right_fork);\n     }\n}\n\n\n\n\nBut this runs into a problem! What if everyone picks up their left fork and is waiting on their right fork? We have deadlocked the program. It is important to note that deadlock doesn't happen all the time and the probability that this solution deadlocks goes down as the number of philosophers goes up. What is really important to note is that eventually that this solution will deadlock, letting threads starve which is bad.\n\n\nTrylock? More like livelock\n\n\nSo now you are thinking about breaking one of the coffman conditions. We have\n- Mutual Exclusion\n- No Preemption\n- Hold and wait\n- Circular Wait\n\n\nWell we can't have two philosophers use a fork at the same time, mutual exclusion is out of the picture. In our current, simple model, we can't have the philosopher let go of the mutex lock once he/she has a hold of it, so we will take this solution out right now -- there are some notes at the bottom of the page about this solution. Let's break Hold and Wait!\n\n\nvoid* philosopher(void* forks){\n     info phil_info = forks;\n     pthread_mutex_t* left_fork = phil_info->left_fork;\n     pthread_mutex_t* right_fork = phil_info->right_fork;\n     while(phil_info->simulation){\n          pthread_mutex_lock(left_fork);\n          int failed = pthread_mutex_trylock(right_fork);\n          if(!failed){\n               eat(left_fork, right_fork);\n               pthread_mutex_unlock(right_fork);\n          }\n          pthread_mutex_unlock(left_fork);\n     }\n}\n\n\n\n\nNow our philosopher picks up the left fork and tries to grab the right. If it's available, they eats. If it's not available, they put the left fork down and try again. No deadlock!\n\n\nBut, there is a problem. What if all the philosophers pick up their left at the same time, try to grab their right, put their left down, pick up their left, try to grab their right.... We have now livelocked our solution! Our poor philosopher are still starving, so let's give them some proper solutions.\n\n\nViable Solutions\n\n\nArbitrator (Naive and Advanced).\n\n\nThe naive arbitrator solution is have one arbitrator (a mutex for example). Have each of the philosopher ask the arbitrator for permission to eat. This solution allows one philosopher to eat at a time. When they are done, another philosopher can ask for permission to eat.\n\n\nThis prevents deadlock because there is no circular wait! No philosopher has to wait on any other philosopher.\n\n\nThe advanced arbitrator solution is to implement a class that determines if the philosopher's forks are in the arbitrator's possession. If they are, they give them to the philosopher, let him eat, and take the forks back. This has the added bonus of being able to have multiple philosopher eat at the same time.\n\n\nProblems:\n\n\n\n\nThese solutions are slow\n\n\nThey have a single point of failure, the arbitrator making it a bottleneck\n\n\nThe arbitrator needs to also be fair, and be able to determine deadlock in the second solution\n\n\nIn practical systems, the arbitrator tends to give the forks repeatedly to philosophers that just ate because of process scheduling\n\n\n\n\nLeaving the Table (Stallings' Solution)\n\n\nWhy does the first solution deadlock? Well there are n philosophers and n chopsticks. What if there is only 1 philsopher at the table? Can we deadlock? No. \n\n\nHow about 2 philsophers? 3? ... You can see where this is going. Stallings' solutions says to remove philosophers from the table until deadlock is not possible -- think about what the magic number of philosophers at the table is. The way to do this in actual system is through semaphores and letting a certain number of philosopher through.\n\n\nProblems:\n\n\n\n\nThe solution requires a lot of context switching which is very expensive for the CPU\n\n\nYou need to know about the number of resources before hand in order to only let that number of philosophers\n\n\nAgain priority is given to the processes who have already eaten.\n\n\n\n\nPartial Ordering (Dijkstra's Solution)\n\n\nThis is Dijkstra's solution (he was the one to propose this problem on an exam). Why does the first solution deadlock? Dijkstra thought that the last philosopher who picks up his left fork (causing the solution to deadlock) should pick up his right. He accomplishes it by number the forks 1..n, and tells each of the philosopher to pick up his lower number fork.\n\n\nLet's run through the deadlock condition again. Everyone tries to pick up their lower number fork first. Philosopher 1 gets fork 1, Philosopher 2 gets fork 2, and so on until we get to Philosopher n. They have to choose between fork 1 and n. fork 1 is already held up by philosopher 1, so they can't pick up that fork, meaning he won't pick up fork n. We have broken circular wait! Meaning deadlock isn't possible.\n\n\nProblems:\n\n\n\n\nThe philosopher needs to know the set of resources in order before grabbing any resources.\n\n\nYou need to define a partial order to all of the resources.\n\n\nPrioritizes philosopher who have already eaten.\n\n\n\n\nAdvanced Solutions\n\n\nThere are many more advanced solutions a non-exhaustive list includes\n- Clean/Dirty Forks (Chandra/Misra Solution)\n- Actor Model (other Message passing models)\n- Super Arbitrators (Complicated pipelines)",
            "title": "Deadlock, Part 3: Dining Philosophers"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#backstory",
            "text": "So you have your philosophers sitting around a table all wanting to eat some pasta (or whatever that is) and they are really hungry. Each of the philosophers are essentially the same, meaning that each philosopher has the same instruction set based on the other philosopher ie you can't tell every even philosopher to do one thing and every odd philosopher to do another thing.",
            "title": "Backstory"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#failed-solutions",
            "text": "",
            "title": "Failed Solutions"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#left-right-deadlock",
            "text": "What do we do? Let's try a simple solution  void* philosopher(void* forks){\n     info phil_info = forks;\n     pthread_mutex_t* left_fork = phil_info->left_fork;\n     pthread_mutex_t* right_fork = phil_info->right_fork;\n     while(phil_info->simulation){\n          pthread_mutex_lock(left_fork);\n          pthread_mutex_lock(right_fork);\n          eat(left_fork, right_fork);\n          pthread_mutex_unlock(left_fork);\n          pthread_mutex_unlock(right_fork);\n     }\n}  But this runs into a problem! What if everyone picks up their left fork and is waiting on their right fork? We have deadlocked the program. It is important to note that deadlock doesn't happen all the time and the probability that this solution deadlocks goes down as the number of philosophers goes up. What is really important to note is that eventually that this solution will deadlock, letting threads starve which is bad.",
            "title": "Left-Right Deadlock"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#trylock-more-like-livelock",
            "text": "So now you are thinking about breaking one of the coffman conditions. We have\n- Mutual Exclusion\n- No Preemption\n- Hold and wait\n- Circular Wait  Well we can't have two philosophers use a fork at the same time, mutual exclusion is out of the picture. In our current, simple model, we can't have the philosopher let go of the mutex lock once he/she has a hold of it, so we will take this solution out right now -- there are some notes at the bottom of the page about this solution. Let's break Hold and Wait!  void* philosopher(void* forks){\n     info phil_info = forks;\n     pthread_mutex_t* left_fork = phil_info->left_fork;\n     pthread_mutex_t* right_fork = phil_info->right_fork;\n     while(phil_info->simulation){\n          pthread_mutex_lock(left_fork);\n          int failed = pthread_mutex_trylock(right_fork);\n          if(!failed){\n               eat(left_fork, right_fork);\n               pthread_mutex_unlock(right_fork);\n          }\n          pthread_mutex_unlock(left_fork);\n     }\n}  Now our philosopher picks up the left fork and tries to grab the right. If it's available, they eats. If it's not available, they put the left fork down and try again. No deadlock!  But, there is a problem. What if all the philosophers pick up their left at the same time, try to grab their right, put their left down, pick up their left, try to grab their right.... We have now livelocked our solution! Our poor philosopher are still starving, so let's give them some proper solutions.",
            "title": "Trylock? More like livelock"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#viable-solutions",
            "text": "",
            "title": "Viable Solutions"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#arbitrator-naive-and-advanced",
            "text": "The naive arbitrator solution is have one arbitrator (a mutex for example). Have each of the philosopher ask the arbitrator for permission to eat. This solution allows one philosopher to eat at a time. When they are done, another philosopher can ask for permission to eat.  This prevents deadlock because there is no circular wait! No philosopher has to wait on any other philosopher.  The advanced arbitrator solution is to implement a class that determines if the philosopher's forks are in the arbitrator's possession. If they are, they give them to the philosopher, let him eat, and take the forks back. This has the added bonus of being able to have multiple philosopher eat at the same time.",
            "title": "Arbitrator (Naive and Advanced)."
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#problems",
            "text": "These solutions are slow  They have a single point of failure, the arbitrator making it a bottleneck  The arbitrator needs to also be fair, and be able to determine deadlock in the second solution  In practical systems, the arbitrator tends to give the forks repeatedly to philosophers that just ate because of process scheduling",
            "title": "Problems:"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#leaving-the-table-stallings-solution",
            "text": "Why does the first solution deadlock? Well there are n philosophers and n chopsticks. What if there is only 1 philsopher at the table? Can we deadlock? No.   How about 2 philsophers? 3? ... You can see where this is going. Stallings' solutions says to remove philosophers from the table until deadlock is not possible -- think about what the magic number of philosophers at the table is. The way to do this in actual system is through semaphores and letting a certain number of philosopher through.",
            "title": "Leaving the Table (Stallings' Solution)"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#problems_1",
            "text": "The solution requires a lot of context switching which is very expensive for the CPU  You need to know about the number of resources before hand in order to only let that number of philosophers  Again priority is given to the processes who have already eaten.",
            "title": "Problems:"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#partial-ordering-dijkstras-solution",
            "text": "This is Dijkstra's solution (he was the one to propose this problem on an exam). Why does the first solution deadlock? Dijkstra thought that the last philosopher who picks up his left fork (causing the solution to deadlock) should pick up his right. He accomplishes it by number the forks 1..n, and tells each of the philosopher to pick up his lower number fork.  Let's run through the deadlock condition again. Everyone tries to pick up their lower number fork first. Philosopher 1 gets fork 1, Philosopher 2 gets fork 2, and so on until we get to Philosopher n. They have to choose between fork 1 and n. fork 1 is already held up by philosopher 1, so they can't pick up that fork, meaning he won't pick up fork n. We have broken circular wait! Meaning deadlock isn't possible.",
            "title": "Partial Ordering (Dijkstra's Solution)"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#problems_2",
            "text": "The philosopher needs to know the set of resources in order before grabbing any resources.  You need to define a partial order to all of the resources.  Prioritizes philosopher who have already eaten.",
            "title": "Problems:"
        },
        {
            "location": "/SystemProgramming/Deadlock,-Part-3:-Dining-Philosophers/#advanced-solutions",
            "text": "There are many more advanced solutions a non-exhaustive list includes\n- Clean/Dirty Forks (Chandra/Misra Solution)\n- Actor Model (other Message passing models)\n- Super Arbitrators (Complicated pipelines)",
            "title": "Advanced Solutions"
        },
        {
            "location": "/SystemProgramming/Deadlock-Review-Questions/",
            "text": "Topics\n\n\nCoffman Conditions\nResource Allocation Graphs\nDining Philosophers\n\n Failed DP Solutions\n\n Livelocking DP Solutions\n* Working DP Solutions: Benefits/Drawbacks\n\n\nQuestions\n\n\n\n\nWhat are the Coffman Conditions?\n\n\nWhat do each of the Coffman conditions mean? (e.g. can you provide a definition of each one)\n\n\nGive a real life example of breaking each Coffman condition in turn. A situation to consider: Painters, Paint, Paintbrushes etc. How would you assure that work would get done?\n\n\nBe able to identify when Dining Philosophers code causes a deadlock (or not).\nFor example, if you saw the following code snippet which Coffman condition is not satisfied?\n\n\n\n\n// Get both locks or none.\npthread_mutex_lock( a );\nif( pthread_mutex_trylock( b ) ) { /*failed*/\n   pthread_mutex_unlock( a );\n   ...\n}\n\n\n\n\n\n\nIf one thread calls\n\n\n\n\n  pthread_mutex_lock(m1) // success\n  pthread_mutex_lock(m2) // blocks\n\n\n\n\nand another threads calls\n\n\n  pthread_mutex_lock(m2) // success\n  pthread_mutex_lock(m1) // blocks\n\n\n\n\nWhat happens and why? What happens if a third thread calls \npthread_mutex_lock(m1)\n ?\n\n\n\n\nHow many processes are blocked? As usual assume that a process is able to complete if it is able to acquire all of the resources listed below.\n\n\nP1 acquires R1\n\n\nP2 acquires R2\n\n\nP1 acquires R3\n\n\nP2 waits for R3\n\n\nP3 acquires R5\n\n\nP1 waits for R4\n\n\nP3 waits for R1\n\n\nP4 waits for R5\n\n\nP5 waits for R1\n\n\n\n\n\n\n\n\n(Draw out the resource graph!)",
            "title": "Deadlock Review Questions"
        },
        {
            "location": "/SystemProgramming/Deadlock-Review-Questions/#topics",
            "text": "Coffman Conditions\nResource Allocation Graphs\nDining Philosophers  Failed DP Solutions  Livelocking DP Solutions\n* Working DP Solutions: Benefits/Drawbacks",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/Deadlock-Review-Questions/#questions",
            "text": "What are the Coffman Conditions?  What do each of the Coffman conditions mean? (e.g. can you provide a definition of each one)  Give a real life example of breaking each Coffman condition in turn. A situation to consider: Painters, Paint, Paintbrushes etc. How would you assure that work would get done?  Be able to identify when Dining Philosophers code causes a deadlock (or not).\nFor example, if you saw the following code snippet which Coffman condition is not satisfied?   // Get both locks or none.\npthread_mutex_lock( a );\nif( pthread_mutex_trylock( b ) ) { /*failed*/\n   pthread_mutex_unlock( a );\n   ...\n}   If one thread calls     pthread_mutex_lock(m1) // success\n  pthread_mutex_lock(m2) // blocks  and another threads calls    pthread_mutex_lock(m2) // success\n  pthread_mutex_lock(m1) // blocks  What happens and why? What happens if a third thread calls  pthread_mutex_lock(m1)  ?   How many processes are blocked? As usual assume that a process is able to complete if it is able to acquire all of the resources listed below.  P1 acquires R1  P2 acquires R2  P1 acquires R3  P2 waits for R3  P3 acquires R5  P1 waits for R4  P3 waits for R1  P4 waits for R5  P5 waits for R1     (Draw out the resource graph!)",
            "title": "Questions"
        },
        {
            "location": "/SystemProgramming/Exam-Topics/",
            "text": "The final exam will likely include multiple choice questions that test your mastery of the following.\n\n\n`\nCSP (critical section problems)\nHTTP\nSIGINT\nTCP\nTLB\nVirtual Memory\narrays\nbarrier\nc strings\nchmod\nclient/server\ncoffman conditions\ncondition variables\ncontext switch\ndeadlock\ndining philosophers\nepoll\nexit\nfile I/O\nfile system representation\nfork/exec/wait\nfprintf\nfree\nheap allocator\nheap/stack\ninode vs name\nmalloc\nmkfifo\nmmap\nmutexes\nnetwork ports\nopen/close\noperating system terms\npage fault\npage tables\npipes\npointer arithmetic\npointers\nprinting (printf)\nproducer/consumer\nprogress/mutex\nrace conditions\nread/write\nreader/writer\nresource allocation graphs\nring buffer\nscanf \nbuffering\nscheduling\nselect\nsemaphores\nsignals\nsizeof\nstat\nstderr/stdout\nsymlinks\nthread control (_create, _join, _exit)\nvariable initializers\nvariable scope\nvm thrashing\nwait macros\nwrite/read with errno, EINTR and partial data",
            "title": "Exam Topics"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/",
            "text": "Navigation/Terminology\n\n\nDesign a file system! What are your design goals?\n\n\nThe design of a file system is difficult problem because there many high-level design goals that we'd like to satisfy. An incomplete list of ideal goals include:\n\n\n\n\nReliable and robust (even with hardware failures or incomplete writes due to power loss)\n\n\nAccess (security) controls\n\n\nAccounting and quotas\n\n\nIndexing and search\n\n\nVersioning and backup capabilities\n\n\nEncryption\n\n\nAutomatic compression\n\n\nHigh performance (e.g. Caching in-memory)\n\n\nEfficient use of storage de-duplication\n\n\n\n\nNot all filesystems natively support all of these goals. For example, many filesystems do not automatically compress rarely-used files\n\n\nWhat are \n.\n, \n..\n, and \n...\n?\n\n\nIn standard unix file systems: \n\n \n.\n represents the current directory\n\n\n \n..\n represents the parent directory\n\n* \n...\n is NOT a valid representation of any directory (this not the grandparent directory). It \ncould\n however be the name of a file on disk.\n\n\nWhat are absolute and relative paths?\n\n\nAbsolute paths are paths that start from the 'root node' of your directory tree. Relative paths are paths that start from your current position in the tree.\n\n\nWhat are some examples of relative and absolute paths?\n\n\nIf you start in your home directory (\"~\" for short), then \nDesktop/cs241\n would be a relative path. Its absolute path counterpart might be something like \n/Users/[yourname]/Desktop/cs241\n.\n\n\nHow do I simplify \na/b/../c/./\n?\n\n\nRemember that \n..\n means 'parent folder' and that \n.\n means 'current folder'.\n\n\nExample: \na/b/../c/./\n\n- Step 1: \ncd a\n (in a)\n- Step 2: \ncd b\n (in a/b)\n- Step 3: \ncd ..\n (in a, because .. represents 'parent folder')\n- Step 4: \ncd c\n (in a/c)\n- Step 5: \ncd .\n (in a/c, because . represents 'current folder')\n\n\nThus, this path can be simplified to \na/c\n.\n\n\nSo what's a File System?\n\n\nA filesystem is how information is organized on disk. Whenever you want to access a file, the filesystem dictates how the file is read. Here is a sample image of a filesystem.\n\n\n\n\nWhoa that's a lot let's break it down\n\n Superblock: This block contains metadata about the filesystem, how large, last modified time, a journal, number of inodes and the first inode start, number of data block and the first data block start.\n\n Inode: This is the the key abstraction. An inode is a file. \n* Disk Blocks: These are where the data is stored. The actual contents of the file\n\n\nHow does inode store the file contents?\n\n\n\n\nFrom \nWikipedia\n:\n\n\n\n\nIn a Unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be one of various things including a file or a directory. Each inode stores the attributes and disk block location(s) of the filesystem object's data. Filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).\n\n\n\n\nTo read the first few bytes of the file, follow the first indirect block pointer to the first indirect block and read the first few bytes, writing is the same process. If you want to read the entire file, keep reading direct blocks until your size runs out (we will talk about indirect blocks in a bit)\n\n\n\n\n\"All problems in computer science can be solved by another level of indirection.\" - David Wheeler\n\n\n\n\nWhy make disk blocks the same size as memory pages?\n\n\nTo support virtual memory, so we can page stuff in and out of memory.\n\n\nWhat information do we want to store for each file?\n\n\n\n\nFilename\n\n\nFile size\n\n\nTime created, last modified, last accessed\n\n\nPermissions\n\n\nFilepath\n\n\nChecksum\n\n\nFile data (inode)\n\n\n\n\nWhat are the traditional permissions: user \u2013 group \u2013 other permissions for a file?\n\n\nSome common file permissions include:\n* 755: \nrwx r-x r-x\n\n\nuser: \nrwx\n, group: \nr-x\n, others: \nr-x\n\n\nUser can read, write and execute. Group and others can only read and execute.\n* 644: \nrw- r-- r--\n\n\nuser: \nrw-\n, group: \nr--\n, others: \nr--\n\n\nUser can read and write. Group and others can only read.\n\n\nWhat are the the 3 permission bits for a regular file for each role?\n\n\n\n\nRead (most significant bit)  \n\n\nWrite (2nd bit)  \n\n\nExecute (least significant bit)\n\n\n\n\nWhat do \"644\" \"755\" mean?\n\n\nThese are examples of permissions in octal format (base 8). Each octal digit corresponds to a different role (user, group, world).\n\n\nWe can read permissions in octal format as follows:\n\n\n 644 - R/W user permissions, R group permissions, R world permissions\n\n\n 755 - R/W/X user permissions, R/X group permissions, R/X world permissions\n\n\nHow many pointers can you store in each indirection table?\n\n\nAs a worked example, suppose we divide the disk into 4KB blocks and we want to address up to 2^32 blocks.\n\n\nThe maximum disk size is 4KB *2^32 = 16TB  (remember 2^10 = 1024)\n\n\nA disk block can store 4KB / 4B (each pointer needs to be 32 bits) = 1024 pointers. Each pointer refers to a 4KB disk block - so you can refer up to 1024*4KB = 4MB of data\n\n\nFor the same disk configuration, a double indirect block stores 1024 pointers to 1024 indirection tables. Thus a double-indirect block can refer up to 1024 * 4MB = 4GB of data.\n\n\nSimilarly, a triple indirect block can refer up to 4TB of data.\n\n\nGo to File System: Part 2",
            "title": "File System, Part 1: Introduction"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#navigationterminology",
            "text": "",
            "title": "Navigation/Terminology"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#design-a-file-system-what-are-your-design-goals",
            "text": "The design of a file system is difficult problem because there many high-level design goals that we'd like to satisfy. An incomplete list of ideal goals include:   Reliable and robust (even with hardware failures or incomplete writes due to power loss)  Access (security) controls  Accounting and quotas  Indexing and search  Versioning and backup capabilities  Encryption  Automatic compression  High performance (e.g. Caching in-memory)  Efficient use of storage de-duplication   Not all filesystems natively support all of these goals. For example, many filesystems do not automatically compress rarely-used files",
            "title": "Design a file system! What are your design goals?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#what-are-and",
            "text": "In standard unix file systems:    .  represents the current directory    ..  represents the parent directory \n*  ...  is NOT a valid representation of any directory (this not the grandparent directory). It  could  however be the name of a file on disk.",
            "title": "What are ., .., and ...?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#what-are-absolute-and-relative-paths",
            "text": "Absolute paths are paths that start from the 'root node' of your directory tree. Relative paths are paths that start from your current position in the tree.",
            "title": "What are absolute and relative paths?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#what-are-some-examples-of-relative-and-absolute-paths",
            "text": "If you start in your home directory (\"~\" for short), then  Desktop/cs241  would be a relative path. Its absolute path counterpart might be something like  /Users/[yourname]/Desktop/cs241 .",
            "title": "What are some examples of relative and absolute paths?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#how-do-i-simplify-abc",
            "text": "Remember that  ..  means 'parent folder' and that  .  means 'current folder'.  Example:  a/b/../c/./ \n- Step 1:  cd a  (in a)\n- Step 2:  cd b  (in a/b)\n- Step 3:  cd ..  (in a, because .. represents 'parent folder')\n- Step 4:  cd c  (in a/c)\n- Step 5:  cd .  (in a/c, because . represents 'current folder')  Thus, this path can be simplified to  a/c .",
            "title": "How do I simplify a/b/../c/./?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#so-whats-a-file-system",
            "text": "A filesystem is how information is organized on disk. Whenever you want to access a file, the filesystem dictates how the file is read. Here is a sample image of a filesystem.   Whoa that's a lot let's break it down  Superblock: This block contains metadata about the filesystem, how large, last modified time, a journal, number of inodes and the first inode start, number of data block and the first data block start.  Inode: This is the the key abstraction. An inode is a file. \n* Disk Blocks: These are where the data is stored. The actual contents of the file",
            "title": "So what's a File System?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#how-does-inode-store-the-file-contents",
            "text": "From  Wikipedia :   In a Unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be one of various things including a file or a directory. Each inode stores the attributes and disk block location(s) of the filesystem object's data. Filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).   To read the first few bytes of the file, follow the first indirect block pointer to the first indirect block and read the first few bytes, writing is the same process. If you want to read the entire file, keep reading direct blocks until your size runs out (we will talk about indirect blocks in a bit)   \"All problems in computer science can be solved by another level of indirection.\" - David Wheeler",
            "title": "How does inode store the file contents?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#why-make-disk-blocks-the-same-size-as-memory-pages",
            "text": "To support virtual memory, so we can page stuff in and out of memory.",
            "title": "Why make disk blocks the same size as memory pages?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#what-information-do-we-want-to-store-for-each-file",
            "text": "Filename  File size  Time created, last modified, last accessed  Permissions  Filepath  Checksum  File data (inode)",
            "title": "What information do we want to store for each file?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#what-are-the-traditional-permissions-user-group-other-permissions-for-a-file",
            "text": "Some common file permissions include:\n* 755:  rwx r-x r-x  user:  rwx , group:  r-x , others:  r-x  User can read, write and execute. Group and others can only read and execute.\n* 644:  rw- r-- r--  user:  rw- , group:  r-- , others:  r--  User can read and write. Group and others can only read.",
            "title": "What are the traditional permissions: user \u2013 group \u2013 other permissions for a file?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#what-are-the-the-3-permission-bits-for-a-regular-file-for-each-role",
            "text": "Read (most significant bit)    Write (2nd bit)    Execute (least significant bit)",
            "title": "What are the the 3 permission bits for a regular file for each role?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#what-do-644-755-mean",
            "text": "These are examples of permissions in octal format (base 8). Each octal digit corresponds to a different role (user, group, world).  We can read permissions in octal format as follows:   644 - R/W user permissions, R group permissions, R world permissions   755 - R/W/X user permissions, R/X group permissions, R/X world permissions",
            "title": "What do \"644\" \"755\" mean?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-1:-Introduction/#how-many-pointers-can-you-store-in-each-indirection-table",
            "text": "As a worked example, suppose we divide the disk into 4KB blocks and we want to address up to 2^32 blocks.  The maximum disk size is 4KB *2^32 = 16TB  (remember 2^10 = 1024)  A disk block can store 4KB / 4B (each pointer needs to be 32 bits) = 1024 pointers. Each pointer refers to a 4KB disk block - so you can refer up to 1024*4KB = 4MB of data  For the same disk configuration, a double indirect block stores 1024 pointers to 1024 indirection tables. Thus a double-indirect block can refer up to 1024 * 4MB = 4GB of data.  Similarly, a triple indirect block can refer up to 4TB of data.  Go to File System: Part 2",
            "title": "How many pointers can you store in each indirection table?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/",
            "text": "Big idea: Forget names of files: The 'inode' is the file.  \n\n\nIt is common to think of the file name as the 'actual' file. It's not! Instead consider the inode as the file. The inode holds the meta-information (last accessed, ownership, size) and points to the disk blocks used to hold the file contents.\n\n\nSo... How do we implement a directory?\n\n\nA directory is just a mapping of names to inode numbers.\nPOSIX provides a small set of functions to read the filename and inode number for each entry (see below)\n\n\nLet's think about what it looks like in the actual file system. Theoretically, directories are just like actual files. The disk blocks will contain \ndirectory entries\n or \ndirents\n. What that means is that our disk block can look like this\n\n\n\n\n\n\n\n\ninode_num\n\n\nname\n\n\n\n\n\n\n\n\n\n\n2043567\n\n\nhi.txt\n\n\n\n\n\n\n...\n\n\n\n\n\n\n\n\n\n\nEach directory entry could either be a fixed size, or a variable c-string. It depends on how the particular filesystem implements it at the lower level.\n\n\nHow can I find the inode number of a file?\n\n\nFrom a shell, use \nls\n with the \n-i\n option\n\n\n$ ls -i\n12983989 dirlist.c      12984068 sandwich.c\n\n\n\n\nFrom C, call one of the stat functions (introduced below).\n\n\nHow do I find out meta-information about a file (or directory)?\n\n\nUse the stat calls. For example, to find out when my 'notes.txt' file was last accessed -\n\n\n   struct stat s;\n   stat(\"notes.txt\", & s);\n   printf(\"Last accessed %s\", ctime(s.st_atime));\n\n\n\n\nThere are actually three versions of \nstat\n;\n\n\n       int stat(const char *path, struct stat *buf);\n       int fstat(int fd, struct stat *buf);\n       int lstat(const char *path, struct stat *buf);\n\n\n\n\nFor example you can use \nfstat\n to find out the meta-information about a file if you already have an file descriptor associated with that file\n\n\n   FILE *file = fopen(\"notes.txt\", \"r\");\n   int fd = fileno(file); /* Just for fun - extract the file descriptor from a C FILE struct */\n   struct stat s;\n   fstat(fd, & s);\n   printf(\"Last accessed %s\", ctime(s.st_atime));\n\n\n\n\nThe third call 'lstat' we will discuss when we introduce symbolic links.\n\n\nIn addition to access,creation, and modified times, the stat structure includes the inode number, length of the file and owner information.\n\n\nstruct stat {\n               dev_t     st_dev;     /* ID of device containing file */\n               ino_t     st_ino;     /* inode number */\n               mode_t    st_mode;    /* protection */\n               nlink_t   st_nlink;   /* number of hard links */\n               uid_t     st_uid;     /* user ID of owner */\n               gid_t     st_gid;     /* group ID of owner */\n               dev_t     st_rdev;    /* device ID (if special file) */\n               off_t     st_size;    /* total size, in bytes */\n               blksize_t st_blksize; /* blocksize for file system I/O */\n               blkcnt_t  st_blocks;  /* number of 512B blocks allocated */\n               time_t    st_atime;   /* time of last access */\n               time_t    st_mtime;   /* time of last modification */\n               time_t    st_ctime;   /* time of last status change */\n           };\n\n\n\n\nHow do I list the contents of a directory ?\n\n\nLet's write our own version of 'ls' to list the contents of a directory.\n\n\n#include <stdio.h>\n#include <dirent.h>\n#include <stdlib.h>\nint main(int argc, char **argv) {\n    if(argc == 1) {\n        printf(\"Usage: %s [directory]\\n\", *argv);\n        exit(0);\n    }\n    struct dirent *dp;\n    DIR *dirp = opendir(argv[1]);\n    while ((dp = readdir(dirp)) != NULL) {\n        puts(dp->d_name);\n    }\n\n    closedir(dirp);\n    return 0;\n}\n\n\n\n\nHow do I read the contents of a directory?\n\n\nAns: Use opendir readdir closedir\nFor example, here's a very simple implementation of 'ls' to list the contents of a directory.\n\n\n#include <stdio.h>\n#include <dirent.h>\n#include <stdlib.h>\nint main(int argc, char **argv) {\n    if(argc ==1) {\n        printf(\"Usage: %s [directory]\\n\", *argv);\n        exit(0);\n    }\n    struct dirent *dp;\n    DIR *dirp = opendir(argv[1]);\n    while ((dp = readdir(dirp)) != NULL) {\n        printf(\"%s %lu\\n\", dp-> d_name, (unsigned long)dp-> d_ino );\n    }\n\n    closedir(dirp);\n    return 0;\n}\n\n\n\n\nNote: after a call to fork(), either (XOR) the parent or the child can use readdir(), rewinddir() or seekdir(). If both the parent and the child use the above, behavior is undefined.\n\n\nHow do I check to see if a file is in the current directory?\n\n\nFor example, to see if a particular directory includes a file (or filename) 'name', we might write the following code. (Hint: Can you spot the bug?)\n\n\nint exists(char *directory, char *name)  {\n    struct dirent *dp;\n    DIR *dirp = opendir(directory);\n    while ((dp = readdir(dirp)) != NULL) {\n        puts(dp->d_name);\n        if (!strcmp(dp->d_name, name)) {\n        return 1; /* Found */\n        }\n    }\n    closedir(dirp);\n    return 0; /* Not Found */\n}\n\n\n\n\nThe above code has a subtle bug: It leaks resources! If a matching filename is found then 'closedir' is never called as part of the early return. Any file descriptors opened, and any memory allocated, by opendir are never released. This means eventually the process will run out of resources and an \nopen\n or \nopendir\n call will fail.\n\n\nThe fix is to ensure we free up resources in every possible code-path. In the above code this means calling \nclosedir\n before \nreturn 1\n. Forgetting to release resources is a common C programming bug because there is no support in the C lanaguage to ensure resources are always released with all codepaths.\n\n\nWhat are the gotcha's of using readdir? For example to recursively search directories?\n\n\nThere are two main gotchas and one consideration:\nThe \nreaddir\n function returns \".\" (current directory) and \"..\" (parent directory). If you are looking for sub-directories, you need to explicitly exclude these directories.\n\n\nFor many applications it's reasonable to check the current directory first before recursively searching sub-directories. This can be achieved by storing the results in a linked list, or resetting the directory struct to restart from the beginning.\n\n\nOne final note of caution: \nreaddir\n is not thread-safe! For multi-threaded searches use \nreaddir_r\n which requires the caller to pass in the address of an existing dirent struct.\n\n\nSee the man page of readdir for more details.\n\n\nHow do I determine if a directory entry is a directory?\n\n\nAns: Use \nS_ISDIR\n to check the mode bits stored in the stat structure\n\n\nAnd to check if a file is regular file use \nS_ISREG\n,\n\n\n   struct stat s;\n   if (0 == stat(name, &s)) {\n      printf(\"%s \", name);\n      if (S_ISDIR( s.st_mode)) puts(\"is a directory\");\n      if (S_ISREG( s.st_mode)) puts(\"is a regular file\");\n   } else {\n      perror(\"stat failed - are you sure I can read this file's meta data?\");\n   }\n\n\n\n\nDoes a directory have an inode too?\n\n\nYes! Though a better way to think about this, is that a directory (like a file) \nis\n an inode (with some data - the directory name and inode contents). It just happens to be a special kind of inode.\n\n\nFrom \nWikipedia\n:\n\n\n\n\nUnix directories are lists of association structures, each of which contains one filename and one inode number.\n\n\n\n\nRemember, inodes don't contain filenames--only other file metadata.\n\n\nHow can I have the same file appear in two different places in my file system?\n\n\nFirst remember that a file name != the file. Think of the inode as 'the file' and a directory as just a list of names with each name mapped to an inode number. Some of those inodes may be regular file inodes, others may be directory inodes.\n\n\nIf we already have a file on a file system we can create another link to the same inode using the 'ln' command\n\n\n$ ln file1.txt blip.txt\n\n\n\n\nHowever blip.txt \nis\n the same file; if I edit blip I'm editing the same file as 'file1.txt!'\nWe can prove this by showing that both file names refer to the same inode:\n\n\n$ ls -i file1.txt blip.txt\n134235 file1.txt\n134235 blip.txt\n\n\n\n\nThese kinds of links (aka directory entries) are called 'hard links'\n\n\nThe equivalent C call is \nlink\n\n\nlink(const char *path1, const char *path2);\n\nlink(\"file1.txt\", \"blip.txt\");\n\n\n\n\nFor simplicity the above examples made hard links inside the same directory however hard links can be created anywhere inside the same filesystem.\n\n\nWhat happens when I \nrm\n (remove) a file?\n\n\nWhen you remove a file (using \nrm\n or \nunlink\n) you are removing an inode reference from a directory.\nHowever the inode may still be referenced from other directories. In order to determine if the contents of the file are still required, each inode keeps a reference count that is updated whenever a new link is created or destroyed.\n\n\nCase study: Back up software that minimizes file duplication\n\n\nAn example use of hard-links is to efficiently create multiple archives of a file system at different points in time. Once the archive area has a copy of a particular file, then future archives can re-use these archive files rather than creating a duplicate file. Apple's \"Time Machine\" software does this.\n\n\nCan I create hard links to directories as well as regular files?\n\n\nNo. Well yes. Not really... Actually you didn't really want to do this, did you?\nThe POSIX standard says no you may not! The \nln\n command will only allow root to do this and only if you provide the \n-d\n option. However even root may not be able to perform this because most filesystems prevent it! \n\n\nWhy?\nThe integrity of the file system assumes the directory structure (excluding softlinks which we will talk about later) is a non-cyclic tree that is reachable from the root directory. It becomes expensive to enforce or verify this constraint if directory linking is allowed. Breaking these assumptions can cause file integrity tools to not be able to repair the file system. Recursive searches potentially never terminate and directories can have more than one parent but \"..\" can only refer to a single parent. All in all, a bad idea.\n\n\n\n\n\n\n\nBack: File System, Part 1\n\n |\n\n\nNext: File System, Part 3",
            "title": "File System, Part 2: Files are inodes (everything else is just data...)"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#so-how-do-we-implement-a-directory",
            "text": "A directory is just a mapping of names to inode numbers.\nPOSIX provides a small set of functions to read the filename and inode number for each entry (see below)  Let's think about what it looks like in the actual file system. Theoretically, directories are just like actual files. The disk blocks will contain  directory entries  or  dirents . What that means is that our disk block can look like this     inode_num  name      2043567  hi.txt    ...      Each directory entry could either be a fixed size, or a variable c-string. It depends on how the particular filesystem implements it at the lower level.",
            "title": "So... How do we implement a directory?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-can-i-find-the-inode-number-of-a-file",
            "text": "From a shell, use  ls  with the  -i  option  $ ls -i\n12983989 dirlist.c      12984068 sandwich.c  From C, call one of the stat functions (introduced below).",
            "title": "How can I find the inode number of a file?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-find-out-meta-information-about-a-file-or-directory",
            "text": "Use the stat calls. For example, to find out when my 'notes.txt' file was last accessed -     struct stat s;\n   stat(\"notes.txt\", & s);\n   printf(\"Last accessed %s\", ctime(s.st_atime));  There are actually three versions of  stat ;         int stat(const char *path, struct stat *buf);\n       int fstat(int fd, struct stat *buf);\n       int lstat(const char *path, struct stat *buf);  For example you can use  fstat  to find out the meta-information about a file if you already have an file descriptor associated with that file     FILE *file = fopen(\"notes.txt\", \"r\");\n   int fd = fileno(file); /* Just for fun - extract the file descriptor from a C FILE struct */\n   struct stat s;\n   fstat(fd, & s);\n   printf(\"Last accessed %s\", ctime(s.st_atime));  The third call 'lstat' we will discuss when we introduce symbolic links.  In addition to access,creation, and modified times, the stat structure includes the inode number, length of the file and owner information.  struct stat {\n               dev_t     st_dev;     /* ID of device containing file */\n               ino_t     st_ino;     /* inode number */\n               mode_t    st_mode;    /* protection */\n               nlink_t   st_nlink;   /* number of hard links */\n               uid_t     st_uid;     /* user ID of owner */\n               gid_t     st_gid;     /* group ID of owner */\n               dev_t     st_rdev;    /* device ID (if special file) */\n               off_t     st_size;    /* total size, in bytes */\n               blksize_t st_blksize; /* blocksize for file system I/O */\n               blkcnt_t  st_blocks;  /* number of 512B blocks allocated */\n               time_t    st_atime;   /* time of last access */\n               time_t    st_mtime;   /* time of last modification */\n               time_t    st_ctime;   /* time of last status change */\n           };",
            "title": "How do I find out meta-information about a file (or directory)?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-list-the-contents-of-a-directory",
            "text": "Let's write our own version of 'ls' to list the contents of a directory.  #include <stdio.h>\n#include <dirent.h>\n#include <stdlib.h>\nint main(int argc, char **argv) {\n    if(argc == 1) {\n        printf(\"Usage: %s [directory]\\n\", *argv);\n        exit(0);\n    }\n    struct dirent *dp;\n    DIR *dirp = opendir(argv[1]);\n    while ((dp = readdir(dirp)) != NULL) {\n        puts(dp->d_name);\n    }\n\n    closedir(dirp);\n    return 0;\n}",
            "title": "How do I list the contents of a directory ?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-read-the-contents-of-a-directory",
            "text": "Ans: Use opendir readdir closedir\nFor example, here's a very simple implementation of 'ls' to list the contents of a directory.  #include <stdio.h>\n#include <dirent.h>\n#include <stdlib.h>\nint main(int argc, char **argv) {\n    if(argc ==1) {\n        printf(\"Usage: %s [directory]\\n\", *argv);\n        exit(0);\n    }\n    struct dirent *dp;\n    DIR *dirp = opendir(argv[1]);\n    while ((dp = readdir(dirp)) != NULL) {\n        printf(\"%s %lu\\n\", dp-> d_name, (unsigned long)dp-> d_ino );\n    }\n\n    closedir(dirp);\n    return 0;\n}  Note: after a call to fork(), either (XOR) the parent or the child can use readdir(), rewinddir() or seekdir(). If both the parent and the child use the above, behavior is undefined.",
            "title": "How do I read the contents of a directory?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-check-to-see-if-a-file-is-in-the-current-directory",
            "text": "For example, to see if a particular directory includes a file (or filename) 'name', we might write the following code. (Hint: Can you spot the bug?)  int exists(char *directory, char *name)  {\n    struct dirent *dp;\n    DIR *dirp = opendir(directory);\n    while ((dp = readdir(dirp)) != NULL) {\n        puts(dp->d_name);\n        if (!strcmp(dp->d_name, name)) {\n        return 1; /* Found */\n        }\n    }\n    closedir(dirp);\n    return 0; /* Not Found */\n}  The above code has a subtle bug: It leaks resources! If a matching filename is found then 'closedir' is never called as part of the early return. Any file descriptors opened, and any memory allocated, by opendir are never released. This means eventually the process will run out of resources and an  open  or  opendir  call will fail.  The fix is to ensure we free up resources in every possible code-path. In the above code this means calling  closedir  before  return 1 . Forgetting to release resources is a common C programming bug because there is no support in the C lanaguage to ensure resources are always released with all codepaths.",
            "title": "How do I check to see if a file is in the current directory?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#what-are-the-gotchas-of-using-readdir-for-example-to-recursively-search-directories",
            "text": "There are two main gotchas and one consideration:\nThe  readdir  function returns \".\" (current directory) and \"..\" (parent directory). If you are looking for sub-directories, you need to explicitly exclude these directories.  For many applications it's reasonable to check the current directory first before recursively searching sub-directories. This can be achieved by storing the results in a linked list, or resetting the directory struct to restart from the beginning.  One final note of caution:  readdir  is not thread-safe! For multi-threaded searches use  readdir_r  which requires the caller to pass in the address of an existing dirent struct.  See the man page of readdir for more details.",
            "title": "What are the gotcha's of using readdir? For example to recursively search directories?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-determine-if-a-directory-entry-is-a-directory",
            "text": "Ans: Use  S_ISDIR  to check the mode bits stored in the stat structure  And to check if a file is regular file use  S_ISREG ,     struct stat s;\n   if (0 == stat(name, &s)) {\n      printf(\"%s \", name);\n      if (S_ISDIR( s.st_mode)) puts(\"is a directory\");\n      if (S_ISREG( s.st_mode)) puts(\"is a regular file\");\n   } else {\n      perror(\"stat failed - are you sure I can read this file's meta data?\");\n   }",
            "title": "How do I determine if a directory entry is a directory?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#does-a-directory-have-an-inode-too",
            "text": "Yes! Though a better way to think about this, is that a directory (like a file)  is  an inode (with some data - the directory name and inode contents). It just happens to be a special kind of inode.  From  Wikipedia :   Unix directories are lists of association structures, each of which contains one filename and one inode number.   Remember, inodes don't contain filenames--only other file metadata.",
            "title": "Does a directory have an inode too?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-can-i-have-the-same-file-appear-in-two-different-places-in-my-file-system",
            "text": "First remember that a file name != the file. Think of the inode as 'the file' and a directory as just a list of names with each name mapped to an inode number. Some of those inodes may be regular file inodes, others may be directory inodes.  If we already have a file on a file system we can create another link to the same inode using the 'ln' command  $ ln file1.txt blip.txt  However blip.txt  is  the same file; if I edit blip I'm editing the same file as 'file1.txt!'\nWe can prove this by showing that both file names refer to the same inode:  $ ls -i file1.txt blip.txt\n134235 file1.txt\n134235 blip.txt  These kinds of links (aka directory entries) are called 'hard links'  The equivalent C call is  link  link(const char *path1, const char *path2);\n\nlink(\"file1.txt\", \"blip.txt\");  For simplicity the above examples made hard links inside the same directory however hard links can be created anywhere inside the same filesystem.",
            "title": "How can I have the same file appear in two different places in my file system?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#what-happens-when-i-rm-remove-a-file",
            "text": "When you remove a file (using  rm  or  unlink ) you are removing an inode reference from a directory.\nHowever the inode may still be referenced from other directories. In order to determine if the contents of the file are still required, each inode keeps a reference count that is updated whenever a new link is created or destroyed.",
            "title": "What happens when I rm (remove) a file?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#case-study-back-up-software-that-minimizes-file-duplication",
            "text": "An example use of hard-links is to efficiently create multiple archives of a file system at different points in time. Once the archive area has a copy of a particular file, then future archives can re-use these archive files rather than creating a duplicate file. Apple's \"Time Machine\" software does this.",
            "title": "Case study: Back up software that minimizes file duplication"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#can-i-create-hard-links-to-directories-as-well-as-regular-files",
            "text": "No. Well yes. Not really... Actually you didn't really want to do this, did you?\nThe POSIX standard says no you may not! The  ln  command will only allow root to do this and only if you provide the  -d  option. However even root may not be able to perform this because most filesystems prevent it!   Why?\nThe integrity of the file system assumes the directory structure (excluding softlinks which we will talk about later) is a non-cyclic tree that is reachable from the root directory. It becomes expensive to enforce or verify this constraint if directory linking is allowed. Breaking these assumptions can cause file integrity tools to not be able to repair the file system. Recursive searches potentially never terminate and directories can have more than one parent but \"..\" can only refer to a single parent. All in all, a bad idea.    \nBack: File System, Part 1  | \nNext: File System, Part 3",
            "title": "Can I create hard links to directories as well as regular files?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/",
            "text": "Remind Me What do Permissions mean again?\n\n\nEvery file and directory has a set of 9 permission bits and a type field\n\n r, permission to read the file\n\n w, permission to write to the file\n* x, permission to execute the file\n\n\nchmod 777 \n\n\n\n\n\n\n\n\nchmod\n\n\n7\n\n\n7\n\n\n7\n\n\n\n\n\n\n\n\n\n\n01\n\n\n111\n\n\n111\n\n\n111\n\n\n\n\n\n\nd\n\n\nrwx\n\n\nrwx\n\n\nrwx\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\n\n\n\n\nType of the file\n\n\nOwner Permissions\n\n\nGroup Permissions\n\n\nEverybody else's permission\n\n\n\n\nmknod\n changes the first field, the type of the file.\n\nchmod\n takes a number and a file and changes the permission bits.\n\n\nThe file has an owner. If your process has the same user id as the owner (or root) then the permissions in the first triad apply to you. If you are in the same group as the file (all files are also owned by a group) then the next set of permission bits applies to you. If none of the above apply, the last triad applies to you.\n\n\nHow do I change the permissions on a file?\n\n\nUse \nchmod\n  (short for \"change the file mode bits\")\n\n\nThere is a system call, \nint chmod(const char *path, mode_t mode);\n but we will concentrate on the shell command. There's two common ways to use \nchmod\n ; with an octal value or with a symbolic string:\n\n\n$ chmod 644 file1\n$ chmod 755 file2\n$ chmod 700 file3\n$ chmod ugo-w file4\n$ chmod o-rx file4\n\n\n\n\nThe base-8 ('octal') digits describe the permissions for each role: The user who owns the file, the group and everyone else. The octal number is the sum of three values given to the three types of permission: read(4), write(2), execute(1)\n\n\nExample: chmod 755 myfile\n\n r + w + x = digit\n\n user has 4+2+1, full permission\n\n group has 4+0+1, read and execute permission\n\n all users have 4+0+1, read and execute permission\n\n\nHow do I read the permission string from ls?\n\n\nUse `ls -l'. \nNote that the permissions will output in the format 'drwxrwxrwx'. The first character indicates the type of file type. \nPossible values for the first character:\n\n (-) regular file\n\n (d) directory\n\n (c) character device file\\\n\n (l) symbolic link\n\n (p) pipe\n\n (b) block device\n* (s) socket\n\n\nWhat is sudo?\n\n\nUse \nsudo\n to become the admin on the machine.\ne.g. Normally (unless explicitly specified in the '/etc/fstab' file, you need root access to mount a filesystem). \nsudo\n can be used to temporarily run a command as root (provided the user has sudo privileges)\n\n\n$ sudo mount /dev/sda2 /stuff/mydisk\n$ sudo adduser fred\n\n\n\n\nHow do I change ownership of a file?\n\n\nUse \nchown username filename\n\n\nHow do I set permissions from code?\n\n\nchmod(const char *path, mode_t mode);\n\n\nWhy are some files 'setuid'? What does this mean?\n\n\nThe set-user-ID-on-execution bit changes the user associated with the process when the file is run. This is typically used for commands that need to run as root but are executed by non-root users. An example of this is \nsudo\n\n\nThe set-group-ID-on-execution changes the group under which the process is run.\n\n\nWhy are they useful?\n\n\nThe most common usecase is so that the user can have root(admin) access for the duration of the program.\n\n\nWhat permissions does sudo run as ?\n\n\n$ ls -l /usr/bin/sudo\n-r-s--x--x  1 root  wheel  327920 Oct 24 09:04 /usr/bin/sudo\n\n\n\n\nThe 's' bit means execute and set-uid; the effective userid of the process will be different from the parent process. In this example it will be root\n\n\nWhat's the difference between getuid() and geteuid()?\n\n\n\n\ngetuid\n returns the real user id (zero if logged in as root)\n\n\ngeteuid\n returns the effective userid (zero if acting as root, e.g. due to the setuid flag set on a program)\n\n\n\n\nHow do I ensure only privileged users can run my code?\n\n\n\n\nCheck the effective permissions of the user by calling \ngeteuid()\n. A return value of zero means the program is running effectively as root.\n\n\n\n\nGo to File System: Part 4",
            "title": "File System, Part 3: Permissions"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#remind-me-what-do-permissions-mean-again",
            "text": "Every file and directory has a set of 9 permission bits and a type field  r, permission to read the file  w, permission to write to the file\n* x, permission to execute the file  chmod 777      chmod  7  7  7      01  111  111  111    d  rwx  rwx  rwx    1  2  3  4      Type of the file  Owner Permissions  Group Permissions  Everybody else's permission   mknod  changes the first field, the type of the file. chmod  takes a number and a file and changes the permission bits.  The file has an owner. If your process has the same user id as the owner (or root) then the permissions in the first triad apply to you. If you are in the same group as the file (all files are also owned by a group) then the next set of permission bits applies to you. If none of the above apply, the last triad applies to you.",
            "title": "Remind Me What do Permissions mean again?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#how-do-i-change-the-permissions-on-a-file",
            "text": "Use  chmod   (short for \"change the file mode bits\")  There is a system call,  int chmod(const char *path, mode_t mode);  but we will concentrate on the shell command. There's two common ways to use  chmod  ; with an octal value or with a symbolic string:  $ chmod 644 file1\n$ chmod 755 file2\n$ chmod 700 file3\n$ chmod ugo-w file4\n$ chmod o-rx file4  The base-8 ('octal') digits describe the permissions for each role: The user who owns the file, the group and everyone else. The octal number is the sum of three values given to the three types of permission: read(4), write(2), execute(1)  Example: chmod 755 myfile  r + w + x = digit  user has 4+2+1, full permission  group has 4+0+1, read and execute permission  all users have 4+0+1, read and execute permission",
            "title": "How do I change the permissions on a file?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#how-do-i-read-the-permission-string-from-ls",
            "text": "Use `ls -l'. \nNote that the permissions will output in the format 'drwxrwxrwx'. The first character indicates the type of file type. \nPossible values for the first character:  (-) regular file  (d) directory  (c) character device file\\  (l) symbolic link  (p) pipe  (b) block device\n* (s) socket",
            "title": "How do I read the permission string from ls?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#what-is-sudo",
            "text": "Use  sudo  to become the admin on the machine.\ne.g. Normally (unless explicitly specified in the '/etc/fstab' file, you need root access to mount a filesystem).  sudo  can be used to temporarily run a command as root (provided the user has sudo privileges)  $ sudo mount /dev/sda2 /stuff/mydisk\n$ sudo adduser fred",
            "title": "What is sudo?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#how-do-i-change-ownership-of-a-file",
            "text": "Use  chown username filename",
            "title": "How do I change ownership of a file?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#how-do-i-set-permissions-from-code",
            "text": "chmod(const char *path, mode_t mode);",
            "title": "How do I set permissions from code?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#why-are-some-files-setuid-what-does-this-mean",
            "text": "The set-user-ID-on-execution bit changes the user associated with the process when the file is run. This is typically used for commands that need to run as root but are executed by non-root users. An example of this is  sudo  The set-group-ID-on-execution changes the group under which the process is run.",
            "title": "Why are some files 'setuid'? What does this mean?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#why-are-they-useful",
            "text": "The most common usecase is so that the user can have root(admin) access for the duration of the program.",
            "title": "Why are they useful?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#what-permissions-does-sudo-run-as",
            "text": "$ ls -l /usr/bin/sudo\n-r-s--x--x  1 root  wheel  327920 Oct 24 09:04 /usr/bin/sudo  The 's' bit means execute and set-uid; the effective userid of the process will be different from the parent process. In this example it will be root",
            "title": "What permissions does sudo run as ?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#whats-the-difference-between-getuid-and-geteuid",
            "text": "getuid  returns the real user id (zero if logged in as root)  geteuid  returns the effective userid (zero if acting as root, e.g. due to the setuid flag set on a program)",
            "title": "What's the difference between getuid() and geteuid()?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-3:-Permissions/#how-do-i-ensure-only-privileged-users-can-run-my-code",
            "text": "Check the effective permissions of the user by calling  geteuid() . A return value of zero means the program is running effectively as root.   Go to File System: Part 4",
            "title": "How do I ensure only privileged users can run my code?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/",
            "text": "How do I find out if file (an inode) is a regular file or directory?\n\n\nUse the \nS_ISDIR\n macro to check the mode bits in the stat struct:\n\n\nstruct stat s;\nstat(\"/tmp\", &s);\nif (S_ISDIR(s.st_mode)) { ... \n\n\n\n\nNote, later we will write robust code to verify that the stat call succeeds (returns 0); if the \nstat\n call fails, we should assume the stat struct content is arbitrary.\n\n\nHow do I recurse into subdirectories?\n\n\nFirst a puzzle - how many bugs can you find in the following code?\n\n\nvoid dirlist(char *path) {\n\n  struct dirent *dp;\n  DIR *dirp = opendir(path);\n  while ((dp = readdir(dirp)) != NULL) {\n     char newpath[strlen(path) + strlen(dp->d_name) + 1];\n     sprintf(newpath,\"%s/%s\", newpath, dp->d_name);\n     printf(\"%s\\n\", dp->d_name);\n     dirlist(newpath);\n  }\n}\n\nint main(int argc, char **argv) { dirlist(argv[1]); return 0; }\n\n\n\n\nDid you find all 5 bugs?\n\n\n// Check opendir result (perhaps user gave us a path that can not be opened as a directory\nif (!dirp) { perror(\"Could not open directory\"); return; }\n// +2 as we need space for the / and the terminating 0\nchar newpath[strlen(path) + strlen(dp->d_name) + 2]; \n// Correct parameter\nsprintf(newpath,\"%s/%s\", path, dp->d_name); \n// Perform stat test (and verify) before recursing\nif (0 == stat(newpath,&s) && S_ISDIR(s.st_mode)) dirlist(newpath)\n// Resource leak: the directory file handle is not closed after the while loop\nclosedir(dirp);\n\n\n\n\nWhat are symbolic links? How do they work? How do I make one?\n\n\nsymlink(const char *target, const char *symlink);\n\n\n\n\nTo create a symbolic link in the shell use \nln -s\n\n\nTo read the contents of the link as just a file use \nreadlink\n\n\n$ readlink myfile.txt\n../../dir1/notes.txt\n\n\n\n\nTo read the meta-(stat) information of a symbolic link use \nlstat\n not \nstat\n\n\nstruct stat s1, s2;\nstat(\"myfile.txt\", &s1); // stat info about  the notes.txt file\nlstat(\"myfile.txt\", &s2); // stat info about the symbolic link\n\n\n\n\nAdvantages of symbolic links\n\n\n\n\nCan refer to files that don't exist yet\n\n\nUnlike hard links, can refer to directories as well as regular files\n\n\nCan refer to files (and directories) that exist outside of the current file system\n\n\n\n\nMain disadvantage: Slower than regular files and directories. When the links contents are read, they must be interpreted as a new path to the target file.\n\n\nWhat is \n/dev/null\n and when is it used?\n\n\nThe file \n/dev/null\n is a great place to store bits that you never need to read!\nBytes sent to \n/dev/null/\n are never stored - they are simply discarded. A common use of \n/dev/null\n is to discard standard output. For example,\n\n\n$ ls . >/dev/null\n\n\n\n\nWhy would I want to set a directory's sticky bit?\n\n\nWhen a directory's sticky bit is set only the file's owner, the directory's owner, and the root user can rename (or delete) the file. This is useful when multiple users have write access to a common directory.\n\n\nA common use of the sticky bit is for the shared and writable \n/tmp\n directory.\n\n\nWhy do shell and script programs start with \n#!/usr/bin/env python\n ?\n\n\nAns: For portability!\nWhile it is possible to write the fully qualified path to a python or perl interpreter, this approach is not portable because you may have installed python in a different directory.\n\n\nTo overcome this use the \nenv\n utility is used to find and execute the program on the user's path.\nThe env utility itself has historically been stored in \n/usr/bin\n - and it must be specified with an absolute path.\n\n\nHow do I make 'hidden' files i.e. not listed by \"ls\"? How do I list them?\n\n\nEasy! Create files (or directories) that start with a \".\" - then (by default) they are not displayed by standard tools and utilities.\n\n\nThis is often used to hide configuration files inside the user's home directory.\nFor example \nssh\n stores its preferences inside a directory called \n.sshd\n\n\nTo list all files including the normally hidden entries use \nls\n with  \n-a\n option \n\n\n$ ls -a\n.           a.c         myls\n..          a.out           other.txt\n.secret \n\n\n\n\nWhat happens if I turn off the execute bit on directories?\n\n\nThe execute bit for a directory is used to control whether the directory contents is listable.\n\n\n$ chmod ugo-x dir1\n$ ls -l\ndrw-r--r--   3 angrave  staff   102 Nov 10 11:22 dir1\n\n\n\n\nHowever when attempting to list the contents of the directory,\n\n\n$ ls dir1\nls: dir1: Permission denied\n\n\n\n\nIn other words, the directory itself is discoverable but its contents cannot be listed.\n\n\nWhat is file globbing (and who does it)?\n\n\nBefore executing the program the shell expands parameters into matching filenames. For example, if the current directory has three filenames that start with my ( my1.txt mytext.txt myomy), then\n\n\n$ echo my*\n\n\n\n\nExpands to \n\n\n$ echo my1.txt mytext.txt myomy\n\n\n\n\nThis is known as file globbing and is processed before the command is executed.\nie the command's parameters are identical to manually typing every matching filename.\n\n\nCreating secure directories\n\n\nSuppose you created your own directory in /tmp and then set the permissions so that only you can use the directory (see below). Is this secure? \n\n\n$ mkdir /tmp/mystuff\n$ chmod 700 /tmp/mystuff\n\n\n\n\nThere is a window of opportunity between when the directory is created and when it's permissions are changed. This leads to several vulnerabilities that are based on a race condition (where an attacker modifies the directory in some way before the privileges are removed). Some examples include:\n\n\nAnother user replaces \nmystuff\n with a hardlink to an existing file or directory owned by the second user, then they would be able to read and control the contents of the \nmystuff\n directory. Oh no - our secrets are no longer secret!\n\n\nHowever in this specific example the \n/tmp\n directory has the sticky bit set, so other users may not delete the \nmystuff\n directory, and the simple attack scenario described above is impossible. This does not mean that creating the directory and then later making the directory private is secure! A better version is to atomically create the directory with the correct permissions from its inception - \n\n\n$ mkdir -m 700 /tmp/mystuff\n\n\n\n\nHow do I automatically create parent directories?\n\n\n$ mkdir -p d1/d2/d3\n\n\n\n\nWill automatically create d1 and d2 if they don't exist.\n\n\nMy default umask 022; what does this mean?\n\n\nThe umask \nsubtracts\n (reduces) permission bits from 777 and is used when new files and new directories are created by open,mkdir etc. Thus \n022\n (octal) means that group and other privileges will not include the writable bit . Each process (including the shell) has a current umask value. When forking, the child inherits the parent's umask value.\n\n\nFor example, by setting the umask to 077 in the shell, ensures that future file and directory creation will only be accessible to the current user,\n\n\n$ umask 077\n$ mkdir secretdir\n\n\n\n\nAs a code example, suppose a new file is created with \nopen()\n and mode bits \n666\n (write and read bits for user,group and other):\n\n\nopen(\"myfile\", O_CREAT, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);\n\n\n\n\nIf umask is octal 022, then the permissions of the created file will be 0666 & ~022\nie.\n\n\n           S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH\n\n\n\n\nHow can I copy bytes from one file to another?\n\n\nUse  the versatile \ndd\n command. For example, the following command copies 1 MB of data from the file \n/dev/urandom\n to the file \n/dev/null\n. The data is copied as 1024 blocks of blocksize 1024 bytes.\n\n\n$ dd if=/dev/urandom of=/dev/null bs=1k count=1024\n\n\n\n\nBoth the input and output files in the example above are virtual - they don't exist on a disk. This means the speed of the transfer is unaffected by hardware power. Instead they are part of the \ndev\n filesystem, which is virtual filesystem provided by the kernel.\nThe virtual file \n/dev/urandom\n provides an infinite stream of random bytes, while the virtal file \n/dev/null\n ignores all bytes written to it. A common use of \n/dev/null\n is to discard the output of a command,\n\n\n$ myverboseexecutable > /dev/null\n\n\n\n\nAnother commonly used /dev virtual file is \n/dev/zero\n which provides an infinite stream of zero bytes.\nFor example, we can benchmark the operating system performance of reading stream zero bytes in the kernel into a process memory and writing the bytes back to the kernel without any disk I/O. Note the throughput (~20GB/s) is strongly dependent on blocksize. For small block sizes the overhead of additional \nread\n and \nwrite\n system calls will  dominate.\n\n\n$ dd if=/dev/zero of=/dev/null bs=1M count=1024\n1024+0 records in\n1024+0 records out\n1073741824 bytes (1.1 GB) copied, 0.0539153 s, 19.9 GB/s\n\n\n\n\nWhat happens when I touch a file?\n\n\nThe \ntouch\n executable creates file if it does not exist and also updates the file's last modified time to be the current time. For example, we can make a new private file with the current time:\n\n\n$ umask 077       # all future new files will maskout all r,w,x bits for group and other access\n$ touch file123   # create a file if it does not exist, and update its modified time\n$ stat file123\n  File: `file123'\n  Size: 0           Blocks: 0          IO Block: 65536  regular empty file\nDevice: 21h/33d Inode: 226148      Links: 1\nAccess: (0600/-rw-------)  Uid: (395606/ angrave)   Gid: (61019/     ews)\nAccess: 2014-11-12 13:42:06.000000000 -0600\nModify: 2014-11-12 13:42:06.001787000 -0600\nChange: 2014-11-12 13:42:06.001787000 -0600\n\n\n\n\nAn example use of touch is to force make to recompile a file that is unchanged after modifying the compiler options inside the makefile. Remeber that make is 'lazy' - it will compare the modified time of the source file with the corresponding output file to see if the file needs to be recompiled\n\n\n$ touch myprogram.c   # force my source file to be recompiled\n$ make\n\n\n\n\nGo to File System: Part 5",
            "title": "File System, Part 4: Working with directories"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#how-do-i-find-out-if-file-an-inode-is-a-regular-file-or-directory",
            "text": "Use the  S_ISDIR  macro to check the mode bits in the stat struct:  struct stat s;\nstat(\"/tmp\", &s);\nif (S_ISDIR(s.st_mode)) { ...   Note, later we will write robust code to verify that the stat call succeeds (returns 0); if the  stat  call fails, we should assume the stat struct content is arbitrary.",
            "title": "How do I find out if file (an inode) is a regular file or directory?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#how-do-i-recurse-into-subdirectories",
            "text": "First a puzzle - how many bugs can you find in the following code?  void dirlist(char *path) {\n\n  struct dirent *dp;\n  DIR *dirp = opendir(path);\n  while ((dp = readdir(dirp)) != NULL) {\n     char newpath[strlen(path) + strlen(dp->d_name) + 1];\n     sprintf(newpath,\"%s/%s\", newpath, dp->d_name);\n     printf(\"%s\\n\", dp->d_name);\n     dirlist(newpath);\n  }\n}\n\nint main(int argc, char **argv) { dirlist(argv[1]); return 0; }  Did you find all 5 bugs?  // Check opendir result (perhaps user gave us a path that can not be opened as a directory\nif (!dirp) { perror(\"Could not open directory\"); return; }\n// +2 as we need space for the / and the terminating 0\nchar newpath[strlen(path) + strlen(dp->d_name) + 2]; \n// Correct parameter\nsprintf(newpath,\"%s/%s\", path, dp->d_name); \n// Perform stat test (and verify) before recursing\nif (0 == stat(newpath,&s) && S_ISDIR(s.st_mode)) dirlist(newpath)\n// Resource leak: the directory file handle is not closed after the while loop\nclosedir(dirp);",
            "title": "How do I recurse into subdirectories?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#what-are-symbolic-links-how-do-they-work-how-do-i-make-one",
            "text": "symlink(const char *target, const char *symlink);  To create a symbolic link in the shell use  ln -s  To read the contents of the link as just a file use  readlink  $ readlink myfile.txt\n../../dir1/notes.txt  To read the meta-(stat) information of a symbolic link use  lstat  not  stat  struct stat s1, s2;\nstat(\"myfile.txt\", &s1); // stat info about  the notes.txt file\nlstat(\"myfile.txt\", &s2); // stat info about the symbolic link",
            "title": "What are symbolic links? How do they work? How do I make one?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#advantages-of-symbolic-links",
            "text": "Can refer to files that don't exist yet  Unlike hard links, can refer to directories as well as regular files  Can refer to files (and directories) that exist outside of the current file system   Main disadvantage: Slower than regular files and directories. When the links contents are read, they must be interpreted as a new path to the target file.",
            "title": "Advantages of symbolic links"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#what-is-devnull-and-when-is-it-used",
            "text": "The file  /dev/null  is a great place to store bits that you never need to read!\nBytes sent to  /dev/null/  are never stored - they are simply discarded. A common use of  /dev/null  is to discard standard output. For example,  $ ls . >/dev/null",
            "title": "What is /dev/null and when is it used?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#why-would-i-want-to-set-a-directorys-sticky-bit",
            "text": "When a directory's sticky bit is set only the file's owner, the directory's owner, and the root user can rename (or delete) the file. This is useful when multiple users have write access to a common directory.  A common use of the sticky bit is for the shared and writable  /tmp  directory.",
            "title": "Why would I want to set a directory's sticky bit?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#why-do-shell-and-script-programs-start-with-usrbinenv-python",
            "text": "Ans: For portability!\nWhile it is possible to write the fully qualified path to a python or perl interpreter, this approach is not portable because you may have installed python in a different directory.  To overcome this use the  env  utility is used to find and execute the program on the user's path.\nThe env utility itself has historically been stored in  /usr/bin  - and it must be specified with an absolute path.",
            "title": "Why do shell and script programs start with #!/usr/bin/env python ?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#how-do-i-make-hidden-files-ie-not-listed-by-ls-how-do-i-list-them",
            "text": "Easy! Create files (or directories) that start with a \".\" - then (by default) they are not displayed by standard tools and utilities.  This is often used to hide configuration files inside the user's home directory.\nFor example  ssh  stores its preferences inside a directory called  .sshd  To list all files including the normally hidden entries use  ls  with   -a  option   $ ls -a\n.           a.c         myls\n..          a.out           other.txt\n.secret",
            "title": "How do I make 'hidden' files i.e. not listed by \"ls\"? How do I list them?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#what-happens-if-i-turn-off-the-execute-bit-on-directories",
            "text": "The execute bit for a directory is used to control whether the directory contents is listable.  $ chmod ugo-x dir1\n$ ls -l\ndrw-r--r--   3 angrave  staff   102 Nov 10 11:22 dir1  However when attempting to list the contents of the directory,  $ ls dir1\nls: dir1: Permission denied  In other words, the directory itself is discoverable but its contents cannot be listed.",
            "title": "What happens if I turn off the execute bit on directories?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#what-is-file-globbing-and-who-does-it",
            "text": "Before executing the program the shell expands parameters into matching filenames. For example, if the current directory has three filenames that start with my ( my1.txt mytext.txt myomy), then  $ echo my*  Expands to   $ echo my1.txt mytext.txt myomy  This is known as file globbing and is processed before the command is executed.\nie the command's parameters are identical to manually typing every matching filename.",
            "title": "What is file globbing (and who does it)?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#creating-secure-directories",
            "text": "Suppose you created your own directory in /tmp and then set the permissions so that only you can use the directory (see below). Is this secure?   $ mkdir /tmp/mystuff\n$ chmod 700 /tmp/mystuff  There is a window of opportunity between when the directory is created and when it's permissions are changed. This leads to several vulnerabilities that are based on a race condition (where an attacker modifies the directory in some way before the privileges are removed). Some examples include:  Another user replaces  mystuff  with a hardlink to an existing file or directory owned by the second user, then they would be able to read and control the contents of the  mystuff  directory. Oh no - our secrets are no longer secret!  However in this specific example the  /tmp  directory has the sticky bit set, so other users may not delete the  mystuff  directory, and the simple attack scenario described above is impossible. This does not mean that creating the directory and then later making the directory private is secure! A better version is to atomically create the directory with the correct permissions from its inception -   $ mkdir -m 700 /tmp/mystuff",
            "title": "Creating secure directories"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#how-do-i-automatically-create-parent-directories",
            "text": "$ mkdir -p d1/d2/d3  Will automatically create d1 and d2 if they don't exist.",
            "title": "How do I automatically create parent directories?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#my-default-umask-022-what-does-this-mean",
            "text": "The umask  subtracts  (reduces) permission bits from 777 and is used when new files and new directories are created by open,mkdir etc. Thus  022  (octal) means that group and other privileges will not include the writable bit . Each process (including the shell) has a current umask value. When forking, the child inherits the parent's umask value.  For example, by setting the umask to 077 in the shell, ensures that future file and directory creation will only be accessible to the current user,  $ umask 077\n$ mkdir secretdir  As a code example, suppose a new file is created with  open()  and mode bits  666  (write and read bits for user,group and other):  open(\"myfile\", O_CREAT, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);  If umask is octal 022, then the permissions of the created file will be 0666 & ~022\nie.             S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH",
            "title": "My default umask 022; what does this mean?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#how-can-i-copy-bytes-from-one-file-to-another",
            "text": "Use  the versatile  dd  command. For example, the following command copies 1 MB of data from the file  /dev/urandom  to the file  /dev/null . The data is copied as 1024 blocks of blocksize 1024 bytes.  $ dd if=/dev/urandom of=/dev/null bs=1k count=1024  Both the input and output files in the example above are virtual - they don't exist on a disk. This means the speed of the transfer is unaffected by hardware power. Instead they are part of the  dev  filesystem, which is virtual filesystem provided by the kernel.\nThe virtual file  /dev/urandom  provides an infinite stream of random bytes, while the virtal file  /dev/null  ignores all bytes written to it. A common use of  /dev/null  is to discard the output of a command,  $ myverboseexecutable > /dev/null  Another commonly used /dev virtual file is  /dev/zero  which provides an infinite stream of zero bytes.\nFor example, we can benchmark the operating system performance of reading stream zero bytes in the kernel into a process memory and writing the bytes back to the kernel without any disk I/O. Note the throughput (~20GB/s) is strongly dependent on blocksize. For small block sizes the overhead of additional  read  and  write  system calls will  dominate.  $ dd if=/dev/zero of=/dev/null bs=1M count=1024\n1024+0 records in\n1024+0 records out\n1073741824 bytes (1.1 GB) copied, 0.0539153 s, 19.9 GB/s",
            "title": "How can I copy bytes from one file to another?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-4:-Working-with-directories/#what-happens-when-i-touch-a-file",
            "text": "The  touch  executable creates file if it does not exist and also updates the file's last modified time to be the current time. For example, we can make a new private file with the current time:  $ umask 077       # all future new files will maskout all r,w,x bits for group and other access\n$ touch file123   # create a file if it does not exist, and update its modified time\n$ stat file123\n  File: `file123'\n  Size: 0           Blocks: 0          IO Block: 65536  regular empty file\nDevice: 21h/33d Inode: 226148      Links: 1\nAccess: (0600/-rw-------)  Uid: (395606/ angrave)   Gid: (61019/     ews)\nAccess: 2014-11-12 13:42:06.000000000 -0600\nModify: 2014-11-12 13:42:06.001787000 -0600\nChange: 2014-11-12 13:42:06.001787000 -0600  An example use of touch is to force make to recompile a file that is unchanged after modifying the compiler options inside the makefile. Remeber that make is 'lazy' - it will compare the modified time of the source file with the corresponding output file to see if the file needs to be recompiled  $ touch myprogram.c   # force my source file to be recompiled\n$ make  Go to File System: Part 5",
            "title": "What happens when I touch a file?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-5:-Virtual-file-systems/",
            "text": "Virtual file systems\n\n\nPOSIX systems, such as Linux and Mac OSX (which is based on BSD) include several virtual filesystems that are mounted (available) as part of the file-system. Files inside these virtual filesystems do not exist on the disk; they are generated dynamically by the kernel when a process requests a directory listing.\nLinux provides 3 main virtual filesystems\n\n\n/dev  - A list of physical and virtual devices (for example network card, cdrom, random number generator)\n/proc - A list of resources used by each process and (by tradition) set of system information\n/sys - An organized list of internal kernel entities\n\n\n\n\nFor example if I want a continuous stream of 0s, I can \ncat /dev/zero\n.\n\n\nHow do I find out what filesystems are currently available (mounted)?\n\n\nUse \nmount\n\nUsing mount without any options generates a list (one filesystem per line) of mounted filesystems including networked, virtual and local (spinning disk / SSD-based) filesystems. Here is a typical output of mount\n\n\n$ mount\n/dev/mapper/cs241--server_sys-root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")\n/dev/sda1 on /boot type ext3 (rw)\n/dev/mapper/cs241--server_sys-srv on /srv type ext4 (rw)\n/dev/mapper/cs241--server_sys-tmp on /tmp type ext4 (rw)\n/dev/mapper/cs241--server_sys-var on /var type ext4 (rw)rw,bind)\n/srv/software/Mathematica-8.0 on /software/Mathematica-8.0 type none (rw,bind)\nengr-ews-homes.engr.illinois.edu:/fs1-homes/angrave/linux on /home/angrave type nfs (rw,soft,intr,tcp,noacl,acregmin=30,vers=3,sec=sys,sloppy,addr=128.174.252.102)\n\n\n\n\nNotice that each line includes the filesystem type source of the filesystem and mount point.\nTo reduce this output we can pipe it into \ngrep\n and only see lines that match a regular expression. \n\n\n>mount | grep proc  # only see lines that contain 'proc'\nproc on /proc type proc (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n\n\n\n\nDifferences between random and urandom?\n\n\n/dev/random is a file which contains number generator where the entropy is determined from environmental noise. Random will block/wait until enough entropy is collected from the environment. \n\n\n/dev/urandom is like random, but differs in the fact that it allows for repetition (lower entropy threshold), thus wont block.\n\n\nOther Filesystems\n\n\n$ cat /proc/sys/kernel/random/entropy_avail\n$ hexdump /dev/random\n$ hexdump /dev/urandom\n\n$ cat /proc/meminfo\n$ cat /proc/cpuinfo\n$ cat /proc/cpuinfo | grep bogomips\n\n$ cat /proc/meminfo | grep Swap\n\n$ cd /proc/self\n$ echo $$; cd /proc/12345; cat maps\n\n\n\n\nMounting a filesystem\n\n\nLet's say I have a filesystem hooked up on \n/dev/cdrom\n that I want to read from. I have to mound it to a directory before I can do any operations.\n\n\n$ sudo mount /dev/cdrom /media/cdrom\n$ mount\n$ mount | grep proc\n\n\n\n\nHow do I mount a disk image?\n\n\nSuppose you had downloaded a bootable linux disk image...\n\n\nwget http://cosmos.cites.illinois.edu/pub/archlinux/iso/2015.04.01/archlinux-2015.04.01-dual.iso\n\n\n\n\nBefore putting the filesystem on a CD, we can mount the file as a filesystem and explore its contents. Note, mount requires root access, so let's run it using sudo\n\n\n$ mkdir arch\n$ sudo mount -o loop archlinux-2015.04.01-dual.iso ./arch\n$ cd arch\n\n\n\n\nBefore the mount command, the arch directory is new and obviously empty. After mounting, the contents of \narch/\n will be drawn from the files and directories stored in the filesystem stored inside the \narchlinux-2014.11.01-dual.iso\n file.\nThe \nloop\n option is required because we want to mount a regular file not a block device such as a physical disk. \n\n\nThe loop option wraps the original file as a block device - in this example we will find out below that the file system is provided under \n/dev/loop0\n : We can check the filesystem type and mount options by running the mount command without any parameters. We will pipe the output into \ngrep\n so that we only see the relevant output line(s) that contain 'arch'\n\n\n$ mount | grep arch\n/home/demo/archlinux-2014.11.01-dual.iso on /home/demo/arch type iso9660 (rw,loop=/dev/loop0)\n\n\n\n\nThe iso9660 filesystem is a read-only filesystem originally designed for optical storage media (i.e. CDRoms). Attempting to change the contents of the filesystem will fail\n\n\n$ touch arch/nocando\ntouch: cannot touch `/home/demo/arch/nocando': Read-only file system\n\n\n\n\nGo to File System: Part 6",
            "title": "File System, Part 5: Virtual file systems"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-5:-Virtual-file-systems/#virtual-file-systems",
            "text": "POSIX systems, such as Linux and Mac OSX (which is based on BSD) include several virtual filesystems that are mounted (available) as part of the file-system. Files inside these virtual filesystems do not exist on the disk; they are generated dynamically by the kernel when a process requests a directory listing.\nLinux provides 3 main virtual filesystems  /dev  - A list of physical and virtual devices (for example network card, cdrom, random number generator)\n/proc - A list of resources used by each process and (by tradition) set of system information\n/sys - An organized list of internal kernel entities  For example if I want a continuous stream of 0s, I can  cat /dev/zero .",
            "title": "Virtual file systems"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-5:-Virtual-file-systems/#how-do-i-find-out-what-filesystems-are-currently-available-mounted",
            "text": "Use  mount \nUsing mount without any options generates a list (one filesystem per line) of mounted filesystems including networked, virtual and local (spinning disk / SSD-based) filesystems. Here is a typical output of mount  $ mount\n/dev/mapper/cs241--server_sys-root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")\n/dev/sda1 on /boot type ext3 (rw)\n/dev/mapper/cs241--server_sys-srv on /srv type ext4 (rw)\n/dev/mapper/cs241--server_sys-tmp on /tmp type ext4 (rw)\n/dev/mapper/cs241--server_sys-var on /var type ext4 (rw)rw,bind)\n/srv/software/Mathematica-8.0 on /software/Mathematica-8.0 type none (rw,bind)\nengr-ews-homes.engr.illinois.edu:/fs1-homes/angrave/linux on /home/angrave type nfs (rw,soft,intr,tcp,noacl,acregmin=30,vers=3,sec=sys,sloppy,addr=128.174.252.102)  Notice that each line includes the filesystem type source of the filesystem and mount point.\nTo reduce this output we can pipe it into  grep  and only see lines that match a regular expression.   >mount | grep proc  # only see lines that contain 'proc'\nproc on /proc type proc (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)",
            "title": "How do I find out what filesystems are currently available (mounted)?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-5:-Virtual-file-systems/#differences-between-random-and-urandom",
            "text": "/dev/random is a file which contains number generator where the entropy is determined from environmental noise. Random will block/wait until enough entropy is collected from the environment.   /dev/urandom is like random, but differs in the fact that it allows for repetition (lower entropy threshold), thus wont block.",
            "title": "Differences between random and urandom?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-5:-Virtual-file-systems/#other-filesystems",
            "text": "$ cat /proc/sys/kernel/random/entropy_avail\n$ hexdump /dev/random\n$ hexdump /dev/urandom\n\n$ cat /proc/meminfo\n$ cat /proc/cpuinfo\n$ cat /proc/cpuinfo | grep bogomips\n\n$ cat /proc/meminfo | grep Swap\n\n$ cd /proc/self\n$ echo $$; cd /proc/12345; cat maps",
            "title": "Other Filesystems"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-5:-Virtual-file-systems/#mounting-a-filesystem",
            "text": "Let's say I have a filesystem hooked up on  /dev/cdrom  that I want to read from. I have to mound it to a directory before I can do any operations.  $ sudo mount /dev/cdrom /media/cdrom\n$ mount\n$ mount | grep proc",
            "title": "Mounting a filesystem"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-5:-Virtual-file-systems/#how-do-i-mount-a-disk-image",
            "text": "Suppose you had downloaded a bootable linux disk image...  wget http://cosmos.cites.illinois.edu/pub/archlinux/iso/2015.04.01/archlinux-2015.04.01-dual.iso  Before putting the filesystem on a CD, we can mount the file as a filesystem and explore its contents. Note, mount requires root access, so let's run it using sudo  $ mkdir arch\n$ sudo mount -o loop archlinux-2015.04.01-dual.iso ./arch\n$ cd arch  Before the mount command, the arch directory is new and obviously empty. After mounting, the contents of  arch/  will be drawn from the files and directories stored in the filesystem stored inside the  archlinux-2014.11.01-dual.iso  file.\nThe  loop  option is required because we want to mount a regular file not a block device such as a physical disk.   The loop option wraps the original file as a block device - in this example we will find out below that the file system is provided under  /dev/loop0  : We can check the filesystem type and mount options by running the mount command without any parameters. We will pipe the output into  grep  so that we only see the relevant output line(s) that contain 'arch'  $ mount | grep arch\n/home/demo/archlinux-2014.11.01-dual.iso on /home/demo/arch type iso9660 (rw,loop=/dev/loop0)  The iso9660 filesystem is a read-only filesystem originally designed for optical storage media (i.e. CDRoms). Attempting to change the contents of the filesystem will fail  $ touch arch/nocando\ntouch: cannot touch `/home/demo/arch/nocando': Read-only file system  Go to File System: Part 6",
            "title": "How do I mount a disk image?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/",
            "text": "How does the operating system load my process and libraries into memory?\n\n\nBy mapping the files' contents into the address-space of the process.\nIf many programs only need read-access to the same file (e.g. /bin/bash, the C library) then the same physical memory can be shared between multiple processes.\n\n\nThe same mechanism can be used by programs to directly map files into memory\n\n\nHow do I map a file into memory?\n\n\nA simple program to map a file into memory is shown below. The key points to notice are:\n\n mmap requires a filedescriptor, so we need to \nopen\n the file first\n\n We seek to our desired size and write one byte to ensure that the file is sufficient length\n* When finished call munmap to unmap the file from memory.\n\n\nThis example also shows the preprocessor constants \"\nLINE\n\" and \"\nFILE\n\" that hold the current line number and filename of the file currently being compiled.\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <sys/mman.h>\n#include <fcntl.h>\n#include <unistd.h>\n#include <errno.h>\n#include <string.h>\n\n\nint fail(char *filename, int linenumber) { \n  fprintf(stderr, \"%s:%d %s\\n\", filename, linenumber, strerror(errno)); \n  exit(1);\n  return 0; /*Make compiler happy */\n}\n#define QUIT fail(__FILE__, __LINE__ )\n\nint main() {\n  // We want a file big enough to hold 10 integers  \n  int size = sizeof(int) * 10;\n\n  int fd = open(\"data\", O_RDWR | O_CREAT | O_TRUNC, 0600); //6 = read+write for me!\n\n  lseek(fd, size, SEEK_SET);\n  write(fd, \"A\", 1);\n\n  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n  printf(\"Mapped at %p\\n\", addr);\n  if (addr == (void*) -1 ) QUIT;\n\n  int *array = addr;\n  array[0] = 0x12345678;\n  array[1] = 0xdeadc0de;\n\n  munmap(addr,size);\n  return 0;\n\n}\n\n\n\n\nThe contents of our binary file can be listed using hexdump\n\n\n$ hexdump data\n0000000 78 56 34 12 de c0 ad de 00 00 00 00 00 00 00 00\n0000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n0000020 00 00 00 00 00 00 00 00 41   \n\n\n\n\nThe careful reader may notice that our integers were written in least-significant-byte format (because that is the endianess of the CPU) and that we allocated a file that is one byte too many!\n\n\nThe \nPROT_READ | PROT_WRITE\n options specify the virtual memory protection. The option \nPROT_EXEC\n (not used here) can be set to allow CPU execution of instructions in memory (e.g. this would be useful if you mapped an executable or library).\n\n\nWhat are the advantages of memory mapping a file\n\n\nFor many applications the main advantages are:\n\nSimplified coding - the file data is immediately available. No need to parse the incoming data and store it in new memory structures.\n\nSharing of files - memory mapped files are particularly efficient when the same data is shared between multiple processes.\n\n\nNote for simple sequential processing memory mapped files are not necessarily faster than standard 'stream-based' approaches of \nread\n / fscanf etc. \n\n\nHow do I share memory between a parent and child process?\n\n\nEasy -  Use \nmmap\n without a file - just specify the MAP_ANONYMOUS and MAP_SHARED options!\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <sys/mman.h> /* mmap() is defined in this header */\n#include <fcntl.h>\n#include <unistd.h>\n#include <errno.h>\n#include <string.h>\n\nint main() {\n\n  int size = 100 * sizeof(int);  \n  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n  printf(\"Mapped at %p\\n\", addr);\n\n  int *shared = addr;\n  pid_t mychild = fork();\n  if (mychild > 0) {\n    shared[0] = 10;\n    shared[1] = 20;\n  } else {\n    sleep(1); // We will talk about synchronization later\n    printf(\"%d\\n\", shared[1] + shared[0]);\n  }\n\n  munmap(addr,size);\n  return 0;\n}\n\n\n\n\nCan I use shared memory for IPC ?\n\n\nYes! As a simple example you could reserve just a few bytes and change the value in shared memory when you want the child process to quit.\nSharing memory is a very efficient form of inter-process communication because there is no copying overhead - the two processes literally share the same \nphysical\n frame of memory.\n\n\nGo to File System: Part 7",
            "title": "File System, Part 6: Memory mapped files and Shared memory"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#how-does-the-operating-system-load-my-process-and-libraries-into-memory",
            "text": "By mapping the files' contents into the address-space of the process.\nIf many programs only need read-access to the same file (e.g. /bin/bash, the C library) then the same physical memory can be shared between multiple processes.  The same mechanism can be used by programs to directly map files into memory",
            "title": "How does the operating system load my process and libraries into memory?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#how-do-i-map-a-file-into-memory",
            "text": "A simple program to map a file into memory is shown below. The key points to notice are:  mmap requires a filedescriptor, so we need to  open  the file first  We seek to our desired size and write one byte to ensure that the file is sufficient length\n* When finished call munmap to unmap the file from memory.  This example also shows the preprocessor constants \" LINE \" and \" FILE \" that hold the current line number and filename of the file currently being compiled.  #include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <sys/mman.h>\n#include <fcntl.h>\n#include <unistd.h>\n#include <errno.h>\n#include <string.h>\n\n\nint fail(char *filename, int linenumber) { \n  fprintf(stderr, \"%s:%d %s\\n\", filename, linenumber, strerror(errno)); \n  exit(1);\n  return 0; /*Make compiler happy */\n}\n#define QUIT fail(__FILE__, __LINE__ )\n\nint main() {\n  // We want a file big enough to hold 10 integers  \n  int size = sizeof(int) * 10;\n\n  int fd = open(\"data\", O_RDWR | O_CREAT | O_TRUNC, 0600); //6 = read+write for me!\n\n  lseek(fd, size, SEEK_SET);\n  write(fd, \"A\", 1);\n\n  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n  printf(\"Mapped at %p\\n\", addr);\n  if (addr == (void*) -1 ) QUIT;\n\n  int *array = addr;\n  array[0] = 0x12345678;\n  array[1] = 0xdeadc0de;\n\n  munmap(addr,size);\n  return 0;\n\n}  The contents of our binary file can be listed using hexdump  $ hexdump data\n0000000 78 56 34 12 de c0 ad de 00 00 00 00 00 00 00 00\n0000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n0000020 00 00 00 00 00 00 00 00 41     The careful reader may notice that our integers were written in least-significant-byte format (because that is the endianess of the CPU) and that we allocated a file that is one byte too many!  The  PROT_READ | PROT_WRITE  options specify the virtual memory protection. The option  PROT_EXEC  (not used here) can be set to allow CPU execution of instructions in memory (e.g. this would be useful if you mapped an executable or library).",
            "title": "How do I map a file into memory?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#what-are-the-advantages-of-memory-mapping-a-file",
            "text": "For many applications the main advantages are: \nSimplified coding - the file data is immediately available. No need to parse the incoming data and store it in new memory structures. \nSharing of files - memory mapped files are particularly efficient when the same data is shared between multiple processes.  Note for simple sequential processing memory mapped files are not necessarily faster than standard 'stream-based' approaches of  read  / fscanf etc.",
            "title": "What are the advantages of memory mapping a file"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#how-do-i-share-memory-between-a-parent-and-child-process",
            "text": "Easy -  Use  mmap  without a file - just specify the MAP_ANONYMOUS and MAP_SHARED options!  #include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <sys/mman.h> /* mmap() is defined in this header */\n#include <fcntl.h>\n#include <unistd.h>\n#include <errno.h>\n#include <string.h>\n\nint main() {\n\n  int size = 100 * sizeof(int);  \n  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n  printf(\"Mapped at %p\\n\", addr);\n\n  int *shared = addr;\n  pid_t mychild = fork();\n  if (mychild > 0) {\n    shared[0] = 10;\n    shared[1] = 20;\n  } else {\n    sleep(1); // We will talk about synchronization later\n    printf(\"%d\\n\", shared[1] + shared[0]);\n  }\n\n  munmap(addr,size);\n  return 0;\n}",
            "title": "How do I share memory between a parent and child process?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#can-i-use-shared-memory-for-ipc",
            "text": "Yes! As a simple example you could reserve just a few bytes and change the value in shared memory when you want the child process to quit.\nSharing memory is a very efficient form of inter-process communication because there is no copying overhead - the two processes literally share the same  physical  frame of memory.  Go to File System: Part 7",
            "title": "Can I use shared memory for IPC ?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/",
            "text": "Reliable Single Disk Filesystems\n\n\nHow and why does the kernel cache the filesystem?\n\n\nMost filesystems cache significant amounts of disk data in physical memory.\nLinux, in this respect, is particularly extreme: All unused memory is used as a giant disk cache.\n\n\nThe disk cache can have significant impact on overall system performance because disk I/O is slow. This is especially true for random access requests on spinning disks where the disk read-write latency is dominated by the seek time required to move the read-write disk head to the correct position.\n\n\nFor efficiency, the kernel caches recently used disk blocks. \nFor writing we have to choose a trade-off between performance and reliability: Disk writes can also be cached (\"Write-back cache\") where modified disk blocks are stored in memory until evicted. Alternatively a 'write-through cache' policy can be employed where disk writes are sent immediately to the disk. The latter is safer (as filesystem modifications are quickly stored to persistent media) but slower than a write-back cache; If writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block.\n\n\nNote this is a simplified description because solid state drives (SSDs) can be used as a secondary write-back cache.\n\n\nBoth solid state disks (SSD) and spinning disks have improved performance when reading or writing sequential data. Thus operating system can often use a read-ahead strategy to amortize the read-request costs (e.g. time cost for a spinning disk) and request several contiguous disk blocks per request. By issuing an I/O request for the next disk block before the user application requires the next disk block, the apparent disk I/O latency can be reduced.\n\n\nMy data is important! Can I force the disk writes to be saved to the physical media and wait for it to complete?\n\n\nYes (almost). Call \nsync\n to request that a filesystem changes be written (flushed) to disk.\nHowever, not all operating systems honor this request and even if the data is evicted from the kernel buffers the disk firmware use an internal on-disk cache or may not yet have finished changing the physical media. \n\n\nNote you can also request that all changes associated with a particular file descriptor are flushed to disk using \nfsync(int fd)\n\n\nWhat if my disk fails in the middle of an important operation?\n\n\nDon't worry most modern file systems do something called \njournalling\n that work around this. What the file system does is before it completes a potentially expensive operation, is that it writes what it is going to do down in a journal. In the case of a crash or failure, one can step through the journal and see which files are corrupt and fix them. This is a way to salvage hard disks in cases there is critical data and there is no apparent backup.\n\n\nHow likely is a disk failure?\n\n\nDisk failures are measured using \"Mean-Time-Failure\". For large arrays, the mean failure time can be surprisingly short. For example if the MTTF(single disk) = 30,000 hours, then the MTTF(100 disks)= 30000/100=300 hours  ie about 12 days!\n\n\nRedundancy\n\n\nHow do I protect my data from disk failure?\n\n\nEasy! Store the data twice! This is the main principle of a \"RAID-1\" disk array. RAID is short for redundant array of inexpensive disks. By duplicating the writes to a disk with writes to another (backup disk) there are exactly two copies of the data. If one disk fails, the other disk serves as the only copy until it can be re-cloned. Reading data is faster (since data can be requested from either disk) but writes are potentially twice as slow (now two write commands need to be issued for every disk block write) and, compared to using a single disk, the cost of storage per byte has doubled.\n\n\nAnother common RAID scheme is RAID-0, meaning that a file could be split up amoung two disks, but if any one of the disks fail then the files are irrecoverable. This has the benefit of halving write times because one part of the file could be writing to hard disk one and another part to hard disk two.\n\n\nIt is also common to combine these systems. If you have a lot of hard disks, consider RAID-10. This is where you have two systems of RAID-1, but the systems are hooked up in RAID-0 to each other. This means you would get roughly the same speed from the slowdowns but now any one disk can fail and you can recover that disk. (If two disks from opposing raid partitions fail there is a chance that recover can happen though we don't could on it most of the time). \n\n\nWhat is RAID-3?\n\n\nRAID-3 uses parity codes instead of mirroring the data. For each N-bits written we will write one extra bit, the 'Parity bit' that ensures the total number of 1s written is even.  The parity bit is written to an additional disk. If any one disk (including the parity disk) is lost, then its contents can still be computed using the contents of the other disks.\n\n\n\n\nOne disadvantage of RAID-3 is that whenever a disk block is written, the parity block will always be written too. This means that there is effectively a bottleneck in a separate disk. In practice, this is more likely to cause a failure because one disk is being used 100% of the time and once that disk fails then the other disks are more prone to failure.\n\n\nHow secure is RAID-3 to data-loss?\n\n\nA single disk failure will not result in data loss (because there is sufficient data to rebuild the array from the remaining disks). Data-loss will occur when a two disks are unusable because there is no longer sufficient data to rebuild the array. We can calculate the probability of a two disk failure based on the repair time which includes not just the time to insert a new disk but the time required to rebuild the entire contents of the array.\n\n\nMTTF = mean time to failure\nMTTR = mean time to repair\nN = number of original disks\n\np = MTTR / (MTTF-one-disk / (N-1))\n\n\n\n\nUsing typical numbers (MTTR=1day, MTTF=1000days, N-1 = 9,, p=0.009\n\n\nThere is a 1% chance that another drive will fail during the rebuild process (at that point you had better hope you still have an accessible backup of your original data.\n\n\nIn practice the probability of a second failure during the repair process is likely higher because rebuilding the array is I/O-intensive (and on top of normal I/O request activity). This higher I/O load will also stress the disk array\n\n\nWhat is RAID-5?\n\n\nRAID-5 is similar to RAID-3 except that the check block (parity information) is assigned to different disks for different blocks. The check-block is 'rotated' through the disk array. RAID-5 provides better read and write performance than RAID-3 because there is no longer the bottleneck of the single parity disk. The one drawback is that you need more disks to have this setup and there are more complicated algorithms need to be used\n\n\n\n\nDistributed storage\n\n\nFailure is the common case\nGoogle reports 2-10% of disks fail per year\nNow multiply that by 60,000+ disks in a single warehouse...\nMust survive failure of not just a disk, but a rack of servers or a whole data center\n\n\nSolutions\nSimple redundancy (2 or 3 copies of each file)\ne.g., Google GFS (2001)\nMore efficient redundancy (analogous to RAID 3++)\ne.g., \nGoogle Colossus filesystem\n (~2010): customizable replication including Reed-Solomon codes with 1.5x redundancy",
            "title": "File System, Part 7: Scalable and Reliable Filesystems"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#reliable-single-disk-filesystems",
            "text": "",
            "title": "Reliable Single Disk Filesystems"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#how-and-why-does-the-kernel-cache-the-filesystem",
            "text": "Most filesystems cache significant amounts of disk data in physical memory.\nLinux, in this respect, is particularly extreme: All unused memory is used as a giant disk cache.  The disk cache can have significant impact on overall system performance because disk I/O is slow. This is especially true for random access requests on spinning disks where the disk read-write latency is dominated by the seek time required to move the read-write disk head to the correct position.  For efficiency, the kernel caches recently used disk blocks. \nFor writing we have to choose a trade-off between performance and reliability: Disk writes can also be cached (\"Write-back cache\") where modified disk blocks are stored in memory until evicted. Alternatively a 'write-through cache' policy can be employed where disk writes are sent immediately to the disk. The latter is safer (as filesystem modifications are quickly stored to persistent media) but slower than a write-back cache; If writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block.  Note this is a simplified description because solid state drives (SSDs) can be used as a secondary write-back cache.  Both solid state disks (SSD) and spinning disks have improved performance when reading or writing sequential data. Thus operating system can often use a read-ahead strategy to amortize the read-request costs (e.g. time cost for a spinning disk) and request several contiguous disk blocks per request. By issuing an I/O request for the next disk block before the user application requires the next disk block, the apparent disk I/O latency can be reduced.",
            "title": "How and why does the kernel cache the filesystem?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#my-data-is-important-can-i-force-the-disk-writes-to-be-saved-to-the-physical-media-and-wait-for-it-to-complete",
            "text": "Yes (almost). Call  sync  to request that a filesystem changes be written (flushed) to disk.\nHowever, not all operating systems honor this request and even if the data is evicted from the kernel buffers the disk firmware use an internal on-disk cache or may not yet have finished changing the physical media.   Note you can also request that all changes associated with a particular file descriptor are flushed to disk using  fsync(int fd)",
            "title": "My data is important! Can I force the disk writes to be saved to the physical media and wait for it to complete?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#what-if-my-disk-fails-in-the-middle-of-an-important-operation",
            "text": "Don't worry most modern file systems do something called  journalling  that work around this. What the file system does is before it completes a potentially expensive operation, is that it writes what it is going to do down in a journal. In the case of a crash or failure, one can step through the journal and see which files are corrupt and fix them. This is a way to salvage hard disks in cases there is critical data and there is no apparent backup.",
            "title": "What if my disk fails in the middle of an important operation?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#how-likely-is-a-disk-failure",
            "text": "Disk failures are measured using \"Mean-Time-Failure\". For large arrays, the mean failure time can be surprisingly short. For example if the MTTF(single disk) = 30,000 hours, then the MTTF(100 disks)= 30000/100=300 hours  ie about 12 days!",
            "title": "How likely is a disk failure?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#redundancy",
            "text": "",
            "title": "Redundancy"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#how-do-i-protect-my-data-from-disk-failure",
            "text": "Easy! Store the data twice! This is the main principle of a \"RAID-1\" disk array. RAID is short for redundant array of inexpensive disks. By duplicating the writes to a disk with writes to another (backup disk) there are exactly two copies of the data. If one disk fails, the other disk serves as the only copy until it can be re-cloned. Reading data is faster (since data can be requested from either disk) but writes are potentially twice as slow (now two write commands need to be issued for every disk block write) and, compared to using a single disk, the cost of storage per byte has doubled.  Another common RAID scheme is RAID-0, meaning that a file could be split up amoung two disks, but if any one of the disks fail then the files are irrecoverable. This has the benefit of halving write times because one part of the file could be writing to hard disk one and another part to hard disk two.  It is also common to combine these systems. If you have a lot of hard disks, consider RAID-10. This is where you have two systems of RAID-1, but the systems are hooked up in RAID-0 to each other. This means you would get roughly the same speed from the slowdowns but now any one disk can fail and you can recover that disk. (If two disks from opposing raid partitions fail there is a chance that recover can happen though we don't could on it most of the time).",
            "title": "How do I protect my data from disk failure?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#what-is-raid-3",
            "text": "RAID-3 uses parity codes instead of mirroring the data. For each N-bits written we will write one extra bit, the 'Parity bit' that ensures the total number of 1s written is even.  The parity bit is written to an additional disk. If any one disk (including the parity disk) is lost, then its contents can still be computed using the contents of the other disks.   One disadvantage of RAID-3 is that whenever a disk block is written, the parity block will always be written too. This means that there is effectively a bottleneck in a separate disk. In practice, this is more likely to cause a failure because one disk is being used 100% of the time and once that disk fails then the other disks are more prone to failure.",
            "title": "What is RAID-3?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#how-secure-is-raid-3-to-data-loss",
            "text": "A single disk failure will not result in data loss (because there is sufficient data to rebuild the array from the remaining disks). Data-loss will occur when a two disks are unusable because there is no longer sufficient data to rebuild the array. We can calculate the probability of a two disk failure based on the repair time which includes not just the time to insert a new disk but the time required to rebuild the entire contents of the array.  MTTF = mean time to failure\nMTTR = mean time to repair\nN = number of original disks\n\np = MTTR / (MTTF-one-disk / (N-1))  Using typical numbers (MTTR=1day, MTTF=1000days, N-1 = 9,, p=0.009  There is a 1% chance that another drive will fail during the rebuild process (at that point you had better hope you still have an accessible backup of your original data.  In practice the probability of a second failure during the repair process is likely higher because rebuilding the array is I/O-intensive (and on top of normal I/O request activity). This higher I/O load will also stress the disk array",
            "title": "How secure is RAID-3 to data-loss?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#what-is-raid-5",
            "text": "RAID-5 is similar to RAID-3 except that the check block (parity information) is assigned to different disks for different blocks. The check-block is 'rotated' through the disk array. RAID-5 provides better read and write performance than RAID-3 because there is no longer the bottleneck of the single parity disk. The one drawback is that you need more disks to have this setup and there are more complicated algorithms need to be used",
            "title": "What is RAID-5?"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#distributed-storage",
            "text": "Failure is the common case\nGoogle reports 2-10% of disks fail per year\nNow multiply that by 60,000+ disks in a single warehouse...\nMust survive failure of not just a disk, but a rack of servers or a whole data center  Solutions\nSimple redundancy (2 or 3 copies of each file)\ne.g., Google GFS (2001)\nMore efficient redundancy (analogous to RAID 3++)\ne.g.,  Google Colossus filesystem  (~2010): customizable replication including Reed-Solomon codes with 1.5x redundancy",
            "title": "Distributed storage"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/",
            "text": "Case study: Removing malware from an Android device\n\n\nThis section uses filesystem features and system programming tools discussed in this wikibook to find and remove unwanted malware from an Android tablet. \n\n\nDISCLAIMER. ENSURE ANY VALUABLE INFORMATION ON YOUR DEVICE IS BACKED UP BEFORE ATTEMPTING TO MODIFY YOUR TABLET. MODIFYING SYSTEM SETTINGS AND SYSTEM FILES IS NOT RECOMMENDED. ATTEMPTING TO MODIFY A DEVICE USING THIS CASE STUDU GUIDE MAY CAUSE YOUR TABLET TO SHARE, LOSE OR CORRUPT YOUR DATA. FURTHER YOUR TABLET MAY FUNCTION INCORRECTLY OR TO STOP FUNCTIONING ALTOGETHER. USE THIS CASE STUDY AT YOUR OWN RISK. THE AUTHORS ASSUME NO RESPONSIBILITY AND MAKE NO WARRANTY ABOUT THE CORRECTNESS OR COMPLETENESS OF THESE INSTRUCTIONS INCLUDED IN THIS CASE STUDY. THE AUTHORS ASSUME NO RESPONSIBILITY AND MAKE NO WARRANTY ABOUT ANY SOFTWARE INCLUDING EXTERNAL THIRD PARTY SOFTWARE DESCRIBED OR LINKED TO IN THIS GUIDE.\n\n\nBackground\n\n\nAn E97 Android tablet purchased from Amazon had developed some peculiar quirks. Most noticeable was that the browser app always opened a website at gotoamazing.com rather than the home page set in the app's preferences (known as browser 'hijacking'). Can we use the knowledge from this wikibook to understand how this unwanted behavior occurs and also remove unwanted pre-installed apps from the device?\n\n\nTools used\n\n\nWhile it is possible to use the Android developer tools installed on a remote USB-connected machine, this guide uses only system tools on the tablet. The following apps were installed - \n\n\n\n\nMalwarebytes - A free vulnerability and malware tool.\n\n\nTerminal emulator - A simple terminal window that gives us shell access on the tablet.\n\n\nKingRoot - A tool that uses known exploits in the linux kernel to gain root access.\n\n\n\n\nInstalling any app can potentially allow arbitrary code to be executed if it is able to break out of the Android security model. Of the apps mentioned above, KingRoot is the most extreme example because it exploits system vulnerabilities to gain root access for our purposes. However in doing so, it could also \nOf these, KingRoot is the most questionable tool to install - we are trusting it not to install any of its own malware. A potentially safer alternative is to use https://github.com/android-rooting-tools/\n\n\nOverview of terminal\n\n\nThe most useful commands are  \nsu grep mount\n and Android's package manager tool, \npm\n.\n\n\n\n\ngrep -s abc * \n/\n   (search for \nabc\n in current directory and immediate sub directories)\n\n\nsu ( aka \"switch user\" become root - requires a rooted device)\n\n\nmount -o rw,remount /system  (allow /system partition to be writeable)\n\n\npm disable  (aka 'package manager' disable an Android app package)\n\n\n\n\nOverview of filesystem layout\n\n\nOn this specific tablet that runs Android 4.4.2, pre-installed apps are unmodifiable and are located in\n\n\n/system/app/\n/system/priv-app/\n\n\n\n\nand preferences and app-data are stored in the \n/data\n partition\nEach app is typically packaged inside an apk file, which is essentially a zip file. When an app is installed the code is expanded into a file that can be directly parsed by the Android virtual machine. The binary code (at least for this particular virtual machine) has an odex extension.\n\n\nWe can search the code of the installed system apps for the string 'gotoamazing'\n\n\ngrep -s gotoamazing /system/app/* /system/priv-app/*\n\n\n\n\nThis didn't find anything; it appears this string was not hardcoded into the source code of the given system apps. To verify that we can find \n\n\nLet's check the data area of all installed apps\n\n\ncd /data/data\ngrep -s gotoamazing * */* */*/*\n\n\n\n\nproduced the following\n\n\ndata/com.android.browser/shared_prefs/xbservice.xml: <string name=\"URL\">http://www.gotoamazing...\n\n\n\n\nThe -s option \"silent option\" stops grep from complaining about trying to grep directories and other invalid files. Note we could have also used -r to recursively search directories but it was fun to use file globbing (wildcard expansion of the * by the shell).\n\n\nNow we are getting somewhere! It looks like this string is part of the app 'com.android.browser' but let's also find out which app binary code opens the 'xbservice' preference. Perhaps this unwanted service is hiding inside another app and managed to secretly load as an extension to the browser?\n\n\nLet's look for any file that contains xbservice. This time we will recursively search in the directories of /system that include the 'app'\n\n\ngrep -r -s xbservice /system/*app*\nBinary file /system/app/Browser.odex matches\n\n\n\n\nFinally - it appears the factory browser was shipped with the the home page hijacking pre-installed. Let's uninstall it. For this, let's become root.\n\n\n`\n$ su\n\n\npm list packages -s\n\n\n`\nAndroid's package manager has many commands and options. The above example lists all currently installed system apps. We can uninstall the browser app using the following command\n\n\npm disable com.android.browser\npm uninstall com.android.browser\n\n\n\n\nUsing \npm list packages\n you can list all installed packages (use \n-s\n options to see just system packages). We disabled the following system apps. Of course there is no real guarantee that we successfully removed all unwanted software, or that one of these is a false-positive. As such we would not recommend keeping sensitive information on such a tablet.\n\n\n\n\ncom.android.browser\n\n\ncom.adups.fota.sysoper\n\n\nelink.com\n\n\ncom.google.android.apps.cloudprint\n\n\ncom.mediatek.CrashService\n\n\ncom.get.googleApps\n\n\ncom.adups.fota (an over-the-air package that can install arbitrary items in the future).\n\n\ncom.mediatek.appguide.plugin\n\n\n\n\nIt is likely that you could just re-enable a package using \npm enable package-name\n or \npm install\n and the relevant .apk file in /system/app or /system/priv-app",
            "title": "File System, Part 8: Removing preinstalled malware from an Android device"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#background",
            "text": "An E97 Android tablet purchased from Amazon had developed some peculiar quirks. Most noticeable was that the browser app always opened a website at gotoamazing.com rather than the home page set in the app's preferences (known as browser 'hijacking'). Can we use the knowledge from this wikibook to understand how this unwanted behavior occurs and also remove unwanted pre-installed apps from the device?",
            "title": "Background"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#tools-used",
            "text": "While it is possible to use the Android developer tools installed on a remote USB-connected machine, this guide uses only system tools on the tablet. The following apps were installed -    Malwarebytes - A free vulnerability and malware tool.  Terminal emulator - A simple terminal window that gives us shell access on the tablet.  KingRoot - A tool that uses known exploits in the linux kernel to gain root access.   Installing any app can potentially allow arbitrary code to be executed if it is able to break out of the Android security model. Of the apps mentioned above, KingRoot is the most extreme example because it exploits system vulnerabilities to gain root access for our purposes. However in doing so, it could also \nOf these, KingRoot is the most questionable tool to install - we are trusting it not to install any of its own malware. A potentially safer alternative is to use https://github.com/android-rooting-tools/",
            "title": "Tools used"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#overview-of-terminal",
            "text": "The most useful commands are   su grep mount  and Android's package manager tool,  pm .   grep -s abc *  /    (search for  abc  in current directory and immediate sub directories)  su ( aka \"switch user\" become root - requires a rooted device)  mount -o rw,remount /system  (allow /system partition to be writeable)  pm disable  (aka 'package manager' disable an Android app package)",
            "title": "Overview of terminal"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#overview-of-filesystem-layout",
            "text": "On this specific tablet that runs Android 4.4.2, pre-installed apps are unmodifiable and are located in  /system/app/\n/system/priv-app/  and preferences and app-data are stored in the  /data  partition\nEach app is typically packaged inside an apk file, which is essentially a zip file. When an app is installed the code is expanded into a file that can be directly parsed by the Android virtual machine. The binary code (at least for this particular virtual machine) has an odex extension.  We can search the code of the installed system apps for the string 'gotoamazing'  grep -s gotoamazing /system/app/* /system/priv-app/*  This didn't find anything; it appears this string was not hardcoded into the source code of the given system apps. To verify that we can find   Let's check the data area of all installed apps  cd /data/data\ngrep -s gotoamazing * */* */*/*  produced the following  data/com.android.browser/shared_prefs/xbservice.xml: <string name=\"URL\">http://www.gotoamazing...  The -s option \"silent option\" stops grep from complaining about trying to grep directories and other invalid files. Note we could have also used -r to recursively search directories but it was fun to use file globbing (wildcard expansion of the * by the shell).  Now we are getting somewhere! It looks like this string is part of the app 'com.android.browser' but let's also find out which app binary code opens the 'xbservice' preference. Perhaps this unwanted service is hiding inside another app and managed to secretly load as an extension to the browser?  Let's look for any file that contains xbservice. This time we will recursively search in the directories of /system that include the 'app'  grep -r -s xbservice /system/*app*\nBinary file /system/app/Browser.odex matches  Finally - it appears the factory browser was shipped with the the home page hijacking pre-installed. Let's uninstall it. For this, let's become root.  `\n$ su",
            "title": "Overview of filesystem layout"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#pm-list-packages-s",
            "text": "`\nAndroid's package manager has many commands and options. The above example lists all currently installed system apps. We can uninstall the browser app using the following command  pm disable com.android.browser\npm uninstall com.android.browser  Using  pm list packages  you can list all installed packages (use  -s  options to see just system packages). We disabled the following system apps. Of course there is no real guarantee that we successfully removed all unwanted software, or that one of these is a false-positive. As such we would not recommend keeping sensitive information on such a tablet.   com.android.browser  com.adups.fota.sysoper  elink.com  com.google.android.apps.cloudprint  com.mediatek.CrashService  com.get.googleApps  com.adups.fota (an over-the-air package that can install arbitrary items in the future).  com.mediatek.appguide.plugin   It is likely that you could just re-enable a package using  pm enable package-name  or  pm install  and the relevant .apk file in /system/app or /system/priv-app",
            "title": "pm list packages -s"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-9:-Disk-blocks-example/",
            "text": "Under construction\n\n\nPlease can you explain a simple model of how the file's content is stored in a simple i-node based filesystem?\n\n\nSure! To answer this question, we'll build a virtual disk and then write some C code to access its contents. Our filesystem will divide the bytes available into space for inodes and a much larger space for disk blocks. Each disk block will be 4096 bytes- \n\n\n// Disk size:\n#define MAX_INODE (1024)\n#define MAX_BLOCK (1024*1024)\n\n// Each block is 4096 bytes:\ntypedef char[4096] block_t;\n\n// A disk is an array of inodes and an array of disk blocks:\nstruct inode[MAX_INODE] inodes;\nblock[MAX_BLOCK] blocks;\n\n\n\n\nNote for clarity we will not use 'unsigned' in this code example. Our fixed-sized inodes will contain the file's size in bytes, permission, user, group information, time meta-data. What is most relevant to the problem-at hand is that it will also include ten pointers to disk blocks that we will use to refer to the actual file's contents!\n\n\nstruct inode {\n int[10] directblocks; // indices for the block array i.e. where to the find the file's content\n long size;\n // ... standard inode meta-data e.g.\n int mode, userid,groupid;\n time_t ctime,atime,mtime;\n}\n\n\n\n\nNow we can work out how to read a byte at offset \nposition\n of our file:\n\n\nchar readbyte(inode*inode,long position) {\n  if(position <0 || position >= inode->size) return -1; // invalid offset\n\n  int  block_count = position / 4096,offset = position % 4096;\n\n  // block count better be 0..9 !\n  int physical_idx = lookup_physical_block_index(inode, block_count );\n\n  // sanity check that the disk block index is reasonable...\n  assert(physical_idx >=0 && physical_idx < MAX_BLOCK);\n\n\n  // read the disk block from our virtual disk 'blocks' and return the specific byte\n  return blocks[physical_idx][offset];\n}\n\n\n\n\nOur initial version of lookup_physical_block is simple - we can use our table of 10 direct blocks!\n\n\nint lookup_physical_block_index(inode*inode, int block_count) {\n  assert(block_count>=0 && block_count < 10);\n\n  return inode->directblocks[ block_count ]; // returns an index value between [0,MAX_BLOCK)\n}\n\n\n\n\nThis simple representation is reasonable provided we can represent all possible files with just ten blocks i.e. up to 40KB. What about larger files? We need the inode struct to always be the same size so just increasing the existing direct block array to 20 would roughly double the size of our inodes. If most of our files require less than 10 blocks, then our inode storage is now wasteful. To solve this problem we will use a disk block called the \nindirect block\n to extend the array of pointers at our disposal. We will only need this for files > 40KB\n\n\nstruct inode {\n int[10] directblocks; // if size<4KB then only the first one is valid\n int indirectblock; // valid value when size >= 40KB\n int size;\n ...\n}\n\n\n\n\nThe indirect block is just a regular disk block of 4096 bytes, but we will use it to hold pointers to disk blocks. Our pointers in this case are just integers, so we need to cast the pointer to an integer pointer:\n\n\nint lookup_physical_block_index(inode*inode, int block_count) {\n  assert(sizeof(int)==4); // Warning this code assumes an index is 4 bytes!\n  assert(block_count>=0 && block_count < 1024 + 10); // 0 <= block_count< 1034\n\n  if( block_count < 10)\n     return inode->directblocks[ block_count ];\n\n  // read the indirect block from disk:\n  block_t* oneblock = & blocks[ inode->indirectblock ];\n\n  // Treat the 4KB as an array of 1024 pointers to other disk blocks\n  int* table = (int*) oneblock;\n\n // Look up the correct entry in the table\n // Offset by 10 because the first 10 blocks of data are already \n // accounted for\n  return table[ block_count - 10 ];\n}\n\n\n\n\nFor a typical filesystem, our index values are 32 bits i.e. 4bytes. Thus in 4096 bytes we can store 4096 / 4 = 1024 entries.\nThis means our indirect block can refer to 1024 * 4KB = 4MB of data. With the first ten direct blocks, we can therefore accommodate files up to 40KB + 1024 * 4KB= 4136KB . Some of the later table entries can be invalid for files that are smaller than this. \n\n\nFor even larger files, we could use two indirect blocks. However there's a better alternative, that will allow us to efficiently scale up to huge files. We will include a double-indirect pointer and if that's not enough a triple indirect pointer. The double indirect pointer means we have a table of 1024 entries to disk blocks that are used as 1024 entries. This means we can refer to 1024*1024 disk blocks of data.\n\n\n\n\n(source: http://uw714doc.sco.com/en/FS_admin/graphics/s5chain.gif)\n\n\nint lookup_physical_block_index(inode*inode, int block_count) {\n  if( block_count < 10)\n     return inode->directblocks[ block_count ];\n\n  // Use indirect block for the next 1024 blocks:\n  // Assumes 1024 ints can fit inside each block!\n  if( block_count < 1024 + 10) {   \n      int* table = (int*) & blocks[ inode->indirectblock ];\n      return table[ block_count - 10 ];\n  }\n  // For huge files we will use a table of tables\n  int i = (block_count - 1034) / 1024 , j = (block_count - 1034) % 1024;\n  assert(i<1024); // triple-indirect is not implemented here!\n\n  int* table1 = (int*) & blocks[ inode->doubleindirectblock ];\n   // The first table tells us where to read the second table ...\n  int* table2 = (int*) & blocks[   table1[i]   ];\n  return table2[j];\n\n   // For gigantic files we will need to implement triple-indirect (table of tables of tables)\n}\n\n\n\n\nNotice that reading a byte using double indirect requires 3 disk block reads (two tables and the actual data block).",
            "title": "File System, Part 9: Disk blocks example"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-9:-Disk-blocks-example/#under-construction",
            "text": "",
            "title": "Under construction"
        },
        {
            "location": "/SystemProgramming/File-System,-Part-9:-Disk-blocks-example/#please-can-you-explain-a-simple-model-of-how-the-files-content-is-stored-in-a-simple-i-node-based-filesystem",
            "text": "Sure! To answer this question, we'll build a virtual disk and then write some C code to access its contents. Our filesystem will divide the bytes available into space for inodes and a much larger space for disk blocks. Each disk block will be 4096 bytes-   // Disk size:\n#define MAX_INODE (1024)\n#define MAX_BLOCK (1024*1024)\n\n// Each block is 4096 bytes:\ntypedef char[4096] block_t;\n\n// A disk is an array of inodes and an array of disk blocks:\nstruct inode[MAX_INODE] inodes;\nblock[MAX_BLOCK] blocks;  Note for clarity we will not use 'unsigned' in this code example. Our fixed-sized inodes will contain the file's size in bytes, permission, user, group information, time meta-data. What is most relevant to the problem-at hand is that it will also include ten pointers to disk blocks that we will use to refer to the actual file's contents!  struct inode {\n int[10] directblocks; // indices for the block array i.e. where to the find the file's content\n long size;\n // ... standard inode meta-data e.g.\n int mode, userid,groupid;\n time_t ctime,atime,mtime;\n}  Now we can work out how to read a byte at offset  position  of our file:  char readbyte(inode*inode,long position) {\n  if(position <0 || position >= inode->size) return -1; // invalid offset\n\n  int  block_count = position / 4096,offset = position % 4096;\n\n  // block count better be 0..9 !\n  int physical_idx = lookup_physical_block_index(inode, block_count );\n\n  // sanity check that the disk block index is reasonable...\n  assert(physical_idx >=0 && physical_idx < MAX_BLOCK);\n\n\n  // read the disk block from our virtual disk 'blocks' and return the specific byte\n  return blocks[physical_idx][offset];\n}  Our initial version of lookup_physical_block is simple - we can use our table of 10 direct blocks!  int lookup_physical_block_index(inode*inode, int block_count) {\n  assert(block_count>=0 && block_count < 10);\n\n  return inode->directblocks[ block_count ]; // returns an index value between [0,MAX_BLOCK)\n}  This simple representation is reasonable provided we can represent all possible files with just ten blocks i.e. up to 40KB. What about larger files? We need the inode struct to always be the same size so just increasing the existing direct block array to 20 would roughly double the size of our inodes. If most of our files require less than 10 blocks, then our inode storage is now wasteful. To solve this problem we will use a disk block called the  indirect block  to extend the array of pointers at our disposal. We will only need this for files > 40KB  struct inode {\n int[10] directblocks; // if size<4KB then only the first one is valid\n int indirectblock; // valid value when size >= 40KB\n int size;\n ...\n}  The indirect block is just a regular disk block of 4096 bytes, but we will use it to hold pointers to disk blocks. Our pointers in this case are just integers, so we need to cast the pointer to an integer pointer:  int lookup_physical_block_index(inode*inode, int block_count) {\n  assert(sizeof(int)==4); // Warning this code assumes an index is 4 bytes!\n  assert(block_count>=0 && block_count < 1024 + 10); // 0 <= block_count< 1034\n\n  if( block_count < 10)\n     return inode->directblocks[ block_count ];\n\n  // read the indirect block from disk:\n  block_t* oneblock = & blocks[ inode->indirectblock ];\n\n  // Treat the 4KB as an array of 1024 pointers to other disk blocks\n  int* table = (int*) oneblock;\n\n // Look up the correct entry in the table\n // Offset by 10 because the first 10 blocks of data are already \n // accounted for\n  return table[ block_count - 10 ];\n}  For a typical filesystem, our index values are 32 bits i.e. 4bytes. Thus in 4096 bytes we can store 4096 / 4 = 1024 entries.\nThis means our indirect block can refer to 1024 * 4KB = 4MB of data. With the first ten direct blocks, we can therefore accommodate files up to 40KB + 1024 * 4KB= 4136KB . Some of the later table entries can be invalid for files that are smaller than this.   For even larger files, we could use two indirect blocks. However there's a better alternative, that will allow us to efficiently scale up to huge files. We will include a double-indirect pointer and if that's not enough a triple indirect pointer. The double indirect pointer means we have a table of 1024 entries to disk blocks that are used as 1024 entries. This means we can refer to 1024*1024 disk blocks of data.   (source: http://uw714doc.sco.com/en/FS_admin/graphics/s5chain.gif)  int lookup_physical_block_index(inode*inode, int block_count) {\n  if( block_count < 10)\n     return inode->directblocks[ block_count ];\n\n  // Use indirect block for the next 1024 blocks:\n  // Assumes 1024 ints can fit inside each block!\n  if( block_count < 1024 + 10) {   \n      int* table = (int*) & blocks[ inode->indirectblock ];\n      return table[ block_count - 10 ];\n  }\n  // For huge files we will use a table of tables\n  int i = (block_count - 1034) / 1024 , j = (block_count - 1034) % 1024;\n  assert(i<1024); // triple-indirect is not implemented here!\n\n  int* table1 = (int*) & blocks[ inode->doubleindirectblock ];\n   // The first table tells us where to read the second table ...\n  int* table2 = (int*) & blocks[   table1[i]   ];\n  return table2[j];\n\n   // For gigantic files we will need to implement triple-indirect (table of tables of tables)\n}  Notice that reading a byte using double indirect requires 3 disk block reads (two tables and the actual data block).",
            "title": "Please can you explain a simple model of how the file's content is stored in a simple i-node based filesystem?"
        },
        {
            "location": "/SystemProgramming/File-Systems-Review-Questions/",
            "text": "Topics\n\n\n\n\nSuperblock\n\n\nData Block\n\n\nInode\n\n\nRelative Path\n\n\nFile Metadata\n\n\nHard and Soft Links\n\n\nPermission Bits\n\n\nWorking with Directories\n\n\nVirtual File System\n\n\nReliable File Systems\n\n\nRAID\n\n\n\n\nQuestions\n\n\n\n\nHow big can files be on a file system with 15 Direct blocks, 2 double, 3 triple indirect, 4kb blocks and 4byte entries? (Assume enough infinite blocks)\n\n\nWhat is a superblock? Inode? Datablock?\n\n\nHow do I simplify \n/./proc/../dev/./random\n/\n\n\nIn ext2, what is stored in an inode, and what is stored in a directory entry?\n\n\nWhat are /sys, /proc, /dev/random, and /dev/urandom?\n\n\nWhat are permission bits?\n\n\nHow do you use chmod to set user/group/owner read/write/execute permissions?\n\n\nWhat does the \"dd\" command do?\n\n\nWhat is the difference between a hard link and a symbolic link? Does the file need to exist?\n\n\n\"ls -l\" shows the size of each file in a directory. Is the size stored in the directory or in the file's inode?",
            "title": "File Systems Review Questions"
        },
        {
            "location": "/SystemProgramming/File-Systems-Review-Questions/#topics",
            "text": "Superblock  Data Block  Inode  Relative Path  File Metadata  Hard and Soft Links  Permission Bits  Working with Directories  Virtual File System  Reliable File Systems  RAID",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/File-Systems-Review-Questions/#questions",
            "text": "How big can files be on a file system with 15 Direct blocks, 2 double, 3 triple indirect, 4kb blocks and 4byte entries? (Assume enough infinite blocks)  What is a superblock? Inode? Datablock?  How do I simplify  /./proc/../dev/./random /  In ext2, what is stored in an inode, and what is stored in a directory entry?  What are /sys, /proc, /dev/random, and /dev/urandom?  What are permission bits?  How do you use chmod to set user/group/owner read/write/execute permissions?  What does the \"dd\" command do?  What is the difference between a hard link and a symbolic link? Does the file need to exist?  \"ls -l\" shows the size of each file in a directory. Is the size stored in the directory or in the file's inode?",
            "title": "Questions"
        },
        {
            "location": "/SystemProgramming/Files,-Part-1:-Working-with-files/",
            "text": "Two types of files\n\n\nOn linux, there are two abstractions with files. The first is the linux \nfd\n level abstraction that means you can use\n\n \nopen\n\n\n \nread\n\n\n \nwrite\n\n\n \nclose\n\n\n \nlseek\n\n\n \nfcntl\n\n...\n\n\nAnd so on. The linux interface is very powerful and expressive, but sometimes we need portability (for example if we are writing for a mac or windows). This is where C's abstraction comes into play. On different operating systems, C uses the low level functions to create a wrapper around files you can use everywhere, meaning that C on linux uses the above calls. C has a few of the following\n\n \nfopen\n\n\n \nfread\n or \nfgetc/fgets\n or \nfscanf\n\n\n \nfwrite\n or \nfprintf\n\n\n \nfclose\n\n* \nfflush\n\n\nBut you don't get the expressiveness that linux gives you with system calls you can convert back and forth between them with \nint fileno(FILE* stream)\n and \nFILE* fdopen(int fd...)\n.\n\n\nAnother important aspect to note is the C files are \nbuffered\n meaning that there contents may not be written write away by default. You can can change that with C options.\n\n\nHow do I tell how large a file is?\n\n\nFor files less than the size of a long, using fseek and ftell is a simple way to accomplish this:\n\n\nMove to the end of the file and find out the current position.\n\n\nfseek(f, 0, SEEK_END);\nlong pos = ftell(f);\n\n\n\n\nThis tells us the current position in the file in bytes - i.e. the length of the file!\n\n\nfseek\n can also be used to set the absolute position.\n\n\nfseek(f, 0, SEEK_SET); // Move to the start of the file \nfseek(f, posn, SEEK_SET);  // Move to 'posn' in the file.\n\n\n\n\nAll future reads and writes in the parent or child processes will honor this position.\nNote writing or reading from the file will change the current position.\n\n\nSee the man pages for fseek and ftell for more information.\n\n\nBut try not to do this\n\n\nNote: This is not recommended in the usual case because of a quirk with the C language\n. That quirk is that longs only need to be \n4 Bytes big\n meaning that the maximum size that ftell can return is a little under 2 Gigabytes (which we know nowadays our files could be hundreds of gigabytes or even terabytes on a distributed file system). What should we do instead? Use \nstat\n! We will cover stat in a later part but here is some code that will tell you the size of the file\n\n\nstruct stat buf;\nif(stat(filename, &buf) != -1){\n    return -1;\n}\nreturn (ssize_t)buf.st_size;\n\n\n\n\nbuf.st_size is of type off_t which is big enough for \ninsanely\n large files.\n\n\nWhat happens if a child process closes a filestream using \nfclose\n or \nclose\n?\n\n\nClosing a file stream is unique to each process. Other processes can continue to use their own file-handle. Remember, everything is copied over when a child is created, even the relative positions of the files.\n\n\nHow about mmap for files?\n\n\nOne of the general uses for mmap is to map a file to memory. This does not mean that the file is malloc'ed to memory right away. Take the following code for example.\n\n\nint fd = open(...); //File is 2 Pages\nchar* addr = mmap(..fd..);\naddr[0] = 'l';\n\n\n\n\nThe kernel may say, \"okay I see that you want to mmap the file into memory, so I'll reserve some space in your address space that is the length of the file\". That means when you write to addr[0] that you are actually writing to the first byte of the file. The kernel can actually do some optimizations too. Instead of loading the file into memory, it may only load pages at a time because if the file is 1024 pages; you may only access 3 or 4 pages making loading the entire file a waste of time (that is why page faults are so powerful! They let the operating system take control of how much you use your files).\n\n\nFor every mmap\n\n\nRemember that once you are done \nmmap\nping that you \nmunmap\n to tell the operating system that you are no longer using the pages allocated, so the OS can write it back to disk and give you the addresses back in case you need to malloc later.",
            "title": "Files, Part 1: Working with files"
        },
        {
            "location": "/SystemProgramming/Files,-Part-1:-Working-with-files/#two-types-of-files",
            "text": "On linux, there are two abstractions with files. The first is the linux  fd  level abstraction that means you can use   open    read    write    close    lseek    fcntl \n...  And so on. The linux interface is very powerful and expressive, but sometimes we need portability (for example if we are writing for a mac or windows). This is where C's abstraction comes into play. On different operating systems, C uses the low level functions to create a wrapper around files you can use everywhere, meaning that C on linux uses the above calls. C has a few of the following   fopen    fread  or  fgetc/fgets  or  fscanf    fwrite  or  fprintf    fclose \n*  fflush  But you don't get the expressiveness that linux gives you with system calls you can convert back and forth between them with  int fileno(FILE* stream)  and  FILE* fdopen(int fd...) .  Another important aspect to note is the C files are  buffered  meaning that there contents may not be written write away by default. You can can change that with C options.",
            "title": "Two types of files"
        },
        {
            "location": "/SystemProgramming/Files,-Part-1:-Working-with-files/#how-do-i-tell-how-large-a-file-is",
            "text": "For files less than the size of a long, using fseek and ftell is a simple way to accomplish this:  Move to the end of the file and find out the current position.  fseek(f, 0, SEEK_END);\nlong pos = ftell(f);  This tells us the current position in the file in bytes - i.e. the length of the file!  fseek  can also be used to set the absolute position.  fseek(f, 0, SEEK_SET); // Move to the start of the file \nfseek(f, posn, SEEK_SET);  // Move to 'posn' in the file.  All future reads and writes in the parent or child processes will honor this position.\nNote writing or reading from the file will change the current position.  See the man pages for fseek and ftell for more information.",
            "title": "How do I tell how large a file is?"
        },
        {
            "location": "/SystemProgramming/Files,-Part-1:-Working-with-files/#but-try-not-to-do-this",
            "text": "Note: This is not recommended in the usual case because of a quirk with the C language . That quirk is that longs only need to be  4 Bytes big  meaning that the maximum size that ftell can return is a little under 2 Gigabytes (which we know nowadays our files could be hundreds of gigabytes or even terabytes on a distributed file system). What should we do instead? Use  stat ! We will cover stat in a later part but here is some code that will tell you the size of the file  struct stat buf;\nif(stat(filename, &buf) != -1){\n    return -1;\n}\nreturn (ssize_t)buf.st_size;  buf.st_size is of type off_t which is big enough for  insanely  large files.",
            "title": "But try not to do this"
        },
        {
            "location": "/SystemProgramming/Files,-Part-1:-Working-with-files/#what-happens-if-a-child-process-closes-a-filestream-using-fclose-or-close",
            "text": "Closing a file stream is unique to each process. Other processes can continue to use their own file-handle. Remember, everything is copied over when a child is created, even the relative positions of the files.",
            "title": "What happens if a child process closes a filestream using fclose or close?"
        },
        {
            "location": "/SystemProgramming/Files,-Part-1:-Working-with-files/#how-about-mmap-for-files",
            "text": "One of the general uses for mmap is to map a file to memory. This does not mean that the file is malloc'ed to memory right away. Take the following code for example.  int fd = open(...); //File is 2 Pages\nchar* addr = mmap(..fd..);\naddr[0] = 'l';  The kernel may say, \"okay I see that you want to mmap the file into memory, so I'll reserve some space in your address space that is the length of the file\". That means when you write to addr[0] that you are actually writing to the first byte of the file. The kernel can actually do some optimizations too. Instead of loading the file into memory, it may only load pages at a time because if the file is 1024 pages; you may only access 3 or 4 pages making loading the entire file a waste of time (that is why page faults are so powerful! They let the operating system take control of how much you use your files).",
            "title": "How about mmap for files?"
        },
        {
            "location": "/SystemProgramming/Files,-Part-1:-Working-with-files/#for-every-mmap",
            "text": "Remember that once you are done  mmap ping that you  munmap  to tell the operating system that you are no longer using the pages allocated, so the OS can write it back to disk and give you the addresses back in case you need to malloc later.",
            "title": "For every mmap"
        },
        {
            "location": "/SystemProgramming/Filesystem:-Review-Questions/",
            "text": "Question numbers subject to change\n\n\n\n\nQ1\n\n\nWrite a function that uses fseek and ftell to replace the middle character of a file with an 'X'\n\n\nvoid xout(char* filename) {\n  FILE *f = fopen(filename, ____ );\n\n\n\n}\n\n\n\n\nQ2\n\n\nIn an \next2\n filesystem how many inodes are read from disk to access the first byte of the file \n/dir1/subdirA/notes.txt\n ? Assume the directory names and inode numbers in the root directory (but not the inodes themselves) are already in memory.\n\n\nQ3\n\n\nIn an \next2\n filesystem what is the minimum number of disk blocks that must be read from disk to access the first byte of the file \n/dir1/subdirA/notes.txt\n ? Assume the directory names and inode numbers in the root directory and all inodes are already in memory.\n\n\nQ4\n\n\nIn an \next2\n filesystem with 32 bit addresses and 4KB disk blocks, an inodes that can store 10 direct disk block numbers. What is the minimum file size required to require an single indirection table? ii) a double direction table?\n\n\nQ5\n\n\nFix the shell command \nchmod\n below to set the permission of a file \nsecret.txt\n  so that the owner can read,write,and execute permissions the group can read and everyone else has no access.\n\n`\nchmod 000 secret.txt",
            "title": "Filesystem: Review Questions"
        },
        {
            "location": "/SystemProgramming/Filesystem:-Review-Questions/#q1",
            "text": "Write a function that uses fseek and ftell to replace the middle character of a file with an 'X'  void xout(char* filename) {\n  FILE *f = fopen(filename, ____ );\n\n\n\n}",
            "title": "Q1"
        },
        {
            "location": "/SystemProgramming/Filesystem:-Review-Questions/#q2",
            "text": "In an  ext2  filesystem how many inodes are read from disk to access the first byte of the file  /dir1/subdirA/notes.txt  ? Assume the directory names and inode numbers in the root directory (but not the inodes themselves) are already in memory.",
            "title": "Q2"
        },
        {
            "location": "/SystemProgramming/Filesystem:-Review-Questions/#q3",
            "text": "In an  ext2  filesystem what is the minimum number of disk blocks that must be read from disk to access the first byte of the file  /dir1/subdirA/notes.txt  ? Assume the directory names and inode numbers in the root directory and all inodes are already in memory.",
            "title": "Q3"
        },
        {
            "location": "/SystemProgramming/Filesystem:-Review-Questions/#q4",
            "text": "In an  ext2  filesystem with 32 bit addresses and 4KB disk blocks, an inodes that can store 10 direct disk block numbers. What is the minimum file size required to require an single indirection table? ii) a double direction table?",
            "title": "Q4"
        },
        {
            "location": "/SystemProgramming/Filesystem:-Review-Questions/#q5",
            "text": "Fix the shell command  chmod  below to set the permission of a file  secret.txt   so that the owner can read,write,and execute permissions the group can read and everyone else has no access. `\nchmod 000 secret.txt",
            "title": "Q5"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/",
            "text": "A word of warning\n\n\nProcess forking is a very powerful (and very dangerous) tool. If you mess up and cause a fork bomb (explained later on this page), \nyou can bring down the entire system\n. To reduce the chances of this, limit your maximum number of processes to a small number e.g\n 40 by typing \nulimit -u 40\n into a command line. Note that this limit is only for the user, which means if you fork bomb, then you won't be able to kill all of the processes you just created since calling \nkillall\n requires your shell to fork() ... ironic right? So what can we do about this. One solution is to spawn another shell instance as another user (for example root) before hand and kill processes from there. Another is to use the built in \nexec\n command to kill all the user processes (careful you only have one shot at this). Finally you could reboot the system :)\n\n\nWhen testing fork() code, ensure that you have either root and/or physical access to the machine involved. If you must work on fork () code remotely, remember that \nkill -9 -1\n will save you in the event of an emergency.\n\n\nTL;DR: Fork can be \nextremely\n dangerous if you aren't prepared for it. \nYou have been warned.\n\n\nIntro to Fork\n\n\nWhat does fork do?\n\n\nThe \nfork\n system call clones the current process to create a new process. It creates a new process (the child process) by duplicating the state of the existing process with a few minor differences (discussed below). The child process does not start from main. Instead it returns from \nfork()\n just as the parent process does.\n\n\nWhat is the simplest \nfork()\n example?\n\n\nHere's a very simple example...\n\n\nprintf(\"I'm printed once!\\n\");\nfork();\n// Now there are two processes running\n// and each process will print out the next line.\nprintf(\"You see this line twice!\\n\");\n\n\n\n\nWhy does this example print 42 twice?\n\n\nThe following program prints out 42 twice - but the \nfork()\n is after the \nprintf\n!? Why?\n\n\n#include <unistd.h> /*fork declared here*/\n#include <stdio.h> /* printf declared here*/\nint main() {\n   int answer = 84 >> 1;\n   printf(\"Answer: %d\", answer);\n   fork();\n   return 0;\n}\n\n\n\n\nThe \nprintf\n line \nis\n executed only once however notice that the printed contents is not flushed to standard out (there's no newline printed, we didn't call \nfflush\n, or change the buffering mode).\nThe output text is therefore still in process memory waiting to be sent.\nWhen \nfork()\n is executed the entire process memory is duplicated including the buffer. Thus the child process starts with a non-empty output buffer which will be flushed when the program exits.\n\n\nHow do you write code that is different for the parent and child process?\n\n\nCheck the return value of \nfork()\n. Return value \n-1\n = failed; \n0\n = in child process; positive = in parent process (and the return value is the child process id).  Here's one way to remember which is which:\n\n\nThe child process can find its parent - the original process that was duplicated -  by calling \ngetppid()\n - so does not need any additional return information from \nfork()\n. The parent process however can only find out the id of the new child process from the return value of \nfork\n:\n\n\npid_t id = fork();\nif (id == -1) exit(1); // fork failed \nif (id > 0)\n{ \n// I'm the original parent and \n// I just created a child process with id 'id'\n// Use waitpid to wait for the child to finish\n} else { // returned zero\n// I must be the newly made child process\n}\n\n\n\n\nWhat is a fork bomb ?\n\n\nA 'fork bomb' is when you attempt to create an infinite number of processes. A simple example is shown below:\n\n\nwhile (1) fork();\n\n\n\n\nThis will often bring a system to a near-standstill as it attempts to allocate CPU time and memory to a very large number of processes that are ready to run. Comment: System administrators don't like fork-bombs and may set upper limits on the number of processes each user can have or may revoke login rights because it creates a disturbance in the force for other users' programs. You can also limit the number of child processes created by using \nsetrlimit()\n.\n\n\nfork bombs are not necessarily malicious - they occasionally occur due to student coding errors.\n\n\nAngrave suggests that the Matrix trilogy, where the machine and man finally work together to defeat the multiplying Agent-Smith, was a cinematic plot based on an AI-driven fork-bomb.\n\n\nWaiting and Execing\n\n\nHow does the parent process wait for the child to finish?\n\n\nUse \nwaitpid\n (or \nwait\n).\n\n\npid_t child_id = fork();\nif (child_id == -1) { perror(\"fork\"); exit(EXIT_FAILURE);}\nif (child_id > 0) { \n  // We have a child! Get their exit code\n  int status; \n  waitpid( child_id, &status, 0 );\n  // code not shown to get exit status from child\n} else { // In child ...\n  // start calculation\n  exit(123);\n}\n\n\n\n\nCan I make the child process execute another program?\n\n\nYes. Use one of the \nexec\n functions after forking. The \nexec\n set of functions replaces the process image with the the process image of what is being called. This means that any lines of code after the \nexec\n call are replaced. Any other work you want the child process to do should be done before the \nexec\n call.  \n\n\nThe \nWikipedia article\n does a great job helping you make sense of the names of the exec family.\n\n\nThe naming schemes can be shortened like this\n\n\n\n\nThe base of each is exec (execute), followed by one or more letters:\n\n\ne \u2013 An array of pointers to environment variables is explicitly passed to the new process image.\n\n\nl \u2013 Command-line arguments are passed individually (a list) to the function.\n\n\np \u2013 Uses the PATH environment variable to find the file named in the file argument to be executed.\n\n\nv \u2013 Command-line arguments are passed to the function as an array (vector) of pointers.\n\n\n\n\n#include <unistd.h>\n#include <sys/types.h> \n#include <sys/wait.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint main(int argc, char**argv) {\n  pid_t child = fork();\n  if (child == -1) return EXIT_FAILURE;\n  if (child) { /* I have a child! */\n    int status;\n    waitpid(child , &status ,0);\n    return EXIT_SUCCESS;\n\n  } else { /* I am the child */\n    // Other versions of exec pass in arguments as arrays\n    // Remember first arg is the program name\n    // Last arg must be a char pointer to NULL\n\n    execl(\"/bin/ls\", \"ls\",\"-alh\", (char *) NULL);\n\n    // If we get to this line, something went wrong!\n    perror(\"exec failed!\");\n  }\n}\n\n\n\n\nA simpler way to execute another program\n\n\nUse \nsystem\n. Here is how to use it:\n\n\n\n#include <unistd.h>\n#include <stdlib.h>\n\nint main(int argc, char**argv) {\n  system(\"ls\");\n  return 0;\n}\n\n\n\n\nThe \nsystem\n call will fork, execute the command passed by parameter and the original parent process will wait for this to finish. This also means that \nsystem\n is a blocking call: The parent process can't continue until the process started by \nsystem\n exits. This may or may not be useful. Also, \nsystem\n actually creates a shell which is then given the string, which is more overhead than just using \nexec\n directly. The standard shell will use the \nPATH\n environment variable to search for a filename that matches the command. Using system will usually be sufficient for many simple run-this-command problems but can quickly become limiting for more complex or subtle problems, and it hides the mechanics of the fork-exec-wait pattern so we encourage you to learn and use \nfork\n \nexec\n and \nwaitpid\n instead.\n\n\nWhat is the silliest fork example?\n\n\nA slightly silly example is shown below. What will it print? Try it with multiple arguments to your program.\n\n\n#include <unistd.h>\n#include <stdio.h>\nint main(int argc, char **argv) {\n  pid_t id;\n  int status; \n  while (--argc && (id=fork())) {\n    waitpid(id,&status,0); /* Wait for child*/\n  }\n  printf(\"%d:%s\\n\", argc, argv[argc]);\n  return 0;\n}\n\n\n\n\nThe amazing parallel apparent-O(N) \nsleepsort\n is today's silly winner. First published on \n4chan in 2011 \n. A version of this awful but amusing sorting algorithm is shown below.\n\n\nint main(int c, char **v)\n{\n        while (--c > 1 && !fork());\n        int val  = atoi(v[c]);\n        sleep(val);\n        printf(\"%d\\n\", val);\n        return 0;\n}\n\n\n\n\nNote: The algorithm isn't actually O(N) because of how the system scheduler works. Though there are parallel algorithms that run in O(log(N)) per process, this is sadly not one of them.\n\n\nWhat is different in the child process than the parent process?\n\n\nThe key differences include:\n\n The process id returned by \ngetpid()\n. The parent process id returned by \ngetppid()\n.\n\n The parent is notified via a signal, SIGCHILD, when the child process finishes but not vice versa.\n* The child does not inherit pending signals or timer alarms.\nFor a complete list see the \nfork man page\n\n\nDo child processes share open filehandles?\n\n\nYes! In fact both processes use the same underlying kernel file descriptor. For example if one process rewinds the random access position back to the beginning of the file, then both processes are affected.\n\n\nBoth child and parent should \nclose\n (or \nfclose\n) their file descriptors or file handle respectively.\n\n\nHow can I find out more?\n\n\nRead the man pages!\n\n \nfork\n\n\n \nexec\n\n* \nwait\n\n\n\n\n\nBack: Processes, Part 1: Introduction\n\n |\n\n\nNext: Forking, Part 2: Fork, Exec, Wait",
            "title": "Forking, Part 1: Introduction"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#a-word-of-warning",
            "text": "Process forking is a very powerful (and very dangerous) tool. If you mess up and cause a fork bomb (explained later on this page),  you can bring down the entire system . To reduce the chances of this, limit your maximum number of processes to a small number e.g\n 40 by typing  ulimit -u 40  into a command line. Note that this limit is only for the user, which means if you fork bomb, then you won't be able to kill all of the processes you just created since calling  killall  requires your shell to fork() ... ironic right? So what can we do about this. One solution is to spawn another shell instance as another user (for example root) before hand and kill processes from there. Another is to use the built in  exec  command to kill all the user processes (careful you only have one shot at this). Finally you could reboot the system :)  When testing fork() code, ensure that you have either root and/or physical access to the machine involved. If you must work on fork () code remotely, remember that  kill -9 -1  will save you in the event of an emergency.  TL;DR: Fork can be  extremely  dangerous if you aren't prepared for it.  You have been warned.",
            "title": "A word of warning"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#intro-to-fork",
            "text": "",
            "title": "Intro to Fork"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#what-does-fork-do",
            "text": "The  fork  system call clones the current process to create a new process. It creates a new process (the child process) by duplicating the state of the existing process with a few minor differences (discussed below). The child process does not start from main. Instead it returns from  fork()  just as the parent process does.",
            "title": "What does fork do?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#what-is-the-simplest-fork-example",
            "text": "Here's a very simple example...  printf(\"I'm printed once!\\n\");\nfork();\n// Now there are two processes running\n// and each process will print out the next line.\nprintf(\"You see this line twice!\\n\");",
            "title": "What is the simplest fork() example?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#why-does-this-example-print-42-twice",
            "text": "The following program prints out 42 twice - but the  fork()  is after the  printf !? Why?  #include <unistd.h> /*fork declared here*/\n#include <stdio.h> /* printf declared here*/\nint main() {\n   int answer = 84 >> 1;\n   printf(\"Answer: %d\", answer);\n   fork();\n   return 0;\n}  The  printf  line  is  executed only once however notice that the printed contents is not flushed to standard out (there's no newline printed, we didn't call  fflush , or change the buffering mode).\nThe output text is therefore still in process memory waiting to be sent.\nWhen  fork()  is executed the entire process memory is duplicated including the buffer. Thus the child process starts with a non-empty output buffer which will be flushed when the program exits.",
            "title": "Why does this example print 42 twice?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#how-do-you-write-code-that-is-different-for-the-parent-and-child-process",
            "text": "Check the return value of  fork() . Return value  -1  = failed;  0  = in child process; positive = in parent process (and the return value is the child process id).  Here's one way to remember which is which:  The child process can find its parent - the original process that was duplicated -  by calling  getppid()  - so does not need any additional return information from  fork() . The parent process however can only find out the id of the new child process from the return value of  fork :  pid_t id = fork();\nif (id == -1) exit(1); // fork failed \nif (id > 0)\n{ \n// I'm the original parent and \n// I just created a child process with id 'id'\n// Use waitpid to wait for the child to finish\n} else { // returned zero\n// I must be the newly made child process\n}",
            "title": "How do you write code that is different for the parent and child process?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#what-is-a-fork-bomb",
            "text": "A 'fork bomb' is when you attempt to create an infinite number of processes. A simple example is shown below:  while (1) fork();  This will often bring a system to a near-standstill as it attempts to allocate CPU time and memory to a very large number of processes that are ready to run. Comment: System administrators don't like fork-bombs and may set upper limits on the number of processes each user can have or may revoke login rights because it creates a disturbance in the force for other users' programs. You can also limit the number of child processes created by using  setrlimit() .  fork bombs are not necessarily malicious - they occasionally occur due to student coding errors.  Angrave suggests that the Matrix trilogy, where the machine and man finally work together to defeat the multiplying Agent-Smith, was a cinematic plot based on an AI-driven fork-bomb.",
            "title": "What is a fork bomb ?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#waiting-and-execing",
            "text": "",
            "title": "Waiting and Execing"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#how-does-the-parent-process-wait-for-the-child-to-finish",
            "text": "Use  waitpid  (or  wait ).  pid_t child_id = fork();\nif (child_id == -1) { perror(\"fork\"); exit(EXIT_FAILURE);}\nif (child_id > 0) { \n  // We have a child! Get their exit code\n  int status; \n  waitpid( child_id, &status, 0 );\n  // code not shown to get exit status from child\n} else { // In child ...\n  // start calculation\n  exit(123);\n}",
            "title": "How does the parent process wait for the child to finish?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#can-i-make-the-child-process-execute-another-program",
            "text": "Yes. Use one of the  exec  functions after forking. The  exec  set of functions replaces the process image with the the process image of what is being called. This means that any lines of code after the  exec  call are replaced. Any other work you want the child process to do should be done before the  exec  call.    The  Wikipedia article  does a great job helping you make sense of the names of the exec family.  The naming schemes can be shortened like this   The base of each is exec (execute), followed by one or more letters:  e \u2013 An array of pointers to environment variables is explicitly passed to the new process image.  l \u2013 Command-line arguments are passed individually (a list) to the function.  p \u2013 Uses the PATH environment variable to find the file named in the file argument to be executed.  v \u2013 Command-line arguments are passed to the function as an array (vector) of pointers.   #include <unistd.h>\n#include <sys/types.h> \n#include <sys/wait.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint main(int argc, char**argv) {\n  pid_t child = fork();\n  if (child == -1) return EXIT_FAILURE;\n  if (child) { /* I have a child! */\n    int status;\n    waitpid(child , &status ,0);\n    return EXIT_SUCCESS;\n\n  } else { /* I am the child */\n    // Other versions of exec pass in arguments as arrays\n    // Remember first arg is the program name\n    // Last arg must be a char pointer to NULL\n\n    execl(\"/bin/ls\", \"ls\",\"-alh\", (char *) NULL);\n\n    // If we get to this line, something went wrong!\n    perror(\"exec failed!\");\n  }\n}",
            "title": "Can I make the child process execute another program?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#a-simpler-way-to-execute-another-program",
            "text": "Use  system . Here is how to use it:  \n#include <unistd.h>\n#include <stdlib.h>\n\nint main(int argc, char**argv) {\n  system(\"ls\");\n  return 0;\n}  The  system  call will fork, execute the command passed by parameter and the original parent process will wait for this to finish. This also means that  system  is a blocking call: The parent process can't continue until the process started by  system  exits. This may or may not be useful. Also,  system  actually creates a shell which is then given the string, which is more overhead than just using  exec  directly. The standard shell will use the  PATH  environment variable to search for a filename that matches the command. Using system will usually be sufficient for many simple run-this-command problems but can quickly become limiting for more complex or subtle problems, and it hides the mechanics of the fork-exec-wait pattern so we encourage you to learn and use  fork   exec  and  waitpid  instead.",
            "title": "A simpler way to execute another program"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#what-is-the-silliest-fork-example",
            "text": "A slightly silly example is shown below. What will it print? Try it with multiple arguments to your program.  #include <unistd.h>\n#include <stdio.h>\nint main(int argc, char **argv) {\n  pid_t id;\n  int status; \n  while (--argc && (id=fork())) {\n    waitpid(id,&status,0); /* Wait for child*/\n  }\n  printf(\"%d:%s\\n\", argc, argv[argc]);\n  return 0;\n}  The amazing parallel apparent-O(N)  sleepsort  is today's silly winner. First published on  4chan in 2011  . A version of this awful but amusing sorting algorithm is shown below.  int main(int c, char **v)\n{\n        while (--c > 1 && !fork());\n        int val  = atoi(v[c]);\n        sleep(val);\n        printf(\"%d\\n\", val);\n        return 0;\n}  Note: The algorithm isn't actually O(N) because of how the system scheduler works. Though there are parallel algorithms that run in O(log(N)) per process, this is sadly not one of them.",
            "title": "What is the silliest fork example?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#what-is-different-in-the-child-process-than-the-parent-process",
            "text": "The key differences include:  The process id returned by  getpid() . The parent process id returned by  getppid() .  The parent is notified via a signal, SIGCHILD, when the child process finishes but not vice versa.\n* The child does not inherit pending signals or timer alarms.\nFor a complete list see the  fork man page",
            "title": "What is different in the child process than the parent process?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#do-child-processes-share-open-filehandles",
            "text": "Yes! In fact both processes use the same underlying kernel file descriptor. For example if one process rewinds the random access position back to the beginning of the file, then both processes are affected.  Both child and parent should  close  (or  fclose ) their file descriptors or file handle respectively.",
            "title": "Do child processes share open filehandles?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-1:-Introduction/#how-can-i-find-out-more",
            "text": "Read the man pages!   fork    exec \n*  wait   \nBack: Processes, Part 1: Introduction  | \nNext: Forking, Part 2: Fork, Exec, Wait",
            "title": "How can I find out more?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/",
            "text": "The Pattern\n\n\nWhat does the following 'exec' example do?\n\n\n#include <unistd.h>\n#include <fcntl.h> // O_CREAT, O_APPEND etc. defined here\n\nint main() {\n   close(1); // close standard out\n   open(\"log.txt\", O_RDWR | O_CREAT | O_APPEND, S_IRUSR | S_IWUSR);\n   puts(\"Captain's log\");\n   chdir(\"/usr/include\");\n   // execl( executable,  arguments for executable including program name and NULL at the end)\n\n   execl(\"/bin/ls\", /* Remaining items sent to ls*/ \"/bin/ls\", \".\", (char *) NULL); // \"ls .\"\n   perror(\"exec failed\");\n   return 0; // Not expected\n}\n\n\n\n\nThere's no error checking in the above code (we assume close,open,chdir etc works as expected).\n\n open: will use the lowest available file descriptor (i.e. 1) ; so standard out now goes to the log file.\n\n chdir : Change the current directory to /usr/include\n\n execl : Replace the program image with /bin/ls and call its main() method\n\n perror : We don't expect to get here - if we did then exec failed.\n\n\nSubtle forkbomb bug\n\n\nWhat's wrong with this code\n\n\n#include <unistd.h>\n#define HELLO_NUMBER 10\n\nint main(){\n    pid_t children[HELLO_NUMBER];\n    int i;\n    for(i = 0; i < HELLO_NUMBER; i++){\n        pid_t child = fork();\n        if(child == -1){\n            break;\n        }\n        if(child == 0){ //I am the child\n             execlp(\"echo\", \"echo\", \"hello\" ,NULL);\n        }\n        else{\n            children[i] = child;\n        }\n    }\n\n    int j;\n    for(j = 0; j < i; j++){\n        waitpid(children[j], NULL, 0);\n    }\n    return 0;\n}\n\n\n\n\n\necho is not a command so we can't \nexec\n it. What does this mean? Instead of creating 10 processes we just created 2**10 processes, fork bombing our machine. How could we prevent this? Put an exit right after exec so in case exec fails we won't end up fork bombing our machine.\n\n\nWhat does the child inherit from the parent?\n\n\n\n\nOpen filehandles. If the parent later seeks, say, to the back to the beginning of the file then this will affect the child too (and vice versa). \n\n\nSignal handlers\n\n\nCurrent working directory\n\n\nEnvironment variables\n\n\n\n\nSee the \nfork man page\n for more details.\n\n\nWhat is different in the child process than the parent process?\n\n\nThe process id is different. In the child calling \ngetppid()\n (notice the two 'p's) will give the same result as calling getpid() in the parent. See the fork man page for more details.\n\n\nHow do I wait for my child to finish?\n\n\nUse \nwaitpid\n or \nwait\n. The parent process will pause until \nwait\n (or \nwaitpid\n) returns. Note this explanation glosses over the restarting discussion.\n\n\nWhat is the fork-exec-wait pattern\n\n\nA common programming pattern is to call \nfork\n followed by \nexec\n and \nwait\n. The original process calls fork, which creates a child process. The child process then uses exec to start execution of a new program. Meanwhile the parent uses \nwait\n (or \nwaitpid\n) to wait for the child process to finish.\nSee below for a complete code example.\n\n\nHow do I start a background process that runs as the same time?\n\n\nDon't wait for them! Your parent process can continue to execute code without having to wait for the child process. Note in practice background processes can also be disconnected from the parent's input and output streams by calling \nclose\n on the open file descriptors before calling exec.\n\n\nHowever child processes that finish before their parent finishes can become zombies. See the zombie page for more information.\n\n\nZombies\n\n\nGood parents don't let their children become zombies!\n\n\nWhen a child finishes (or terminates) it still takes up a slot in the kernel process table. \nOnly when the child has been 'waited on' will the slot be available again.\n\n\nA long running program could create many zombies by continually creating processes and never \nwait\n-ing for them.\n\n\nWhat would be effect of too many zombies?\n\n\nEventually there would be insufficient space in the kernel process table to create a new processes. Thus \nfork()\n would fail and could make the system difficult / impossible to use - for example just logging in requires a new process!\n\n\nWhat does the system do to help prevent zombies?\n\n\nOnce a process completes, any of its children will be assigned to \"init\" - the first process with pid of 1. Thus these children would see getppid() return a value of 1. These orphans will eventually finish and for a breif moment become a zombie. Fortunately, the init process automatically waits for all of its children, thus removing these zombies from the system.\n\n\nHow do I prevent zombies? (Warning: Simplified answer)\n\n\nWait on your child!\n\n\nwaitpid(child, &status, 0); // Clean up and wait for my child process to finish.\n\n\n\n\nNote we assume that the only reason to get a SIGCHLD event is that a child has finished (this is not quite true - see man page for more details).\n\n\nA robust implementation would also check for interrupted status and include the above in a loop.\nRead on for a discussion of a more robust implementation.\n\n\nHow can I asynchronously wait for my child using SIGCHLD? (ADVANCED)\n\n\nWarning: This section uses signals which we have not yet fully introduced.\nThe parent gets the signal SIGCHLD when a child completes, so the signal handler can wait on the process. A slightly simplified version is shown below.\n\n\npid_t child;\n\nvoid cleanup(int signal) {\n  int status;\n  waitpid(child, &status, 0);\n  write(1,\"cleanup!\\n\",9);\n}\nint main() {\n   // Register signal handler BEFORE the child can finish\n   signal(SIGCHLD, cleanup); // or better - sigaction\n   child = fork();\n   if (child == -1) { exit(EXIT_FAILURE);}\n\n   if (child == 0) { /* I am the child!*/\n     // Do background stuff e.g. call exec   \n   } else { /* I'm the parent! */\n      sleep(4); // so we can see the cleanup\n      puts(\"Parent is done\");\n   }\n   return 0;\n} \n\n\n\n\nThe above example however misses a couple of subtle points:\n\n More than one child may have finished but the parent will only get one SIGCHLD signal (signals are not queued)\n\n SIGCHLD signals can be sent for other reasons (e.g. a child process is temporarily stopped)\n\n\nA more robust code to reap zombies is shown below.\n\n\nvoid cleanup(int signal) {\n  int status;\n  while (waitpid((pid_t) (-1), 0, WNOHANG) > 0) {}\n}\n\n\n\n\nSo what are environment variables?\n\n\nEnvironment variables are variables that the system keeps for all processes to use. Your system has these set up right now! In Bash, you can check some of these\n\n\n$ echo $HOME\n/home/bhuvy\n$ echo $PATH\n/usr/local/sbin:/usr/bin:...\n\n\n\n\nHow would you get these in C/C++? You can use the \ngetenv\n and \nsetenv\n function\n\n\nchar* home = getenv(\"HOME\"); // Will return /home/bhuvy\nsetenv(\"HOME\", \"/home/bhuvan\", 1 /*set overwrite to true*/ );\n\n\n\n\nRight, so how do these environment variables mean anything to parent/child?\n\n\nWell each process gets its own dictionary of environment variables that are copied over to the child. Meaning, if the parent changes their environment variables it won't be transferred to the child and vice versa. This is important in the fork-exec-wait trilogy if you want to exec a program with different environment variables than your parent (or any other process).\n\n\nFor example, you can write a C program that loops through all of the time zones and executes the \ndate\n command to print out the date and time in all locals. Environment variables are used for all sorts of programs so modifying them is important.\n\n\n\n\n\nBack: Forking, Part 1: Introduction\n\n |\n\n\nNext: Process Control, Part 1: Wait macros, using signals",
            "title": "Forking, Part 2: Fork, Exec, Wait"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#the-pattern",
            "text": "",
            "title": "The Pattern"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#what-does-the-following-exec-example-do",
            "text": "#include <unistd.h>\n#include <fcntl.h> // O_CREAT, O_APPEND etc. defined here\n\nint main() {\n   close(1); // close standard out\n   open(\"log.txt\", O_RDWR | O_CREAT | O_APPEND, S_IRUSR | S_IWUSR);\n   puts(\"Captain's log\");\n   chdir(\"/usr/include\");\n   // execl( executable,  arguments for executable including program name and NULL at the end)\n\n   execl(\"/bin/ls\", /* Remaining items sent to ls*/ \"/bin/ls\", \".\", (char *) NULL); // \"ls .\"\n   perror(\"exec failed\");\n   return 0; // Not expected\n}  There's no error checking in the above code (we assume close,open,chdir etc works as expected).  open: will use the lowest available file descriptor (i.e. 1) ; so standard out now goes to the log file.  chdir : Change the current directory to /usr/include  execl : Replace the program image with /bin/ls and call its main() method  perror : We don't expect to get here - if we did then exec failed.",
            "title": "What does the following 'exec' example do?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#subtle-forkbomb-bug",
            "text": "What's wrong with this code  #include <unistd.h>\n#define HELLO_NUMBER 10\n\nint main(){\n    pid_t children[HELLO_NUMBER];\n    int i;\n    for(i = 0; i < HELLO_NUMBER; i++){\n        pid_t child = fork();\n        if(child == -1){\n            break;\n        }\n        if(child == 0){ //I am the child\n             execlp(\"echo\", \"echo\", \"hello\" ,NULL);\n        }\n        else{\n            children[i] = child;\n        }\n    }\n\n    int j;\n    for(j = 0; j < i; j++){\n        waitpid(children[j], NULL, 0);\n    }\n    return 0;\n}  echo is not a command so we can't  exec  it. What does this mean? Instead of creating 10 processes we just created 2**10 processes, fork bombing our machine. How could we prevent this? Put an exit right after exec so in case exec fails we won't end up fork bombing our machine.",
            "title": "Subtle forkbomb bug"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#what-does-the-child-inherit-from-the-parent",
            "text": "Open filehandles. If the parent later seeks, say, to the back to the beginning of the file then this will affect the child too (and vice versa).   Signal handlers  Current working directory  Environment variables   See the  fork man page  for more details.",
            "title": "What does the child inherit from the parent?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#what-is-different-in-the-child-process-than-the-parent-process",
            "text": "The process id is different. In the child calling  getppid()  (notice the two 'p's) will give the same result as calling getpid() in the parent. See the fork man page for more details.",
            "title": "What is different in the child process than the parent process?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#how-do-i-wait-for-my-child-to-finish",
            "text": "Use  waitpid  or  wait . The parent process will pause until  wait  (or  waitpid ) returns. Note this explanation glosses over the restarting discussion.",
            "title": "How do I wait for my child to finish?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#what-is-the-fork-exec-wait-pattern",
            "text": "A common programming pattern is to call  fork  followed by  exec  and  wait . The original process calls fork, which creates a child process. The child process then uses exec to start execution of a new program. Meanwhile the parent uses  wait  (or  waitpid ) to wait for the child process to finish.\nSee below for a complete code example.",
            "title": "What is the fork-exec-wait pattern"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#how-do-i-start-a-background-process-that-runs-as-the-same-time",
            "text": "Don't wait for them! Your parent process can continue to execute code without having to wait for the child process. Note in practice background processes can also be disconnected from the parent's input and output streams by calling  close  on the open file descriptors before calling exec.  However child processes that finish before their parent finishes can become zombies. See the zombie page for more information.",
            "title": "How do I start a background process that runs as the same time?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#zombies",
            "text": "",
            "title": "Zombies"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#good-parents-dont-let-their-children-become-zombies",
            "text": "When a child finishes (or terminates) it still takes up a slot in the kernel process table. \nOnly when the child has been 'waited on' will the slot be available again.  A long running program could create many zombies by continually creating processes and never  wait -ing for them.",
            "title": "Good parents don't let their children become zombies!"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#what-would-be-effect-of-too-many-zombies",
            "text": "Eventually there would be insufficient space in the kernel process table to create a new processes. Thus  fork()  would fail and could make the system difficult / impossible to use - for example just logging in requires a new process!",
            "title": "What would be effect of too many zombies?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#what-does-the-system-do-to-help-prevent-zombies",
            "text": "Once a process completes, any of its children will be assigned to \"init\" - the first process with pid of 1. Thus these children would see getppid() return a value of 1. These orphans will eventually finish and for a breif moment become a zombie. Fortunately, the init process automatically waits for all of its children, thus removing these zombies from the system.",
            "title": "What does the system do to help prevent zombies?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#how-do-i-prevent-zombies-warning-simplified-answer",
            "text": "Wait on your child!  waitpid(child, &status, 0); // Clean up and wait for my child process to finish.  Note we assume that the only reason to get a SIGCHLD event is that a child has finished (this is not quite true - see man page for more details).  A robust implementation would also check for interrupted status and include the above in a loop.\nRead on for a discussion of a more robust implementation.",
            "title": "How do I prevent zombies? (Warning: Simplified answer)"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#how-can-i-asynchronously-wait-for-my-child-using-sigchld-advanced",
            "text": "Warning: This section uses signals which we have not yet fully introduced.\nThe parent gets the signal SIGCHLD when a child completes, so the signal handler can wait on the process. A slightly simplified version is shown below.  pid_t child;\n\nvoid cleanup(int signal) {\n  int status;\n  waitpid(child, &status, 0);\n  write(1,\"cleanup!\\n\",9);\n}\nint main() {\n   // Register signal handler BEFORE the child can finish\n   signal(SIGCHLD, cleanup); // or better - sigaction\n   child = fork();\n   if (child == -1) { exit(EXIT_FAILURE);}\n\n   if (child == 0) { /* I am the child!*/\n     // Do background stuff e.g. call exec   \n   } else { /* I'm the parent! */\n      sleep(4); // so we can see the cleanup\n      puts(\"Parent is done\");\n   }\n   return 0;\n}   The above example however misses a couple of subtle points:  More than one child may have finished but the parent will only get one SIGCHLD signal (signals are not queued)  SIGCHLD signals can be sent for other reasons (e.g. a child process is temporarily stopped)  A more robust code to reap zombies is shown below.  void cleanup(int signal) {\n  int status;\n  while (waitpid((pid_t) (-1), 0, WNOHANG) > 0) {}\n}",
            "title": "How can I asynchronously wait for my child using SIGCHLD? (ADVANCED)"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#so-what-are-environment-variables",
            "text": "Environment variables are variables that the system keeps for all processes to use. Your system has these set up right now! In Bash, you can check some of these  $ echo $HOME\n/home/bhuvy\n$ echo $PATH\n/usr/local/sbin:/usr/bin:...  How would you get these in C/C++? You can use the  getenv  and  setenv  function  char* home = getenv(\"HOME\"); // Will return /home/bhuvy\nsetenv(\"HOME\", \"/home/bhuvan\", 1 /*set overwrite to true*/ );",
            "title": "So what are environment variables?"
        },
        {
            "location": "/SystemProgramming/Forking,-Part-2:-Fork,-Exec,-Wait/#right-so-how-do-these-environment-variables-mean-anything-to-parentchild",
            "text": "Well each process gets its own dictionary of environment variables that are copied over to the child. Meaning, if the parent changes their environment variables it won't be transferred to the child and vice versa. This is important in the fork-exec-wait trilogy if you want to exec a program with different environment variables than your parent (or any other process).  For example, you can write a C program that loops through all of the time zones and executes the  date  command to print out the date and time in all locals. Environment variables are used for all sorts of programs so modifying them is important.   \nBack: Forking, Part 1: Introduction  | \nNext: Process Control, Part 1: Wait macros, using signals",
            "title": "Right, so how do these environment variables mean anything to parent/child?"
        },
        {
            "location": "/SystemProgramming/HW0/",
            "text": "Welcome!\n\n\nIf you are taking CS241 you can submit this homework at this \nGoogle Form\n.\n\n\n// First can you guess which lyrics have been transformed into this C-like system code?\nchar q[] = \"Do you wanna build a C99 program?\";\n#define or \"go debugging with gdb?\"\nstatic unsigned int i = sizeof(or) != strlen(or);\nchar* ptr = \"lathe\"; size_t come = fprintf(stdout,\"%s door\", ptr+2);\nint away = ! (int) * \"\";\n\nint* shared = mmap(NULL, sizeof(int*), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\nmunmap(shared,sizeof(int*));\n\nif(!fork()) { execlp(\"man\",\"man\",\"-3\",\"ftell\", (char*)0); perror(\"failed\"); }\nif(!fork()) { execlp(\"make\",\"make\", \"snowman\", (char*)0); execlp(\"make\",\"make\", (char*)0)); }\n\nexit(0);\n\n\n\n\nSo you want to master System Programming? And get a better grade than B?\n\n\nint main(int argc, char** argv) {\n puts(\"Great! We have plenty of useful resources for you but it's up to you to\");\n puts(\"be an active learner and learn how to solve problems and debug code.\");\n puts(\"Bring your near-completed answers the problems below\");\n puts(\" to the first lab to show that you've been working on this\");\n printf(\"A few \\\"don't knows\\\" or \\\"unsure\\\" is fine for lab 1\"); \n puts(\"Warning; your peers will be working hard for this class\");\n puts(\"This is not CS225; you will be pushed much harder to\");\n puts(\" work things out on your own\");\n fprintf(stdout,\"the point is that this homework is a stepping stone to all future assignments\");\n char p[] = \"so you will want to clear up any confusions or misconceptions.\";\n write(1, p, strlen(p) );\n char buffer[1024];\n sprintf(buffer,\"For grading purposes this homework 0 will be graded as part of your lab %d work.\", 1);\n write(1, buffer, strlen(buffer));\n printf(\"Press Return to continue\\n\");\n read(0, buffer, sizeof(buffer));\n return 0;\n}\n\n\n\n\nWatch the videos and write up your answers to the following questions.\n\n\nhttp://cs-education.github.io/sys/\n\n\nThere is also the course wikibook - \n\n\nhttps://github.com/angrave/SystemProgramming/wiki\n\n\nQuestions? Comments? Use Piazza,\nhttps://piazza.com/illinois/spring2017/cs241/home\n\n\nThe in-browser virtual machine runs entirely in Javascript and is fastest in Chrome. Note the VM and any code you write is reset when you reload the page \nSo copy your code to a separate document.\n The post-video challenges (e.g. Haiku poem) are not part of homework 0. \n\n\nChapter 1\n\n\n\n\nHello World (System call style)\n\n\nWrite a program that uses \nwrite()\n to print out \"Hi! My name is \n\".\n\n\nHello Standard Error Stream\n\n\nWrite a program that uses \nwrite()\n to print out a triangle of height \nn\n to Standard Error\n\n\nn should be a variable and the triangle should look like this for n = 3\n```C\n\n\n\n*\n\n\n\n\n\n\n```\n\n\n\n\nWriting to files\n\n\nTake your program from \"Hello World\" and have it write to a file\n\n\nMake sure to to use some interesting flags and mode for \nopen()\n\n\nman 2 open\n is your friend\n\n\n\n\n\n\nNot everything is a system call\n\n\nTake your program from \"Writing to files\" and replace it with \nprintf()\n\n\nName some differences from \nwrite()\n and \nprintf()\n\n\n\n\nChapter 2\n\n\n\n\nNot all bytes are 8 bits?\n\n\nHow many bits are there in a byte?\n\n\nHow many bytes is a \nchar\n?\n\n\nTell me how many bytes the following are on your machine: \nint, double, float, long, long long\n\n\nFollow the int pointer\n\n\nOn a machine with 8 byte integers:\n  \nC\n  int main(){\n      int data[8];\n  }\n\n  If the address of data is \n0x7fbd9d40\n, then what is the address of \ndata+2\n?\n\n\nWhat is \ndata[3]\n equivalent to in C?\n\n\nsizeof\n character arrays, incrementing pointers\n\n\n\n\nRemember the type of a string constant \n\"abc\"\n is an array.\n  - Why does this segfault?\n  \nC\n  char *ptr = \"hello\";\n  *ptr = 'J';\n\n  - What does \nsizeof(\"Hello\\0World\")\n return?\n  - What does \nstrlen(\"Hello\\0World\")\n return?\n  - Give an example of X such that \nsizeof(X)\n is 3\n  - Give an example of Y such that \nsizeof(Y)\n might be 4 or 8 depending on the machine.\n\n\nChapter 3\n\n\n\n\nProgram arguments \nargc\n \nargv\n\n\nName me two ways to find the length of \nargv\n\n\nWhat is \nargv[0]\n\n\nEnvironment Variables\n\n\nWhere are the pointers to environment variables stored?\n\n\nString searching (Strings are just char arrays)\n\n\n\n\nOn a machine where pointers are 8 bytes and with the following code:\n  \nC\n  char *ptr = \"Hello\";\n  char array[] = \"Hello\";\n\n  What is the results of \nsizeof(ptr)\n and \nsizeof(array)\n? Explain why.\n\n\n\n\n\n\nLifetime of automatic variables\n\n\n\n\nWhat datastucture is managing the lifetime of automatic variables?\n\n\n\n\nChapter 4\n\n\n\n\nMemory allocation using \nmalloc\n, heap and time\n\n\nIf I want to use data after the lifetime of the function it was created in, then where should I put it and how do I put it there?\n\n\nFill in the blank. In a good C program: \"For every malloc there is a ___\".\n\n\nHeap allocation Gotchas\n\n\nName one reason \nmalloc\n can fail.\n\n\nName some differences between \ntime()\n and \nctime()\n\n\nWhat is wrong with this code snippet?\n  \nC\n  free(ptr);\n  free(ptr);\n\n\nWhat is wrong with this code snippet?\n  \nC\n  free(ptr);\n  printf(\"%s\\n\", ptr);\n\n\nHow can one avoid the previous 2 mistakes? \n\n\nstruct, typedefs and a linked list\n\n\nCreate a struct that represents a Person and typedef, so that \"struct Person\" can be replaced with a single word.\n\n\nA person should contain the following information: name, age, friends (pointer to an array of pointers to People).\n\n\n\n\n\n\nNow make two persons \"Agent Smith\" and \"Sonny Moore\" on the heap who are 128 and 256 years old respectively and are friends with each other.\n\n\nDuplicating strings, memory allocation and deallocation of structures\n\n\nCreate functions to create and destroy a Person (Person's and their names should live on the heap).\n\n\ncreate()\n should take a name and make a copy of the name and also an age. Use malloc to reserve sufficient memory. Be sure initialize all fields (why?).\n\n\ndestroy()\n should free up not only the memory of the person struct but also free all its attributes that are stored on the heap (the array if it exists and the string). Destroying one person however should not destroy any others.\n\n\n\n\n\n\n\n\nChapter 5\n\n\n\n\nReading characters, Trouble with gets\n\n\nWhat functions can be used for getting characters for \nstdin\n and writing them to \nstdout\n?\n\n\nName one issue with \ngets()\n\n\nIntroducing \nsscanf\n and friends\n\n\nWrite code that parses a the string \"Hello 5 World\" and initializes 3 variables to (\"Hello\", 5, \"World\") respectively.\n\n\ngetline\n is useful\n\n\nWhat does one need to define before using \ngetline()\n?\n\n\nWrite a C program to print out the content of a file line by line using \ngetline()\n\n\n\n\nC Development (A web search is useful here)\n\n\n\n\nWhat compiler flag is used to generate a debug build?\n\n\nYou modify the makefile to generate debug builds and type \nmake\n again. Explain why this is insufficient to generate a new build.\n\n\nAre tabs or spaces used in Makefiles?\n\n\nWhat are the differences between heap and stack memory?\n\n\nAre there other kinds of memory in a process?\n\n\n\n\nOptional (Just for fun)\n\n\n\n\nConvert your a song lyrics into System Programming and C code covered in this wiki book and share on Piazza.\n\n\nFind, in your opinion the best and worst C code on the web and post the link to Piazza\n\n\nWrite a short C program with a deliberate subtle C bug and post it on Piazza to see if others can spot your bug",
            "title": "HW0"
        },
        {
            "location": "/SystemProgramming/HW0/#welcome",
            "text": "If you are taking CS241 you can submit this homework at this  Google Form .  // First can you guess which lyrics have been transformed into this C-like system code?\nchar q[] = \"Do you wanna build a C99 program?\";\n#define or \"go debugging with gdb?\"\nstatic unsigned int i = sizeof(or) != strlen(or);\nchar* ptr = \"lathe\"; size_t come = fprintf(stdout,\"%s door\", ptr+2);\nint away = ! (int) * \"\";\n\nint* shared = mmap(NULL, sizeof(int*), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\nmunmap(shared,sizeof(int*));\n\nif(!fork()) { execlp(\"man\",\"man\",\"-3\",\"ftell\", (char*)0); perror(\"failed\"); }\nif(!fork()) { execlp(\"make\",\"make\", \"snowman\", (char*)0); execlp(\"make\",\"make\", (char*)0)); }\n\nexit(0);",
            "title": "Welcome!"
        },
        {
            "location": "/SystemProgramming/HW0/#so-you-want-to-master-system-programming-and-get-a-better-grade-than-b",
            "text": "int main(int argc, char** argv) {\n puts(\"Great! We have plenty of useful resources for you but it's up to you to\");\n puts(\"be an active learner and learn how to solve problems and debug code.\");\n puts(\"Bring your near-completed answers the problems below\");\n puts(\" to the first lab to show that you've been working on this\");\n printf(\"A few \\\"don't knows\\\" or \\\"unsure\\\" is fine for lab 1\"); \n puts(\"Warning; your peers will be working hard for this class\");\n puts(\"This is not CS225; you will be pushed much harder to\");\n puts(\" work things out on your own\");\n fprintf(stdout,\"the point is that this homework is a stepping stone to all future assignments\");\n char p[] = \"so you will want to clear up any confusions or misconceptions.\";\n write(1, p, strlen(p) );\n char buffer[1024];\n sprintf(buffer,\"For grading purposes this homework 0 will be graded as part of your lab %d work.\", 1);\n write(1, buffer, strlen(buffer));\n printf(\"Press Return to continue\\n\");\n read(0, buffer, sizeof(buffer));\n return 0;\n}",
            "title": "So you want to master System Programming? And get a better grade than B?"
        },
        {
            "location": "/SystemProgramming/HW0/#watch-the-videos-and-write-up-your-answers-to-the-following-questions",
            "text": "http://cs-education.github.io/sys/  There is also the course wikibook -   https://github.com/angrave/SystemProgramming/wiki  Questions? Comments? Use Piazza,\nhttps://piazza.com/illinois/spring2017/cs241/home  The in-browser virtual machine runs entirely in Javascript and is fastest in Chrome. Note the VM and any code you write is reset when you reload the page  So copy your code to a separate document.  The post-video challenges (e.g. Haiku poem) are not part of homework 0.",
            "title": "Watch the videos and write up your answers to the following questions."
        },
        {
            "location": "/SystemProgramming/HW0/#chapter-1",
            "text": "Hello World (System call style)  Write a program that uses  write()  to print out \"Hi! My name is  \".  Hello Standard Error Stream  Write a program that uses  write()  to print out a triangle of height  n  to Standard Error  n should be a variable and the triangle should look like this for n = 3\n```C  *    ```   Writing to files  Take your program from \"Hello World\" and have it write to a file  Make sure to to use some interesting flags and mode for  open()  man 2 open  is your friend    Not everything is a system call  Take your program from \"Writing to files\" and replace it with  printf()  Name some differences from  write()  and  printf()",
            "title": "Chapter 1"
        },
        {
            "location": "/SystemProgramming/HW0/#chapter-2",
            "text": "Not all bytes are 8 bits?  How many bits are there in a byte?  How many bytes is a  char ?  Tell me how many bytes the following are on your machine:  int, double, float, long, long long  Follow the int pointer  On a machine with 8 byte integers:\n   C\n  int main(){\n      int data[8];\n  } \n  If the address of data is  0x7fbd9d40 , then what is the address of  data+2 ?  What is  data[3]  equivalent to in C?  sizeof  character arrays, incrementing pointers   Remember the type of a string constant  \"abc\"  is an array.\n  - Why does this segfault?\n   C\n  char *ptr = \"hello\";\n  *ptr = 'J'; \n  - What does  sizeof(\"Hello\\0World\")  return?\n  - What does  strlen(\"Hello\\0World\")  return?\n  - Give an example of X such that  sizeof(X)  is 3\n  - Give an example of Y such that  sizeof(Y)  might be 4 or 8 depending on the machine.",
            "title": "Chapter 2"
        },
        {
            "location": "/SystemProgramming/HW0/#chapter-3",
            "text": "Program arguments  argc   argv  Name me two ways to find the length of  argv  What is  argv[0]  Environment Variables  Where are the pointers to environment variables stored?  String searching (Strings are just char arrays)   On a machine where pointers are 8 bytes and with the following code:\n   C\n  char *ptr = \"Hello\";\n  char array[] = \"Hello\"; \n  What is the results of  sizeof(ptr)  and  sizeof(array) ? Explain why.    Lifetime of automatic variables   What datastucture is managing the lifetime of automatic variables?",
            "title": "Chapter 3"
        },
        {
            "location": "/SystemProgramming/HW0/#chapter-4",
            "text": "Memory allocation using  malloc , heap and time  If I want to use data after the lifetime of the function it was created in, then where should I put it and how do I put it there?  Fill in the blank. In a good C program: \"For every malloc there is a ___\".  Heap allocation Gotchas  Name one reason  malloc  can fail.  Name some differences between  time()  and  ctime()  What is wrong with this code snippet?\n   C\n  free(ptr);\n  free(ptr);  What is wrong with this code snippet?\n   C\n  free(ptr);\n  printf(\"%s\\n\", ptr);  How can one avoid the previous 2 mistakes?   struct, typedefs and a linked list  Create a struct that represents a Person and typedef, so that \"struct Person\" can be replaced with a single word.  A person should contain the following information: name, age, friends (pointer to an array of pointers to People).    Now make two persons \"Agent Smith\" and \"Sonny Moore\" on the heap who are 128 and 256 years old respectively and are friends with each other.  Duplicating strings, memory allocation and deallocation of structures  Create functions to create and destroy a Person (Person's and their names should live on the heap).  create()  should take a name and make a copy of the name and also an age. Use malloc to reserve sufficient memory. Be sure initialize all fields (why?).  destroy()  should free up not only the memory of the person struct but also free all its attributes that are stored on the heap (the array if it exists and the string). Destroying one person however should not destroy any others.",
            "title": "Chapter 4"
        },
        {
            "location": "/SystemProgramming/HW0/#chapter-5",
            "text": "Reading characters, Trouble with gets  What functions can be used for getting characters for  stdin  and writing them to  stdout ?  Name one issue with  gets()  Introducing  sscanf  and friends  Write code that parses a the string \"Hello 5 World\" and initializes 3 variables to (\"Hello\", 5, \"World\") respectively.  getline  is useful  What does one need to define before using  getline() ?  Write a C program to print out the content of a file line by line using  getline()",
            "title": "Chapter 5"
        },
        {
            "location": "/SystemProgramming/HW0/#c-development-a-web-search-is-useful-here",
            "text": "What compiler flag is used to generate a debug build?  You modify the makefile to generate debug builds and type  make  again. Explain why this is insufficient to generate a new build.  Are tabs or spaces used in Makefiles?  What are the differences between heap and stack memory?  Are there other kinds of memory in a process?",
            "title": "C Development (A web search is useful here)"
        },
        {
            "location": "/SystemProgramming/HW0/#optional-just-for-fun",
            "text": "Convert your a song lyrics into System Programming and C code covered in this wiki book and share on Piazza.  Find, in your opinion the best and worst C code on the web and post the link to Piazza  Write a short C program with a deliberate subtle C bug and post it on Piazza to see if others can spot your bug",
            "title": "Optional (Just for fun)"
        },
        {
            "location": "/SystemProgramming/Home/",
            "text": "Welcome to Angrave's crowd-sourced System Programming wiki-book!\nThis wiki is being built by students and faculty from the University of Illinois and is a crowd-source authoring experiment by Lawrence Angrave from CS @ Illinois.\n\n\nRather than requiring an existing paper-based book this semester, we will build our own set of resources here.\n\n\n0. HW0/Resources\n\n\n\n\n[[HW0]]\n\n\n[[Informal Glossary of basic terms|#Informal Glossary]]\n\n\n[[#Piazza: When And How to Ask For Help]]\n\n\n[[ Programming Tricks, Part 1 ]]\n\n\n[[ System Programming Short Stories and Songs ]]\n\n\n\n\n1. Learning C\n\n\n\n\n[[C Programming, Part 1: Introduction]]\n\n\n[[C Programming, Part 2: Text Input And Output]]\n\n\nPrinting to Streams\n\n\nParsing Input\n\n\n[[C Programming, Part 3: Common Gotchas]]\n\n\nMemory Mistakes\n\n\nLogic/Programming Flow\n\n\nOther Gotchas\n\n\n[[C Programming, Part 4: Strings and Structs]]\n\n\nStrings\n\n\nStructs\n\n\n[[C Programming, Part 5: Debugging]]\n\n\nIn Code Debugging\n\n\nValgrind\n\n\nTsan\n\n\nGDB\n\n\n[[C Programming, Review Questions]]\n\n\n\n\n2. Processes\n\n\n\n\n[[Processes, Part 1: Introduction]]\n\n\nOverview\n\n\nProcess Contents\n\n\nBonus: More Contents\n\n\n[[Forking, Part 1: Introduction]]\n\n\nIntroduction\n\n\nWaiting and Execing\n\n\n[[Forking, Part 2: Fork, Exec, Wait]]\n\n\nThe Pattern\n\n\nZombies\n\n\n[[Process Control, Part 1: Wait macros, using signals]]\n\n\nWait Macros\n\n\nSignals\n\n\n[[Processes Review Questions]]\n\n\n\n\n3. Memory and Allocators\n\n\n\n\n[[Memory, Part 1: Heap Memory Introduction]]\n\n\nC Dynamic Memory Allocation\n\n\nIntroduction to Allocating\n\n\n[[Memory, Part 2: Implementing a Memory Allocator]]\n\n\nMemory Allocator Tutorial\n\n\n[[Memory, Part 3: Smashing the Stack Example]]\n\n\n[[Memory Review Questions]]\n\n\n\n\n4. Intro to Pthreads\n\n\n\n\n[[Pthreads, Part 1: Introduction]]\n\n\nIntro to Threads\n\n\nSimple Pthreads\n\n\n[[Pthreads, Part 2: Usage in Practice]]\n\n\nMore pthread Functions\n\n\nIntro to Race Conditions\n\n\n[[Pthreads, Part 3: Parallel Problems (Bonus) ]]\n\n\n[[Pthread Review Questions]]\n\n\n\n\n5. Synchronization\n\n\n\n\n[[Synchronization, Part 1: Mutex Locks]]\n\n\nSolving Critical Sections\n\n\nMutex Gotchas\n\n\n\n\n\n\n[[Synchronization, Part 2: Counting Semaphores]]\n\n\n[[Synchronization, Part 3: Working with Mutexes And Semaphores]]\n\n\nThread Safe Stack\n\n\nStack Semaphores\n\n\n\n\n\n\n[[Synchronization, Part 4: The Critical Section Problem]]\n\n\nCandidate Solutions\n\n\nWorking Solutions\n\n\nHardware Solutions\n\n\n\n\n\n\n[[Synchronization, Part 5: Condition Variables]]\n\n\nIntro To Condition Variables\n\n\nImplementing a Counting Semaphore\n\n\n\n\n\n\n[[Synchronization, Part 6: Implementing a barrier]]\n\n\n[[Synchronization, Part 7: The Reader Writer Problem]]\n\n\n[[Synchronization, Part 8: Ring Buffer Example]]\n\n\n[[Synchronization Review Questions]]\n\n\n\n\n6. Deadlock\n\n\n\n\n[[Deadlock, Part 1: Resource Allocation Graph]]\n\n\n[[Deadlock, Part 2: Deadlock Conditions]]\n\n\n[[Deadlock, Part 3: Dining Philosophers]]\n\n\nFailed Solutions\n\n\nViable Solutions\n\n\n\n\n\n\n[[Deadlock Review Questions]]\n\n\n\n\n7. Inter-process Communication & Scheduling\n\n\n\n\n[[Virtual Memory, Part 1: Introduction to Virtual Memory]]\n\n\nWhat is Virtual Memory?\n\n\nAdvanced Frames and Protections\n\n\n\n\n\n\n[[Pipes, Part 1: Introduction to pipes]]\n\n\n[[Pipes, Part 2: Pipe programming secrets]]\n\n\nPipe Gotchas\n\n\nNamed Pipes\n\n\n\n\n\n\n[[ Files, Part 1: Working with files]]\n\n\n[[ Scheduling, Part 1: Scheduling Processes ]]\n\n\nThinking about Scheduling\n\n\nMeasures of Efficiency\n\n\n\n\n\n\n[[ Scheduling, Part 2: Scheduling Processes: Algorithms ]]\n\n\n[[ IPC Review Questions]]\n\n\n\n\n8. Networking\n\n\n\n\n[[ POSIX, Part 1: Error handling]]\n\n\n[[ Networking, Part 1: Introduction]]\n\n\n[[ Networking, Part 2: Using getaddrinfo ]]\n\n\n[[ Networking, Part 3: Building a simple TCP Client ]]\n\n\n[[ Networking, Part 4: Building a simple TCP Server ]]\n\n\n[[ Networking, Part 5: Shutting down ports, reusing ports and other tricks ]]\n\n\n[[ Networking, Part 6: Creating a UDP server ]]\n\n\n[[ Networking, Part 7: Nonblocking I O, select(), and epoll ]]\n\n\n[[ RPC, Part 1: Introduction to Remote Procedure Calls ]]\n\n\n[[ Networking Review Questions ]]\n\n\n\n\n9. File Systems\n\n\n\n\n[[ File System, Part 1: Introduction ]]\n\n\nNavigation/Terminology\n\n\nWhat's a File System?\n\n\n\n\n\n\n[[ File System, Part 2: Files are inodes (everything else is just data...) ]]\n\n\n[[ File System, Part 3: Permissions ]]\n\n\n[[ File System, Part 4: Working with directories ]]\n\n\n[[ File System, Part 5: Virtual file systems ]]\n\n\n[[ File System, Part 6: Memory mapped files and Shared memory ]]\n\n\n[[ File System, Part 7: Scalable and Reliable Filesystems ]]\n\n\nReliability with a Single Disk\n\n\nRedundancy\n\n\n\n\n\n\n[[ File System, Part 8: Removing preinstalled malware from an Android device ]]\n\n\n[[ File System, Part 9: Disk blocks example ]]\n\n\n[[ File Systems Review Questions ]]\n\n\n\n\n10. Signals\n\n\n\n\n[[Process Control, Part 1: Wait macros, using signals]]\n\n\nWait Macros\n\n\nSignals\n\n\n[[ Signals, Part 2: Pending Signals and Signal Masks ]]\n\n\nSignals in Depth\n\n\nDisposition in Threads/Children\n\n\n[[ Signals, Part 3: Raising signals ]]\n\n\n[[ Signals, Part 4: Sigaction ]]\n\n\n[[ Signals Review Questions ]]\n\n\n\n\nExam Practice Questions\n\n\nWarning these are good practice but not comprehensive. The CS241 final assumes you fully understand and can apply all topics of the course. Questions will focus mostly but not entirely on topics that you have used in the lab and programming assignments.\n\n\n\n\n[[Exam Topics]]\n\n\n[[C Programming: Review Questions]]\n\n\n[[Multi-threaded Programming: Review Questions]]\n\n\n[[Synchronization Concepts: Review Questions]]\n\n\n[[Memory: Review Questions]]\n\n\n[[Pipe: Review Questions]]\n\n\n[[Filesystem: Review Questions]]\n\n\n[[Networking: Review Questions]]\n\n\n[[Signals: Review Questions]] (todo)\n\n\n[[System Programming Jokes]]",
            "title": "Home"
        },
        {
            "location": "/SystemProgramming/Home/#0-hw0resources",
            "text": "[[HW0]]  [[Informal Glossary of basic terms|#Informal Glossary]]  [[#Piazza: When And How to Ask For Help]]  [[ Programming Tricks, Part 1 ]]  [[ System Programming Short Stories and Songs ]]",
            "title": "0. HW0/Resources"
        },
        {
            "location": "/SystemProgramming/Home/#1-learning-c",
            "text": "[[C Programming, Part 1: Introduction]]  [[C Programming, Part 2: Text Input And Output]]  Printing to Streams  Parsing Input  [[C Programming, Part 3: Common Gotchas]]  Memory Mistakes  Logic/Programming Flow  Other Gotchas  [[C Programming, Part 4: Strings and Structs]]  Strings  Structs  [[C Programming, Part 5: Debugging]]  In Code Debugging  Valgrind  Tsan  GDB  [[C Programming, Review Questions]]",
            "title": "1. Learning C"
        },
        {
            "location": "/SystemProgramming/Home/#2-processes",
            "text": "[[Processes, Part 1: Introduction]]  Overview  Process Contents  Bonus: More Contents  [[Forking, Part 1: Introduction]]  Introduction  Waiting and Execing  [[Forking, Part 2: Fork, Exec, Wait]]  The Pattern  Zombies  [[Process Control, Part 1: Wait macros, using signals]]  Wait Macros  Signals  [[Processes Review Questions]]",
            "title": "2. Processes"
        },
        {
            "location": "/SystemProgramming/Home/#3-memory-and-allocators",
            "text": "[[Memory, Part 1: Heap Memory Introduction]]  C Dynamic Memory Allocation  Introduction to Allocating  [[Memory, Part 2: Implementing a Memory Allocator]]  Memory Allocator Tutorial  [[Memory, Part 3: Smashing the Stack Example]]  [[Memory Review Questions]]",
            "title": "3. Memory and Allocators"
        },
        {
            "location": "/SystemProgramming/Home/#4-intro-to-pthreads",
            "text": "[[Pthreads, Part 1: Introduction]]  Intro to Threads  Simple Pthreads  [[Pthreads, Part 2: Usage in Practice]]  More pthread Functions  Intro to Race Conditions  [[Pthreads, Part 3: Parallel Problems (Bonus) ]]  [[Pthread Review Questions]]",
            "title": "4. Intro to Pthreads"
        },
        {
            "location": "/SystemProgramming/Home/#5-synchronization",
            "text": "[[Synchronization, Part 1: Mutex Locks]]  Solving Critical Sections  Mutex Gotchas    [[Synchronization, Part 2: Counting Semaphores]]  [[Synchronization, Part 3: Working with Mutexes And Semaphores]]  Thread Safe Stack  Stack Semaphores    [[Synchronization, Part 4: The Critical Section Problem]]  Candidate Solutions  Working Solutions  Hardware Solutions    [[Synchronization, Part 5: Condition Variables]]  Intro To Condition Variables  Implementing a Counting Semaphore    [[Synchronization, Part 6: Implementing a barrier]]  [[Synchronization, Part 7: The Reader Writer Problem]]  [[Synchronization, Part 8: Ring Buffer Example]]  [[Synchronization Review Questions]]",
            "title": "5. Synchronization"
        },
        {
            "location": "/SystemProgramming/Home/#6-deadlock",
            "text": "[[Deadlock, Part 1: Resource Allocation Graph]]  [[Deadlock, Part 2: Deadlock Conditions]]  [[Deadlock, Part 3: Dining Philosophers]]  Failed Solutions  Viable Solutions    [[Deadlock Review Questions]]",
            "title": "6. Deadlock"
        },
        {
            "location": "/SystemProgramming/Home/#7-inter-process-communication-scheduling",
            "text": "[[Virtual Memory, Part 1: Introduction to Virtual Memory]]  What is Virtual Memory?  Advanced Frames and Protections    [[Pipes, Part 1: Introduction to pipes]]  [[Pipes, Part 2: Pipe programming secrets]]  Pipe Gotchas  Named Pipes    [[ Files, Part 1: Working with files]]  [[ Scheduling, Part 1: Scheduling Processes ]]  Thinking about Scheduling  Measures of Efficiency    [[ Scheduling, Part 2: Scheduling Processes: Algorithms ]]  [[ IPC Review Questions]]",
            "title": "7. Inter-process Communication &amp; Scheduling"
        },
        {
            "location": "/SystemProgramming/Home/#8-networking",
            "text": "[[ POSIX, Part 1: Error handling]]  [[ Networking, Part 1: Introduction]]  [[ Networking, Part 2: Using getaddrinfo ]]  [[ Networking, Part 3: Building a simple TCP Client ]]  [[ Networking, Part 4: Building a simple TCP Server ]]  [[ Networking, Part 5: Shutting down ports, reusing ports and other tricks ]]  [[ Networking, Part 6: Creating a UDP server ]]  [[ Networking, Part 7: Nonblocking I O, select(), and epoll ]]  [[ RPC, Part 1: Introduction to Remote Procedure Calls ]]  [[ Networking Review Questions ]]",
            "title": "8. Networking"
        },
        {
            "location": "/SystemProgramming/Home/#9-file-systems",
            "text": "[[ File System, Part 1: Introduction ]]  Navigation/Terminology  What's a File System?    [[ File System, Part 2: Files are inodes (everything else is just data...) ]]  [[ File System, Part 3: Permissions ]]  [[ File System, Part 4: Working with directories ]]  [[ File System, Part 5: Virtual file systems ]]  [[ File System, Part 6: Memory mapped files and Shared memory ]]  [[ File System, Part 7: Scalable and Reliable Filesystems ]]  Reliability with a Single Disk  Redundancy    [[ File System, Part 8: Removing preinstalled malware from an Android device ]]  [[ File System, Part 9: Disk blocks example ]]  [[ File Systems Review Questions ]]",
            "title": "9. File Systems"
        },
        {
            "location": "/SystemProgramming/Home/#10-signals",
            "text": "[[Process Control, Part 1: Wait macros, using signals]]  Wait Macros  Signals  [[ Signals, Part 2: Pending Signals and Signal Masks ]]  Signals in Depth  Disposition in Threads/Children  [[ Signals, Part 3: Raising signals ]]  [[ Signals, Part 4: Sigaction ]]  [[ Signals Review Questions ]]",
            "title": "10. Signals"
        },
        {
            "location": "/SystemProgramming/Home/#exam-practice-questions",
            "text": "Warning these are good practice but not comprehensive. The CS241 final assumes you fully understand and can apply all topics of the course. Questions will focus mostly but not entirely on topics that you have used in the lab and programming assignments.   [[Exam Topics]]  [[C Programming: Review Questions]]  [[Multi-threaded Programming: Review Questions]]  [[Synchronization Concepts: Review Questions]]  [[Memory: Review Questions]]  [[Pipe: Review Questions]]  [[Filesystem: Review Questions]]  [[Networking: Review Questions]]  [[Signals: Review Questions]] (todo)  [[System Programming Jokes]]",
            "title": "Exam Practice Questions"
        },
        {
            "location": "/SystemProgramming/IPC-Review-Questions/",
            "text": "Topics\n\n\nVirtual Memory\nPage Table\nMMU/TLB\nAddress Translation\nPage Faults\nFrames/Pages\nSingle level vs multi level page table\nCalculating offsets for multi-level page table\nPipes\nPipe read write ends\nWriting to a zero reader pipe\nReading from a zero writer pipe\nNamed pipe and Unnamed Pipes\nBuffer Size/Atomicity\nScheduling Algorithms\nMeasures of Efficiency\n\n\nQuestions\n\n\n\n\nWhat is virtual memory?\n\n\nWhat are the following and what is their purpose?\n\n\nTranslation Lookaside Buffer\n\n\nPhysical Address\n\n\nMemory Management Unit. Multilevel page table. Frame number. Page number and page offset.\n\n\nThe dirty bit\n\n\nThe NX Bit\n\n\n\n\n\n\nWhat is a page table? How about a physical frame? Does a page always need to point to a physical frame?\n\n\nWhat is a page fault? What are the types? When does it result in a segfault?\n\n\nWhat are the advantages to a single level page table? Disadvantages? How about a multi leveled table?\n\n\nWhat does a multi leveled table look like in memory?\n\n\nHow do you determine how many bits are used in the page offset?\n\n\nGiven a 64 bit address space, 4kb pages and frames, and a 3 level page table, how many bits is the Virtual page number 1, VPN2, VPN3 and the offset?\n\n\nWhat is a pipe? How do I create a pipe?\n\n\nWhen is SIGPIPE delivered to a process?\n\n\nUnder what conditions will calling read() on a pipe block? Under what conditions will read() immediately return 0\n\n\nWhat is the difference between a named pipe and an unnamed pipe?\n\n\nIs a pipe thread safe?\n\n\nWrite a function that uses fseek and ftell to replace the middle character of a file with an 'X'\n\n\nWrite a function that create a pipe and uses write to send 5 bytes, \"HELLO\" to the pipe. Return the read file descriptor of the pipe.\n\n\nWhat happens when you mmap a file?\n\n\nWhy is getting the file size with ftell not recommended? How should you do it instead?\n\n\nWhat is scheduling?\n\n\nWhat is turnaround time? Response Time? Wait time?\n\n\nWhat is the convoy effect?\n\n\nWhich algorithms have the best turnaround/response/wait time on average",
            "title": "IPC Review Questions"
        },
        {
            "location": "/SystemProgramming/IPC-Review-Questions/#topics",
            "text": "Virtual Memory\nPage Table\nMMU/TLB\nAddress Translation\nPage Faults\nFrames/Pages\nSingle level vs multi level page table\nCalculating offsets for multi-level page table\nPipes\nPipe read write ends\nWriting to a zero reader pipe\nReading from a zero writer pipe\nNamed pipe and Unnamed Pipes\nBuffer Size/Atomicity\nScheduling Algorithms\nMeasures of Efficiency",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/IPC-Review-Questions/#questions",
            "text": "What is virtual memory?  What are the following and what is their purpose?  Translation Lookaside Buffer  Physical Address  Memory Management Unit. Multilevel page table. Frame number. Page number and page offset.  The dirty bit  The NX Bit    What is a page table? How about a physical frame? Does a page always need to point to a physical frame?  What is a page fault? What are the types? When does it result in a segfault?  What are the advantages to a single level page table? Disadvantages? How about a multi leveled table?  What does a multi leveled table look like in memory?  How do you determine how many bits are used in the page offset?  Given a 64 bit address space, 4kb pages and frames, and a 3 level page table, how many bits is the Virtual page number 1, VPN2, VPN3 and the offset?  What is a pipe? How do I create a pipe?  When is SIGPIPE delivered to a process?  Under what conditions will calling read() on a pipe block? Under what conditions will read() immediately return 0  What is the difference between a named pipe and an unnamed pipe?  Is a pipe thread safe?  Write a function that uses fseek and ftell to replace the middle character of a file with an 'X'  Write a function that create a pipe and uses write to send 5 bytes, \"HELLO\" to the pipe. Return the read file descriptor of the pipe.  What happens when you mmap a file?  Why is getting the file size with ftell not recommended? How should you do it instead?  What is scheduling?  What is turnaround time? Response Time? Wait time?  What is the convoy effect?  Which algorithms have the best turnaround/response/wait time on average",
            "title": "Questions"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/",
            "text": "C Dynamic Memory Allocation\n\n\nWhat happens when I call malloc?\n\n\nThe function \nmalloc\n is a C library call and is used to reserve a contiguous block of memory. Unlike stack memory, the memory remains allocated until \nfree\n is called with the same pointer. There is also \ncalloc\n and \nrealloc\n which are discussed below.\n\n\nCan malloc fail?\n\n\nIf \nmalloc\n fails to reserve any more memory then it returns \nNULL\n. Robust programs should check the return value. If your code assumes \nmalloc\n succeeds and it does not, then your program will likely crash (segfault) when it tries to write to address 0.\n\n\nWhere is the heap and how big is it?\n\n\nThe heap is part of the process memory and it does not have a fixed size. Heap memory allocation is performed by the C library when you call \nmalloc\n (\ncalloc\n, \nrealloc\n) and \nfree\n.\n\n\nFirst a quick review on process memory: A process is a running instance of your program. Each process has its own address space. For example on a 32 bit machine your process gets about 4 billion addresses to play with, however not all of these are valid or even mapped to actual physical memory (RAM). Inside the process's memory you will find the executable code, space for the stack, environment variables, global (static) variables and the heap.\n\n\nBy calling \nsbrk\n the C library can increase the size of the heap as your program demands more heap memory. As the heap and stack (one for each thread) need to grow, we put them at opposite ends of the address space. So for typical architectures the heap will grow upwards and the stack grows downwards. \n\n\nTruthiness: Modern operating system memory allocators no longer need \nsbrk\n - instead they can request independent regions of virtual memory and maintain multiple memory regions. For example gigabyte requests may be placed in a different memory region than small allocation requests. However this detail is an unwanted complexity: The problems of fragmentation and allocating memory efficiently still apply, so we will ignore this implementation nicety here and will write as if the heap is a single region.\n\n\nIf we write a multi-threaded program (more about that later) we will need multiple stacks (one per thread) but there's only ever one heap.\n\n\nOn typical architectures, the heap is part of the \nData segment\n and starts just above the code and global variables. \n\n\nDo programs need to call brk or sbrk?\n\n\nNot typically (though calling \nsbrk(0)\n can be interesting because it tells you where your heap currently ends). Instead programs use \nmalloc,calloc,realloc\n and \nfree\n which are part of the C library. The internal implementation of these functions will call \nsbrk\n when additional heap memory is required.\n\n\nvoid *top_of_heap = sbrk(0);\nmalloc(16384);\nvoid *top_of_heap2 = sbrk(0);\nprintf(\"The top of heap went from %p to %p \\n\", top_of_heap, top_of_heap2);\n\n\n\n\nExample output: \nThe top of heap went from 0x4000 to 0xa000\n\n\nWhat is calloc?\n\n\nUnlike \nmalloc\n, \ncalloc\n initializes memory contents to zero and also takes two arguments (the number of items and the size in bytes of each item). A naive but readable implementation of \ncalloc\n looks like this:\n\n\nvoid *calloc(size_t n, size_t size)\n{\n    size_t total = n * size; // Does not check for overflow!\n    void *result = malloc(total);\n\n    if (!result) return NULL;\n\n// If we're using new memory pages \n// just allocated from the system by calling sbrk\n// then they will be zero so zero-ing out is unnecessary,\n\n    memset(result, 0, total);\n    return result; \n}\n\n\n\n\nAn advanced discussion of these limitations is \nhere\n.\n\n\nProgrammers often use \ncalloc\n rather than explicitly calling \nmemset\n after \nmalloc\n, to set the memory contents to zero. Note \ncalloc(x,y)\n is identical to \ncalloc(y,x)\n, but you should follow the conventions of the manual.\n\n\n// Ensure our memory is initialized to zero\nlink_t *link  = malloc(256);\nmemset(link, 0, 256); // Assumes malloc returned a valid address!\n\nlink_t *link = calloc(1, 256); // safer: calloc(1, sizeof(link_t));\n\n\n\n\nWhy is the memory that is first returned by sbrk initialized to zero?\n\n\nIf the operating system did not zero out contents of physical RAM it might be possible for one process to learn about the memory of another process that had previously used the memory. This would be a security leak.\n\n\nUnfortunately this means that for \nmalloc\n requests before any memory has been freed and simple programs (which end up using newly reserved memory from the system) the memory is \noften\n zero. Then programmers mistaken write C programs that assume malloc'd memory will \nalways\n be zero.\n\n\nchar* ptr = malloc(300);\n// contents is probably zero because we get brand new memory\n// so beginner programs appear to work!\n// strcpy(ptr, \"Some data\"); // work with the data\nfree(ptr);\n// later\nchar *ptr2 = malloc(308); // Contents might now contain existing data and is probably not zero\n\n\n\n\nWhy doesn't malloc always initialize memory to zero?\n\n\nPerformance! We want malloc to be as fast as possible. Zeroing out memory may be unnecessary.\n\n\nWhat is realloc and when would you use it?\n\n\nrealloc\n allows you to resize an existing memory allocation that was previously allocated on the heap (via malloc,calloc or realloc). The most common use of realloc is to resize memory used to hold an array of values. A naive but readable version of realloc is suggested below\n\n\nvoid * realloc(void * ptr, size_t newsize) {\n  // Simple implementation always reserves more memory\n  // and has no error checking\n  void *result = malloc(newsize); \n  size_t oldsize =  ... //(depends on allocator's internal data structure)\n  if (ptr) memcpy(result, ptr, newsize < oldsize ? newsize : oldsize);\n  free(ptr);\n  return result;\n}\n\n\n\n\nAn INCORRECT use of realloc is shown below:\n\n\nint *array = malloc(sizeof(int) * 2);\narray[0] = 10; array[1] = 20;\n// Ooops need a bigger array - so use realloc..\nrealloc (array, 3); // ERRORS!\narray[2] = 30; \n\n\n\n\nThe above code contains two mistakes. Firstly we needed 3*sizeof(int) bytes not 3 bytes.\nSecondly realloc may need to move the existing contents of the memory to a new location. For example, there may not be sufficient space because the neighboring bytes are already allocated. A correct use of realloc is shown below.\n\n\narray = realloc(array, 3 * sizeof(int));\n// If array is copied to a new location then old allocation will be freed.\n\n\n\n\nA robust version would also check for a \nNULL\n return value. Note \nrealloc\n can be used to grow and shrink allocations. \n\n\nWhere can I read more?\n\n\nSee \nthe man page\n!\n\n\nHow important is that memory allocation is fast?\n\n\nVery! Allocating and de-allocating heap memory is a common operation in most applications.\n\n\nIntro to Allocating\n\n\nWhat is the silliest malloc and free implementation and what is wrong with it?\n\n\nvoid* malloc(size_t size)\n// Ask the system for more bytes by extending the heap space. \n// sbrk Returns -1 on failure\n   void *p = sbrk(size); \n   if(p == (void *) -1) return NULL; // No space left\n   return p;\n}\nvoid free() {/* Do nothing */}\n\n\n\n\nThe above implementation suffers from two major drawbacks:\n\n System calls are slow (compared to library calls). We should reserve a large amount of memory and only occasionally ask for more from the system.\n\n No reuse of freed memory. Our program never re-uses heap memory - it just keeps asking for a bigger heap.\n\n\nIf this allocator was used in a typical program, the process would quickly exhaust all available memory.\nInstead we need an allocator that can efficiently use heap space and only ask for more memory when necessary.\n\n\nWhat are placement strategies?\n\n\nDuring program execution memory is allocated and de-allocated (freed), so there will be gaps (holes) in the heap memory that can be re-used for future memory requests. The memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available.\n\n\nSuppose our current heap size is 64K, though not all of it is in use because some earlier malloc'd memory has already been freed by the program: \n\n\n\n\n\n\n\n\n16KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n30KB free\n\n\n4KB allocated\n\n\n2KB free\n\n\n\n\n\n\n\n\n\n\nIf a new malloc request for 2KB is executed (\nmalloc(2048)\n), where should \nmalloc\n reserve the memory? It could use the last 2KB hole (which happens to be the perfect size!) or it could split one of the other two free holes. These choices represent different placement strategies.\n\n\nWhichever hole is chosen, the allocator will need to split the hole into two: The newly allocated space (which will be returned to the program) and a smaller hole (if there is spare space left over).\n\n\nA perfect-fit strategy finds the smallest hole that is of sufficient size (at least 2KB):\n\n\n\n\n\n\n\n\n16KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n30KB free\n\n\n4KB allocated\n\n\n2KB HERE!\n\n\n\n\n\n\n\n\n\n\n\n\nA worst-fit strategy finds the largest hole that is of sufficient size (so break the 30KB hole into two):\n\n\n\n\n\n\n\n\n16KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n2KB HERE!\n\n\n28KB free\n\n\n4KB allocated\n\n\n2KB free\n\n\n\n\n\n\n\n\n\n\nA first-fit strategy finds the first available hole that is of sufficient size (break the 16KB hole into two):\n\n\n\n\n\n\n\n\n2KB HERE!\n\n\n14KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n30KB free\n\n\n4KB allocated\n\n\n2KB free\n\n\n\n\n\n\n\n\n\n\nWhat is external fragmentation?\n\n\nIn the example below, of the 64KB of heap memory, 17KB is allocated, and 47KB is free. However the largest available block is only 30KB because our available unallocated heap memory is fragmented into smaller pieces. \n\n\n\n\n\n\n\n\n16KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n30KB free\n\n\n4KB allocated\n\n\n2KB free\n\n\n\n\n\n\n\n\n\n\nWhat effect do placement strategies have on external fragmentation and performance?\n\n\nDifferent strategies affect the fragmentation of heap memory in non-obvious ways, which only are discovered by mathematical analysis or careful simulations under real-world conditions (for example simulating the memory allocation requests of a database or webserver).\nFor example, best-fit at first glance appears to be an excellent strategy however, if we can not find a perfectly-sized hole then this placement creates many tiny unusable holes, leading to high fragmentation. It also requires a scan of all possible holes.\n\n\nFirst fit has the advantage that it will not evaluate all possible placements and therefore be faster. \n\n\nSince Worst-fit targets the largest unallocated space, it is a poor choice if large allocations are required.\n\n\nIn practice first-fit and next-fit (which is not discussed here) are often common placement strategy. Hybrid approaches and many other alternatives exist (see implementing a memory allocator page).\n\n\nWhat are the challenges of writing a heap allocator?\n\n\nThe main challenges are,\n\n Need to minimize fragmentation (i.e. maximize memory utilization)\n\n Need high performance\n* Fiddly implementation (lots of pointer manipulation using linked lists and pointer arithmetic)\n\n\nSome additional comments:\n\n\nBoth fragmentation and performance depend on the application allocation profile, which can be evaluated but not predicted and in practice, under-specific usage conditions, a special-purpose allocator can often out-perform a general purpose implementation.\n\n\nThe allocator doesn't know the program's memory allocation requests in advance. Even if we did, this is the \nKnapsack problem\n which is known to be NP hard!\n\n\nHow do you implement a memory allocator?\n\n\nGood question. \nImplementing a memory allocator",
            "title": "Memory, Part 1: Heap Memory Introduction"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#c-dynamic-memory-allocation",
            "text": "",
            "title": "C Dynamic Memory Allocation"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#what-happens-when-i-call-malloc",
            "text": "The function  malloc  is a C library call and is used to reserve a contiguous block of memory. Unlike stack memory, the memory remains allocated until  free  is called with the same pointer. There is also  calloc  and  realloc  which are discussed below.",
            "title": "What happens when I call malloc?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#can-malloc-fail",
            "text": "If  malloc  fails to reserve any more memory then it returns  NULL . Robust programs should check the return value. If your code assumes  malloc  succeeds and it does not, then your program will likely crash (segfault) when it tries to write to address 0.",
            "title": "Can malloc fail?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#where-is-the-heap-and-how-big-is-it",
            "text": "The heap is part of the process memory and it does not have a fixed size. Heap memory allocation is performed by the C library when you call  malloc  ( calloc ,  realloc ) and  free .  First a quick review on process memory: A process is a running instance of your program. Each process has its own address space. For example on a 32 bit machine your process gets about 4 billion addresses to play with, however not all of these are valid or even mapped to actual physical memory (RAM). Inside the process's memory you will find the executable code, space for the stack, environment variables, global (static) variables and the heap.  By calling  sbrk  the C library can increase the size of the heap as your program demands more heap memory. As the heap and stack (one for each thread) need to grow, we put them at opposite ends of the address space. So for typical architectures the heap will grow upwards and the stack grows downwards.   Truthiness: Modern operating system memory allocators no longer need  sbrk  - instead they can request independent regions of virtual memory and maintain multiple memory regions. For example gigabyte requests may be placed in a different memory region than small allocation requests. However this detail is an unwanted complexity: The problems of fragmentation and allocating memory efficiently still apply, so we will ignore this implementation nicety here and will write as if the heap is a single region.  If we write a multi-threaded program (more about that later) we will need multiple stacks (one per thread) but there's only ever one heap.  On typical architectures, the heap is part of the  Data segment  and starts just above the code and global variables.",
            "title": "Where is the heap and how big is it?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#do-programs-need-to-call-brk-or-sbrk",
            "text": "Not typically (though calling  sbrk(0)  can be interesting because it tells you where your heap currently ends). Instead programs use  malloc,calloc,realloc  and  free  which are part of the C library. The internal implementation of these functions will call  sbrk  when additional heap memory is required.  void *top_of_heap = sbrk(0);\nmalloc(16384);\nvoid *top_of_heap2 = sbrk(0);\nprintf(\"The top of heap went from %p to %p \\n\", top_of_heap, top_of_heap2);  Example output:  The top of heap went from 0x4000 to 0xa000",
            "title": "Do programs need to call brk or sbrk?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#what-is-calloc",
            "text": "Unlike  malloc ,  calloc  initializes memory contents to zero and also takes two arguments (the number of items and the size in bytes of each item). A naive but readable implementation of  calloc  looks like this:  void *calloc(size_t n, size_t size)\n{\n    size_t total = n * size; // Does not check for overflow!\n    void *result = malloc(total);\n\n    if (!result) return NULL;\n\n// If we're using new memory pages \n// just allocated from the system by calling sbrk\n// then they will be zero so zero-ing out is unnecessary,\n\n    memset(result, 0, total);\n    return result; \n}  An advanced discussion of these limitations is  here .  Programmers often use  calloc  rather than explicitly calling  memset  after  malloc , to set the memory contents to zero. Note  calloc(x,y)  is identical to  calloc(y,x) , but you should follow the conventions of the manual.  // Ensure our memory is initialized to zero\nlink_t *link  = malloc(256);\nmemset(link, 0, 256); // Assumes malloc returned a valid address!\n\nlink_t *link = calloc(1, 256); // safer: calloc(1, sizeof(link_t));",
            "title": "What is calloc?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#why-is-the-memory-that-is-first-returned-by-sbrk-initialized-to-zero",
            "text": "If the operating system did not zero out contents of physical RAM it might be possible for one process to learn about the memory of another process that had previously used the memory. This would be a security leak.  Unfortunately this means that for  malloc  requests before any memory has been freed and simple programs (which end up using newly reserved memory from the system) the memory is  often  zero. Then programmers mistaken write C programs that assume malloc'd memory will  always  be zero.  char* ptr = malloc(300);\n// contents is probably zero because we get brand new memory\n// so beginner programs appear to work!\n// strcpy(ptr, \"Some data\"); // work with the data\nfree(ptr);\n// later\nchar *ptr2 = malloc(308); // Contents might now contain existing data and is probably not zero",
            "title": "Why is the memory that is first returned by sbrk initialized to zero?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#why-doesnt-malloc-always-initialize-memory-to-zero",
            "text": "Performance! We want malloc to be as fast as possible. Zeroing out memory may be unnecessary.",
            "title": "Why doesn't malloc always initialize memory to zero?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#what-is-realloc-and-when-would-you-use-it",
            "text": "realloc  allows you to resize an existing memory allocation that was previously allocated on the heap (via malloc,calloc or realloc). The most common use of realloc is to resize memory used to hold an array of values. A naive but readable version of realloc is suggested below  void * realloc(void * ptr, size_t newsize) {\n  // Simple implementation always reserves more memory\n  // and has no error checking\n  void *result = malloc(newsize); \n  size_t oldsize =  ... //(depends on allocator's internal data structure)\n  if (ptr) memcpy(result, ptr, newsize < oldsize ? newsize : oldsize);\n  free(ptr);\n  return result;\n}  An INCORRECT use of realloc is shown below:  int *array = malloc(sizeof(int) * 2);\narray[0] = 10; array[1] = 20;\n// Ooops need a bigger array - so use realloc..\nrealloc (array, 3); // ERRORS!\narray[2] = 30;   The above code contains two mistakes. Firstly we needed 3*sizeof(int) bytes not 3 bytes.\nSecondly realloc may need to move the existing contents of the memory to a new location. For example, there may not be sufficient space because the neighboring bytes are already allocated. A correct use of realloc is shown below.  array = realloc(array, 3 * sizeof(int));\n// If array is copied to a new location then old allocation will be freed.  A robust version would also check for a  NULL  return value. Note  realloc  can be used to grow and shrink allocations.",
            "title": "What is realloc and when would you use it?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#where-can-i-read-more",
            "text": "See  the man page !",
            "title": "Where can I read more?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#how-important-is-that-memory-allocation-is-fast",
            "text": "Very! Allocating and de-allocating heap memory is a common operation in most applications.",
            "title": "How important is that memory allocation is fast?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#intro-to-allocating",
            "text": "",
            "title": "Intro to Allocating"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#what-is-the-silliest-malloc-and-free-implementation-and-what-is-wrong-with-it",
            "text": "void* malloc(size_t size)\n// Ask the system for more bytes by extending the heap space. \n// sbrk Returns -1 on failure\n   void *p = sbrk(size); \n   if(p == (void *) -1) return NULL; // No space left\n   return p;\n}\nvoid free() {/* Do nothing */}  The above implementation suffers from two major drawbacks:  System calls are slow (compared to library calls). We should reserve a large amount of memory and only occasionally ask for more from the system.  No reuse of freed memory. Our program never re-uses heap memory - it just keeps asking for a bigger heap.  If this allocator was used in a typical program, the process would quickly exhaust all available memory.\nInstead we need an allocator that can efficiently use heap space and only ask for more memory when necessary.",
            "title": "What is the silliest malloc and free implementation and what is wrong with it?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#what-are-placement-strategies",
            "text": "During program execution memory is allocated and de-allocated (freed), so there will be gaps (holes) in the heap memory that can be re-used for future memory requests. The memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available.  Suppose our current heap size is 64K, though not all of it is in use because some earlier malloc'd memory has already been freed by the program:      16KB free  10KB allocated  1KB free  1KB allocated  30KB free  4KB allocated  2KB free      If a new malloc request for 2KB is executed ( malloc(2048) ), where should  malloc  reserve the memory? It could use the last 2KB hole (which happens to be the perfect size!) or it could split one of the other two free holes. These choices represent different placement strategies.  Whichever hole is chosen, the allocator will need to split the hole into two: The newly allocated space (which will be returned to the program) and a smaller hole (if there is spare space left over).  A perfect-fit strategy finds the smallest hole that is of sufficient size (at least 2KB):     16KB free  10KB allocated  1KB free  1KB allocated  30KB free  4KB allocated  2KB HERE!       A worst-fit strategy finds the largest hole that is of sufficient size (so break the 30KB hole into two):     16KB free  10KB allocated  1KB free  1KB allocated  2KB HERE!  28KB free  4KB allocated  2KB free      A first-fit strategy finds the first available hole that is of sufficient size (break the 16KB hole into two):     2KB HERE!  14KB free  10KB allocated  1KB free  1KB allocated  30KB free  4KB allocated  2KB free",
            "title": "What are placement strategies?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#what-is-external-fragmentation",
            "text": "In the example below, of the 64KB of heap memory, 17KB is allocated, and 47KB is free. However the largest available block is only 30KB because our available unallocated heap memory is fragmented into smaller pieces.      16KB free  10KB allocated  1KB free  1KB allocated  30KB free  4KB allocated  2KB free",
            "title": "What is external fragmentation?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#what-effect-do-placement-strategies-have-on-external-fragmentation-and-performance",
            "text": "Different strategies affect the fragmentation of heap memory in non-obvious ways, which only are discovered by mathematical analysis or careful simulations under real-world conditions (for example simulating the memory allocation requests of a database or webserver).\nFor example, best-fit at first glance appears to be an excellent strategy however, if we can not find a perfectly-sized hole then this placement creates many tiny unusable holes, leading to high fragmentation. It also requires a scan of all possible holes.  First fit has the advantage that it will not evaluate all possible placements and therefore be faster.   Since Worst-fit targets the largest unallocated space, it is a poor choice if large allocations are required.  In practice first-fit and next-fit (which is not discussed here) are often common placement strategy. Hybrid approaches and many other alternatives exist (see implementing a memory allocator page).",
            "title": "What effect do placement strategies have on external fragmentation and performance?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#what-are-the-challenges-of-writing-a-heap-allocator",
            "text": "The main challenges are,  Need to minimize fragmentation (i.e. maximize memory utilization)  Need high performance\n* Fiddly implementation (lots of pointer manipulation using linked lists and pointer arithmetic)  Some additional comments:  Both fragmentation and performance depend on the application allocation profile, which can be evaluated but not predicted and in practice, under-specific usage conditions, a special-purpose allocator can often out-perform a general purpose implementation.  The allocator doesn't know the program's memory allocation requests in advance. Even if we did, this is the  Knapsack problem  which is known to be NP hard!",
            "title": "What are the challenges of writing a heap allocator?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-1:-Heap-Memory-Introduction/#how-do-you-implement-a-memory-allocator",
            "text": "Good question.  Implementing a memory allocator",
            "title": "How do you implement a memory allocator?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/",
            "text": "Memory Allocator Tutorial\n\n\nA memory allocator needs to keep track of which bytes are currently allocated and which are available for use. This page introduces the implementation and conceptual details of building an allocator, i.e. the actual code that implements \nmalloc\n and \nfree\n.\n\n\nThis page talks about links of blocks - do I malloc memory for them instead?\n\n\nThough conceptually we are thinking about creating linked lists and lists of blocks, we don't need to \"malloc memory\" to create them! Instead we are writing integers and pointers into memory that we already control so that we can later consistently hop from one address to the next. This internal information represents some overhead. So even if we had requested 1024 KB of contiguous memory from the system, we will not be able to provide all of it to the running program.\n\n\nThinking in blocks\n\n\nWe can think of our heap memory as a list of blocks where each block is either allocated or unallocated.\nRather than storing an explicit list of pointers we store information about the block's size \nas part of the block\n. Thus there is conceptually a list of free blocks, but it is implicit, i.e. in the form of block size information that we store as part of each block.\n\n\nWe could navigate from one block to the next block just by adding the block's size. For example if you have a pointer \np\n that points to the start of a block, then \nnext_block\n  with be at \n((char *)p) +  *(size_t *) p\n, if you are storing the size of the blocks in bytes. The cast to \nchar *\n ensures that pointer arithmetic is calculated in bytes. The cast to \nsize_t *\n ensures the memory at \np\n is read as a size value and would be necessarily if \np\n was a \nvoid *\n or \nchar *\n type.\n\n\nThe calling program never sees these values; they are internal to the implementation of the memory allocator. \n\n\nAs an example, suppose your allocator is asked to reserve 80 bytes (\nmalloc(80)\n) and requires 8 bytes of internal header data. The allocator would need to find an unallocated space of at least 88 bytes. After updating the heap data it would return a pointer to the block. However, the returned pointer does not point to the start of the block because that's where the internal size data is stored! Instead we would return the start of the block + 8 bytes.\nIn the implementation, remember that pointer arithmetic depends on type. For example, \np += 8\n adds \n8 * sizeof(p)\n, not necessarily 8 bytes!\n\n\nImplementing malloc\n\n\nThe simplest implementation uses first fit: Start at the first block, assuming it exists, and iterate until a block that represents unallocated space of sufficient size is found, or we've checked all the blocks.\n\n\nIf no suitable block is found, it's time to call \nsbrk()\n again to sufficiently extend the size of the heap. A fast implementation might extend it a significant amount so that we will not need to request more heap memory in the near future.\n\n\nWhen a free block is found, it may be larger than the space we need. If so, we will create two entries in our implicit list. The first entry is the allocated block, the second entry is the remaining space.\n\n\nThere are two simple ways to note if a block is in use or available. The first is to store it as a byte in the header information along with the size and the second to encode it as the lowest bit in the size!\nThus block size information would be limited to only even values:\n\n\n// Assumes p is a reasonable pointer type, e.g. 'size_t *'.\nisallocated = (*p) & 1;\nrealsize = (*p) & ~1;  // mask out the lowest bit\n\n\n\n\nAlignment and rounding up considerations\n\n\nMany architectures expect multi-byte primitives to be aligned to some multiple of 2^n. For example, it's common to require 4-byte types to be aligned to 4-byte boundaries (and 8-byte types on 8-byte boundaries). If multi-byte primitives are not stored on a reasonable boundary (for example starting at an odd address) then the performance can be significantly impacted because it may require two memory read requests instead of one. On some architectures the penalty is even greater - the program will crash with a \nbus error\n.\n\n\nAs \nmalloc\n does not know how the user will use the allocated memory (array of doubles? array of chars?), the pointer returned to the program needs to be aligned for the worst case, which is architecture dependent.\n\n\nFrom glibc documentation, the glibc \nmalloc\n uses the following heuristic:\n\"    The block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. On GNU systems, the address is always a multiple of eight on most systems, and a multiple of 16 on 64-bit systems.\"\n\n\nFor example, if you need to calculate how many 16 byte units are required, don't forget to round up -\n\n\nint s = (requested_bytes + tag_overhead_bytes + 15) / 16\n\n\n\n\nThe additional constant ensures incomplete units are rounded up. Note, real code is more likely to symbol sizes e.g. \nsizeof(x) - 1\n, rather than coding numerical constant 15.\n\n\nA note about internal fragmentation\n\n\nInternal fragmentation happens when the block you give them is larger than their allocation size. Let's say that we have a free block of size 16B (not including metadata). If they allocate 7 bytes, you may want to round up to 16B and just return the entire block.\n\n\nThis gets very sinister when you implementing coalescing and splitting (next section). If you don't implement either, then you may end up returning a block of size 64B for a 7B allocation! There is a \nlot\n of overhead for that allocation which is what we are trying to avoid.\n\n\nImplementing free\n\n\nWhen \nfree\n is called we need to re-apply the offset to get back to the 'real' start of the block (remember we didn't give the user a pointer to the actual start of the block?), i.e. to where we stored the size information.\n\n\nA naive implementation would simply mark the block as unused. If we are storing the block allocation status in the lowest size bit, then we just need to clear the bit:\n\n\n*p = (*p) & ~1; // Clear lowest bit \n\n\n\n\nHowever, we have a bit more work to do: If the current block and the next block (if it exists) are both free we need to coalesce these blocks into a single block.\nSimilarly, we also need to check the previous block, too. If that exists and represents an unallocated memory, then we need to coalesce the blocks into a single large block.\n\n\nTo be able to coalesce a free block with a previous free block we will also need to find the previous block, so we store the block's size at the end of the block, too. These are called \"boundary tags\" (ref Knuth73). As the blocks are contiguous, the end of one blocks sits right next to the start of the next block. So the current block (apart from the first one) can look a few bytes further back to lookup the size of the previous block. With this information you can now jump backwards!\n\n\nPerformance\n\n\nWith the above description it's possible to build a memory allocator. It's main advantage is simplicity - at least simple compared to other allocators!\nAllocating memory is a worst-case linear time operation (search linked lists for a sufficiently large free block) and de-allocation is constant time (no more than 3 blocks will need to be coalesced into a single block). Using this allocator it is possible to experiment with different placement strategies. For example, you could start searching from where you last free'd a block, or where you last allocated from. If you do store pointers to blocks, you need to be very careful that they always remain valid (e.g. when coalescing blocks or other malloc or free calls that change the heap structure)\n\n\nExplicit Free Lists Allocators\n\n\nBetter performance can be achieved by implementing an explicit doubly-linked list of free nodes. In that case, we can immediately traverse to the next free block and the previous free block. This can halve the search time, because the linked list only includes unallocated blocks.\n\n\nA second advantage is that we now have some control over the ordering of the linked list. For example, when a block is free'd, we could choose to insert it into the beginning of the linked list rather than always between its neighbors. This is discussed below.\n\n\nWhere do we store the pointers of our linked list? A simple trick is to realize that the block itself is not being used and store the next and previous pointers as part of the block (though now you have to ensure that the free blocks are always sufficiently large to hold two pointers).\n\n\nWe still need to implement Boundary Tags (i.e. an implicit list using sizes), so that we can correctly free blocks and coalesce them with their two neighbors. Consequently, explicit free lists require more code and complexity.\n\n\nWith explicit linked lists a fast and simple 'Find-First' algorithm is used to find the first sufficiently large link. However, since the link order can be modified, this corresponds to different placement strategies. For example if the links are maintained from largest to smallest, then this produces a 'Worst-Fit' placement strategy.\n\n\nExplicit linked list insertion policy\n\n\nThe newly free'd block can be inserted easily into two possible positions: at the beginning or in address order (by using the boundary tags to first find the neighbors).\n\n\nInserting at the beginning creates a LIFO (last-in, first-out) policy: The most recently free'd spaces will be reused. Studies suggest fragmentation is worse than using address order.\n\n\nInserting in address order (\"Address ordered policy\") inserts free'd blocks so that the blocks are visited in increasing address order. This policy required more time to free a block because the boundary tags (size data) must be used to find the next and previous unallocated blocks. However, there is less fragmentation.\n\n\nCase study: Buddy Allocator (an example of a segregated list)\n\n\nA segregated allocator is one that divides the heap into different areas that are handled by different sub-allocators dependent on the size of the allocation request. Sizes are grouped into classes (e.g. powers of two) and each size is handled by a different sub-allocator and each size maintains its own free list.\n\n\nA well known allocator of this type is the buddy allocator. We'll discuss the binary buddy allocator which splits allocation into blocks of size 2^n (n = 1, 2, 3, ...) times some base unit number of bytes, but others also exist (e.g. Fibonacci split - can you see why it's named?). The basic concept is simple: If there are no free blocks of size 2^n, go to the next level and steal that block and split it into two. If two neighboring blocks of the same size become unallocated, they can be coalesced back together into a single large block of twice the size.\n\n\nBuddy allocators are fast because the neighboring blocks to coalesce with can be calculated from the free'd block's address, rather than traversing the size tags. Ultimate performance often requires a small amount of assembler code to use a specialized CPU instruction to find the lowest non-zero bit. \n\n\nThe main disadvantage of the Buddy allocator is that they suffer from \ninternal fragmentation\n, because allocations are rounded up to the nearest block size. For example, a 68-byte allocation will require a 128-byte block.\n\n\nFurther Reading and References\n\n\n\n\nSee \nFoundations of Software Technology and Theoretical Computer Science 1999 proceedings\n (Google books,page 85)\n\n\nThanksForTheMemory UIUC lecture Slides (\npptx\n) (\npdf\n)\nand \n\n\nWikipedia's buddy memory allocation page\n\n\n\n\nOther allocators\n\n\nThere are many other allocation schemes. For example \nSLUB\n (wikipedia) - one of three allocators used internally by the Linux Kernel.",
            "title": "Memory, Part 2: Implementing a Memory Allocator"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#memory-allocator-tutorial",
            "text": "A memory allocator needs to keep track of which bytes are currently allocated and which are available for use. This page introduces the implementation and conceptual details of building an allocator, i.e. the actual code that implements  malloc  and  free .",
            "title": "Memory Allocator Tutorial"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#this-page-talks-about-links-of-blocks-do-i-malloc-memory-for-them-instead",
            "text": "Though conceptually we are thinking about creating linked lists and lists of blocks, we don't need to \"malloc memory\" to create them! Instead we are writing integers and pointers into memory that we already control so that we can later consistently hop from one address to the next. This internal information represents some overhead. So even if we had requested 1024 KB of contiguous memory from the system, we will not be able to provide all of it to the running program.",
            "title": "This page talks about links of blocks - do I malloc memory for them instead?"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#thinking-in-blocks",
            "text": "We can think of our heap memory as a list of blocks where each block is either allocated or unallocated.\nRather than storing an explicit list of pointers we store information about the block's size  as part of the block . Thus there is conceptually a list of free blocks, but it is implicit, i.e. in the form of block size information that we store as part of each block.  We could navigate from one block to the next block just by adding the block's size. For example if you have a pointer  p  that points to the start of a block, then  next_block   with be at  ((char *)p) +  *(size_t *) p , if you are storing the size of the blocks in bytes. The cast to  char *  ensures that pointer arithmetic is calculated in bytes. The cast to  size_t *  ensures the memory at  p  is read as a size value and would be necessarily if  p  was a  void *  or  char *  type.  The calling program never sees these values; they are internal to the implementation of the memory allocator.   As an example, suppose your allocator is asked to reserve 80 bytes ( malloc(80) ) and requires 8 bytes of internal header data. The allocator would need to find an unallocated space of at least 88 bytes. After updating the heap data it would return a pointer to the block. However, the returned pointer does not point to the start of the block because that's where the internal size data is stored! Instead we would return the start of the block + 8 bytes.\nIn the implementation, remember that pointer arithmetic depends on type. For example,  p += 8  adds  8 * sizeof(p) , not necessarily 8 bytes!",
            "title": "Thinking in blocks"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#implementing-malloc",
            "text": "The simplest implementation uses first fit: Start at the first block, assuming it exists, and iterate until a block that represents unallocated space of sufficient size is found, or we've checked all the blocks.  If no suitable block is found, it's time to call  sbrk()  again to sufficiently extend the size of the heap. A fast implementation might extend it a significant amount so that we will not need to request more heap memory in the near future.  When a free block is found, it may be larger than the space we need. If so, we will create two entries in our implicit list. The first entry is the allocated block, the second entry is the remaining space.  There are two simple ways to note if a block is in use or available. The first is to store it as a byte in the header information along with the size and the second to encode it as the lowest bit in the size!\nThus block size information would be limited to only even values:  // Assumes p is a reasonable pointer type, e.g. 'size_t *'.\nisallocated = (*p) & 1;\nrealsize = (*p) & ~1;  // mask out the lowest bit",
            "title": "Implementing malloc"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#alignment-and-rounding-up-considerations",
            "text": "Many architectures expect multi-byte primitives to be aligned to some multiple of 2^n. For example, it's common to require 4-byte types to be aligned to 4-byte boundaries (and 8-byte types on 8-byte boundaries). If multi-byte primitives are not stored on a reasonable boundary (for example starting at an odd address) then the performance can be significantly impacted because it may require two memory read requests instead of one. On some architectures the penalty is even greater - the program will crash with a  bus error .  As  malloc  does not know how the user will use the allocated memory (array of doubles? array of chars?), the pointer returned to the program needs to be aligned for the worst case, which is architecture dependent.  From glibc documentation, the glibc  malloc  uses the following heuristic:\n\"    The block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. On GNU systems, the address is always a multiple of eight on most systems, and a multiple of 16 on 64-bit systems.\"  For example, if you need to calculate how many 16 byte units are required, don't forget to round up -  int s = (requested_bytes + tag_overhead_bytes + 15) / 16  The additional constant ensures incomplete units are rounded up. Note, real code is more likely to symbol sizes e.g.  sizeof(x) - 1 , rather than coding numerical constant 15.",
            "title": "Alignment and rounding up considerations"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#a-note-about-internal-fragmentation",
            "text": "Internal fragmentation happens when the block you give them is larger than their allocation size. Let's say that we have a free block of size 16B (not including metadata). If they allocate 7 bytes, you may want to round up to 16B and just return the entire block.  This gets very sinister when you implementing coalescing and splitting (next section). If you don't implement either, then you may end up returning a block of size 64B for a 7B allocation! There is a  lot  of overhead for that allocation which is what we are trying to avoid.",
            "title": "A note about internal fragmentation"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#implementing-free",
            "text": "When  free  is called we need to re-apply the offset to get back to the 'real' start of the block (remember we didn't give the user a pointer to the actual start of the block?), i.e. to where we stored the size information.  A naive implementation would simply mark the block as unused. If we are storing the block allocation status in the lowest size bit, then we just need to clear the bit:  *p = (*p) & ~1; // Clear lowest bit   However, we have a bit more work to do: If the current block and the next block (if it exists) are both free we need to coalesce these blocks into a single block.\nSimilarly, we also need to check the previous block, too. If that exists and represents an unallocated memory, then we need to coalesce the blocks into a single large block.  To be able to coalesce a free block with a previous free block we will also need to find the previous block, so we store the block's size at the end of the block, too. These are called \"boundary tags\" (ref Knuth73). As the blocks are contiguous, the end of one blocks sits right next to the start of the next block. So the current block (apart from the first one) can look a few bytes further back to lookup the size of the previous block. With this information you can now jump backwards!",
            "title": "Implementing free"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#performance",
            "text": "With the above description it's possible to build a memory allocator. It's main advantage is simplicity - at least simple compared to other allocators!\nAllocating memory is a worst-case linear time operation (search linked lists for a sufficiently large free block) and de-allocation is constant time (no more than 3 blocks will need to be coalesced into a single block). Using this allocator it is possible to experiment with different placement strategies. For example, you could start searching from where you last free'd a block, or where you last allocated from. If you do store pointers to blocks, you need to be very careful that they always remain valid (e.g. when coalescing blocks or other malloc or free calls that change the heap structure)",
            "title": "Performance"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#explicit-free-lists-allocators",
            "text": "Better performance can be achieved by implementing an explicit doubly-linked list of free nodes. In that case, we can immediately traverse to the next free block and the previous free block. This can halve the search time, because the linked list only includes unallocated blocks.  A second advantage is that we now have some control over the ordering of the linked list. For example, when a block is free'd, we could choose to insert it into the beginning of the linked list rather than always between its neighbors. This is discussed below.  Where do we store the pointers of our linked list? A simple trick is to realize that the block itself is not being used and store the next and previous pointers as part of the block (though now you have to ensure that the free blocks are always sufficiently large to hold two pointers).  We still need to implement Boundary Tags (i.e. an implicit list using sizes), so that we can correctly free blocks and coalesce them with their two neighbors. Consequently, explicit free lists require more code and complexity.  With explicit linked lists a fast and simple 'Find-First' algorithm is used to find the first sufficiently large link. However, since the link order can be modified, this corresponds to different placement strategies. For example if the links are maintained from largest to smallest, then this produces a 'Worst-Fit' placement strategy.",
            "title": "Explicit Free Lists Allocators"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#explicit-linked-list-insertion-policy",
            "text": "The newly free'd block can be inserted easily into two possible positions: at the beginning or in address order (by using the boundary tags to first find the neighbors).  Inserting at the beginning creates a LIFO (last-in, first-out) policy: The most recently free'd spaces will be reused. Studies suggest fragmentation is worse than using address order.  Inserting in address order (\"Address ordered policy\") inserts free'd blocks so that the blocks are visited in increasing address order. This policy required more time to free a block because the boundary tags (size data) must be used to find the next and previous unallocated blocks. However, there is less fragmentation.",
            "title": "Explicit linked list insertion policy"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#case-study-buddy-allocator-an-example-of-a-segregated-list",
            "text": "A segregated allocator is one that divides the heap into different areas that are handled by different sub-allocators dependent on the size of the allocation request. Sizes are grouped into classes (e.g. powers of two) and each size is handled by a different sub-allocator and each size maintains its own free list.  A well known allocator of this type is the buddy allocator. We'll discuss the binary buddy allocator which splits allocation into blocks of size 2^n (n = 1, 2, 3, ...) times some base unit number of bytes, but others also exist (e.g. Fibonacci split - can you see why it's named?). The basic concept is simple: If there are no free blocks of size 2^n, go to the next level and steal that block and split it into two. If two neighboring blocks of the same size become unallocated, they can be coalesced back together into a single large block of twice the size.  Buddy allocators are fast because the neighboring blocks to coalesce with can be calculated from the free'd block's address, rather than traversing the size tags. Ultimate performance often requires a small amount of assembler code to use a specialized CPU instruction to find the lowest non-zero bit.   The main disadvantage of the Buddy allocator is that they suffer from  internal fragmentation , because allocations are rounded up to the nearest block size. For example, a 68-byte allocation will require a 128-byte block.",
            "title": "Case study: Buddy Allocator (an example of a segregated list)"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#further-reading-and-references",
            "text": "See  Foundations of Software Technology and Theoretical Computer Science 1999 proceedings  (Google books,page 85)  ThanksForTheMemory UIUC lecture Slides ( pptx ) ( pdf )\nand   Wikipedia's buddy memory allocation page",
            "title": "Further Reading and References"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-2:-Implementing-a-Memory-Allocator/#other-allocators",
            "text": "There are many other allocation schemes. For example  SLUB  (wikipedia) - one of three allocators used internally by the Linux Kernel.",
            "title": "Other allocators"
        },
        {
            "location": "/SystemProgramming/Memory,-Part-3:-Smashing-the-Stack-Example/",
            "text": "Each thread uses a stack memory. The stack 'grows downwards' - if a function calls another function, then the stack is extended to smaller memory addresses.\nStack memory includes non-static automatic (temporary) variables, parameter values and the return address.\nIf a buffer is too small some data (e.g. input values from the user), then there is a real possibility that other stack variables and even the return address will be overwritten.\nThe precise layout of the stack's contents and order of the automatic variables is architecture and compiler dependent. However with a little investigative work we can learn how to deliberately smash the stack for a particular architecture.\n\n\nThe example below demonstrates how the return address is stored on the stack. For a particular 32 bit architecture \nLive Linux Machine\n, we determine that the return address is stored at an address two pointers (8 bytes) above the address of the automatic variable. The code deliberately changes the stack value so that when the input function returns, rather than continuing on inside the main method, it jumps to the exploit function instead.\n\n\n// Overwrites the return address on the following machine:\n// http://cs-education.github.io/sys/\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n\nvoid breakout() {\n    puts(\"Welcome. Have a shell...\");\n    system(\"/bin/sh\");\n}\nvoid input() {\n  void *p;\n  printf(\"Address of stack variable: %p\\n\", &p);\n  printf(\"Something that looks like a return address on stack: %p\\n\", *((&p)+2));\n  // Let's change it to point to the start of our sneaky function.\n  *((&p)+2) = breakout;\n}\nint main() {\n    printf(\"main() code starts at %p\\n\",main);\n\n    input();\n    while (1) {\n        puts(\"Hello\");\n        sleep(1);\n    }\n\n    return 0;\n}\n\n\n\n\nThere are \na lot\n of ways that computers tend to get around this.",
            "title": "Memory, Part 3: Smashing the Stack Example"
        },
        {
            "location": "/SystemProgramming/Memory-Review-Questions/",
            "text": "Topics\n\n\n\n\nBest Fit\n\n\nWorst Fit\n\n\nFirst Fit\n\n\nBuddy Allocator\n\n\nInternal Fragmentation\n\n\nExternal Fragmentation\n\n\nsbrk\n\n\nNatural Alignment\n\n\nBoundary Tag\n\n\nCoalescing\n\n\nSplitting\n\n\nSlab Allocation/Memory Pool\n\n\n\n\nQuestions/Exercises\n\n\n\n\nWhat is Internal Fragmentation? When does it become an issue?\n\n\nWhat is External Fragmentation? When does it become an issue?\n\n\nWhat is a Best Fit placement strategy? How is it with External Fragmentation? Time Complexity?\n\n\nWhat is a Worst Fit placement strategy? Is it any better with External Fragmentation? Time Complexity?\n\n\nWhat is the First Fit Placement strategy? It's a little bit better with Fragmentation, right? Expected Time Complexity?\n\n\nLet's say that we are using a buddy allocator with a new slab of 64kb. How does it go about allocating 1.5kb?\n\n\nWhen does the 5 line \nsbrk\n implementation of malloc have a use?\n\n\nWhat is natural alignment?\n\n\nWhat is Coalescing/Splitting? How do they increase/decrease fragmentation? When can you coalesce or split?\n\n\nHow do boundary tags work? How can they be used to coalesce or split?",
            "title": "Memory Review Questions"
        },
        {
            "location": "/SystemProgramming/Memory-Review-Questions/#topics",
            "text": "Best Fit  Worst Fit  First Fit  Buddy Allocator  Internal Fragmentation  External Fragmentation  sbrk  Natural Alignment  Boundary Tag  Coalescing  Splitting  Slab Allocation/Memory Pool",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/Memory-Review-Questions/#questionsexercises",
            "text": "What is Internal Fragmentation? When does it become an issue?  What is External Fragmentation? When does it become an issue?  What is a Best Fit placement strategy? How is it with External Fragmentation? Time Complexity?  What is a Worst Fit placement strategy? Is it any better with External Fragmentation? Time Complexity?  What is the First Fit Placement strategy? It's a little bit better with Fragmentation, right? Expected Time Complexity?  Let's say that we are using a buddy allocator with a new slab of 64kb. How does it go about allocating 1.5kb?  When does the 5 line  sbrk  implementation of malloc have a use?  What is natural alignment?  What is Coalescing/Splitting? How do they increase/decrease fragmentation? When can you coalesce or split?  How do boundary tags work? How can they be used to coalesce or split?",
            "title": "Questions/Exercises"
        },
        {
            "location": "/SystemProgramming/Memory:-Review-Questions/",
            "text": "Question numbers subject to change\n\n\n\n\nQ1\n\n\nWhat are the following and what is their purpose?\n\n Translation Lookaside Buffer\n\n Physical Address\n\n Memory Management Unit\n\n The dirty bit\n\n\nQ2\n\n\nHow do you determine how many bits are used in the page offset?\n\n\nQ3\n\n\n20 ms after a context switch the TLB contains all logical addresses used by your numerical code which performs main memory access 100% of the time. What is the overhead (slowdown) of a two-level page table compared to a single-level page table?\n\n\nQ4\n\n\nExplain why the TLB must be flushed when a context switch occurs (i.e. the CPU is assigned to work on a different process).",
            "title": "Memory: Review Questions"
        },
        {
            "location": "/SystemProgramming/Memory:-Review-Questions/#q1",
            "text": "What are the following and what is their purpose?  Translation Lookaside Buffer  Physical Address  Memory Management Unit  The dirty bit",
            "title": "Q1"
        },
        {
            "location": "/SystemProgramming/Memory:-Review-Questions/#q2",
            "text": "How do you determine how many bits are used in the page offset?",
            "title": "Q2"
        },
        {
            "location": "/SystemProgramming/Memory:-Review-Questions/#q3",
            "text": "20 ms after a context switch the TLB contains all logical addresses used by your numerical code which performs main memory access 100% of the time. What is the overhead (slowdown) of a two-level page table compared to a single-level page table?",
            "title": "Q3"
        },
        {
            "location": "/SystemProgramming/Memory:-Review-Questions/#q4",
            "text": "Explain why the TLB must be flushed when a context switch occurs (i.e. the CPU is assigned to work on a different process).",
            "title": "Q4"
        },
        {
            "location": "/SystemProgramming/Multi-threaded-Programming:-Review-Questions/",
            "text": "Warning - question numbers subject to change\n\n\n\n\nQ1\n\n\nIs the following code thread-safe? Redesign the following code to be thread-safe. Hint: A mutex is unnecessary if the message memory is unique to each call.\n\n\nstatic char message[20];\npthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\n\nvoid format(int v) {\n  pthread_mutex_lock(&mutex);\n  sprintf(message, \":%d:\" ,v);\n  pthread_mutex_unlock(&mutex);\n  return message;\n}\n\n\n\n\nQ2\n\n\nWhich one of the following does not cause a process to exit?\n\n Returning from the pthread's starting function in the last running thread.\n\n The original thread returning from main.\n\n Any thread causing a segmentation fault.\n\n Any thread calling \nexit\n.\n* Calling \npthread_exit\n in the main thread with other threads still running.\n\n\nQ3\n\n\nWrite a mathematical expression for the number of \"W\" characters that will be printed by the following program. Assume a,b,c,d are small positive integers. Your answer may use a 'min' function that returns its lowest valued argument.\n\n\nunsigned int a=...,b=...,c=...,d=...;\n\nvoid* func(void* ptr) {\n  char m = * (char*)ptr;\n  if(m == 'P') sem_post(s);\n  if(m == 'W') sem_wait(s);\n  putchar(m);\n  return NULL;\n}\n\nint main(int argv, char** argc) {\n  sem_init(s,0, a);\n  while(b--) pthread_create(&tid, NULL, func, \"W\"); \n  while(c--) pthread_create(&tid, NULL, func, \"P\"); \n  while(d--) pthread_create(&tid, NULL, func, \"W\"); \n  pthread_exit(NULL); \n  /*Process will finish when all threads have exited */\n}\n\n\n\n\nQ4\n\n\nComplete the following code. The following code is supposed to print alternating \nA\n and \nB\n. It represents two threads that take turns to execute.  Add condition variable calls to \nfunc\n so that the waiting thread does not need to continually check the \nturn\n variable. Q: Is pthread_cond_broadcast necessary or is pthread_cond_signal sufficient?\n\n\npthread_cond_t cv = PTHREAD_COND_INITIALIZER;\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid* turn;\n\nvoid* func(void* mesg) {\n  while(1) {\n// Add mutex lock and condition variable calls ...\n\n    while(turn == mesg) { \n        /* poll again ... Change me - This busy loop burns CPU time! */ \n    }\n\n    /* Do stuff on this thread */\n    puts( (char*) mesg);\n    turn = mesg;\n\n  }\n  return 0;\n}\n\nint main(int argc, char** argv){\n  pthread_t tid1;\n  pthread_create(&tid1, NULL, func, \"A\");\n  func(\"B\"); // no need to create another thread - just use the main thread\n  return 0;\n}\n\n\n\n\nQ5\n\n\nIdentify the critical sections in the given code. Add mutex locking to make the code thread safe. Add condition variable calls so that \ntotal\n never becomes negative or above 1000. Instead the call should block until it is safe to proceed. Explain why \npthread_cond_broadcast\n is necessary.\n\n\nint total;\nvoid add(int value) {\n if(value < 1) return;\n total += value;\n}\nvoid sub(int value) {\n if(value < 1) return;\n total -= value;\n}\n\n\n\n\nQ6\n\n\nA non-threadsafe data structure has \nsize()\n \nenq\n and \ndeq\n methods. Use condition variable and mutex lock to complete the thread-safe, blocking versions.\n\n\nvoid enqueue(void* data) {\n  // should block if the size() would become greater than 256\n  enq(data);\n}\nvoid* dequeue() {\n  // should block if size() is 0\n  return deq();\n}\n\n\n\n\nQ7\n\n\nYour startup offers path planning using latest traffic information. Your overpaid intern has created a non-threadsafe data structure with two functions: \nshortest\n (which uses but does not modify the graph) and \nset_edge\n (which modifies the graph).\n\n\ngraph_t* create_graph(char* filename); // called once\n\n// returns a new heap object that is the shortest path from vertex i to j\npath_t* shortest(graph_t* graph, int i, int j); \n\n// updates edge from vertex i to j\nvoid set_edge(graph_t* graph, int i, int j, double time); \n\n\n\n\n\nFor performance, multiple threads must be able to call \nshortest\n at the same time but the graph can only be modified by one thread when no threads other are executing inside \nshortest\n or \nset_edge\n.\n\n\nUse mutex lock and condition variables to implement a reader-writer solution. An incomplete attempt is shown below. Though this attempt is threadsafe (thus sufficient for demo day!), it does not allow multiple threads to calculate \nshortest\n path at the same time and will not have sufficient throughput.\n\n\npath_t* shortest_safe(graph_t* graph, int i, int j) {\n  pthread_mutex_lock(&m);\n  path_t* path = shortest(graph, i, j);\n  pthread_mutex_unlock(&m);\n  return path;\n}\nvoid set_edge_safe(graph_t* graph, int i, int j, double dist) {\n  pthread_mutex_lock(&m);\n  set_edge(graph, i, j, dist);\n  pthread_mutex_unlock(&m);\n}",
            "title": "Multi threaded Programming: Review Questions"
        },
        {
            "location": "/SystemProgramming/Multi-threaded-Programming:-Review-Questions/#q1",
            "text": "Is the following code thread-safe? Redesign the following code to be thread-safe. Hint: A mutex is unnecessary if the message memory is unique to each call.  static char message[20];\npthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\n\nvoid format(int v) {\n  pthread_mutex_lock(&mutex);\n  sprintf(message, \":%d:\" ,v);\n  pthread_mutex_unlock(&mutex);\n  return message;\n}",
            "title": "Q1"
        },
        {
            "location": "/SystemProgramming/Multi-threaded-Programming:-Review-Questions/#q2",
            "text": "Which one of the following does not cause a process to exit?  Returning from the pthread's starting function in the last running thread.  The original thread returning from main.  Any thread causing a segmentation fault.  Any thread calling  exit .\n* Calling  pthread_exit  in the main thread with other threads still running.",
            "title": "Q2"
        },
        {
            "location": "/SystemProgramming/Multi-threaded-Programming:-Review-Questions/#q3",
            "text": "Write a mathematical expression for the number of \"W\" characters that will be printed by the following program. Assume a,b,c,d are small positive integers. Your answer may use a 'min' function that returns its lowest valued argument.  unsigned int a=...,b=...,c=...,d=...;\n\nvoid* func(void* ptr) {\n  char m = * (char*)ptr;\n  if(m == 'P') sem_post(s);\n  if(m == 'W') sem_wait(s);\n  putchar(m);\n  return NULL;\n}\n\nint main(int argv, char** argc) {\n  sem_init(s,0, a);\n  while(b--) pthread_create(&tid, NULL, func, \"W\"); \n  while(c--) pthread_create(&tid, NULL, func, \"P\"); \n  while(d--) pthread_create(&tid, NULL, func, \"W\"); \n  pthread_exit(NULL); \n  /*Process will finish when all threads have exited */\n}",
            "title": "Q3"
        },
        {
            "location": "/SystemProgramming/Multi-threaded-Programming:-Review-Questions/#q4",
            "text": "Complete the following code. The following code is supposed to print alternating  A  and  B . It represents two threads that take turns to execute.  Add condition variable calls to  func  so that the waiting thread does not need to continually check the  turn  variable. Q: Is pthread_cond_broadcast necessary or is pthread_cond_signal sufficient?  pthread_cond_t cv = PTHREAD_COND_INITIALIZER;\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid* turn;\n\nvoid* func(void* mesg) {\n  while(1) {\n// Add mutex lock and condition variable calls ...\n\n    while(turn == mesg) { \n        /* poll again ... Change me - This busy loop burns CPU time! */ \n    }\n\n    /* Do stuff on this thread */\n    puts( (char*) mesg);\n    turn = mesg;\n\n  }\n  return 0;\n}\n\nint main(int argc, char** argv){\n  pthread_t tid1;\n  pthread_create(&tid1, NULL, func, \"A\");\n  func(\"B\"); // no need to create another thread - just use the main thread\n  return 0;\n}",
            "title": "Q4"
        },
        {
            "location": "/SystemProgramming/Multi-threaded-Programming:-Review-Questions/#q5",
            "text": "Identify the critical sections in the given code. Add mutex locking to make the code thread safe. Add condition variable calls so that  total  never becomes negative or above 1000. Instead the call should block until it is safe to proceed. Explain why  pthread_cond_broadcast  is necessary.  int total;\nvoid add(int value) {\n if(value < 1) return;\n total += value;\n}\nvoid sub(int value) {\n if(value < 1) return;\n total -= value;\n}",
            "title": "Q5"
        },
        {
            "location": "/SystemProgramming/Multi-threaded-Programming:-Review-Questions/#q6",
            "text": "A non-threadsafe data structure has  size()   enq  and  deq  methods. Use condition variable and mutex lock to complete the thread-safe, blocking versions.  void enqueue(void* data) {\n  // should block if the size() would become greater than 256\n  enq(data);\n}\nvoid* dequeue() {\n  // should block if size() is 0\n  return deq();\n}",
            "title": "Q6"
        },
        {
            "location": "/SystemProgramming/Multi-threaded-Programming:-Review-Questions/#q7",
            "text": "Your startup offers path planning using latest traffic information. Your overpaid intern has created a non-threadsafe data structure with two functions:  shortest  (which uses but does not modify the graph) and  set_edge  (which modifies the graph).  graph_t* create_graph(char* filename); // called once\n\n// returns a new heap object that is the shortest path from vertex i to j\npath_t* shortest(graph_t* graph, int i, int j); \n\n// updates edge from vertex i to j\nvoid set_edge(graph_t* graph, int i, int j, double time);   For performance, multiple threads must be able to call  shortest  at the same time but the graph can only be modified by one thread when no threads other are executing inside  shortest  or  set_edge .  Use mutex lock and condition variables to implement a reader-writer solution. An incomplete attempt is shown below. Though this attempt is threadsafe (thus sufficient for demo day!), it does not allow multiple threads to calculate  shortest  path at the same time and will not have sufficient throughput.  path_t* shortest_safe(graph_t* graph, int i, int j) {\n  pthread_mutex_lock(&m);\n  path_t* path = shortest(graph, i, j);\n  pthread_mutex_unlock(&m);\n  return path;\n}\nvoid set_edge_safe(graph_t* graph, int i, int j, double dist) {\n  pthread_mutex_lock(&m);\n  set_edge(graph, i, j, dist);\n  pthread_mutex_unlock(&m);\n}",
            "title": "Q7"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-1:-Introduction/",
            "text": "What is \"IP4\" \"IP6\"?\n\n\nThe following is the \"30 second\" introduction to internet protocol (IP) - which is the primary way to send packets (\"datagrams\") of information from one machine to another.\n\n\n\"IP4\", or more precisely, \"IPv4\" is version 4 of the Internet Protocol that describes how to send packets of information across a network from one machine to another . Roughly 95% of all packets on the Internet today are IPv4 packets. A significant limitation of IPv4 is that source and destination addresses are limited to 32 bits (IPv4 was designed at a time when the idea of 4 billion devices connected to the same network was unthinkable - or at least not worth making the packet size larger) \n\n\nEach IPv4 packet includes a very small header - typically 20 bytes (more precisely, \"octets\"), that includes a source and destination address.\n\n\nConceptually the source and destination addresses can be split into two: a network number (the upper bits) and the lower bits represent a particular host number on that network.\n\n\nA newer packet protocol \"IPv6\" solves many of the limitations of IPv4 (e.g. makes routing tables simpler and 128 bit addresses) however less than 5% of web traffic is IPv6 based.\n\n\nA machine can have an IPv6 address and an IPv4 address.\n\n\n\"There's no place like 127.0.0.1\"!\n\n\nA special IPv4 address is \n127.0.0.1\n also known as localhost. Packets sent to 127.0.0.1 will never leave the machine; the address is specified to be the same machine.\n\n\nNotice that the 32 bits address is split into 4 octets i.e. each number in the dot notation can be 0-255 inclusive. However IPv4 addresses can also be written as an integer.\n\n\n... and ... \"There's no place like 0:0:0:0:0:0:0:1?\"\n\n\nThe 128bit localhost address in IPv6 is \n0:0:0:0:0:0:0:1\n which can be written in its shortened form, \n::1\n\n\nWhat is a port?\n\n\nTo send a datagram (packet) to a host on the Internet using IPv4 (or IPv6) you need to specify the host address and a port. The port is an unsigned 16 bit number (i.e. the maximum port number is 65535).\n\n\nA process can listen for incoming packets on a particular port. However only processes with super-user (root) access can listen on ports < 1024. Any process can listen on ports 1024 or higher.\n\n\nAn often used port is port 80: Port 80 is used for unencrypted http requests (i.e. web pages).\nFor example, if a web browser connects to http://www.bbc.com/, then it will be connecting to port 80.\n\n\nWhat is UDP? When is it used?\n\n\nUDP is a connectionless protocol that is built on top of IPv4 and IPv6. It's very simple to use: Decide the destination address and port and send your data packet! However the network makes no guarantee about whether the packets will arrive.\nPackets (aka Datagrams) may be dropped if the network is congested. Packets may be duplicated or arrive out of order.\n\n\nBetween two distant data-centers it's typical to see 3% packet loss.\n\n\nA typical use case for UDP is when receiving up to date data is more important than receiving all of the data. For example, a game may send continuous updates of player positions. A streaming video signal may send picture updates using UDP\n\n\nWhat is TCP? When is it used?\n\n\nTCP is a connection-based protocol that is built on top of IPv4 and IPv6 (and therefore can be described as \"TCP/IP\" or \"TCP over IP\"). TCP creates a \npipe\n between two machines and abstracts away the low level packet-nature of the Internet: Thus, under most conditions, bytes sent from one machine will eventually arrive at the other end without duplication or data loss. \n\n\nTCP will automatically manage resending packets, ignoring duplicate packets, re-arranging out-of-order packets and changing the rate at which packets are sent.\n\n\nTCP's three way handshake is known as SYN, SYN-ACK, and ACK. The diagram on this page helps with understanding the TCP handshake. \nTCP Handshake\n\n\nMost services on the Internet today (e.g. a web service) use TCP because it hides the complexity of lower, packet-level nature of the Internet.",
            "title": "Networking, Part 1: Introduction"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-1:-Introduction/#what-is-ip4-ip6",
            "text": "The following is the \"30 second\" introduction to internet protocol (IP) - which is the primary way to send packets (\"datagrams\") of information from one machine to another.  \"IP4\", or more precisely, \"IPv4\" is version 4 of the Internet Protocol that describes how to send packets of information across a network from one machine to another . Roughly 95% of all packets on the Internet today are IPv4 packets. A significant limitation of IPv4 is that source and destination addresses are limited to 32 bits (IPv4 was designed at a time when the idea of 4 billion devices connected to the same network was unthinkable - or at least not worth making the packet size larger)   Each IPv4 packet includes a very small header - typically 20 bytes (more precisely, \"octets\"), that includes a source and destination address.  Conceptually the source and destination addresses can be split into two: a network number (the upper bits) and the lower bits represent a particular host number on that network.  A newer packet protocol \"IPv6\" solves many of the limitations of IPv4 (e.g. makes routing tables simpler and 128 bit addresses) however less than 5% of web traffic is IPv6 based.  A machine can have an IPv6 address and an IPv4 address.",
            "title": "What is \"IP4\" \"IP6\"?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-1:-Introduction/#theres-no-place-like-127001",
            "text": "A special IPv4 address is  127.0.0.1  also known as localhost. Packets sent to 127.0.0.1 will never leave the machine; the address is specified to be the same machine.  Notice that the 32 bits address is split into 4 octets i.e. each number in the dot notation can be 0-255 inclusive. However IPv4 addresses can also be written as an integer.",
            "title": "\"There's no place like 127.0.0.1\"!"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-1:-Introduction/#and-theres-no-place-like-00000001",
            "text": "The 128bit localhost address in IPv6 is  0:0:0:0:0:0:0:1  which can be written in its shortened form,  ::1",
            "title": "... and ... \"There's no place like 0:0:0:0:0:0:0:1?\""
        },
        {
            "location": "/SystemProgramming/Networking,-Part-1:-Introduction/#what-is-a-port",
            "text": "To send a datagram (packet) to a host on the Internet using IPv4 (or IPv6) you need to specify the host address and a port. The port is an unsigned 16 bit number (i.e. the maximum port number is 65535).  A process can listen for incoming packets on a particular port. However only processes with super-user (root) access can listen on ports < 1024. Any process can listen on ports 1024 or higher.  An often used port is port 80: Port 80 is used for unencrypted http requests (i.e. web pages).\nFor example, if a web browser connects to http://www.bbc.com/, then it will be connecting to port 80.",
            "title": "What is a port?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-1:-Introduction/#what-is-udp-when-is-it-used",
            "text": "UDP is a connectionless protocol that is built on top of IPv4 and IPv6. It's very simple to use: Decide the destination address and port and send your data packet! However the network makes no guarantee about whether the packets will arrive.\nPackets (aka Datagrams) may be dropped if the network is congested. Packets may be duplicated or arrive out of order.  Between two distant data-centers it's typical to see 3% packet loss.  A typical use case for UDP is when receiving up to date data is more important than receiving all of the data. For example, a game may send continuous updates of player positions. A streaming video signal may send picture updates using UDP",
            "title": "What is UDP? When is it used?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-1:-Introduction/#what-is-tcp-when-is-it-used",
            "text": "TCP is a connection-based protocol that is built on top of IPv4 and IPv6 (and therefore can be described as \"TCP/IP\" or \"TCP over IP\"). TCP creates a  pipe  between two machines and abstracts away the low level packet-nature of the Internet: Thus, under most conditions, bytes sent from one machine will eventually arrive at the other end without duplication or data loss.   TCP will automatically manage resending packets, ignoring duplicate packets, re-arranging out-of-order packets and changing the rate at which packets are sent.  TCP's three way handshake is known as SYN, SYN-ACK, and ACK. The diagram on this page helps with understanding the TCP handshake.  TCP Handshake  Most services on the Internet today (e.g. a web service) use TCP because it hides the complexity of lower, packet-level nature of the Internet.",
            "title": "What is TCP? When is it used?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/",
            "text": "How do I use \ngetaddrinfo\n to convert the hostname into an IP address?\n\n\nThe function \ngetaddrinfo\n can convert a human readable domain name (e.g. \nwww.illinois.edu\n) into an IPv4 and IPv6 address. In fact it will return a linked-list of addrinfo structs:\n\n\nstruct addrinfo {\n    int              ai_flags;\n    int              ai_family;\n    int              ai_socktype;\n    int              ai_protocol;\n    socklen_t        ai_addrlen;\n    struct sockaddr *ai_addr;\n    char            *ai_canonname;\n    struct addrinfo *ai_next;\n};\n\n\n\n\nIt's very easy to use. For example, suppose you wanted to find out the numeric IPv4 address of a webserver at www.bbc.com. We do this in two stages. First use getaddrinfo to build a linked-list of possible connections. Secondly use \ngetnameinfo\n to convert the binary address into a readable form.\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n\nstruct addrinfo hints, *infoptr; // So no need to use memset global variables\n\nint main() {\n  hints.ai_family = AF_INET; // AF_INET means IPv4 only addresses\n\n  int result = getaddrinfo(\"www.bbc.com\", NULL, &hints, &infoptr);\n  if (result) {\n    fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(result));\n    exit(1);\n  }\n\n  struct addrinfo *p;\n  char host[256],service[256];\n\n  for(p = infoptr; p != NULL; p = p->ai_next) {\n\n    getnameinfo(p->ai_addr, p->ai_addrlen, host, sizeof(host), service, sizeof(service), NI_NUMERICHOST);\n    puts(host);\n  }\n\n  freeaddrinfo(infoptr);\n  return 0;\n}\n\n\n\n\nTypical output:\n\n\n212.58.244.70\n212.58.244.71\n\n\n\n\nHow is www.cs.illinois.edu converted into an IP address?\n\n\nMagic! No seriously, a system called \"DNS\" (Domain Name Service) is used. If a machine does not hold the answer locally then it sends a UDP packet to a local DNS server. This server in turn may query other upstream DNS servers. \n\n\nIs DNS secure?\n\n\nDNS by itself is fast but not secure. DNS requests are not encrypted and susceptible to 'man-in-the-middle' attacks. For example, a coffee shop internet connection could easily subvert your DNS requests and send back different IP addresses for a particular domain\n\n\nHow do I connect to a TCP server (e.g. web server?)\n\n\nTODO\nThere are three basic system calls you need to connect to a remote machine:\n\n\ngetaddrinfo -- Determine the remote addresses of a remote host\nsocket  -- Create a socket\nconnect  -- Connect to the remote host using the socket and address information\n\n\n\n\nThe \ngetaddrinfo\n call if successful, creates a linked-list of \naddrinfo\n structs and sets the given pointer to point to the first one.\n\n\nThe socket call creates an outgoing socket and returns a descriptor (sometimes called a 'file descriptor') that can be used with \nread\n and \nwrite\n etc.In this sense it is the network analog of \nopen\n that opens a file stream - except that we haven't connected the socket to anything yet!\n\n\nFinally the connect call attempts the connection to the remote machine. We pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. There are different kinds of socket address structures (e.g. IPv4 vs IPv6) which can require more memory. So in addition to passing the pointer, the size of the structure is also passed:\n\n\n// Pull out the socket address info from the addrinfo struct:\nconnect(sockfd, p->ai_addr, p->ai_addrlen)\n\n\n\n\nHow do I free the memory allocated for the linked-list of addrinfo structs?\n\n\nAs part of the clean up code call \nfreeaddrinfo\n on the top-most \naddrinfo\n struct:\n\n\nvoid freeaddrinfo(struct addrinfo *ai);\n\n\n\n\nIf getaddrinfo fails can I use \nstrerror\n to print out the error?\n\n\nNo. Error handling with \ngetaddrinfo\n is a little different:\n\n  The return value \nis\n the error code (i.e. don't use \nerrno\n)\n\n Use \ngai_strerror\n to get the equivalent short English error text:\n\n\nint result = getaddrinfo(...);\nif(result) { \n   const char *mesg = gai_strerror(result); \n   ...\n}\n\n\n\n\nCan I request only IPv4 or IPv6 connection? TCP only?\n\n\nYes! Use the addrinfo structure that is passed into \ngetaddrinfo\n to define the kind of connection you'd like.\n\n\nFor example, to specify stream-based protocols over IPv6:\n\n\nstruct addrinfo hints;\nmemset(hints, 0, sizeof(hints));\n\nhints.ai_family = AF_INET6; // Only want IPv6 (use AF_INET for IPv4)\nhints.ai_socktype = SOCK_STREAM; // Only want stream-based connection\n\n\n\n\nWhat about code examples that use \ngethostbyname\n?\n\n\nThe old function \ngethostbyname\n is deprecated; it's the old way convert a host name into an IP address. The port address still needs to be manually set using htons function. It's much easier to write code to support IPv4 AND IPv6 using the newer \ngetaddrinfo\n\n\nIs it that easy!?\n\n\nYes and no. It's easy to create a simple TCP client - however network communications offers many different levels of abstraction and several attributes and options that can be set at each level of abstraction (for example we haven't talked about \nsetsockopt\n which can manipulate options for the socket).\nFor more information see this \nguide\n.",
            "title": "Networking, Part 2: Using getaddrinfo"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#how-do-i-use-getaddrinfo-to-convert-the-hostname-into-an-ip-address",
            "text": "The function  getaddrinfo  can convert a human readable domain name (e.g.  www.illinois.edu ) into an IPv4 and IPv6 address. In fact it will return a linked-list of addrinfo structs:  struct addrinfo {\n    int              ai_flags;\n    int              ai_family;\n    int              ai_socktype;\n    int              ai_protocol;\n    socklen_t        ai_addrlen;\n    struct sockaddr *ai_addr;\n    char            *ai_canonname;\n    struct addrinfo *ai_next;\n};  It's very easy to use. For example, suppose you wanted to find out the numeric IPv4 address of a webserver at www.bbc.com. We do this in two stages. First use getaddrinfo to build a linked-list of possible connections. Secondly use  getnameinfo  to convert the binary address into a readable form.  #include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n\nstruct addrinfo hints, *infoptr; // So no need to use memset global variables\n\nint main() {\n  hints.ai_family = AF_INET; // AF_INET means IPv4 only addresses\n\n  int result = getaddrinfo(\"www.bbc.com\", NULL, &hints, &infoptr);\n  if (result) {\n    fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(result));\n    exit(1);\n  }\n\n  struct addrinfo *p;\n  char host[256],service[256];\n\n  for(p = infoptr; p != NULL; p = p->ai_next) {\n\n    getnameinfo(p->ai_addr, p->ai_addrlen, host, sizeof(host), service, sizeof(service), NI_NUMERICHOST);\n    puts(host);\n  }\n\n  freeaddrinfo(infoptr);\n  return 0;\n}  Typical output:  212.58.244.70\n212.58.244.71",
            "title": "How do I use getaddrinfo to convert the hostname into an IP address?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#how-is-wwwcsillinoisedu-converted-into-an-ip-address",
            "text": "Magic! No seriously, a system called \"DNS\" (Domain Name Service) is used. If a machine does not hold the answer locally then it sends a UDP packet to a local DNS server. This server in turn may query other upstream DNS servers.",
            "title": "How is www.cs.illinois.edu converted into an IP address?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#is-dns-secure",
            "text": "DNS by itself is fast but not secure. DNS requests are not encrypted and susceptible to 'man-in-the-middle' attacks. For example, a coffee shop internet connection could easily subvert your DNS requests and send back different IP addresses for a particular domain",
            "title": "Is DNS secure?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#how-do-i-connect-to-a-tcp-server-eg-web-server",
            "text": "TODO\nThere are three basic system calls you need to connect to a remote machine:  getaddrinfo -- Determine the remote addresses of a remote host\nsocket  -- Create a socket\nconnect  -- Connect to the remote host using the socket and address information  The  getaddrinfo  call if successful, creates a linked-list of  addrinfo  structs and sets the given pointer to point to the first one.  The socket call creates an outgoing socket and returns a descriptor (sometimes called a 'file descriptor') that can be used with  read  and  write  etc.In this sense it is the network analog of  open  that opens a file stream - except that we haven't connected the socket to anything yet!  Finally the connect call attempts the connection to the remote machine. We pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. There are different kinds of socket address structures (e.g. IPv4 vs IPv6) which can require more memory. So in addition to passing the pointer, the size of the structure is also passed:  // Pull out the socket address info from the addrinfo struct:\nconnect(sockfd, p->ai_addr, p->ai_addrlen)",
            "title": "How do I connect to a TCP server (e.g. web server?)"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#how-do-i-free-the-memory-allocated-for-the-linked-list-of-addrinfo-structs",
            "text": "As part of the clean up code call  freeaddrinfo  on the top-most  addrinfo  struct:  void freeaddrinfo(struct addrinfo *ai);",
            "title": "How do I free the memory allocated for the linked-list of addrinfo structs?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#if-getaddrinfo-fails-can-i-use-strerror-to-print-out-the-error",
            "text": "No. Error handling with  getaddrinfo  is a little different:   The return value  is  the error code (i.e. don't use  errno )  Use  gai_strerror  to get the equivalent short English error text:  int result = getaddrinfo(...);\nif(result) { \n   const char *mesg = gai_strerror(result); \n   ...\n}",
            "title": "If getaddrinfo fails can I use strerror to print out the error?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#can-i-request-only-ipv4-or-ipv6-connection-tcp-only",
            "text": "Yes! Use the addrinfo structure that is passed into  getaddrinfo  to define the kind of connection you'd like.  For example, to specify stream-based protocols over IPv6:  struct addrinfo hints;\nmemset(hints, 0, sizeof(hints));\n\nhints.ai_family = AF_INET6; // Only want IPv6 (use AF_INET for IPv4)\nhints.ai_socktype = SOCK_STREAM; // Only want stream-based connection",
            "title": "Can I request only IPv4 or IPv6 connection? TCP only?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#what-about-code-examples-that-use-gethostbyname",
            "text": "The old function  gethostbyname  is deprecated; it's the old way convert a host name into an IP address. The port address still needs to be manually set using htons function. It's much easier to write code to support IPv4 AND IPv6 using the newer  getaddrinfo",
            "title": "What about code examples that use gethostbyname?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-2:-Using-getaddrinfo/#is-it-that-easy",
            "text": "Yes and no. It's easy to create a simple TCP client - however network communications offers many different levels of abstraction and several attributes and options that can be set at each level of abstraction (for example we haven't talked about  setsockopt  which can manipulate options for the socket).\nFor more information see this  guide .",
            "title": "Is it that easy!?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-3:-Building-a-simple-TCP-Client/",
            "text": "socket\n\n\nint socket(int domain, int type, int protocol);\n\n\nSocket creates a socket with domain (usually AF_INET for IPv4), type is whether to use UDP or TCP, protocol is any addition options. This creates a socket object in the kernel with which one can communicate with the outside world/network. This returns a fd so you can use it like a normal file descriptor! Remember you want to do your reads or writes from the socketfd because that represents the socket object only as a client, otherwise you want to respect the convention of the server.\n\n\ngetaddressinfo\n\n\nWe saw this in the last section! You're experts at this.\n\n\nconnect\n\n\nint connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);\n\n\nPass it the sockfd, then the address you want to go to and its length and you will be off connecting (as long as you check the error). Remember, network calls are ultra perceptible to failing.\n\n\nread\n/\nwrite\n\n\nOnce we have a successful connection we can read or write like any old file descriptor. Keep in mind if you are connected to a website, you want to conform to the HTTP protocol specification in order to get any sort of meaningful results back. There are libraries to do this, usually you don't connect at the socket level because there are other libraries or packages around it\n\n\nComplete Simple TCP Client Example\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <unistd.h>\n\nint main(int argc, char **argv)\n{\n    int s;\n    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    struct addrinfo hints, *result;\n    memset(&hints, 0, sizeof(struct addrinfo));\n    hints.ai_family = AF_INET; /* IPv4 only */\n    hints.ai_socktype = SOCK_STREAM; /* TCP */\n\n    s = getaddrinfo(\"www.illinois.edu\", \"80\", &hints, &result);\n    if (s != 0) {\n            fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(s));\n            exit(1);\n    }\n\n    if(connect(sock_fd, result->ai_addr, result->ai_addrlen) == -1){\n                perror(\"connect\");\n                exit(2);\n        }\n\n    char *buffer = \"GET / HTTP/1.0\\r\\n\\r\\n\";\n    printf(\"SENDING: %s\", buffer);\n    printf(\"===\\n\");\n    write(sock_fd, buffer, strlen(buffer));\n\n\n    char resp[1000];\n    int len = read(sock_fd, resp, 999);\n    resp[len] = '\\0';\n    printf(\"%s\\n\", resp);\n\n    return 0;\n}\n\n\n\n\nExample output:\n\n\nSENDING: GET / HTTP/1.0\n\n===\nHTTP/1.1 200 OK\nDate: Mon, 27 Oct 2014 19:19:05 GMT\nServer: Apache/2.2.15 (Red Hat) mod_ssl/2.2.15 OpenSSL/1.0.1e-fips mod_jk/1.2.32\nLast-Modified: Fri, 03 Feb 2012 16:51:10 GMT\nETag: \"401b0-49-4b8121ea69b80\"\nAccept-Ranges: bytes\nContent-Length: 73\nConnection: close\nContent-Type: text/html\n\nProvided by Web Services at Public Affairs at the University of Illinois\n\n\n\n\nComment on HTTP request and response\n\n\nThe example above demonstrates a request to the server using Hypertext Transfer Protocol.\nA web page (or other resources) are requested using the following request:\n\n\nGET / HTTP/1.0\n\n\n\n\n\nThere are four parts (the method e.g. GET,POST,...); the resource (e.g. / /index.html /image.png); the proctocol \"HTTP/1.0\" and two new lines (\\r\\n\\r\\n)\n\n\nThe server's first response line describes the HTTP version used and whether the request is successful using a 3 digit response code:\n\n\nHTTP/1.1 200 OK\n\n\n\n\nIf the client had requested a non existing file, e.g. \nGET /nosuchfile.html HTTP/1.0\n\nThen the first line includes the response code is the well-known \n404\n response code:\n\n\nHTTP/1.1 404 Not Found",
            "title": "Networking, Part 3: Building a simple TCP Client"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-3:-Building-a-simple-TCP-Client/#socket",
            "text": "int socket(int domain, int type, int protocol);  Socket creates a socket with domain (usually AF_INET for IPv4), type is whether to use UDP or TCP, protocol is any addition options. This creates a socket object in the kernel with which one can communicate with the outside world/network. This returns a fd so you can use it like a normal file descriptor! Remember you want to do your reads or writes from the socketfd because that represents the socket object only as a client, otherwise you want to respect the convention of the server.",
            "title": "socket"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-3:-Building-a-simple-TCP-Client/#getaddressinfo",
            "text": "We saw this in the last section! You're experts at this.",
            "title": "getaddressinfo"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-3:-Building-a-simple-TCP-Client/#connect",
            "text": "int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);  Pass it the sockfd, then the address you want to go to and its length and you will be off connecting (as long as you check the error). Remember, network calls are ultra perceptible to failing.",
            "title": "connect"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-3:-Building-a-simple-TCP-Client/#readwrite",
            "text": "Once we have a successful connection we can read or write like any old file descriptor. Keep in mind if you are connected to a website, you want to conform to the HTTP protocol specification in order to get any sort of meaningful results back. There are libraries to do this, usually you don't connect at the socket level because there are other libraries or packages around it",
            "title": "read/write"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-3:-Building-a-simple-TCP-Client/#complete-simple-tcp-client-example",
            "text": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <unistd.h>\n\nint main(int argc, char **argv)\n{\n    int s;\n    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    struct addrinfo hints, *result;\n    memset(&hints, 0, sizeof(struct addrinfo));\n    hints.ai_family = AF_INET; /* IPv4 only */\n    hints.ai_socktype = SOCK_STREAM; /* TCP */\n\n    s = getaddrinfo(\"www.illinois.edu\", \"80\", &hints, &result);\n    if (s != 0) {\n            fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(s));\n            exit(1);\n    }\n\n    if(connect(sock_fd, result->ai_addr, result->ai_addrlen) == -1){\n                perror(\"connect\");\n                exit(2);\n        }\n\n    char *buffer = \"GET / HTTP/1.0\\r\\n\\r\\n\";\n    printf(\"SENDING: %s\", buffer);\n    printf(\"===\\n\");\n    write(sock_fd, buffer, strlen(buffer));\n\n\n    char resp[1000];\n    int len = read(sock_fd, resp, 999);\n    resp[len] = '\\0';\n    printf(\"%s\\n\", resp);\n\n    return 0;\n}  Example output:  SENDING: GET / HTTP/1.0\n\n===\nHTTP/1.1 200 OK\nDate: Mon, 27 Oct 2014 19:19:05 GMT\nServer: Apache/2.2.15 (Red Hat) mod_ssl/2.2.15 OpenSSL/1.0.1e-fips mod_jk/1.2.32\nLast-Modified: Fri, 03 Feb 2012 16:51:10 GMT\nETag: \"401b0-49-4b8121ea69b80\"\nAccept-Ranges: bytes\nContent-Length: 73\nConnection: close\nContent-Type: text/html\n\nProvided by Web Services at Public Affairs at the University of Illinois",
            "title": "Complete Simple TCP Client Example"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-3:-Building-a-simple-TCP-Client/#comment-on-http-request-and-response",
            "text": "The example above demonstrates a request to the server using Hypertext Transfer Protocol.\nA web page (or other resources) are requested using the following request:  GET / HTTP/1.0  There are four parts (the method e.g. GET,POST,...); the resource (e.g. / /index.html /image.png); the proctocol \"HTTP/1.0\" and two new lines (\\r\\n\\r\\n)  The server's first response line describes the HTTP version used and whether the request is successful using a 3 digit response code:  HTTP/1.1 200 OK  If the client had requested a non existing file, e.g.  GET /nosuchfile.html HTTP/1.0 \nThen the first line includes the response code is the well-known  404  response code:  HTTP/1.1 404 Not Found",
            "title": "Comment on HTTP request and response"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/",
            "text": "What is \nhtons\n and when is it used?\n\n\nIntegers can be represented in least significant byte first or most-significant byte first. Either approach is reasonable as long as the machine itself is internally consistent. For network communications we need to standardize on agreed format.\n\n\nhtons(xyz)\n returns the 16 bit unsigned integer 'short' value xyz in network byte order.\n\nhtonl(xyz)\n returns the 32 bit unsigned integer 'long' value xyz in network byte order.\n\n\nThese functions are read as 'host to network'; the inverse functions (ntohs, ntohl) convert network ordered byte values to host-ordered ordering. So, is host-ordering  little-endian or big-endian? The answer is - it depends on your machine! It depends on the actual architecture of the host running the code. If the architecture happens to be the same as network ordering then the result of these functions is just the argument. For x86 machines, the host and network ordering \nis\n different.\n\n\nSummary: Whenever you read or write the low level C network structures (e.g. port and address information), remember to use the above functions to ensure correct conversion to/from a machine format. Otherwise the displayed or specified value may be incorrect.\n\n\nWhat are the 'big 4' network calls used to create a server?\n\n\nThe four system calls required to create a TCP server are: \nsocket\n, \nbind\n \nlisten\n and \naccept\n. Each has a specific purpose and should be called in the above order\n\n\nThe port information (used by bind) can be set manually (many older IPv4-only C code examples do this), or be created using \ngetaddrinfo\n\n\nWe also see examples of setsockopt later too.\n\n\nWhat is the purpose of calling \nsocket\n?\n\n\nTo create a endpoint for networking communication. A new socket by itself is not particularly useful; though we've specified either a packet or stream-based connections it is not bound to a particular network interface or port. Instead socket returns a network descriptor that can be used with later calls to bind,listen and accept.\n\n\nWhat is the purpose of calling \nbind\n\n\nThe \nbind\n call associates an abstract socket with an actual network interface and port. It is possible to call bind on a TCP client however it's unusually unnecessary to specify the outgoing port.\n\n\nWhat is the purpose of calling \nlisten\n\n\nThe \nlisten\n call specifies the queue size for the number of incoming, unhandled connections i.e. that have not yet been assigned a network descriptor by \naccept\n\nTypical values for a high performance server are 128 or more.\n\n\nWhy are server sockets passive?\n\n\nServer sockets do not actively try to connect to another host; instead they wait for incoming connections. Additionally, server sockets are not closed when the peer disconnects. Instead when a remote client connects, it is immediately bumped to an unused port number for future communications.\n\n\nWhat is the purpose of calling \naccept\n\n\nOnce the server socket has been initialized the server calls \naccept\n to wait for new connections. Unlike \nsocket\n \nbind\n and \nlisten\n, this call will block. i.e. if there are no new connections, this call will block and only return when a new client connects.\n\n\nNote the \naccept\n call returns a new file descriptor. This file descriptor is specific to a particular client. It is common programming mistake to use the original server socket descriptor for server I/O and then wonder why networking code has failed.\n\n\nWhat are the gotchas of creating a TCP-server?\n\n\n\n\nUsing the socket descriptor of the passive server socket (described above)\n\n\nNot specifying SOCK_STREAM requirement for getaddrinfo\n\n\nNot being able to re-use an existing port.\n\n\nNot initializing the unused struct entries\n\n\nThe \nbind\n call will fail if the port is currently in use\n\n\n\n\nNote, ports are per machine- not per process or per user. In other words,  you cannot use port 1234 while another process is using that port. Worse, ports are by default 'tied up' after a process has finished.\n\n\nServer code example\n\n\nA working simple server example is shown below. Note this example is incomplete - for example it does not close either socket descriptor, or free up memory created by \ngetaddrinfo\n\n\n\n#include <string.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <unistd.h>\n#include <arpa/inet.h>\n\nint main(int argc, char **argv)\n{\n    int s;\n    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    struct addrinfo hints, *result;\n    memset(&hints, 0, sizeof(struct addrinfo));\n    hints.ai_family = AF_INET;\n    hints.ai_socktype = SOCK_STREAM;\n    hints.ai_flags = AI_PASSIVE;\n\n    s = getaddrinfo(NULL, \"1234\", &hints, &result);\n    if (s != 0) {\n            fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(s));\n            exit(1);\n    }\n\n    if (bind(sock_fd, result->ai_addr, result->ai_addrlen) != 0) {\n        perror(\"bind()\");\n        exit(1);\n    }\n\n    if (listen(sock_fd, 10) != 0) {\n        perror(\"listen()\");\n        exit(1);\n    }\n\n    struct sockaddr_in *result_addr = (struct sockaddr_in *) result->ai_addr;\n    printf(\"Listening on file descriptor %d, port %d\\n\", sock_fd, ntohs(result_addr->sin_port));\n\n    printf(\"Waiting for connection...\\n\");\n    int client_fd = accept(sock_fd, NULL, NULL);\n    printf(\"Connection made: client_fd=%d\\n\", client_fd);\n\n    char buffer[1000];\n    int len = read(client_fd, buffer, sizeof(buffer) - 1);\n    buffer[len] = '\\0';\n\n    printf(\"Read %d chars\\n\", len);\n    printf(\"===\\n\");\n    printf(\"%s\\n\", buffer);\n\n    return 0;\n}\n\n\n\n\nWhy can't my server re-use the port?\n\n\nBy default a port is not immediately released when the socket is closed. Instead, the port enters a \"TIMED-WAIT\" state. This can lead to significant confusion during development because the timeout can make valid networking code appear to fail.\n\n\nTo be able to immediately re-use a port, specify \nSO_REUSEPORT\n before binding to the port.\n\n\nint optval = 1;\nsetsockopt(sfd, SOL_SOCKET, SO_REUSEPORT, &optval, sizeof(optval));\n\nbind(....\n\n\n\n\nHere's \nan extended stackoverflow introductory discussion of \nSO_REUSEPORT\n.",
            "title": "Networking, Part 4: Building a simple TCP Server"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-htons-and-when-is-it-used",
            "text": "Integers can be represented in least significant byte first or most-significant byte first. Either approach is reasonable as long as the machine itself is internally consistent. For network communications we need to standardize on agreed format.  htons(xyz)  returns the 16 bit unsigned integer 'short' value xyz in network byte order. htonl(xyz)  returns the 32 bit unsigned integer 'long' value xyz in network byte order.  These functions are read as 'host to network'; the inverse functions (ntohs, ntohl) convert network ordered byte values to host-ordered ordering. So, is host-ordering  little-endian or big-endian? The answer is - it depends on your machine! It depends on the actual architecture of the host running the code. If the architecture happens to be the same as network ordering then the result of these functions is just the argument. For x86 machines, the host and network ordering  is  different.  Summary: Whenever you read or write the low level C network structures (e.g. port and address information), remember to use the above functions to ensure correct conversion to/from a machine format. Otherwise the displayed or specified value may be incorrect.",
            "title": "What is htons and when is it used?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-are-the-big-4-network-calls-used-to-create-a-server",
            "text": "The four system calls required to create a TCP server are:  socket ,  bind   listen  and  accept . Each has a specific purpose and should be called in the above order  The port information (used by bind) can be set manually (many older IPv4-only C code examples do this), or be created using  getaddrinfo  We also see examples of setsockopt later too.",
            "title": "What are the 'big 4' network calls used to create a server?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-the-purpose-of-calling-socket",
            "text": "To create a endpoint for networking communication. A new socket by itself is not particularly useful; though we've specified either a packet or stream-based connections it is not bound to a particular network interface or port. Instead socket returns a network descriptor that can be used with later calls to bind,listen and accept.",
            "title": "What is the purpose of calling socket?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-the-purpose-of-calling-bind",
            "text": "The  bind  call associates an abstract socket with an actual network interface and port. It is possible to call bind on a TCP client however it's unusually unnecessary to specify the outgoing port.",
            "title": "What is the purpose of calling bind"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-the-purpose-of-calling-listen",
            "text": "The  listen  call specifies the queue size for the number of incoming, unhandled connections i.e. that have not yet been assigned a network descriptor by  accept \nTypical values for a high performance server are 128 or more.",
            "title": "What is the purpose of calling listen"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#why-are-server-sockets-passive",
            "text": "Server sockets do not actively try to connect to another host; instead they wait for incoming connections. Additionally, server sockets are not closed when the peer disconnects. Instead when a remote client connects, it is immediately bumped to an unused port number for future communications.",
            "title": "Why are server sockets passive?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-the-purpose-of-calling-accept",
            "text": "Once the server socket has been initialized the server calls  accept  to wait for new connections. Unlike  socket   bind  and  listen , this call will block. i.e. if there are no new connections, this call will block and only return when a new client connects.  Note the  accept  call returns a new file descriptor. This file descriptor is specific to a particular client. It is common programming mistake to use the original server socket descriptor for server I/O and then wonder why networking code has failed.",
            "title": "What is the purpose of calling accept"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-are-the-gotchas-of-creating-a-tcp-server",
            "text": "Using the socket descriptor of the passive server socket (described above)  Not specifying SOCK_STREAM requirement for getaddrinfo  Not being able to re-use an existing port.  Not initializing the unused struct entries  The  bind  call will fail if the port is currently in use   Note, ports are per machine- not per process or per user. In other words,  you cannot use port 1234 while another process is using that port. Worse, ports are by default 'tied up' after a process has finished.",
            "title": "What are the gotchas of creating a TCP-server?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#server-code-example",
            "text": "A working simple server example is shown below. Note this example is incomplete - for example it does not close either socket descriptor, or free up memory created by  getaddrinfo  \n#include <string.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <unistd.h>\n#include <arpa/inet.h>\n\nint main(int argc, char **argv)\n{\n    int s;\n    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    struct addrinfo hints, *result;\n    memset(&hints, 0, sizeof(struct addrinfo));\n    hints.ai_family = AF_INET;\n    hints.ai_socktype = SOCK_STREAM;\n    hints.ai_flags = AI_PASSIVE;\n\n    s = getaddrinfo(NULL, \"1234\", &hints, &result);\n    if (s != 0) {\n            fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(s));\n            exit(1);\n    }\n\n    if (bind(sock_fd, result->ai_addr, result->ai_addrlen) != 0) {\n        perror(\"bind()\");\n        exit(1);\n    }\n\n    if (listen(sock_fd, 10) != 0) {\n        perror(\"listen()\");\n        exit(1);\n    }\n\n    struct sockaddr_in *result_addr = (struct sockaddr_in *) result->ai_addr;\n    printf(\"Listening on file descriptor %d, port %d\\n\", sock_fd, ntohs(result_addr->sin_port));\n\n    printf(\"Waiting for connection...\\n\");\n    int client_fd = accept(sock_fd, NULL, NULL);\n    printf(\"Connection made: client_fd=%d\\n\", client_fd);\n\n    char buffer[1000];\n    int len = read(client_fd, buffer, sizeof(buffer) - 1);\n    buffer[len] = '\\0';\n\n    printf(\"Read %d chars\\n\", len);\n    printf(\"===\\n\");\n    printf(\"%s\\n\", buffer);\n\n    return 0;\n}",
            "title": "Server code example"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-4:-Building-a-simple-TCP-Server/#why-cant-my-server-re-use-the-port",
            "text": "By default a port is not immediately released when the socket is closed. Instead, the port enters a \"TIMED-WAIT\" state. This can lead to significant confusion during development because the timeout can make valid networking code appear to fail.  To be able to immediately re-use a port, specify  SO_REUSEPORT  before binding to the port.  int optval = 1;\nsetsockopt(sfd, SOL_SOCKET, SO_REUSEPORT, &optval, sizeof(optval));\n\nbind(....  Here's  an extended stackoverflow introductory discussion of  SO_REUSEPORT .",
            "title": "Why can't my server re-use the port?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-5:-Shutting-down-ports,-reusing-ports-and-other-tricks/",
            "text": "What is the difference shutdown and close?\n\n\nUse the \nshutdown\n call when you no longer need to read any more data from the socket, write more data, or have finished doing both.\nWhen you shutdown a socket for further writing (or reading) that information is also sent to the other end of the connection. For example if you shutdown the socket for further writing at the server end, then a moment later,a blocked \nread\n call could return 0 to indicate that no more bytes are expected.\n\n\nUse \nclose\n when your process no longer needs the socket file descriptor. \n\n\nIf you \nfork\n-ed after creating a socket file descriptor, all processes need to close the socket before the socket resources can be re-used.  If you shutdown a socket for further read then all process are be affected because you've changed the socket, not just the file descriptor.\n\n\nWell written code will \nshutdown\n a socket before calling \nclose\n it.\n\n\nWhen I re-run my server code it doesn't work! Why?\n\n\nBy default, after a socket is closed the port enters a time-out state during which time it cannot be re-used ('bound to a new socket').\n\n\nThis behavior can be disabled by setting the socket option REUSEPORT before bind-ing to a port:\n\n\n    int optval = 1;\n    setsockopt(sock_fd, SOL_SOCKET, SO_REUSEPORT, &optval, sizeof(optval));\n\n    bind(sock_fd, ...);\n\n\n\n\nCan a TCP client bind to a particular port?\n\n\nYes! In fact outgoing TCP connections are automatically bound to an unused port on the client. Usually it's unnecessary to explicitly set the port on the client because the system will intelligently find an unusued port on a reasonable interface (e.g. the wireless card, if currently connected by WiFi connection). However it can be useful if you needed to specifically choose a particular ethernet card, or if a firewall only allows outgoing connections from a particular range of port values.\n\n\nTo explicitly bind to an ethernet interface and port, call \nbind\n before \nconnect\n\n\nWho connected to my server?\n\n\nThe \naccept\n system call can optionally provide information about the remote client, by passing in a sockaddr struct. Different protocols have differently variants of the  \nstruct sockaddr\n, which are different sizes. The simplest struct to use is the \nsockaddr_storage\n which is sufficiently large to represent all possible types of sockaddr. Notice that C does not have any model of inheritance. Therefore we need to explicitly cast our struct to the 'base type' struct sockaddr.\n\n\n    struct sockaddr_storage clientaddr;\n    socklen_t clientaddrsize = sizeof(clientaddr);\n    int client_id = accept(passive_socket,\n            (struct sockaddr *) &clientaddr,\n             &clientaddrsize);\n\n\n\n\nWe've already seen \ngetaddrinfo\n that can build a linked list of addrinfo entries (and each one of these can include socket configuration data). What if we wanted to turn socket data into IP and port addresses? Enter \ngetnameinfo\n that can be used to convert a local or remote socket information into a domain name or numeric IP. Similarly the port number can be represented as a service name (e.g. \"http\" for port 80). In the example below we request numeric versions for the client IP address and client port number.\n\n\n    socklen_t clientaddrsize = sizeof(clientaddr);\n    int client_id = accept(sock_id, (struct sockaddr *) &clientaddr, &clientaddrsize);\n    char host[256], port[256];\n    getnameinfo((struct sockaddr *) &clientaddr,\n          clientaddrsize, host, sizeof(host), port, sizeof(port),\n          NI_NUMERICHOST | NI_NUMERICSERV);\n\n\n\n\nTodo: Discuss NI_MAXHOST and NI_MAXSERV, and NI_NUMERICHOST \n\n\ngetnameinfo Example: What's my IP address?\n\n\nTo obtain a linked list of IP addresses of the current machine use \ngetifaddrs\n which will return a linked list of IPv4 and IPv6 IP addresses (and potentially other interfaces too). We can examine each entry and use \ngetnameinfo\n to print the host's IP address.\nThe  ifaddrs struct includes the family but does not include the sizeof the struct. Therefore we need to manually determine the struct sized based on the family (IPv4 v IPv6)\n\n\n (family == AF_INET) ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6)\n\n\n\n\nThe complete code is shown below.\n\n\n    int required_family = AF_INET; // Change to AF_INET6 for IPv6\n    struct ifaddrs *myaddrs, *ifa;\n    getifaddrs(&myaddrs);\n    char host[256], port[256];\n    for (ifa = myaddrs; ifa != NULL; ifa = ifa->ifa_next) {\n        int family = ifa->ifa_addr->sa_family;\n        if (family == required_family && ifa->ifa_addr) {\n            if (0 == getnameinfo(ifa->ifa_addr,\n                                (family == AF_INET) ? sizeof(struct sockaddr_in) :\n                                sizeof(struct sockaddr_in6),\n                                host, sizeof(host), port, sizeof(port)\n                                 , NI_NUMERICHOST | NI_NUMERICSERV  ))\n                puts(host);\n            }\n        }\n\n\n\n\nWhat's my machine's IP address (shell version)\n\n\nAnswer: use \nifconfig\n (or Windows's ipconfig)\nHowever this command generates a lot of output for each interface, so we can filter the output using grep\n\n\nifconfig | grep inet\n\nExample output:\n    inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 \n    inet 127.0.0.1 netmask 0xff000000 \n    inet6 ::1 prefixlen 128 \n    inet6 fe80::7256:81ff:fe9a:9141%en1 prefixlen 64 scopeid 0x5 \n    inet 192.168.1.100 netmask 0xffffff00 broadcast 192.168.1.255",
            "title": "Networking, Part 5: Shutting down ports, reusing ports and other tricks"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-5:-Shutting-down-ports,-reusing-ports-and-other-tricks/#what-is-the-difference-shutdown-and-close",
            "text": "Use the  shutdown  call when you no longer need to read any more data from the socket, write more data, or have finished doing both.\nWhen you shutdown a socket for further writing (or reading) that information is also sent to the other end of the connection. For example if you shutdown the socket for further writing at the server end, then a moment later,a blocked  read  call could return 0 to indicate that no more bytes are expected.  Use  close  when your process no longer needs the socket file descriptor.   If you  fork -ed after creating a socket file descriptor, all processes need to close the socket before the socket resources can be re-used.  If you shutdown a socket for further read then all process are be affected because you've changed the socket, not just the file descriptor.  Well written code will  shutdown  a socket before calling  close  it.",
            "title": "What is the difference shutdown and close?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-5:-Shutting-down-ports,-reusing-ports-and-other-tricks/#when-i-re-run-my-server-code-it-doesnt-work-why",
            "text": "By default, after a socket is closed the port enters a time-out state during which time it cannot be re-used ('bound to a new socket').  This behavior can be disabled by setting the socket option REUSEPORT before bind-ing to a port:      int optval = 1;\n    setsockopt(sock_fd, SOL_SOCKET, SO_REUSEPORT, &optval, sizeof(optval));\n\n    bind(sock_fd, ...);",
            "title": "When I re-run my server code it doesn't work! Why?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-5:-Shutting-down-ports,-reusing-ports-and-other-tricks/#can-a-tcp-client-bind-to-a-particular-port",
            "text": "Yes! In fact outgoing TCP connections are automatically bound to an unused port on the client. Usually it's unnecessary to explicitly set the port on the client because the system will intelligently find an unusued port on a reasonable interface (e.g. the wireless card, if currently connected by WiFi connection). However it can be useful if you needed to specifically choose a particular ethernet card, or if a firewall only allows outgoing connections from a particular range of port values.  To explicitly bind to an ethernet interface and port, call  bind  before  connect",
            "title": "Can a TCP client bind to a particular port?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-5:-Shutting-down-ports,-reusing-ports-and-other-tricks/#who-connected-to-my-server",
            "text": "The  accept  system call can optionally provide information about the remote client, by passing in a sockaddr struct. Different protocols have differently variants of the   struct sockaddr , which are different sizes. The simplest struct to use is the  sockaddr_storage  which is sufficiently large to represent all possible types of sockaddr. Notice that C does not have any model of inheritance. Therefore we need to explicitly cast our struct to the 'base type' struct sockaddr.      struct sockaddr_storage clientaddr;\n    socklen_t clientaddrsize = sizeof(clientaddr);\n    int client_id = accept(passive_socket,\n            (struct sockaddr *) &clientaddr,\n             &clientaddrsize);  We've already seen  getaddrinfo  that can build a linked list of addrinfo entries (and each one of these can include socket configuration data). What if we wanted to turn socket data into IP and port addresses? Enter  getnameinfo  that can be used to convert a local or remote socket information into a domain name or numeric IP. Similarly the port number can be represented as a service name (e.g. \"http\" for port 80). In the example below we request numeric versions for the client IP address and client port number.      socklen_t clientaddrsize = sizeof(clientaddr);\n    int client_id = accept(sock_id, (struct sockaddr *) &clientaddr, &clientaddrsize);\n    char host[256], port[256];\n    getnameinfo((struct sockaddr *) &clientaddr,\n          clientaddrsize, host, sizeof(host), port, sizeof(port),\n          NI_NUMERICHOST | NI_NUMERICSERV);  Todo: Discuss NI_MAXHOST and NI_MAXSERV, and NI_NUMERICHOST",
            "title": "Who connected to my server?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-5:-Shutting-down-ports,-reusing-ports-and-other-tricks/#getnameinfo-example-whats-my-ip-address",
            "text": "To obtain a linked list of IP addresses of the current machine use  getifaddrs  which will return a linked list of IPv4 and IPv6 IP addresses (and potentially other interfaces too). We can examine each entry and use  getnameinfo  to print the host's IP address.\nThe  ifaddrs struct includes the family but does not include the sizeof the struct. Therefore we need to manually determine the struct sized based on the family (IPv4 v IPv6)   (family == AF_INET) ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6)  The complete code is shown below.      int required_family = AF_INET; // Change to AF_INET6 for IPv6\n    struct ifaddrs *myaddrs, *ifa;\n    getifaddrs(&myaddrs);\n    char host[256], port[256];\n    for (ifa = myaddrs; ifa != NULL; ifa = ifa->ifa_next) {\n        int family = ifa->ifa_addr->sa_family;\n        if (family == required_family && ifa->ifa_addr) {\n            if (0 == getnameinfo(ifa->ifa_addr,\n                                (family == AF_INET) ? sizeof(struct sockaddr_in) :\n                                sizeof(struct sockaddr_in6),\n                                host, sizeof(host), port, sizeof(port)\n                                 , NI_NUMERICHOST | NI_NUMERICSERV  ))\n                puts(host);\n            }\n        }",
            "title": "getnameinfo Example: What's my IP address?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-5:-Shutting-down-ports,-reusing-ports-and-other-tricks/#whats-my-machines-ip-address-shell-version",
            "text": "Answer: use  ifconfig  (or Windows's ipconfig)\nHowever this command generates a lot of output for each interface, so we can filter the output using grep  ifconfig | grep inet\n\nExample output:\n    inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 \n    inet 127.0.0.1 netmask 0xff000000 \n    inet6 ::1 prefixlen 128 \n    inet6 fe80::7256:81ff:fe9a:9141%en1 prefixlen 64 scopeid 0x5 \n    inet 192.168.1.100 netmask 0xffffff00 broadcast 192.168.1.255",
            "title": "What's my machine's IP address (shell version)"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-6:-Building-a-web-server/",
            "text": "How can you serve a simple static web page?\n\n\nA web server needs to implement HTTP (Hypertext transfer protocol), which is a specification of how client machine can request resources from a server and how a server can respond to a client message.\n\n\nWe're not going to build a fully compliant web server. Instead we will build the simplest possible web server, using the parts of TCP server that have already been introduced.\n\n\nFirst, we'll set up a passive server socket\n\n\n hints.ai_family = AF_INET;\n hints.ai_socktype = SOCK_STREAM;\n hints.ai_flags = AI_PASSIVE;\n getaddrinfo(NULL, \"0\", &hints, &result); // \"0\"  = use any port that is free\n\n\n\n\nWe don't need the timeout after we've finished using the port - it can be reused immediately,\n\n\n int optval = 1;\n setsockopt(sock_fd, SOL_SOCKET, SO_REUSEPORT, &optval, sizeof(optval));\n\n\n\n\nBind the socket to a port on the local machine and start listening for connections\n\n\n bind(sock_fd, result->ai_addr, result->ai_addrlen)\n listen(sock_fd, 10)\n\n\n\n\nNow that we've bound to a port, we can find out the actual port number used. Notice we need convert it from network to host byte ordering, (\nntohs\n)\n\n\n  struct sockaddr_in sin;\n  socklen_t socklen = sizeof(sin);\n  getsockname(sock_fd, (struct sockaddr *)&sin, &socklen)\n  printf(\"Listening on port %d\\n\", ntohs(sin.sin_port)) \n````\n\nNow call `accept` block until we have a client request to service. For each client, wait for the web browser's request then always send a friendly message back. This starts with the response header that includes the MIMETYPE - the type of data that is represented by the bytes that follow. The response header, like the request header is terminated by two blank lines together `\\r\\n\\r\\n`/ Note this code also demonstrates use of Linux's `dprintf` which allows printf-like formatting directly to a file descriptor.\n```C\n  while(1) {\n    int client_fd = accept(sock_fd, (struct sockaddr*) &client_info, &size);\n\n    char *connected_ip= inet_ntoa(client_info.sin_addr);\n  // ^^^^ Does this look thread-safe to you?\n    int port = ntohs(client_info.sin_port);\n\n\n    char buffer[1000];\n    read(client_fd, buffer, sizeof(buffer)-1);\n    dprintf(client_fd, \"HTTP/1.0 200 OK\\r\\n\");\n    dprintf(client_fd, \"Content-Type: text/html\\r\\n\\r\\n\");\n    dprintf(client_fd, \"<html><body><h1>Hello</h1></body></html>\");\n    shutdown(client_fd , SHUT_RDWR);\n    close(client_fd);\n  }\n\n\n\n\nHow can you serve a simple static image file?\n\n\n  FILE*file = fopen(\"apicture.jpg\",\"r\");\n  if(file) {\n    fseek(file,0, SEEK_END);\n    long size = ftell(file);\n    fseek(file,0,SEEK_SET);\n    printf(\"Sending %ld bytes\\n\", size);\n\n    char*buf = malloc(size);\n    fread(buf,1,size,file);\n\n    char response[2048];\n\n    sprintf( response, \"HTTP/1.0 200 OK\\r\\n\"\n             \"Content-Type: image/jpeg\\r\\n\"\n             \"Content-Length: %ld\\r\\n\\r\\n\" , size);\n\n    write(client_fd, response, strlen(response));\n\n    write(client_fd, buf, size);\n    fclose(file);\n    free(buf);\n  }     \n  shutdown(client_fd , SHUT_RDWR);\n  close(client_fd);",
            "title": "Networking, Part 6: Building a web server"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-6:-Building-a-web-server/#how-can-you-serve-a-simple-static-web-page",
            "text": "A web server needs to implement HTTP (Hypertext transfer protocol), which is a specification of how client machine can request resources from a server and how a server can respond to a client message.  We're not going to build a fully compliant web server. Instead we will build the simplest possible web server, using the parts of TCP server that have already been introduced.  First, we'll set up a passive server socket   hints.ai_family = AF_INET;\n hints.ai_socktype = SOCK_STREAM;\n hints.ai_flags = AI_PASSIVE;\n getaddrinfo(NULL, \"0\", &hints, &result); // \"0\"  = use any port that is free  We don't need the timeout after we've finished using the port - it can be reused immediately,   int optval = 1;\n setsockopt(sock_fd, SOL_SOCKET, SO_REUSEPORT, &optval, sizeof(optval));  Bind the socket to a port on the local machine and start listening for connections   bind(sock_fd, result->ai_addr, result->ai_addrlen)\n listen(sock_fd, 10)  Now that we've bound to a port, we can find out the actual port number used. Notice we need convert it from network to host byte ordering, ( ntohs )    struct sockaddr_in sin;\n  socklen_t socklen = sizeof(sin);\n  getsockname(sock_fd, (struct sockaddr *)&sin, &socklen)\n  printf(\"Listening on port %d\\n\", ntohs(sin.sin_port)) \n````\n\nNow call `accept` block until we have a client request to service. For each client, wait for the web browser's request then always send a friendly message back. This starts with the response header that includes the MIMETYPE - the type of data that is represented by the bytes that follow. The response header, like the request header is terminated by two blank lines together `\\r\\n\\r\\n`/ Note this code also demonstrates use of Linux's `dprintf` which allows printf-like formatting directly to a file descriptor.\n```C\n  while(1) {\n    int client_fd = accept(sock_fd, (struct sockaddr*) &client_info, &size);\n\n    char *connected_ip= inet_ntoa(client_info.sin_addr);\n  // ^^^^ Does this look thread-safe to you?\n    int port = ntohs(client_info.sin_port);\n\n\n    char buffer[1000];\n    read(client_fd, buffer, sizeof(buffer)-1);\n    dprintf(client_fd, \"HTTP/1.0 200 OK\\r\\n\");\n    dprintf(client_fd, \"Content-Type: text/html\\r\\n\\r\\n\");\n    dprintf(client_fd, \"<html><body><h1>Hello</h1></body></html>\");\n    shutdown(client_fd , SHUT_RDWR);\n    close(client_fd);\n  }",
            "title": "How can you serve a simple static web page?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-6:-Building-a-web-server/#how-can-you-serve-a-simple-static-image-file",
            "text": "FILE*file = fopen(\"apicture.jpg\",\"r\");\n  if(file) {\n    fseek(file,0, SEEK_END);\n    long size = ftell(file);\n    fseek(file,0,SEEK_SET);\n    printf(\"Sending %ld bytes\\n\", size);\n\n    char*buf = malloc(size);\n    fread(buf,1,size,file);\n\n    char response[2048];\n\n    sprintf( response, \"HTTP/1.0 200 OK\\r\\n\"\n             \"Content-Type: image/jpeg\\r\\n\"\n             \"Content-Length: %ld\\r\\n\\r\\n\" , size);\n\n    write(client_fd, response, strlen(response));\n\n    write(client_fd, buf, size);\n    fclose(file);\n    free(buf);\n  }     \n  shutdown(client_fd , SHUT_RDWR);\n  close(client_fd);",
            "title": "How can you serve a simple static image file?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-6:-Creating-a-UDP-server/",
            "text": "How do I create a UDP server?\n\n\nThere are a variety of function calls available to send UDP sockets. We will use the newer getaddrinfo to help set up a socket structure.\n\n\nRemember that UDP is a simple packet-based ('data-gram') protocol ; there is no connection to set up between the two hosts.\n\n\nFirst, initialize the hints addrinfo struct to request an IPv6, passive datagram socket.\n\n\nmemset(&hints, 0, sizeof(hints));\nhints.ai_family = AF_INET6; // INET for IPv4\nhints.ai_socktype =  SOCK_DGRAM;\nhints.ai_flags =  AI_PASSIVE;\n\n\n\n\nNext, use getaddrinfo to specify the port number (we don't need to specify a host as we are creating a server socket, not sending a packet to a remote host).\n\n\ngetaddrinfo(NULL, \"300\", &hints, &res);\n\nsockfd = socket(res->ai_family, res->ai_socktype, res->ai_protocol);\nbind(sockfd, res->ai_addr, res->ai_addrlen);\n\n\n\n\nThe port number is <1024, so the program will need \nroot\n privileges. We could have also specified a service name instead of a numeric port value.\n\n\nSo far the calls have been similar to a TCP server. For a stream-based service we would call \nlisten\n and accept. For our UDP-serve we can just start waiting for the arrival of a packet on the socket-\n\n\nstruct sockaddr_storage addr;\nint addrlen = sizeof(addr);\n\n// ssize_t recvfrom(int socket, void* buffer, size_t buflen, int flags, struct sockaddr *addr, socklen_t * address_len);\n\nbyte_count = recvfrom(sockfd, buf, sizeof(buf), 0, &addr, &addrlen);\n\n\n\n\nThe addr struct will hold sender (source) information about the arriving packet.\nNote the \nsockaddr_storage\n type is a sufficiently large enough to hold all possible types of socket addresses (e.g. IPv4, IPv6 and other socket types).\n\n\nFull Code\n\n\n#include <string.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <unistd.h>\n#include <arpa/inet.h>\n\nint main(int argc, char **argv)\n{\n    int s;\n\n    struct addrinfo hints, *result;\n    memset(&hints, 0, sizeof(hints));\n    hints.ai_family = AF_INET6; // INET for IPv4\n    hints.ai_socktype =  SOCK_DGRAM;\n    hints.ai_flags =  AI_PASSIVE;\n\n    getaddrinfo(NULL, \"300\", &hints, &res);\n\n    int sockfd = socket(res->ai_family, res->ai_socktype, res->ai_protocol);\n\n    if (bind(sockfd, res->ai_addr, res->ai_addrlen) != 0) {\n        perror(\"bind()\");\n        exit(1);\n    }\n    struct sockaddr_storage addr;\n    int addrlen = sizeof(addr);\n\n    while(1){\n        char buffer[1000];\n        ssize_t byte_count = recvfrom(sockfd, buf, sizeof(buf), 0, &addr, &addrlen);\n        buffer[byte_count] = '\\0';\n    }\n\n    printf(\"Read %d chars\\n\", len);\n    printf(\"===\\n\");\n    printf(\"%s\\n\", buffer);\n\n    return 0;\n}",
            "title": "Networking, Part 6: Creating a UDP server"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-6:-Creating-a-UDP-server/#how-do-i-create-a-udp-server",
            "text": "There are a variety of function calls available to send UDP sockets. We will use the newer getaddrinfo to help set up a socket structure.  Remember that UDP is a simple packet-based ('data-gram') protocol ; there is no connection to set up between the two hosts.  First, initialize the hints addrinfo struct to request an IPv6, passive datagram socket.  memset(&hints, 0, sizeof(hints));\nhints.ai_family = AF_INET6; // INET for IPv4\nhints.ai_socktype =  SOCK_DGRAM;\nhints.ai_flags =  AI_PASSIVE;  Next, use getaddrinfo to specify the port number (we don't need to specify a host as we are creating a server socket, not sending a packet to a remote host).  getaddrinfo(NULL, \"300\", &hints, &res);\n\nsockfd = socket(res->ai_family, res->ai_socktype, res->ai_protocol);\nbind(sockfd, res->ai_addr, res->ai_addrlen);  The port number is <1024, so the program will need  root  privileges. We could have also specified a service name instead of a numeric port value.  So far the calls have been similar to a TCP server. For a stream-based service we would call  listen  and accept. For our UDP-serve we can just start waiting for the arrival of a packet on the socket-  struct sockaddr_storage addr;\nint addrlen = sizeof(addr);\n\n// ssize_t recvfrom(int socket, void* buffer, size_t buflen, int flags, struct sockaddr *addr, socklen_t * address_len);\n\nbyte_count = recvfrom(sockfd, buf, sizeof(buf), 0, &addr, &addrlen);  The addr struct will hold sender (source) information about the arriving packet.\nNote the  sockaddr_storage  type is a sufficiently large enough to hold all possible types of socket addresses (e.g. IPv4, IPv6 and other socket types).",
            "title": "How do I create a UDP server?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-6:-Creating-a-UDP-server/#full-code",
            "text": "#include <string.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <unistd.h>\n#include <arpa/inet.h>\n\nint main(int argc, char **argv)\n{\n    int s;\n\n    struct addrinfo hints, *result;\n    memset(&hints, 0, sizeof(hints));\n    hints.ai_family = AF_INET6; // INET for IPv4\n    hints.ai_socktype =  SOCK_DGRAM;\n    hints.ai_flags =  AI_PASSIVE;\n\n    getaddrinfo(NULL, \"300\", &hints, &res);\n\n    int sockfd = socket(res->ai_family, res->ai_socktype, res->ai_protocol);\n\n    if (bind(sockfd, res->ai_addr, res->ai_addrlen) != 0) {\n        perror(\"bind()\");\n        exit(1);\n    }\n    struct sockaddr_storage addr;\n    int addrlen = sizeof(addr);\n\n    while(1){\n        char buffer[1000];\n        ssize_t byte_count = recvfrom(sockfd, buf, sizeof(buf), 0, &addr, &addrlen);\n        buffer[byte_count] = '\\0';\n    }\n\n    printf(\"Read %d chars\\n\", len);\n    printf(\"===\\n\");\n    printf(\"%s\\n\", buffer);\n\n    return 0;\n}",
            "title": "Full Code"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-7:-Nonblocking-I-O,-select(),-and-epoll/",
            "text": "Don't waste time waiting\n\n\nNormally, when you call \nread()\n, if the data is not available yet it will wait until the data is ready before the function returns.  When you're reading data from a disk, that delay may not be long, but when you're reading from a slow network connection it may take a long time for that data to arrive, if it ever arrives.  \n\n\nPOSIX lets you set a flag on a file descriptor such that any call to \nread()\n on that file descriptor will return immediately, whether it has finished or not.  With your file descriptor in this mode, your call to \nread()\n will start\nthe read operation, and while it's working you can do other useful work.  This is called \"nonblocking\" mode,\nsince the call to \nread()\n doesn't block.\n\n\nTo set a file descriptor to be nonblocking:\n\n\n    // fd is my file descriptor\n    int flags = fcntl(fd, F_GETFL, 0);\n    fcntl(fd, F_SETFL, flags | O_NONBLOCK);\n\n\n\n\nFor a socket, you can create it in nonblocking mode by adding \nSOCK_NONBLOCK\n to the second argument to \nsocket()\n:\n\n\n    fd = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, 0);\n\n\n\n\nWhen a file is in nonblocking mode and you call \nread()\n, it will return immediately with whatever bytes are available.\nSay 100 bytes have arrived from the server at the other end of your socket and you call \nread(fd, buf, 150)\n.\nRead will return immediately with a value of 100, meaning it read 100 of the 150 bytes you asked for.\nSay you tried to read the remaining data with a call to \nread(fd, buf+100, 50)\n, but the last 50 bytes still hadn't\narrived yet.  \nread()\n would return -1 and set the global error variable \nerrno\n to either\nEAGAIN or EWOULDBLOCK.  That's the system's way of telling you the data isn't ready yet.\n\n\nwrite()\n also works in nonblocking mode.  Say you want to send 40,000 bytes to a remote server using a socket.\nThe system can only send so many bytes at a time. Common systems can send about 23,000 bytes at a time. In nonblocking mode, \nwrite(fd, buf, 40000)\n would return the number of bytes it was able to\nsend immediately, or about 23,000.  If you called \nwrite()\n right away again, it would return -1 and set errno to\nEAGAIN or EWOULDBLOCK. That's the system's way of telling you it's still busy sending the last chunk of data,\nand isn't ready to send more yet.\n\n\nHow do I check when the I/O has finished?\n\n\nThere are a few ways.  Let's see how to do it using \nselect\n and \nepoll\n.\n\n\nselect\n\n\n    int select(int nfds, \n               fd_set *readfds, \n               fd_set *writefds,\n               fd_set *exceptfds, \n               struct timeval *timeout);\n\n\n\n\nGiven three sets of file descriptors, \nselect()\n will wait for any of those file descriptors to become 'ready'.\n\n \nreadfds\n - a file descriptor in \nreadfds\n is ready when there is data that can be read or EOF has been reached.\n\n \nwritefds\n - a file descriptor in \nwritefds\n is ready when a call to write() will succeed.\n* \nexceptfds\n - system-specific, not well-defined.  Just pass NULL for this.\n\n\nselect()\n returns the total number of file descriptors that are ready.  If none of them become\nready during the time defined by \ntimeout\n, it will return 0.  After \nselect()\n returns, the \ncaller will need to loop\nthrough the file descriptors in readfds and/or writefds to see which ones are ready.\n\n\n    fd_set readfds, writefds;\n    FD_ZERO(&readfds);\n    FD_ZERO(&writefds);\n    for (int i=0; i < read_fd_count; i++)\n      FD_SET(my_read_fds[i], &readfds);\n    for (int i=0; i < write_fd_count; i++)\n      FD_SET(my_write_fds[i], &writefds);\n\n    struct timeval timeout;\n    timeout.tv_sec = 3;\n    timeout.tv_usec = 0;\n\n    int num_ready = select(FD_SETSIZE, &readfds, &writefds, NULL, &timeout);\n\n    if (num_ready < 0) {\n      perror(\"error in select()\");\n    } else if (num_ready == 0) {\n      printf(\"timeout\\n\");\n    } else {\n      for (int i=0; i < read_fd_count; i++)\n        if (FD_ISSET(my_read_fds[i], &readfds))\n          printf(\"fd %d is ready for reading\\n\", my_read_fds[i]);\n      for (int i=0; i < write_fd_count; i++)\n        if (FD_ISSET(my_write_fds[i], &writefds))\n          printf(\"fd %d is ready for writing\\n\", my_write_fds[i]);\n    }\n\n\n\n\nFor more information on select()\n\n\nepoll\n\n\nepoll\n is not part of POSIX, but it is supported by Linux.  It is a more efficient way to wait for many\nfile descriptors.  It will tell you exactly which descriptors are ready. It even gives you a way to store\na small amount of data with each descriptor, like an array index or a pointer, making it easier to access\nyour data associated with that descriptor.\n\n\nTo use epoll, first you must create a special file descriptor with \nepoll_create()\n.  You won't read or write to this file\ndescriptor; you'll just pass it to the other epoll_xxx functions and call\nclose() on it at the end.\n\n\n    epfd = epoll_create(1);\n\n\n\n\nFor each file descriptor you want to monitor with epoll, you'll need to add it \nto the epoll data structures \nusing \nepoll_ctl()\n with the \nEPOLL_CTL_ADD\n option.  You can add any\nnumber of file descriptors to it.\n\n\n    struct epoll_event event;\n    event.events = EPOLLOUT;  // EPOLLIN==read, EPOLLOUT==write\n    event.data.ptr = mypointer;\n    epoll_ctl(epfd, EPOLL_CTL_ADD, mypointer->fd, &event)\n\n\n\n\nTo wait for some of the file descriptors to become ready, use \nepoll_wait()\n.\nThe epoll_event struct that it fills out will contain the data you provided in event.data when you\nadded this file descriptor. This makes it easy for you to look up your own data associated\nwith this file descriptor.\n\n\n    int num_ready = epoll_wait(epfd, &event, 1, timeout_milliseconds);\n    if (num_ready > 0) {\n      MyData *mypointer = (MyData*) event.data.ptr;\n      printf(\"ready to write on %d\\n\", mypointer->fd);\n    }\n\n\n\n\nSay you were waiting to write data to a file descriptor, but now you want to wait to read data from it.\nJust use \nepoll_ctl()\n with the \nEPOLL_CTL_MOD\n option to change the type of operation you're monitoring.\n\n\n    event.events = EPOLLOUT;\n    event.data.ptr = mypointer;\n    epoll_ctl(epfd, EPOLL_CTL_MOD, mypointer->fd, &event);\n\n\n\n\nTo unsubscribe one file descriptor from epoll while leaving others active, use \nepoll_ctl()\n with the \nEPOLL_CTL_DEL\n option.\n\n\n    epoll_ctl(epfd, EPOLL_CTL_DEL, mypointer->fd, NULL);\n\n\n\n\nTo shut down an epoll instance, close its file descriptor.\n\n\n    close(epfd);\n\n\n\n\nIn addition to nonblocking \nread()\n and \nwrite()\n, any calls to \nconnect()\n on a nonblocking socket will also be\nnonblocking. To wait for the connection to complete, use \nselect()\n or epoll to wait for the socket to be writable.\n\n\nInteresting Blogpost about edge cases with select\n\n\nhttps://idea.popcount.org/2017-01-06-select-is-fundamentally-broken/",
            "title": "Networking, Part 7: Nonblocking I O, select(), and epoll"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-7:-Nonblocking-I-O,-select(),-and-epoll/#dont-waste-time-waiting",
            "text": "Normally, when you call  read() , if the data is not available yet it will wait until the data is ready before the function returns.  When you're reading data from a disk, that delay may not be long, but when you're reading from a slow network connection it may take a long time for that data to arrive, if it ever arrives.    POSIX lets you set a flag on a file descriptor such that any call to  read()  on that file descriptor will return immediately, whether it has finished or not.  With your file descriptor in this mode, your call to  read()  will start\nthe read operation, and while it's working you can do other useful work.  This is called \"nonblocking\" mode,\nsince the call to  read()  doesn't block.  To set a file descriptor to be nonblocking:      // fd is my file descriptor\n    int flags = fcntl(fd, F_GETFL, 0);\n    fcntl(fd, F_SETFL, flags | O_NONBLOCK);  For a socket, you can create it in nonblocking mode by adding  SOCK_NONBLOCK  to the second argument to  socket() :      fd = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, 0);  When a file is in nonblocking mode and you call  read() , it will return immediately with whatever bytes are available.\nSay 100 bytes have arrived from the server at the other end of your socket and you call  read(fd, buf, 150) .\nRead will return immediately with a value of 100, meaning it read 100 of the 150 bytes you asked for.\nSay you tried to read the remaining data with a call to  read(fd, buf+100, 50) , but the last 50 bytes still hadn't\narrived yet.   read()  would return -1 and set the global error variable  errno  to either\nEAGAIN or EWOULDBLOCK.  That's the system's way of telling you the data isn't ready yet.  write()  also works in nonblocking mode.  Say you want to send 40,000 bytes to a remote server using a socket.\nThe system can only send so many bytes at a time. Common systems can send about 23,000 bytes at a time. In nonblocking mode,  write(fd, buf, 40000)  would return the number of bytes it was able to\nsend immediately, or about 23,000.  If you called  write()  right away again, it would return -1 and set errno to\nEAGAIN or EWOULDBLOCK. That's the system's way of telling you it's still busy sending the last chunk of data,\nand isn't ready to send more yet.",
            "title": "Don't waste time waiting"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-7:-Nonblocking-I-O,-select(),-and-epoll/#how-do-i-check-when-the-io-has-finished",
            "text": "There are a few ways.  Let's see how to do it using  select  and  epoll .",
            "title": "How do I check when the I/O has finished?"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-7:-Nonblocking-I-O,-select(),-and-epoll/#select",
            "text": "int select(int nfds, \n               fd_set *readfds, \n               fd_set *writefds,\n               fd_set *exceptfds, \n               struct timeval *timeout);  Given three sets of file descriptors,  select()  will wait for any of those file descriptors to become 'ready'.   readfds  - a file descriptor in  readfds  is ready when there is data that can be read or EOF has been reached.   writefds  - a file descriptor in  writefds  is ready when a call to write() will succeed.\n*  exceptfds  - system-specific, not well-defined.  Just pass NULL for this.  select()  returns the total number of file descriptors that are ready.  If none of them become\nready during the time defined by  timeout , it will return 0.  After  select()  returns, the \ncaller will need to loop\nthrough the file descriptors in readfds and/or writefds to see which ones are ready.      fd_set readfds, writefds;\n    FD_ZERO(&readfds);\n    FD_ZERO(&writefds);\n    for (int i=0; i < read_fd_count; i++)\n      FD_SET(my_read_fds[i], &readfds);\n    for (int i=0; i < write_fd_count; i++)\n      FD_SET(my_write_fds[i], &writefds);\n\n    struct timeval timeout;\n    timeout.tv_sec = 3;\n    timeout.tv_usec = 0;\n\n    int num_ready = select(FD_SETSIZE, &readfds, &writefds, NULL, &timeout);\n\n    if (num_ready < 0) {\n      perror(\"error in select()\");\n    } else if (num_ready == 0) {\n      printf(\"timeout\\n\");\n    } else {\n      for (int i=0; i < read_fd_count; i++)\n        if (FD_ISSET(my_read_fds[i], &readfds))\n          printf(\"fd %d is ready for reading\\n\", my_read_fds[i]);\n      for (int i=0; i < write_fd_count; i++)\n        if (FD_ISSET(my_write_fds[i], &writefds))\n          printf(\"fd %d is ready for writing\\n\", my_write_fds[i]);\n    }  For more information on select()",
            "title": "select"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-7:-Nonblocking-I-O,-select(),-and-epoll/#epoll",
            "text": "epoll  is not part of POSIX, but it is supported by Linux.  It is a more efficient way to wait for many\nfile descriptors.  It will tell you exactly which descriptors are ready. It even gives you a way to store\na small amount of data with each descriptor, like an array index or a pointer, making it easier to access\nyour data associated with that descriptor.  To use epoll, first you must create a special file descriptor with  epoll_create() .  You won't read or write to this file\ndescriptor; you'll just pass it to the other epoll_xxx functions and call\nclose() on it at the end.      epfd = epoll_create(1);  For each file descriptor you want to monitor with epoll, you'll need to add it \nto the epoll data structures \nusing  epoll_ctl()  with the  EPOLL_CTL_ADD  option.  You can add any\nnumber of file descriptors to it.      struct epoll_event event;\n    event.events = EPOLLOUT;  // EPOLLIN==read, EPOLLOUT==write\n    event.data.ptr = mypointer;\n    epoll_ctl(epfd, EPOLL_CTL_ADD, mypointer->fd, &event)  To wait for some of the file descriptors to become ready, use  epoll_wait() .\nThe epoll_event struct that it fills out will contain the data you provided in event.data when you\nadded this file descriptor. This makes it easy for you to look up your own data associated\nwith this file descriptor.      int num_ready = epoll_wait(epfd, &event, 1, timeout_milliseconds);\n    if (num_ready > 0) {\n      MyData *mypointer = (MyData*) event.data.ptr;\n      printf(\"ready to write on %d\\n\", mypointer->fd);\n    }  Say you were waiting to write data to a file descriptor, but now you want to wait to read data from it.\nJust use  epoll_ctl()  with the  EPOLL_CTL_MOD  option to change the type of operation you're monitoring.      event.events = EPOLLOUT;\n    event.data.ptr = mypointer;\n    epoll_ctl(epfd, EPOLL_CTL_MOD, mypointer->fd, &event);  To unsubscribe one file descriptor from epoll while leaving others active, use  epoll_ctl()  with the  EPOLL_CTL_DEL  option.      epoll_ctl(epfd, EPOLL_CTL_DEL, mypointer->fd, NULL);  To shut down an epoll instance, close its file descriptor.      close(epfd);  In addition to nonblocking  read()  and  write() , any calls to  connect()  on a nonblocking socket will also be\nnonblocking. To wait for the connection to complete, use  select()  or epoll to wait for the socket to be writable.",
            "title": "epoll"
        },
        {
            "location": "/SystemProgramming/Networking,-Part-7:-Nonblocking-I-O,-select(),-and-epoll/#interesting-blogpost-about-edge-cases-with-select",
            "text": "https://idea.popcount.org/2017-01-06-select-is-fundamentally-broken/",
            "title": "Interesting Blogpost about edge cases with select"
        },
        {
            "location": "/SystemProgramming/Networking-Review-Questions/",
            "text": "Topics\n\n\n\n\nIPv4 vs IPv6\n\n\nTCP vs UDP\n\n\nPacket Loss/Connection Based\n\n\nGet address info\n\n\nDNS\n\n\nTCP client calls\n\n\nTCP server calls\n\n\nshutdown\n\n\nrecvfrom\n\n\nepoll vs select\n\n\nRPC\n\n\n\n\nQuestions\n\n\n\n\nWhat is IPv4? IPv6? What are the differences between them?\n\n\nWhat is TCP? UDP? Give me advantages and disadvantages of both of them. When would I use one and not the other?\n\n\nWhich protocol is connection less and which one is connection based?\n\n\nWhat is DNS? What is the route that DNS takes?\n\n\nWhat does socket do?\n\n\nWhat are the calls to set up a TCP client?\n\n\nWhat are the calls to set up a TCP server?\n\n\nWhat is the difference between a socket shutdown and closing?\n\n\nWhen can you use \nread\n and \nwrite\n? How about \nrecvfrom\n and \nsendto\n?\n\n\nWhat are some advantages to \nepoll\n over \nselect\n? How about \nselect\n over \nepoll\n?\n\n\nWhat is a remote procedure call? When should I use it?\n\n\nWhat is marshalling/unmarshalling? Why is HTTP \nnot\n an RPC?",
            "title": "Networking Review Questions"
        },
        {
            "location": "/SystemProgramming/Networking-Review-Questions/#topics",
            "text": "IPv4 vs IPv6  TCP vs UDP  Packet Loss/Connection Based  Get address info  DNS  TCP client calls  TCP server calls  shutdown  recvfrom  epoll vs select  RPC",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/Networking-Review-Questions/#questions",
            "text": "What is IPv4? IPv6? What are the differences between them?  What is TCP? UDP? Give me advantages and disadvantages of both of them. When would I use one and not the other?  Which protocol is connection less and which one is connection based?  What is DNS? What is the route that DNS takes?  What does socket do?  What are the calls to set up a TCP client?  What are the calls to set up a TCP server?  What is the difference between a socket shutdown and closing?  When can you use  read  and  write ? How about  recvfrom  and  sendto ?  What are some advantages to  epoll  over  select ? How about  select  over  epoll ?  What is a remote procedure call? When should I use it?  What is marshalling/unmarshalling? Why is HTTP  not  an RPC?",
            "title": "Questions"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/",
            "text": "Wiki w/Interactive MC Questions\n\n\nSee \nCoding questions\n\n\nSee \nShort answer questions\n\n\nSee \nMP Wearables\n Food For Thought questions\n\n\n\n\nShort answer questions\n\n\nQ1\n\n\nWhat is a socket?\n\n\nQ2\n\n\nWhat is special about listening on port 1000 vs port 2000?\n\n\n\n\nPort 2000 is twice as slow as port 1000\n\n\nPort 2000 is twice as fast as port 1000\n\n\nPort 1000 requires root privileges\n\n\nNothing\n\n\n\n\nQ3\n\n\nDescribe one significant difference between IPv4 and IPv6\n\n\nQ4\n\n\nWhen and why would you use ntohs?\n\n\nQ5\n\n\nIf a host address is 32 bits which IP scheme am I most likely using? 128 bits?\n\n\nQ6\n\n\nWhich common network protocol is packet based and may not successfully deliver the data?\n\n\nQ7\n\n\nWhich common protocol is stream-based and will resend data if packets are lost?\n\n\nQ8\n\n\nWhat is the SYN ACK ACK-SYN handshake?\n\n\nQ9\n\n\nWhich one of the following is NOT a feature of TCP? \n- Packet re-ordering\n- Flow control\n- Packet re-tranmission\n- Simple error detection\n- Encryption\n\n\nQ10\n\n\nWhat protocol uses sequence numbers? What is their initial value? And why?\n\n\nQ11\n\n\nWhat are the minimum network calls are required to build a TCP server? What is their correct order?\n\n\nQ12\n\n\nWhat are the minimum network calls are required to build a TCP client? What is their correct order?\n\n\nQ13\n\n\nWhen would you call bind on a TCP client?\n\n\nQ14\n\n\nWhat is the purpose of\nsocket\nbind\nlisten\naccept\n?\n\n\nQ15\n\n\nWhich of the above calls can block, waiting for a new client to connect?\n\n\nQ16\n\n\nWhat is DNS? What does it do for you? Which of the CS241 network calls will use it for you?\n\n\nQ17\n\n\nFor getaddrinfo, how do you specify a server socket?\n\n\nQ18\n\n\nWhy may getaddrinfo generate network packets?\n\n\nQ19\n\n\nWhich network call specifies the size of the allowed backlog?\n\n\nQ20\n\n\nWhich network call returns a new file descriptor?\n\n\nQ21\n\n\nWhen are passive sockets used?\n\n\nQ22\n\n\nWhen is epoll a better choice than select? When is select a better choice than epoll?\n\n\nQ23\n\n\nWill  \nwrite(fd, data, 5000)\n  always send 5000 bytes of data? When can it fail?\n\n\nQ24\n\n\nHow does Network Address Translation (NAT) work? \n\n\nQ25\n\n\n@MCQ\nAssuming a network has a 20ms Transmit Time between Client and Server, how much time would it take to establish a TCP Connection?\n20 ms\n40 ms \n100 ms\n60 ms @ANS\n3 Way Handshake @EXP\n@END\n\n\nQ26\n\n\nWhat are some of the differences between HTTP 1.0 and HTTP 1.1? How many ms will it take to transmit 3 files from server to client if the network has a 20ms transmit time? How does the time taken differ between HTTP 1.0 and HTTP 1.1?\n\n\nCoding questions\n\n\nQ 2.1\n\n\nWriting to a network socket may not send all of the bytes and may be interrupted due to a signal. Check the return value of \nwrite\n to implement \nwrite_all\n that will repeatedly call \nwrite\n with any remaining data. If \nwrite\n returns -1 then immediately return -1 unless the \nerrno\n is \nEINTR\n - in which case repeat the last \nwrite\n attempt. You will need to use pointer arithmetic.\n\n\n// Returns -1 if write fails (unless EINTR in which case it recalls write\n// Repeated calls write until all of the buffer is written.\nssize_t write_all(int fd, const char *buf, size_t nbyte) {\n  ssize_t nb = write(fd, buf, nbyte);\n  return nb;\n}\n\n\n\n\nQ 2.2\n\n\nImplement a multithreaded TCP server that listens on port 2000. Each thread should read 128 bytes from the client file descriptor and echo it back to the client, before closing the connection and ending the thread.\n\n\nQ 2.3\n\n\nImplement a UDP server that listens on port 2000. Reserve a buffer of 200 bytes. Listen for an arriving packet. Valid packets are 200 bytes or less and start with four bytes 0x65 0x66 0x67 0x68. Ignore invalid packets. For valid packets add the value of the fifth byte as an unsigned value to a running total and print the total so far. If the running total is greater than 255 then exit.",
            "title": "Networking: Review Questions"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#short-answer-questions",
            "text": "",
            "title": "Short answer questions"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q1",
            "text": "What is a socket?",
            "title": "Q1"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q2",
            "text": "What is special about listening on port 1000 vs port 2000?   Port 2000 is twice as slow as port 1000  Port 2000 is twice as fast as port 1000  Port 1000 requires root privileges  Nothing",
            "title": "Q2"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q3",
            "text": "Describe one significant difference between IPv4 and IPv6",
            "title": "Q3"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q4",
            "text": "When and why would you use ntohs?",
            "title": "Q4"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q5",
            "text": "If a host address is 32 bits which IP scheme am I most likely using? 128 bits?",
            "title": "Q5"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q6",
            "text": "Which common network protocol is packet based and may not successfully deliver the data?",
            "title": "Q6"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q7",
            "text": "Which common protocol is stream-based and will resend data if packets are lost?",
            "title": "Q7"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q8",
            "text": "What is the SYN ACK ACK-SYN handshake?",
            "title": "Q8"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q9",
            "text": "Which one of the following is NOT a feature of TCP? \n- Packet re-ordering\n- Flow control\n- Packet re-tranmission\n- Simple error detection\n- Encryption",
            "title": "Q9"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q10",
            "text": "What protocol uses sequence numbers? What is their initial value? And why?",
            "title": "Q10"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q11",
            "text": "What are the minimum network calls are required to build a TCP server? What is their correct order?",
            "title": "Q11"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q12",
            "text": "What are the minimum network calls are required to build a TCP client? What is their correct order?",
            "title": "Q12"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q13",
            "text": "When would you call bind on a TCP client?",
            "title": "Q13"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q14",
            "text": "What is the purpose of\nsocket\nbind\nlisten\naccept\n?",
            "title": "Q14"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q15",
            "text": "Which of the above calls can block, waiting for a new client to connect?",
            "title": "Q15"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q16",
            "text": "What is DNS? What does it do for you? Which of the CS241 network calls will use it for you?",
            "title": "Q16"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q17",
            "text": "For getaddrinfo, how do you specify a server socket?",
            "title": "Q17"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q18",
            "text": "Why may getaddrinfo generate network packets?",
            "title": "Q18"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q19",
            "text": "Which network call specifies the size of the allowed backlog?",
            "title": "Q19"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q20",
            "text": "Which network call returns a new file descriptor?",
            "title": "Q20"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q21",
            "text": "When are passive sockets used?",
            "title": "Q21"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q22",
            "text": "When is epoll a better choice than select? When is select a better choice than epoll?",
            "title": "Q22"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q23",
            "text": "Will   write(fd, data, 5000)   always send 5000 bytes of data? When can it fail?",
            "title": "Q23"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q24",
            "text": "How does Network Address Translation (NAT) work?",
            "title": "Q24"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q25",
            "text": "@MCQ\nAssuming a network has a 20ms Transmit Time between Client and Server, how much time would it take to establish a TCP Connection?\n20 ms\n40 ms \n100 ms\n60 ms @ANS\n3 Way Handshake @EXP\n@END",
            "title": "Q25"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q26",
            "text": "What are some of the differences between HTTP 1.0 and HTTP 1.1? How many ms will it take to transmit 3 files from server to client if the network has a 20ms transmit time? How does the time taken differ between HTTP 1.0 and HTTP 1.1?",
            "title": "Q26"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#coding-questions",
            "text": "",
            "title": "Coding questions"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q-21",
            "text": "Writing to a network socket may not send all of the bytes and may be interrupted due to a signal. Check the return value of  write  to implement  write_all  that will repeatedly call  write  with any remaining data. If  write  returns -1 then immediately return -1 unless the  errno  is  EINTR  - in which case repeat the last  write  attempt. You will need to use pointer arithmetic.  // Returns -1 if write fails (unless EINTR in which case it recalls write\n// Repeated calls write until all of the buffer is written.\nssize_t write_all(int fd, const char *buf, size_t nbyte) {\n  ssize_t nb = write(fd, buf, nbyte);\n  return nb;\n}",
            "title": "Q 2.1"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q-22",
            "text": "Implement a multithreaded TCP server that listens on port 2000. Each thread should read 128 bytes from the client file descriptor and echo it back to the client, before closing the connection and ending the thread.",
            "title": "Q 2.2"
        },
        {
            "location": "/SystemProgramming/Networking:-Review-Questions/#q-23",
            "text": "Implement a UDP server that listens on port 2000. Reserve a buffer of 200 bytes. Listen for an arriving packet. Valid packets are 200 bytes or less and start with four bytes 0x65 0x66 0x67 0x68. Ignore invalid packets. For valid packets add the value of the fifth byte as an unsigned value to a running total and print the total so far. If the running total is greater than 255 then exit.",
            "title": "Q 2.3"
        },
        {
            "location": "/SystemProgramming/OSI-Model/",
            "text": "@MCQ \nWhich one of the following is NOT a feature of TCP?\n -Packet re-ordering\n -Flow control\n -Packet re-transmission\n -Simple error detection\n -Encryption @ans\nNo Hind available @hint\n@EXP\n@END",
            "title": "OSI Model"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/",
            "text": "What is POSIX error handling?\n\n\nIn other languages, you may see error handling implemented with exceptions. Although you technically can use them in c (You keep a stack of very try/catch block and use \nsetjmp\n and \nlongjmp\n to go to those blocks respectively), error handling in C is typically done with posix error handling the code typically looks like this.\n\n\nint ret = some_system_call()\nif(ret == ERROR_CODE){\nswitch(errno){\n// Do different stuff based on the errno number.\n}\n}\n\n\n\n\n\nIn the kernel, the use of \ngoto\n is heavily used to clean up different parts of the application. \nYou should not use gotos\n because they make code harder to read. gotos in the kernel are there out of necessity, so don't take lessons.\n\n\nWhat is \nerrno\n and when is it set?\n\n\nPOSIX defines a special integer \nerrno\n that is set when a system call fails.\nThe initial value of \nerrno\n is zero (i.e. no error).\nWhen a system call fails it will typically return -1 to indicate an error and set \nerrno\n\n\nWhat about multiple threads?\n\n\nEach thread has it's own copy of \nerrno\n. This is very useful; otherwise an error in one thread would interfere with the error status of another thread.\n\n\nWhen is \nerrno\n reset to zero?\n\n\nIt's not unless you specifically reset it to zero!  When system calls are successful they do \nnot\n reset the value of \nerrno\n.\n\n\nThis means you should only rely on the value of errno if you know a system call has failed (e.g. it returned -1).\n\n\nWhat are the gotchas and best practices of using \nerrno\n?\n\n\nBe careful when complex error handling use of library calls or system calls that may change the value of \nerrno\n. In practice it's safer to copy the value of errno into a int variable:\n\n\n// Unsafe - the first fprintf may change the value of errno before we use it!\nif (-1 == sem_wait(&s)) {\n   fprintf(stderr, \"An error occurred!\");\n   fprintf(stderr, \"The error value is %d\\n\", errno);\n}\n// Better, copy the value before making more system and library calls\nif (-1 == sem_wait(&s)) {\n   int errno_saved = errno;\n   fprintf(stderr, \"An error occurred!\");\n   fprintf(stderr, \"The error value is %d\\n\", errno_saved);\n}\n\n\n\n\nIn a similar vein, if your signal handler makes any system or library calls, then it is good practice to save the original value of errno and restore the value before returning:\n\n\nvoid handler(int signal) {\n   int errno_saved = errno;\n\n   // make system calls that might change errno\n\n   errno = errno_saved;\n}\n\n\n\n\nHow can you print out the string message associated with a particular error number?\n\n\nUse \nstrerror\n to get a short (English) description of the error value\n\n\nchar *mesg = strerror(errno);\nfprintf(stderr, \"An error occurred (errno=%d): %s\", errno, mesg);\n\n\n\n\nHow are perror and strerror related?\n\n\nIn previous pages we've used perror to print out the error to standard error. Using \nstrerror\n, we can now write a simple implementation of \nperror\n:\n\n\nvoid perror(char *what) {\n   fprintf(stderr, \"%s: %s\\n\", what, strerror(errno));\n}\n\n\n\n\nWhat are the gotchas of using strerror?\n\n\nUnfortunately \nstrerror\n is not threadsafe. In other words, two threads cannot call it at the same time!\n\n\nThere are two workarounds: Firstly we can use a mutex lock to define a critical section and a local buffer. The same mutex should be used by all threads in all places that call \nstrerror\n\n\npthread_mutex_lock(&m);\nchar *result = strerror(errno);\nchar *message = malloc(strlen(result) + 1);\nstrcpy(message, result);\npthread_mutex_unlock(&m);\nfprintf(stderr, \"An error occurred (errno=%d): %s\", errno, message);\nfree(message);\n\n\n\n\nAlternatively use the less portable but thread-safe \nstrerror_r\n\n\nWhat is EINTR? What does it mean for sem_wait? read? write?\n\n\nSome system calls can be interrupted when a signal (e.g SIGCHLD, SIGPIPE,...) is delivered to the process. At this point the system call may return without performing any action! For example, bytes may not have been read/written, semaphore wait may not have waited.\n\n\nThis interruption can be detected by checking the return value and if \nerrno\n is EINTR. In which case the system call should be retried. It's common to see the following kind of loop that wraps a system call (such as sem_wait).\n\n\nwhile ((-1 == systemcall(...)) && (errno == EINTR)) { /* repeat! */}\n\n\n\n\nBe careful to write \n== EINTR\n, not \n= EINTR\n.\n\n\nOr, if the result value needs to be used later...\n\n\nwhile ((-1 == (result = systemcall(...))) && (errno == EINTR)) { /* repeat! */}\n\n\n\n\nOn Linux,calling \nread\n and \nwrite\n to a local disk will normally not return with EINTR (instead the function is automatically restarted for you). However, calling \nread\n and \nwrite\n on a file descriptor that corresponds to a network stream \ncan\n return with EINTR.\n\n\nWhich system calls may be interrupted and need to be wrapped?\n\n\nUse man the page! The man page includes a list of errors (i.e. errno values) that may be set by the system call. A rule of thumb is 'slow' (blocking) calls (e.g. writing to a socket) may be interrupted but fast non-blocking calls (e.g. pthread_mutex_lock) will not.\n\n\nFrom the linux signal 7 man page.\n\n\n\"If a signal handler is invoked while a system call or library function call is blocked, then either:\n\n the call is automatically restarted after the signal handler returns; or\n\n the call fails with the error EINTR.\nWhich of these two behaviors occurs depends on the interface and whether or not the signal handler was established using the SA_RESTART flag (see sigaction(2)). The details vary across UNIX systems; below, the details for Linux.\n\n\nIf a blocked call to one of the following interfaces is interrupted by a signal handler, then the call will be automatically restarted after the signal handler returns if the SA_RESTART flag was used; otherwise the call will fail with the error EINTR:\n\n\n\n\nread(2), readv(2), write(2), writev(2), and ioctl(2) calls on \"slow\" devices. A \"slow\" device is one where the I/O call may block for an indefinite time, for example, a terminal, pipe, or socket. (A disk is not a slow device according to this definition.) If an I/O call on a slow device has already transferred some data by the time it is interrupted by a signal handler, then the call will return a success status (normally, the number of bytes transferred).\n\"\n\n\n\n\nNote, it is easy to believe that setting 'SA_RESTART' flag is sufficient to make this whole problem disappear. Unfortunately that's not true: there are still system calls that may return early and set \nEINTR\n! See \nsignal(7)\n for details. \n\n\nErrno exceptions?\n\n\nThere are some POSIX utilities that have their own errno per say. One is when you call \ngetaddrinfo\n the function to check that error and convert to a string is \ngai_strerr\n. Don't get them mixed up!",
            "title": "POSIX, Part 1: Error handling"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#what-is-posix-error-handling",
            "text": "In other languages, you may see error handling implemented with exceptions. Although you technically can use them in c (You keep a stack of very try/catch block and use  setjmp  and  longjmp  to go to those blocks respectively), error handling in C is typically done with posix error handling the code typically looks like this.  int ret = some_system_call()\nif(ret == ERROR_CODE){\nswitch(errno){\n// Do different stuff based on the errno number.\n}\n}  In the kernel, the use of  goto  is heavily used to clean up different parts of the application.  You should not use gotos  because they make code harder to read. gotos in the kernel are there out of necessity, so don't take lessons.",
            "title": "What is POSIX error handling?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#what-is-errno-and-when-is-it-set",
            "text": "POSIX defines a special integer  errno  that is set when a system call fails.\nThe initial value of  errno  is zero (i.e. no error).\nWhen a system call fails it will typically return -1 to indicate an error and set  errno",
            "title": "What is errno and when is it set?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#what-about-multiple-threads",
            "text": "Each thread has it's own copy of  errno . This is very useful; otherwise an error in one thread would interfere with the error status of another thread.",
            "title": "What about multiple threads?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#when-is-errno-reset-to-zero",
            "text": "It's not unless you specifically reset it to zero!  When system calls are successful they do  not  reset the value of  errno .  This means you should only rely on the value of errno if you know a system call has failed (e.g. it returned -1).",
            "title": "When is errno reset to zero?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#what-are-the-gotchas-and-best-practices-of-using-errno",
            "text": "Be careful when complex error handling use of library calls or system calls that may change the value of  errno . In practice it's safer to copy the value of errno into a int variable:  // Unsafe - the first fprintf may change the value of errno before we use it!\nif (-1 == sem_wait(&s)) {\n   fprintf(stderr, \"An error occurred!\");\n   fprintf(stderr, \"The error value is %d\\n\", errno);\n}\n// Better, copy the value before making more system and library calls\nif (-1 == sem_wait(&s)) {\n   int errno_saved = errno;\n   fprintf(stderr, \"An error occurred!\");\n   fprintf(stderr, \"The error value is %d\\n\", errno_saved);\n}  In a similar vein, if your signal handler makes any system or library calls, then it is good practice to save the original value of errno and restore the value before returning:  void handler(int signal) {\n   int errno_saved = errno;\n\n   // make system calls that might change errno\n\n   errno = errno_saved;\n}",
            "title": "What are the gotchas and best practices of using errno?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#how-can-you-print-out-the-string-message-associated-with-a-particular-error-number",
            "text": "Use  strerror  to get a short (English) description of the error value  char *mesg = strerror(errno);\nfprintf(stderr, \"An error occurred (errno=%d): %s\", errno, mesg);",
            "title": "How can you print out the string message associated with a particular error number?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#how-are-perror-and-strerror-related",
            "text": "In previous pages we've used perror to print out the error to standard error. Using  strerror , we can now write a simple implementation of  perror :  void perror(char *what) {\n   fprintf(stderr, \"%s: %s\\n\", what, strerror(errno));\n}",
            "title": "How are perror and strerror related?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#what-are-the-gotchas-of-using-strerror",
            "text": "Unfortunately  strerror  is not threadsafe. In other words, two threads cannot call it at the same time!  There are two workarounds: Firstly we can use a mutex lock to define a critical section and a local buffer. The same mutex should be used by all threads in all places that call  strerror  pthread_mutex_lock(&m);\nchar *result = strerror(errno);\nchar *message = malloc(strlen(result) + 1);\nstrcpy(message, result);\npthread_mutex_unlock(&m);\nfprintf(stderr, \"An error occurred (errno=%d): %s\", errno, message);\nfree(message);  Alternatively use the less portable but thread-safe  strerror_r",
            "title": "What are the gotchas of using strerror?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#what-is-eintr-what-does-it-mean-for-sem_wait-read-write",
            "text": "Some system calls can be interrupted when a signal (e.g SIGCHLD, SIGPIPE,...) is delivered to the process. At this point the system call may return without performing any action! For example, bytes may not have been read/written, semaphore wait may not have waited.  This interruption can be detected by checking the return value and if  errno  is EINTR. In which case the system call should be retried. It's common to see the following kind of loop that wraps a system call (such as sem_wait).  while ((-1 == systemcall(...)) && (errno == EINTR)) { /* repeat! */}  Be careful to write  == EINTR , not  = EINTR .  Or, if the result value needs to be used later...  while ((-1 == (result = systemcall(...))) && (errno == EINTR)) { /* repeat! */}  On Linux,calling  read  and  write  to a local disk will normally not return with EINTR (instead the function is automatically restarted for you). However, calling  read  and  write  on a file descriptor that corresponds to a network stream  can  return with EINTR.",
            "title": "What is EINTR? What does it mean for sem_wait? read? write?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#which-system-calls-may-be-interrupted-and-need-to-be-wrapped",
            "text": "Use man the page! The man page includes a list of errors (i.e. errno values) that may be set by the system call. A rule of thumb is 'slow' (blocking) calls (e.g. writing to a socket) may be interrupted but fast non-blocking calls (e.g. pthread_mutex_lock) will not.  From the linux signal 7 man page.  \"If a signal handler is invoked while a system call or library function call is blocked, then either:  the call is automatically restarted after the signal handler returns; or  the call fails with the error EINTR.\nWhich of these two behaviors occurs depends on the interface and whether or not the signal handler was established using the SA_RESTART flag (see sigaction(2)). The details vary across UNIX systems; below, the details for Linux.  If a blocked call to one of the following interfaces is interrupted by a signal handler, then the call will be automatically restarted after the signal handler returns if the SA_RESTART flag was used; otherwise the call will fail with the error EINTR:   read(2), readv(2), write(2), writev(2), and ioctl(2) calls on \"slow\" devices. A \"slow\" device is one where the I/O call may block for an indefinite time, for example, a terminal, pipe, or socket. (A disk is not a slow device according to this definition.) If an I/O call on a slow device has already transferred some data by the time it is interrupted by a signal handler, then the call will return a success status (normally, the number of bytes transferred).\n\"   Note, it is easy to believe that setting 'SA_RESTART' flag is sufficient to make this whole problem disappear. Unfortunately that's not true: there are still system calls that may return early and set  EINTR ! See  signal(7)  for details.",
            "title": "Which system calls may be interrupted and need to be wrapped?"
        },
        {
            "location": "/SystemProgramming/POSIX,-Part-1:-Error-handling/#errno-exceptions",
            "text": "There are some POSIX utilities that have their own errno per say. One is when you call  getaddrinfo  the function to check that error and convert to a string is  gai_strerr . Don't get them mixed up!",
            "title": "Errno exceptions?"
        },
        {
            "location": "/SystemProgramming/Pipe:-Review-Questions/",
            "text": "Question numbers subject to change\n\n\n\n\nQ1\n\n\nFill in the blanks to make the following program print 123456789. If \ncat\n is given no arguments it simply prints its input until EOF. Bonus: Explain why the \nclose\n call below is necessary.\n\n\nint main() {\n  int i = 0;\n  while(++i < 10) {\n    pid_t pid = fork();\n    if(pid == 0) { /* child */\n      char buffer[16];\n      sprintf(buffer, ______,i);\n      int fds[ ______];\n      pipe( fds);\n      write( fds[1], ______,______ ); // Write the buffer into the pipe\n      close(  ______ );\n      dup2( fds[0],  ______);\n      execlp( \"cat\", \"cat\",  ______ );\n      perror(\"exec\"); exit(1);\n    }\n    waitpid(pid, NULL, 0);\n  }\n  return 0;\n}\n\n\n\n\nQ2\n\n\nUse POSIX calls \nfork\n \npipe\n \ndup2\n and \nclose\n to implement an autograding program. Capture the standard output of a child process into a pipe. The child process should \nexec\n the program \n./test\n with no additional arguments (other than the process name). In the parent process read from the pipe: Exit the parent process as soon as the captured output contains the ! character. Before exiting the parent process send SIGKILL to the child process. Exit 0 if the output contained a !. Otherwise if the child process exits causing the pipe write end to be closed, then exit with a value of 1. Be sure to close the unused ends of the pipe in the parent and child process\n\n\nQ3 (Advanced)\n\n\nThis advanced challenge uses pipes to get an \"AI player\" to play itself until the game is complete.\nThe program \ntictactoe\n accepts a line of input - the sequence of turns made so far, prints the same sequence followed by another turn, and then exits. A turn is specified using two characters. For example \"A1\" and \"C3\" are two opposite corner positions. The string \nB2A1A3\n is a game of 3 turns/plys. A valid response is \nB2A1A3C1\n (the C1 response blocks the diagonal B2 A3 threat). The output line may also include a suffix \n-I win\n \n-You win\n \n-invalid\n or \n-draw\n\nUse pipes to control the input and output of each child process created. When the output contains a \n-\n, print the final output line (the entire game sequence and the result) and exit.",
            "title": "Pipe: Review Questions"
        },
        {
            "location": "/SystemProgramming/Pipe:-Review-Questions/#q1",
            "text": "Fill in the blanks to make the following program print 123456789. If  cat  is given no arguments it simply prints its input until EOF. Bonus: Explain why the  close  call below is necessary.  int main() {\n  int i = 0;\n  while(++i < 10) {\n    pid_t pid = fork();\n    if(pid == 0) { /* child */\n      char buffer[16];\n      sprintf(buffer, ______,i);\n      int fds[ ______];\n      pipe( fds);\n      write( fds[1], ______,______ ); // Write the buffer into the pipe\n      close(  ______ );\n      dup2( fds[0],  ______);\n      execlp( \"cat\", \"cat\",  ______ );\n      perror(\"exec\"); exit(1);\n    }\n    waitpid(pid, NULL, 0);\n  }\n  return 0;\n}",
            "title": "Q1"
        },
        {
            "location": "/SystemProgramming/Pipe:-Review-Questions/#q2",
            "text": "Use POSIX calls  fork   pipe   dup2  and  close  to implement an autograding program. Capture the standard output of a child process into a pipe. The child process should  exec  the program  ./test  with no additional arguments (other than the process name). In the parent process read from the pipe: Exit the parent process as soon as the captured output contains the ! character. Before exiting the parent process send SIGKILL to the child process. Exit 0 if the output contained a !. Otherwise if the child process exits causing the pipe write end to be closed, then exit with a value of 1. Be sure to close the unused ends of the pipe in the parent and child process",
            "title": "Q2"
        },
        {
            "location": "/SystemProgramming/Pipe:-Review-Questions/#q3-advanced",
            "text": "This advanced challenge uses pipes to get an \"AI player\" to play itself until the game is complete.\nThe program  tictactoe  accepts a line of input - the sequence of turns made so far, prints the same sequence followed by another turn, and then exits. A turn is specified using two characters. For example \"A1\" and \"C3\" are two opposite corner positions. The string  B2A1A3  is a game of 3 turns/plys. A valid response is  B2A1A3C1  (the C1 response blocks the diagonal B2 A3 threat). The output line may also include a suffix  -I win   -You win   -invalid  or  -draw \nUse pipes to control the input and output of each child process created. When the output contains a  - , print the final output line (the entire game sequence and the result) and exit.",
            "title": "Q3 (Advanced)"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-1:-Introduction-to-pipes/",
            "text": "What is IPC?\n\n\nInter process communication is any way for one process to talk to another process. You've already seen one form of this virtual memory! A piece of virtual memory can be shared between parent and child, leading to communication. You may want to wrap that memory in \npthread_mutexattr_setpshared(&attrmutex, PTHREAD_PROCESS_SHARED);\n mutex (or a process wide mutex) to prevent race conditions.\n\n\nThere are more standard ways of IPC, like pipes! Consider if you type the following into your terminal\n\n\n$ ls -1 | cut -d'.' -f1 | uniq | sort | tee dir_contents\n\n\n\n\nWhat does the following code do (It doesn't really matter so you can skip this if you want)? Well it \nls\n's the current directory (the -1 means that it outputs one entry per line). The \ncut\n command then takes everything before the first period. Uniq makes sure all the lines are uniq, sort sorts them and tee outputs to a file. \n\n\nThe important part is that bash creates \n5 separate processes\n and connects their standard outs/stdins with pipes the trail looks something like this.\n\n\n(0) ls (1)------>(0) cut (1)------->(0) uniq (1)------>(0) sort (1)------>(0) tee (1)\n\n\nThe numbers in the pipes are the file descriptors for each process and the arrow represents the redirect or where the output of the pipe is going.\n\n\nWhat is a pipe?\n\n\nA POSIX pipe is almost like its real counterpart - you can stuff bytes down one end and they will appear at the other end in the same order. Unlike real pipes however, the flow is always in the same direction, one file descriptor is used for reading and the other for writing. The \npipe\n system call is used to create a pipe.\n\n\nint filedes[2];\npipe (filedes);\nprintf(\"read from %d, write to %d\\n\", filedes[0], filedes[1]);\n\n\n\n\nThese file descriptors can be used with \nread\n -\n\n\n// To read...\nchar buffer[80];\nint bytesread = read(filedes[0], buffer, sizeof(buffer));\n\n\n\n\nAnd \nwrite\n - \n\n\nwrite(filedes[1], \"Go!\", 4);\n\n\n\n\nHow can I use pipe to communicate with a child process?\n\n\nA common method of using pipes is to create the pipe before forking.\n\n\nint filedes[2];\npipe (filedes);\npid_t child = fork();\nif (child > 0) { /* I must be the parent */\n    char buffer[80];\n    int bytesread = read(filedes[0], buffer, sizeof(buffer));\n    // do something with the bytes read    \n}\n\n\n\n\nThe child can then send a message back to the parent:\n\n\nif (child == 0) {\n   write(filedes[1], \"done\", 4);\n}\n\n\n\n\nCan I use pipes inside a single process?\n\n\nShort answer: Yes, but I'm not sure why you would want to LOL!\n\n\nHere's an example program that sends a message to itself:\n\n\n#include <unistd.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint main() {\n    int fh[2];\n    pipe(fh);\n    FILE *reader = fdopen(fh[0], \"r\");\n    FILE *writer = fdopen(fh[1], \"w\");\n    // Hurrah now I can use printf rather than using low-level read() write()\n    printf(\"Writing...\\n\");\n    fprintf(writer,\"%d %d %d\\n\", 10, 20, 30);\n    fflush(writer);\n\n    printf(\"Reading...\\n\");\n    int results[3];\n    int ok = fscanf(reader,\"%d %d %d\", results, results + 1, results + 2);\n    printf(\"%d values parsed: %d %d %d\\n\", ok, results[0], results[1], results[2]);\n\n    return 0;\n}\n\n\n\n\nThe problem with using a pipe in this fashion is that writing to a pipe can block i.e. the pipe only has a limited buffering capacity. If the pipe is full the writing process will block! The maximum size of the buffer is system dependent; typical values from  4KB upto 128KB.\n\n\nint main() {\n    int fh[2];\n    pipe(fh);\n    int b = 0;\n    #define MESG \"...............................\"\n    while(1) {\n        printf(\"%d\\n\",b);\n        write(fh[1], MESG, sizeof(MESG))\n        b+=sizeof(MESG);\n    }\n    return 0;\n}\n\n\n\n\nSee [[Pipes, Part 2: Pipe programming secrets]]",
            "title": "Pipes, Part 1: Introduction to pipes"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-1:-Introduction-to-pipes/#what-is-ipc",
            "text": "Inter process communication is any way for one process to talk to another process. You've already seen one form of this virtual memory! A piece of virtual memory can be shared between parent and child, leading to communication. You may want to wrap that memory in  pthread_mutexattr_setpshared(&attrmutex, PTHREAD_PROCESS_SHARED);  mutex (or a process wide mutex) to prevent race conditions.  There are more standard ways of IPC, like pipes! Consider if you type the following into your terminal  $ ls -1 | cut -d'.' -f1 | uniq | sort | tee dir_contents  What does the following code do (It doesn't really matter so you can skip this if you want)? Well it  ls 's the current directory (the -1 means that it outputs one entry per line). The  cut  command then takes everything before the first period. Uniq makes sure all the lines are uniq, sort sorts them and tee outputs to a file.   The important part is that bash creates  5 separate processes  and connects their standard outs/stdins with pipes the trail looks something like this.  (0) ls (1)------>(0) cut (1)------->(0) uniq (1)------>(0) sort (1)------>(0) tee (1)  The numbers in the pipes are the file descriptors for each process and the arrow represents the redirect or where the output of the pipe is going.",
            "title": "What is IPC?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-1:-Introduction-to-pipes/#what-is-a-pipe",
            "text": "A POSIX pipe is almost like its real counterpart - you can stuff bytes down one end and they will appear at the other end in the same order. Unlike real pipes however, the flow is always in the same direction, one file descriptor is used for reading and the other for writing. The  pipe  system call is used to create a pipe.  int filedes[2];\npipe (filedes);\nprintf(\"read from %d, write to %d\\n\", filedes[0], filedes[1]);  These file descriptors can be used with  read  -  // To read...\nchar buffer[80];\nint bytesread = read(filedes[0], buffer, sizeof(buffer));  And  write  -   write(filedes[1], \"Go!\", 4);",
            "title": "What is a pipe?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-1:-Introduction-to-pipes/#how-can-i-use-pipe-to-communicate-with-a-child-process",
            "text": "A common method of using pipes is to create the pipe before forking.  int filedes[2];\npipe (filedes);\npid_t child = fork();\nif (child > 0) { /* I must be the parent */\n    char buffer[80];\n    int bytesread = read(filedes[0], buffer, sizeof(buffer));\n    // do something with the bytes read    \n}  The child can then send a message back to the parent:  if (child == 0) {\n   write(filedes[1], \"done\", 4);\n}",
            "title": "How can I use pipe to communicate with a child process?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-1:-Introduction-to-pipes/#can-i-use-pipes-inside-a-single-process",
            "text": "Short answer: Yes, but I'm not sure why you would want to LOL!  Here's an example program that sends a message to itself:  #include <unistd.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint main() {\n    int fh[2];\n    pipe(fh);\n    FILE *reader = fdopen(fh[0], \"r\");\n    FILE *writer = fdopen(fh[1], \"w\");\n    // Hurrah now I can use printf rather than using low-level read() write()\n    printf(\"Writing...\\n\");\n    fprintf(writer,\"%d %d %d\\n\", 10, 20, 30);\n    fflush(writer);\n\n    printf(\"Reading...\\n\");\n    int results[3];\n    int ok = fscanf(reader,\"%d %d %d\", results, results + 1, results + 2);\n    printf(\"%d values parsed: %d %d %d\\n\", ok, results[0], results[1], results[2]);\n\n    return 0;\n}  The problem with using a pipe in this fashion is that writing to a pipe can block i.e. the pipe only has a limited buffering capacity. If the pipe is full the writing process will block! The maximum size of the buffer is system dependent; typical values from  4KB upto 128KB.  int main() {\n    int fh[2];\n    pipe(fh);\n    int b = 0;\n    #define MESG \"...............................\"\n    while(1) {\n        printf(\"%d\\n\",b);\n        write(fh[1], MESG, sizeof(MESG))\n        b+=sizeof(MESG);\n    }\n    return 0;\n}  See [[Pipes, Part 2: Pipe programming secrets]]",
            "title": "Can I use pipes inside a single process?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/",
            "text": "Pipe Gotchas\n\n\nHere's a complete example that doesn't work! The child reads one byte at a time from the pipe and prints it out - but we never see the message! Can you see why?\n\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <signal.h>\n\nint main() {\n    int fd[2];\n    pipe(fd);\n    //You must read from fd[0] and write from fd[1]\n    printf(\"Reading from %d, writing to %d\\n\", fd[0], fd[1]);\n\n    pid_t p = fork();\n    if (p > 0) {\n        /* I have a child therefore I am the parent*/\n        write(fd[1],\"Hi Child!\",9);\n\n        /*don't forget your child*/\n        wait(NULL);\n    } else {\n        char buf;\n        int bytesread;\n        // read one byte at a time.\n        while ((bytesread = read(fd[0], &buf, 1)) > 0) {\n            putchar(buf);\n        }\n    }\n    return 0;\n}\n\n\n\n\n\nThe parent sends the bytes \nH,i,(space),C...!\n into the pipe (this may block if the pipe is full).\nThe child starts reading the pipe one byte at a time. In the above case, the child process will read and print each character. However it never leaves the while loop! When there are no characters left to read it simply blocks and waits for more. \n\n\nThe call \nputchar\n writes the characters out but we never flush the \nstdout\n buffer. i.e. We have transferred the message from one process to another but it has not yet been printed. To see the message we could flush the buffer e.g. \nfflush(stdout)\n (or \nprintf(\"\\n\")\n if the output is going to a terminal). A better solution would also exit the loop by checking for an end-of-message marker,\n\n\n        while ((bytesread = read(fd[0], &buf, 1)) > 0) {\n            putchar(buf);\n            if (buf == '!') break; /* End of message */\n        }\n\n\n\n\nAnd the message will be flushed to the terminal when the child process exits.\n\n\nWant to use pipes with printf and scanf? Use fdopen!\n\n\nPOSIX file descriptors are simple integers 0,1,2,3...\nAt the C library level, C wraps these with a buffer and useful functions like printf and scanf, so we that we can easily print or parse integers, strings etc.\nIf you already have a file descriptor then you can 'wrap' it yourself into a FILE pointer using \nfdopen\n :\n\n\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h>\n\nint main() {\n    char *name=\"Fred\";\n    int score = 123;\n    int filedes = open(\"mydata.txt\", \"w\", O_CREAT, S_IWUSR | S_IRUSR);\n\n    FILE *f = fdopen(filedes, \"w\");\n    fprintf(f, \"Name:%s Score:%d\\n\", name, score);\n    fclose(f);\n\n\n\n\nFor writing to files this is unnecessary - just use \nfopen\n which does the same as \nopen\n and \nfdopen\n\nHowever for pipes, we already have a file descriptor - so this is great time to use \nfdopen\n!\n\n\nHere's a complete example using pipes that almost works! Can you spot the error? Hint: The parent never prints anything!\n\n\n#include <unistd.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint main() {\n    int fh[2];\n    pipe(fh);\n    FILE *reader = fdopen(fh[0], \"r\");\n    FILE *writer = fdopen(fh[1], \"w\");\n    pid_t p = fork();\n    if (p > 0) {\n        int score;\n        fscanf(reader, \"Score %d\", &score);\n        printf(\"The child says the score is %d\\n\", score);\n    } else {\n        fprintf(writer, \"Score %d\", 10 + 10);\n        fflush(writer);\n    }\n    return 0;\n}\n\n\n\n\nNote the (unnamed) pipe resource will disappear once both the child and parent have exited. In the above example the child will send the bytes and the parent will receive the bytes from the pipe. However, no end-of-line character is ever sent, so \nfscanf\n will continue to ask for bytes because it is waiting for the end of the line i.e. it will wait forever! The fix is to ensure we send a newline character, so that \nfscanf\n will return.\n\n\nchange:   fprintf(writer, \"Score %d\", 10 + 10);\nto:       fprintf(writer, \"Score %d\\n\", 10 + 10);\n\n\n\n\nSo do we need to \nfflush\n too?\n\n\nYes, if you want your bytes to be sent to the pipe immediately! At the beginning of this course we assumed that file streams are always \nline buffered\n i.e. the C library will flush its buffer everytime you send a newline character. Actually this is only true for terminal streams - for other filestreams the C library attempts to improve performance by only flushing when it's internal buffer is full or the file is closed.\n\n\nWhen do I need two pipes?\n\n\nIf you need to send data to and from a child asynchronously, then two pipes are required (one for each direction).\nOtherwise the child would attempt to read its own data intended for the parent (and vice versa)!\n\n\nClosing pipes gotchas\n\n\nProcesses receive the signal SIGPIPE when no process is listening! From the pipe(2) man page - \n\n\nIf all file descriptors referring to the read end of a pipe have been closed,\n then a write(2) will cause a SIGPIPE signal to be generated for the calling process. \n\n\n\n\nTip: Notice only the writer (not a reader) can use this signal.\nTo inform the reader that a writer is closing their end of the pipe, you could write your own special byte (e.g. 0xff) or a message ( \n\"Bye!\"\n)\n\n\nHere's an example of catching this signal that does not work! Can you see why?\n\n\n#include <stdio.h>\n#include <stdio.h>\n#include <unistd.h>\n#include <signal.h>\n\nvoid no_one_listening(int signal) {\n    write(1, \"No one is listening!\\n\", 21);\n}\n\nint main() {\n    signal(SIGPIPE, no_one_listening);\n    int filedes[2];\n\n    pipe(filedes);\n    pid_t child = fork();\n    if (child > 0) { \n        /* I must be the parent. Close the listening end of the pipe */\n        /* I'm not listening anymore!*/\n        close(filedes[0]);\n    } else {\n        /* Child writes messages to the pipe */\n        write(filedes[1], \"One\", 3);\n        sleep(2);\n        // Will this write generate SIGPIPE ?\n        write(filedes[1], \"Two\", 3);\n        write(1, \"Done\\n\", 5);\n    }\n    return 0;\n}\n\n\n\n\nThe mistake in above code is that there is still a reader for the pipe! The child still has the pipe's first file descriptor open and remember the specification? All readers must be closed.\n\n\nWhen forking, \nIt is common practice\n to close the unnecessary (unused) end of each pipe in the child and parent process. For example the parent might close the reading end and the child might close the writing end (and vice versa if you have two pipes)\n\n\nWhat is filling up the pipe? What happens when the pipe becomes full?\n\n\nA pipe gets filled up when the writer writes too much to the pipe without the reader reading any of it. When the pipes become full, all writes fail until a read occurs. Even then, a write may partial fail if the pipe has a little bit of space left but not enough for the entire message.\n\n\nTo avoid this, usually two things are done. Either increase the size of the pipe. Or more commonly, fix your program design so that the pipe is constantly being read from.\n\n\nAre pipes process safe?\n\n\nYes! Pipe write are atomic up to the size of the pipe. Meaning that if two processes try to write to the same pipe, the kernel has internal mutexes with the pipe that it will lock, do the write, and return. The only gotcha is when the pipe is about to become full. If two processes are trying to write and the pipe can only satisfy a partial write, that pipe write is not atomic -- be careful about that!\n\n\nThe lifetime of pipes\n\n\nUnnamed pipes (the kind we've seen up to this point) live in memory (do not take up any disk space) and are a simple and efficient form of inter-process communication (IPC) that is useful for streaming data and simple messages. Once all processes have closed, the pipe resources are freed.\n\n\nAn alternative to \nunamed\n pipes is \nnamed\n pipes created using \nmkfifo\n.\n\n\nNamed Pipes\n\n\nHow do I create named pipes?\n\n\nFrom the command line: \nmkfifo\n\nFrom C: \nint mkfifo(const char *pathname, mode_t mode);\n\n\nYou give it the path name and the operation mode, it will be ready to go! Named pipes take up no space on the disk. What the operating system is essentially telling you when you have a named pipe is that it will create an unnamed pipe that refers to the named pipe, and that's it! There is no additional magic. This is just for programming convenience if processes are started without forking (meaning that there would be no way to get the file descriptor to the child process for an unnamed pipe)\n\n\nWhy is my pipe hanging?\n\n\nReads and writes hang on Named Pipes until there is at least one reader and one writer, take this\n\n\n1$ mkfifo fifo\n1$ echo Hello > fifo\n# This will hang until I do this on another terminal or another process\n2$ cat fifo\nHello\n\n\n\n\nAny \nopen\n is called on a named pipe the kernel blocks until another process calls the opposite open. Meaning, echo calls \nopen(.., O_RDONLY)\n but that blocks until cat calls \nopen(.., O_WRONLY)\n, then the programs are allowed to continue.\n\n\nRace condition with named pipes.\n\n\nWhat is wrong with the following program?\n\n\n//Program 1\n\nint main(){\n    int fd = open(\"fifo\", O_RDWR | O_TRUNC);\n    write(fd, \"Hello!\", 6);\n    close(fd);\n    return 0;\n}\n\n//Program 2\nint main() {\n    char buffer[7];\n    int fd = open(\"fifo\", O_RDONLY);\n    read(fd, buffer, 6);\n    buffer[6] = '\\0';\n    printf(\"%s\\n\", buffer);\n    return 0;\n}\n\n\n\n\nThis may never print hello because of a race condition. Since you opened the pipe in the first process under both permissions, open won't wait for a reader because you told the operating system that you are a reader! Sometimes it looks like it works because the execution of the code looks something like this.\n\n\n\n\n\n\n\n\nProcess 1\n\n\nProcess 2\n\n\n\n\n\n\n\n\n\n\nopen(O_RDWR) & write()\n\n\n\n\n\n\n\n\n\n\nopen(O_RDONLY) & read()\n\n\n\n\n\n\nclose() & exit()\n\n\n\n\n\n\n\n\n\n\nprint() & exit()\n\n\n\n\n\n\n\n\nSometimes it won't\n\n\n\n\n\n\n\n\nProcess 1\n\n\nProcess 2\n\n\n\n\n\n\n\n\n\n\nopen(O_RDWR) & write()\n\n\n\n\n\n\n\n\nclose() & exit()\n\n\n(Named pipe is destroyed)\n\n\n\n\n\n\n(Blocks indefinitely)\n\n\nopen(O_RDONLY)",
            "title": "Pipes, Part 2: Pipe programming secrets"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#pipe-gotchas",
            "text": "Here's a complete example that doesn't work! The child reads one byte at a time from the pipe and prints it out - but we never see the message! Can you see why?  #include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <signal.h>\n\nint main() {\n    int fd[2];\n    pipe(fd);\n    //You must read from fd[0] and write from fd[1]\n    printf(\"Reading from %d, writing to %d\\n\", fd[0], fd[1]);\n\n    pid_t p = fork();\n    if (p > 0) {\n        /* I have a child therefore I am the parent*/\n        write(fd[1],\"Hi Child!\",9);\n\n        /*don't forget your child*/\n        wait(NULL);\n    } else {\n        char buf;\n        int bytesread;\n        // read one byte at a time.\n        while ((bytesread = read(fd[0], &buf, 1)) > 0) {\n            putchar(buf);\n        }\n    }\n    return 0;\n}  The parent sends the bytes  H,i,(space),C...!  into the pipe (this may block if the pipe is full).\nThe child starts reading the pipe one byte at a time. In the above case, the child process will read and print each character. However it never leaves the while loop! When there are no characters left to read it simply blocks and waits for more.   The call  putchar  writes the characters out but we never flush the  stdout  buffer. i.e. We have transferred the message from one process to another but it has not yet been printed. To see the message we could flush the buffer e.g.  fflush(stdout)  (or  printf(\"\\n\")  if the output is going to a terminal). A better solution would also exit the loop by checking for an end-of-message marker,          while ((bytesread = read(fd[0], &buf, 1)) > 0) {\n            putchar(buf);\n            if (buf == '!') break; /* End of message */\n        }  And the message will be flushed to the terminal when the child process exits.",
            "title": "Pipe Gotchas"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#want-to-use-pipes-with-printf-and-scanf-use-fdopen",
            "text": "POSIX file descriptors are simple integers 0,1,2,3...\nAt the C library level, C wraps these with a buffer and useful functions like printf and scanf, so we that we can easily print or parse integers, strings etc.\nIf you already have a file descriptor then you can 'wrap' it yourself into a FILE pointer using  fdopen  :  #include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h>\n\nint main() {\n    char *name=\"Fred\";\n    int score = 123;\n    int filedes = open(\"mydata.txt\", \"w\", O_CREAT, S_IWUSR | S_IRUSR);\n\n    FILE *f = fdopen(filedes, \"w\");\n    fprintf(f, \"Name:%s Score:%d\\n\", name, score);\n    fclose(f);  For writing to files this is unnecessary - just use  fopen  which does the same as  open  and  fdopen \nHowever for pipes, we already have a file descriptor - so this is great time to use  fdopen !  Here's a complete example using pipes that almost works! Can you spot the error? Hint: The parent never prints anything!  #include <unistd.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint main() {\n    int fh[2];\n    pipe(fh);\n    FILE *reader = fdopen(fh[0], \"r\");\n    FILE *writer = fdopen(fh[1], \"w\");\n    pid_t p = fork();\n    if (p > 0) {\n        int score;\n        fscanf(reader, \"Score %d\", &score);\n        printf(\"The child says the score is %d\\n\", score);\n    } else {\n        fprintf(writer, \"Score %d\", 10 + 10);\n        fflush(writer);\n    }\n    return 0;\n}  Note the (unnamed) pipe resource will disappear once both the child and parent have exited. In the above example the child will send the bytes and the parent will receive the bytes from the pipe. However, no end-of-line character is ever sent, so  fscanf  will continue to ask for bytes because it is waiting for the end of the line i.e. it will wait forever! The fix is to ensure we send a newline character, so that  fscanf  will return.  change:   fprintf(writer, \"Score %d\", 10 + 10);\nto:       fprintf(writer, \"Score %d\\n\", 10 + 10);",
            "title": "Want to use pipes with printf and scanf? Use fdopen!"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#so-do-we-need-to-fflush-too",
            "text": "Yes, if you want your bytes to be sent to the pipe immediately! At the beginning of this course we assumed that file streams are always  line buffered  i.e. the C library will flush its buffer everytime you send a newline character. Actually this is only true for terminal streams - for other filestreams the C library attempts to improve performance by only flushing when it's internal buffer is full or the file is closed.",
            "title": "So do we need to fflush too?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#when-do-i-need-two-pipes",
            "text": "If you need to send data to and from a child asynchronously, then two pipes are required (one for each direction).\nOtherwise the child would attempt to read its own data intended for the parent (and vice versa)!",
            "title": "When do I need two pipes?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#closing-pipes-gotchas",
            "text": "Processes receive the signal SIGPIPE when no process is listening! From the pipe(2) man page -   If all file descriptors referring to the read end of a pipe have been closed,\n then a write(2) will cause a SIGPIPE signal to be generated for the calling process.   Tip: Notice only the writer (not a reader) can use this signal.\nTo inform the reader that a writer is closing their end of the pipe, you could write your own special byte (e.g. 0xff) or a message (  \"Bye!\" )  Here's an example of catching this signal that does not work! Can you see why?  #include <stdio.h>\n#include <stdio.h>\n#include <unistd.h>\n#include <signal.h>\n\nvoid no_one_listening(int signal) {\n    write(1, \"No one is listening!\\n\", 21);\n}\n\nint main() {\n    signal(SIGPIPE, no_one_listening);\n    int filedes[2];\n\n    pipe(filedes);\n    pid_t child = fork();\n    if (child > 0) { \n        /* I must be the parent. Close the listening end of the pipe */\n        /* I'm not listening anymore!*/\n        close(filedes[0]);\n    } else {\n        /* Child writes messages to the pipe */\n        write(filedes[1], \"One\", 3);\n        sleep(2);\n        // Will this write generate SIGPIPE ?\n        write(filedes[1], \"Two\", 3);\n        write(1, \"Done\\n\", 5);\n    }\n    return 0;\n}  The mistake in above code is that there is still a reader for the pipe! The child still has the pipe's first file descriptor open and remember the specification? All readers must be closed.  When forking,  It is common practice  to close the unnecessary (unused) end of each pipe in the child and parent process. For example the parent might close the reading end and the child might close the writing end (and vice versa if you have two pipes)",
            "title": "Closing pipes gotchas"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#what-is-filling-up-the-pipe-what-happens-when-the-pipe-becomes-full",
            "text": "A pipe gets filled up when the writer writes too much to the pipe without the reader reading any of it. When the pipes become full, all writes fail until a read occurs. Even then, a write may partial fail if the pipe has a little bit of space left but not enough for the entire message.  To avoid this, usually two things are done. Either increase the size of the pipe. Or more commonly, fix your program design so that the pipe is constantly being read from.",
            "title": "What is filling up the pipe? What happens when the pipe becomes full?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#are-pipes-process-safe",
            "text": "Yes! Pipe write are atomic up to the size of the pipe. Meaning that if two processes try to write to the same pipe, the kernel has internal mutexes with the pipe that it will lock, do the write, and return. The only gotcha is when the pipe is about to become full. If two processes are trying to write and the pipe can only satisfy a partial write, that pipe write is not atomic -- be careful about that!",
            "title": "Are pipes process safe?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#the-lifetime-of-pipes",
            "text": "Unnamed pipes (the kind we've seen up to this point) live in memory (do not take up any disk space) and are a simple and efficient form of inter-process communication (IPC) that is useful for streaming data and simple messages. Once all processes have closed, the pipe resources are freed.  An alternative to  unamed  pipes is  named  pipes created using  mkfifo .",
            "title": "The lifetime of pipes"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#named-pipes",
            "text": "",
            "title": "Named Pipes"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#how-do-i-create-named-pipes",
            "text": "From the command line:  mkfifo \nFrom C:  int mkfifo(const char *pathname, mode_t mode);  You give it the path name and the operation mode, it will be ready to go! Named pipes take up no space on the disk. What the operating system is essentially telling you when you have a named pipe is that it will create an unnamed pipe that refers to the named pipe, and that's it! There is no additional magic. This is just for programming convenience if processes are started without forking (meaning that there would be no way to get the file descriptor to the child process for an unnamed pipe)",
            "title": "How do I create named pipes?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#why-is-my-pipe-hanging",
            "text": "Reads and writes hang on Named Pipes until there is at least one reader and one writer, take this  1$ mkfifo fifo\n1$ echo Hello > fifo\n# This will hang until I do this on another terminal or another process\n2$ cat fifo\nHello  Any  open  is called on a named pipe the kernel blocks until another process calls the opposite open. Meaning, echo calls  open(.., O_RDONLY)  but that blocks until cat calls  open(.., O_WRONLY) , then the programs are allowed to continue.",
            "title": "Why is my pipe hanging?"
        },
        {
            "location": "/SystemProgramming/Pipes,-Part-2:-Pipe-programming-secrets/#race-condition-with-named-pipes",
            "text": "What is wrong with the following program?  //Program 1\n\nint main(){\n    int fd = open(\"fifo\", O_RDWR | O_TRUNC);\n    write(fd, \"Hello!\", 6);\n    close(fd);\n    return 0;\n}\n\n//Program 2\nint main() {\n    char buffer[7];\n    int fd = open(\"fifo\", O_RDONLY);\n    read(fd, buffer, 6);\n    buffer[6] = '\\0';\n    printf(\"%s\\n\", buffer);\n    return 0;\n}  This may never print hello because of a race condition. Since you opened the pipe in the first process under both permissions, open won't wait for a reader because you told the operating system that you are a reader! Sometimes it looks like it works because the execution of the code looks something like this.     Process 1  Process 2      open(O_RDWR) & write()      open(O_RDONLY) & read()    close() & exit()      print() & exit()     Sometimes it won't     Process 1  Process 2      open(O_RDWR) & write()     close() & exit()  (Named pipe is destroyed)    (Blocks indefinitely)  open(O_RDONLY)",
            "title": "Race condition with named pipes."
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/",
            "text": "Wait Macros\n\n\nCan I find out the exit value of my child?\n\n\nYou can find the lowest 8 bits of the child's exit value (the return value of \nmain()\n or value included in \nexit()\n): Use the \"Wait macros\" - typically you will use \"WIFEXITED\" and \"WEXITSTATUS\" . See \nwait\n/\nwaitpid\n man page for more information).\n\n\nint status;\npid_t child = fork();\nif (child == -1) return 1; //Failed\nif (child > 0) { /* I am the parent - wait for the child to finish */\n  pid_t pid = waitpid(child, &status, 0);\n  if (pid != -1 && WIFEXITED(status)) {\n     int low8bits = WEXITSTATUS(status);\n     printf(\"Process %d returned %d\" , pid, low8bits);\n  }\n} else { /* I am the child */\n // do something interesting\n  execl(\"/bin/ls\", \"/bin/ls\", \".\", (char *) NULL); // \"ls .\"\n}\n\n\n\n\nA process can only have 256 return values, the rest of the bits are informational.\n\n\nBit Shifting\n\n\nNote there is no need to memorize this, this is just a high level overview of how information is stored inside the status variables\n\n\nAndroid source code\n\n\n/\n If WIFEXITED(STATUS), the low-order 8 bits of the status. \n/\n\n\n#define __WEXITSTATUS(status) (((status) & 0xff00) >> 8)\n\n\n/\n If WIFSIGNALED(STATUS), the terminating signal. \n/\n\n\n#define __WTERMSIG(status) ((status) & 0x7f)\n\n\n/\n If WIFSTOPPED(STATUS), the signal that stopped the child. \n/\n\n\n#define \nWSTOPSIG(status) \nWEXITSTATUS(status)\n\n\n/\n Nonzero if STATUS indicates normal termination. \n/\n\n\n#define \nWIFEXITED(status) (\nWTERMSIG(status) == 0)\n\n\nThe kernel has an internal way of keeping track of signaled, exited, or stopped. That API is abstracted so that that the kernel developers are free to change at will.\n\n\nBeing careful.\n\n\nRemember that the the macros only make sense if the precondition is met. Meaning that a process' exit status won't be defined if the process is signaled. The macros will not do the checking for you, so it's up to the programming to make sure the logic checks out.\n\n\nSignals\n\n\nWhat's a signal?\n\n\nA signal is a construct provided to us by the kernel. It allows one process to asynchronously send a signal (think a message) to another process. If that process wants to accept the signal, it can, and then, for most signals, can decide what to do with that signal. Here is a short list (non comprehensive) of signals.\n\n\n\n\n\n\n\n\nName\n\n\nDefault Action\n\n\nUsual Use Case\n\n\n\n\n\n\n\n\n\n\nSIGINT\n\n\nTerminate Process (Can be caught)\n\n\nTell the process to stop nicely\n\n\n\n\n\n\nSIGQUIT\n\n\nTerminate Process (Can be caught)\n\n\nTells the process to stop harshly\n\n\n\n\n\n\nSIGSTOP\n\n\nStop Process (Can be caught)\n\n\nStops the process to be continued\n\n\n\n\n\n\nSIGCONT\n\n\nContinues a Process\n\n\nContinues to run the process\n\n\n\n\n\n\nSIGKILL\n\n\nTerminate Process (Cannot be Ignored)\n\n\nYou want your process gone\n\n\n\n\n\n\n\n\nCan I pause my child?\n\n\nYes ! You can temporarily pause a running process by sending it a SIGSTOP signal.\nIf it succeeds it will freeze a process; i.e. the process will not be allocated anymore CPU time.\n\n\nTo allow a process to resume execution send it the SIGCONT signal.\n\n\nFor example,\nHere's program that slowly prints a dot every second, up to 59 dots.\n\n\n#include <unistd.h>\n#include <stdio.h>\nint main() {\n  printf(\"My pid is %d\\n\", getpid() );\n  int i = 60;\n  while(--i) { \n    write(1, \".\",1);\n    sleep(1);\n  }\n  write(1, \"Done!\",5);\n  return 0;\n}\n\n\n\n\nWe will first start the process in the background (notice the & at the end).\nThen send it a signal from the shell process by using the kill command.\n\n\n>./program &\nMy pid is 403\n...\n>kill -SIGSTOP 403\n>kill -SIGCONT 403\n\n\n\n\nHow do I kill/stop/suspend my child from C?\n\n\nIn C, send a signal to the child using \nkill\n POSIX call,\n\n\nkill(child, SIGUSR1); // Send a user-defined signal\nkill(child, SIGSTOP); // Stop the child process (the child cannot prevent this)\nkill(child, SIGTERM); // Terminate the child process (the child can prevent this)\nkill(child, SIGINT); // Equivalent to CTRL-C (by default closes the process)\n\n\n\n\nAs we saw above there is also a kill command available in the shell\ne.g. get a list of running processes and then terminate process 45 and process 46\n\n\nps\nkill -l \nkill -9 45\nkill -s TERM 46\n\n\n\n\nHow can I detect \"CTRL-C\" and clean up gracefully?\n\n\nWe will return to signals later on - this is just a short introduction. On a Linux system, see \nman -s7 signal\n if you are interested in finding out more (for example a list of system and library calls that are async-signal-safe.\n\n\nThere are strict limitations on the executable code inside a signal handler. Most library and system calls are not 'async-signal-safe' - they may be not use used inside a signal handler because they are not re-entrant safe. In a single-threaded program, signal handling momentarily interrupts the program execution to execute the signal handler code instead. Suppose your original program was interrupted while executing the library code of \nmalloc\n ;  the memory structures used by malloc will not be in a consistent state. Calling \nprintf\n (which uses \nmalloc\n) as part of the signal handler is unsafe and will result in \"undefined behavior\" i.e. it is no longer a useful,predictable program. In practice your program might crash, compute or generate incorrect results or stop functioning (\"deadlock\"), depending on exactly what your program was executing when it was interrupted to execute the signal handler code.\n\n\nOne common use of signal handlers is to set a boolean flag that is occasionally polled (read) as part of the normal running of the program. For example,\n\n\nint pleaseStop ; // See notes on why \"volatile sig_atomic_t\" is better\n\nvoid handle_sigint(int signal) {\n  pleaseStop = 1;\n}\n\nint main() {\n  signal(SIGINT, handle_sigint);\n  pleaseStop = 0;\n  while ( ! pleaseStop) { \n     /* application logic here */ \n   }\n  /* cleanup code here */\n}\n\n\n\n\nThe above code might appear to be correct on paper. However, we need to provide a hint to the compiler and to the CPU core that will execute the \nmain()\n loop. We need to prevent a compiler optimization: The expression \n! pleaseStop\n appears to be a loop invariant i.e. true forever, so can be simplified to \ntrue\n.  Secondly, we need to ensure that the value of \npleaseStop\n is not cached using a CPU register and instead always read from and written to main memory. The \nsig_atomic_t\n type implies that all the bits of the variable can be read or modified as an \"atomic operation\" - a single uninterruptable operation. It is impossible to read a value that is composed of some new bit values and old bit values.\n\n\nBy specifying \npleaseStop\n with the correct type \nvolatile sig_atomic_t\n we can write portable code where the main loop will be exited after the signal handler returns. The \nsig_atomic_t\n type can be as large as an \nint\n on most modern platforms but on embedded systems can be as small as a \nchar\n and only able to represent (-127 to 127) values.\n\n\nvolatile sig_atomic_t pleaseStop;\n\n\n\n\nTwo examples of this pattern can be found in \"COMP\" a terminal based 1Hz 4bit computer (https://github.com/gto76/comp-cpp/blob/1bf9a77eaf8f57f7358a316e5bbada97f2dc8987/src/output.c#L121).\nTwo boolean flags are used. One to mark the delivery of \nSIGINT\n (CTRL-C), and gracefully shutdown the program, and the other to mark \nSIGWINCH\n signal to detect terminal resize and redraw the entire display. \n\n\n\n\n\nBack: Forking, Part 2: Fork, Exec, Wait\n\n |\n\n\nNext: Processes Review Questions",
            "title": "Process Control, Part 1: Wait macros, using signals"
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#wait-macros",
            "text": "",
            "title": "Wait Macros"
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#can-i-find-out-the-exit-value-of-my-child",
            "text": "You can find the lowest 8 bits of the child's exit value (the return value of  main()  or value included in  exit() ): Use the \"Wait macros\" - typically you will use \"WIFEXITED\" and \"WEXITSTATUS\" . See  wait / waitpid  man page for more information).  int status;\npid_t child = fork();\nif (child == -1) return 1; //Failed\nif (child > 0) { /* I am the parent - wait for the child to finish */\n  pid_t pid = waitpid(child, &status, 0);\n  if (pid != -1 && WIFEXITED(status)) {\n     int low8bits = WEXITSTATUS(status);\n     printf(\"Process %d returned %d\" , pid, low8bits);\n  }\n} else { /* I am the child */\n // do something interesting\n  execl(\"/bin/ls\", \"/bin/ls\", \".\", (char *) NULL); // \"ls .\"\n}  A process can only have 256 return values, the rest of the bits are informational.",
            "title": "Can I find out the exit value of my child?"
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#bit-shifting",
            "text": "Note there is no need to memorize this, this is just a high level overview of how information is stored inside the status variables  Android source code  /  If WIFEXITED(STATUS), the low-order 8 bits of the status.  /  #define __WEXITSTATUS(status) (((status) & 0xff00) >> 8)  /  If WIFSIGNALED(STATUS), the terminating signal.  /  #define __WTERMSIG(status) ((status) & 0x7f)  /  If WIFSTOPPED(STATUS), the signal that stopped the child.  /  #define  WSTOPSIG(status)  WEXITSTATUS(status)  /  Nonzero if STATUS indicates normal termination.  /  #define  WIFEXITED(status) ( WTERMSIG(status) == 0)  The kernel has an internal way of keeping track of signaled, exited, or stopped. That API is abstracted so that that the kernel developers are free to change at will.",
            "title": "Bit Shifting"
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#being-careful",
            "text": "Remember that the the macros only make sense if the precondition is met. Meaning that a process' exit status won't be defined if the process is signaled. The macros will not do the checking for you, so it's up to the programming to make sure the logic checks out.",
            "title": "Being careful."
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#signals",
            "text": "",
            "title": "Signals"
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#whats-a-signal",
            "text": "A signal is a construct provided to us by the kernel. It allows one process to asynchronously send a signal (think a message) to another process. If that process wants to accept the signal, it can, and then, for most signals, can decide what to do with that signal. Here is a short list (non comprehensive) of signals.     Name  Default Action  Usual Use Case      SIGINT  Terminate Process (Can be caught)  Tell the process to stop nicely    SIGQUIT  Terminate Process (Can be caught)  Tells the process to stop harshly    SIGSTOP  Stop Process (Can be caught)  Stops the process to be continued    SIGCONT  Continues a Process  Continues to run the process    SIGKILL  Terminate Process (Cannot be Ignored)  You want your process gone",
            "title": "What's a signal?"
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#can-i-pause-my-child",
            "text": "Yes ! You can temporarily pause a running process by sending it a SIGSTOP signal.\nIf it succeeds it will freeze a process; i.e. the process will not be allocated anymore CPU time.  To allow a process to resume execution send it the SIGCONT signal.  For example,\nHere's program that slowly prints a dot every second, up to 59 dots.  #include <unistd.h>\n#include <stdio.h>\nint main() {\n  printf(\"My pid is %d\\n\", getpid() );\n  int i = 60;\n  while(--i) { \n    write(1, \".\",1);\n    sleep(1);\n  }\n  write(1, \"Done!\",5);\n  return 0;\n}  We will first start the process in the background (notice the & at the end).\nThen send it a signal from the shell process by using the kill command.  >./program &\nMy pid is 403\n...\n>kill -SIGSTOP 403\n>kill -SIGCONT 403",
            "title": "Can I pause my child?"
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#how-do-i-killstopsuspend-my-child-from-c",
            "text": "In C, send a signal to the child using  kill  POSIX call,  kill(child, SIGUSR1); // Send a user-defined signal\nkill(child, SIGSTOP); // Stop the child process (the child cannot prevent this)\nkill(child, SIGTERM); // Terminate the child process (the child can prevent this)\nkill(child, SIGINT); // Equivalent to CTRL-C (by default closes the process)  As we saw above there is also a kill command available in the shell\ne.g. get a list of running processes and then terminate process 45 and process 46  ps\nkill -l \nkill -9 45\nkill -s TERM 46",
            "title": "How do I kill/stop/suspend my child from C?"
        },
        {
            "location": "/SystemProgramming/Process-Control,-Part-1:-Wait-macros,-using-signals/#how-can-i-detect-ctrl-c-and-clean-up-gracefully",
            "text": "We will return to signals later on - this is just a short introduction. On a Linux system, see  man -s7 signal  if you are interested in finding out more (for example a list of system and library calls that are async-signal-safe.  There are strict limitations on the executable code inside a signal handler. Most library and system calls are not 'async-signal-safe' - they may be not use used inside a signal handler because they are not re-entrant safe. In a single-threaded program, signal handling momentarily interrupts the program execution to execute the signal handler code instead. Suppose your original program was interrupted while executing the library code of  malloc  ;  the memory structures used by malloc will not be in a consistent state. Calling  printf  (which uses  malloc ) as part of the signal handler is unsafe and will result in \"undefined behavior\" i.e. it is no longer a useful,predictable program. In practice your program might crash, compute or generate incorrect results or stop functioning (\"deadlock\"), depending on exactly what your program was executing when it was interrupted to execute the signal handler code.  One common use of signal handlers is to set a boolean flag that is occasionally polled (read) as part of the normal running of the program. For example,  int pleaseStop ; // See notes on why \"volatile sig_atomic_t\" is better\n\nvoid handle_sigint(int signal) {\n  pleaseStop = 1;\n}\n\nint main() {\n  signal(SIGINT, handle_sigint);\n  pleaseStop = 0;\n  while ( ! pleaseStop) { \n     /* application logic here */ \n   }\n  /* cleanup code here */\n}  The above code might appear to be correct on paper. However, we need to provide a hint to the compiler and to the CPU core that will execute the  main()  loop. We need to prevent a compiler optimization: The expression  ! pleaseStop  appears to be a loop invariant i.e. true forever, so can be simplified to  true .  Secondly, we need to ensure that the value of  pleaseStop  is not cached using a CPU register and instead always read from and written to main memory. The  sig_atomic_t  type implies that all the bits of the variable can be read or modified as an \"atomic operation\" - a single uninterruptable operation. It is impossible to read a value that is composed of some new bit values and old bit values.  By specifying  pleaseStop  with the correct type  volatile sig_atomic_t  we can write portable code where the main loop will be exited after the signal handler returns. The  sig_atomic_t  type can be as large as an  int  on most modern platforms but on embedded systems can be as small as a  char  and only able to represent (-127 to 127) values.  volatile sig_atomic_t pleaseStop;  Two examples of this pattern can be found in \"COMP\" a terminal based 1Hz 4bit computer (https://github.com/gto76/comp-cpp/blob/1bf9a77eaf8f57f7358a316e5bbada97f2dc8987/src/output.c#L121).\nTwo boolean flags are used. One to mark the delivery of  SIGINT  (CTRL-C), and gracefully shutdown the program, and the other to mark  SIGWINCH  signal to detect terminal resize and redraw the entire display.    \nBack: Forking, Part 2: Fork, Exec, Wait  | \nNext: Processes Review Questions",
            "title": "How can I detect \"CTRL-C\" and clean up gracefully?"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/",
            "text": "Overview\n\n\nA process is program that is running (kinda). A process is also just one instance of that computer program running. Processes have a lot of things at their disposal. At the start of each program you get one process, but each program can make more processes. In fact, your operating system starts up with only one process and all other processes are forked off of that -- all of that is done under the hood when booting up.\n\n\nIn the beginning\n\n\nWhen your operating system starts on a linux machine, there is a process called \ninit.d\n that gets created. That process is a special one handling signals, interrupts, and a persistence module for certain kernel elements. Whenever you want to make a new process, you call \nfork\n (to be discussed in a later section) and use another function to load another program.\n\n\nProcess Isolation\n\n\nProcesses are very powerful but they are isolated! That means that by default, no process can communicate with another process. This is very important because if you have a large system (let's say EWS) then you want some processes to have higher privilages (monitoring, admin) than your average user, and one certainly doesn't want the average user to be able to bring down the entire system either on purpose or accidentally by modifying a process.\n\n\nIf I run the following code,\n\n\nint secrets; //maybe defined in the kernel or else where\nsecrets++;\nprintf(\"%d\\n\", secrets);\n\n\n\n\nOn two different terminals, as you would guess they would both print out 1 not too. Even if we changed the code to do something really hacky (apart from reading the memory directly) there would be no way to change another process' state (okay maybe \nthis\n but that is getting a little too in depth).\n\n\nProcess Contents\n\n\nMemory Layout\n\n\n\n\nWhen a process starts, it gets its own address space. Meaning that each process gets (for Memory\n\n \nA Stack\n. The Stack is the place where automatic variable and function call return addresses are stored. Every time a new variable is declared, the program moves the stack pointer down to reserve space for the variable. This segment of the stack is Writable but not executable. If the stack grows too far (meaning that it either grows beyond a preset boundary or intersects the heap) you will get a stackoverflow most likely resulting in a SEGFAULT or something similar. \nThe stack is statically allocated by default meaning that there is only a certain amount of space to which one can write\n\n\n \nA Heap\n. The heap is an expanding region of memory. If you want to allocate a large object, it goes here. The heap starts at the top of the text segment and grows upward (meaning sometimes when you call \nmalloc\n that it asks the operating system to push the heap boundary upward). This area is also Writable but not Executable. One can run out of heap memory if the system is constrained or if you run out of addresses (more common on a 32bit system).\n\n \nA Data Segment\n This contains all of your globals. This section starts at the end of the Text segment and is static in size because the amount of globals is known at compile time. This section is Writable but not Executable and there isn't anything else too fancy here.\n\n \nA Text Segment\n. This is, arguably, the most important section of the address. This is where all your code is stored. Since assembly compiles to 1's and 0's, this is where the 1's and 0's get stored. The program counter moves through this segment executing instructions and moving down the next instruction. It is important to note that this is the only Executable section of the code. If you try to change the code while it's running, most likely you will segfault (there are ways around it but just assume that it segfaults).\n* Why doesn't it start at zero? It is outside the \nscope\n of this class but it is for security.\n\n\nFile Descriptors\n\n\n\n\nAs the little zine shows, the OS keeps track of the file descriptors and what they point to. We will see later that file descriptors need not point to actual files and the OS keeps track of them for you. Also, notice that between processes file descriptors may be reused but inside of a process they are unique.\n\n\nFile descriptors also have a notion of position. You can read a file on disk completely because the OS keeps track of the position in the file, and that belongs to your process as well.\n\n\nSecurity/Permissions\n\n\nProcess Functions/Limitations (Bonus)\n\n\nWhen you are reviewing for the final, you can come back and see that a process also has all of these things. The first time around -- it won't make much sense.\n\n\nProcess ID (PID)\n\n\nTo keep track of all these processes, your operating system gives each process a number and that process is called the PID, process ID.\n\n\nProcesses could also contain\n\n Mappings\n\n State\n\n File Descriptors\n\n Permissions\n\n\n\n\n\nForking, Part 1: Introduction",
            "title": "Processes, Part 1: Introduction"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#overview",
            "text": "A process is program that is running (kinda). A process is also just one instance of that computer program running. Processes have a lot of things at their disposal. At the start of each program you get one process, but each program can make more processes. In fact, your operating system starts up with only one process and all other processes are forked off of that -- all of that is done under the hood when booting up.",
            "title": "Overview"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#in-the-beginning",
            "text": "When your operating system starts on a linux machine, there is a process called  init.d  that gets created. That process is a special one handling signals, interrupts, and a persistence module for certain kernel elements. Whenever you want to make a new process, you call  fork  (to be discussed in a later section) and use another function to load another program.",
            "title": "In the beginning"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#process-isolation",
            "text": "Processes are very powerful but they are isolated! That means that by default, no process can communicate with another process. This is very important because if you have a large system (let's say EWS) then you want some processes to have higher privilages (monitoring, admin) than your average user, and one certainly doesn't want the average user to be able to bring down the entire system either on purpose or accidentally by modifying a process.  If I run the following code,  int secrets; //maybe defined in the kernel or else where\nsecrets++;\nprintf(\"%d\\n\", secrets);  On two different terminals, as you would guess they would both print out 1 not too. Even if we changed the code to do something really hacky (apart from reading the memory directly) there would be no way to change another process' state (okay maybe  this  but that is getting a little too in depth).",
            "title": "Process Isolation"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#process-contents",
            "text": "",
            "title": "Process Contents"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#memory-layout",
            "text": "When a process starts, it gets its own address space. Meaning that each process gets (for Memory   A Stack . The Stack is the place where automatic variable and function call return addresses are stored. Every time a new variable is declared, the program moves the stack pointer down to reserve space for the variable. This segment of the stack is Writable but not executable. If the stack grows too far (meaning that it either grows beyond a preset boundary or intersects the heap) you will get a stackoverflow most likely resulting in a SEGFAULT or something similar.  The stack is statically allocated by default meaning that there is only a certain amount of space to which one can write    A Heap . The heap is an expanding region of memory. If you want to allocate a large object, it goes here. The heap starts at the top of the text segment and grows upward (meaning sometimes when you call  malloc  that it asks the operating system to push the heap boundary upward). This area is also Writable but not Executable. One can run out of heap memory if the system is constrained or if you run out of addresses (more common on a 32bit system).   A Data Segment  This contains all of your globals. This section starts at the end of the Text segment and is static in size because the amount of globals is known at compile time. This section is Writable but not Executable and there isn't anything else too fancy here.   A Text Segment . This is, arguably, the most important section of the address. This is where all your code is stored. Since assembly compiles to 1's and 0's, this is where the 1's and 0's get stored. The program counter moves through this segment executing instructions and moving down the next instruction. It is important to note that this is the only Executable section of the code. If you try to change the code while it's running, most likely you will segfault (there are ways around it but just assume that it segfaults).\n* Why doesn't it start at zero? It is outside the  scope  of this class but it is for security.",
            "title": "Memory Layout"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#file-descriptors",
            "text": "As the little zine shows, the OS keeps track of the file descriptors and what they point to. We will see later that file descriptors need not point to actual files and the OS keeps track of them for you. Also, notice that between processes file descriptors may be reused but inside of a process they are unique.  File descriptors also have a notion of position. You can read a file on disk completely because the OS keeps track of the position in the file, and that belongs to your process as well.",
            "title": "File Descriptors"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#securitypermissions",
            "text": "",
            "title": "Security/Permissions"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#process-functionslimitations-bonus",
            "text": "When you are reviewing for the final, you can come back and see that a process also has all of these things. The first time around -- it won't make much sense.",
            "title": "Process Functions/Limitations (Bonus)"
        },
        {
            "location": "/SystemProgramming/Processes,-Part-1:-Introduction/#process-id-pid",
            "text": "To keep track of all these processes, your operating system gives each process a number and that process is called the PID, process ID.  Processes could also contain  Mappings  State  File Descriptors  Permissions   \nForking, Part 1: Introduction",
            "title": "Process ID (PID)"
        },
        {
            "location": "/SystemProgramming/Processes-Review-Questions/",
            "text": "Topics\n\n\n\n\nCorrect use of fork, exec and waitpid\n\n\nUsing exec with a path\n\n\nUnderstanding what fork and exec and waitpid do. E.g. how to use their return values.\n\n\nSIGKILL vs SIGSTOP vs SIGINT. \n\n\nWhat signal is sent when you press CTRL-C\n\n\nUsing kill from the shell or the kill POSIX call.\n\n\nProcess memory isolation.\n\n\nProcess memory layout (where is the heap, stack etc; invalid memory addresses).\n\n\nWhat is a fork bomb, zombie and orphan? How to create/remove them.\n\n\ngetpid vs getppid\n\n\nHow to use the WAIT exit status macros WIFEXITED etc.\n\n\n\n\nQuestions/Exercises\n\n\n\n\nWhat is the difference between execs with a p and without a p? What does the operating system\n\n\nHow do you pass in command line arguments to \nexecl*\n? How about \nexecv*\n? What should be the first command line argument by convention?\n\n\nHow do you know if \nexec\n or \nfork\n failed?\n\n\nWhat is the \nint *status\n pointer passed into wait? When does wait fail?\n\n\nWhat are some differences between \nSIGKILL\n, \nSIGSTOP\n, \nSIGCONT\n, \nSIGINT\n? What are the default behaviors? Which ones can you set up a signal handler for?\n\n\nWhat signal is sent when you press \nCTRL-C\n?\n\n\nMy terminal is anchored to PID = 1337 and has just become unresponsive. Write me the terminal command and the C code to send \nSIGQUIT\n to it.\n\n\nCan one process alter another processes memory through normal means? Why?\n\n\nWhere is the heap, stack, data, and text segment? Which segments can you write to? What are invalid memory addresses?\n\n\nCode me up a fork bomb in C (please don't run it).\n\n\nWhat is an orphan? How does it become a zombie? How do I be a good parent?\n\n\nDon't you hate it when your parents tell you that you can't do something? Write me a program that sends \nSIGSTOP\n to your parent.\n\n\nWrite a function that fork exec waits an executable, and using the wait macros tells me if the process exited normally or if it was signaled. If the process exited normally, then print that with the return value. If not, then print the signal number that caused the process to terminate.\n\n\n\n\nBack: Process Control, Part 1: Wait macros, using signals",
            "title": "Processes Review Questions"
        },
        {
            "location": "/SystemProgramming/Processes-Review-Questions/#topics",
            "text": "Correct use of fork, exec and waitpid  Using exec with a path  Understanding what fork and exec and waitpid do. E.g. how to use their return values.  SIGKILL vs SIGSTOP vs SIGINT.   What signal is sent when you press CTRL-C  Using kill from the shell or the kill POSIX call.  Process memory isolation.  Process memory layout (where is the heap, stack etc; invalid memory addresses).  What is a fork bomb, zombie and orphan? How to create/remove them.  getpid vs getppid  How to use the WAIT exit status macros WIFEXITED etc.",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/Processes-Review-Questions/#questionsexercises",
            "text": "What is the difference between execs with a p and without a p? What does the operating system  How do you pass in command line arguments to  execl* ? How about  execv* ? What should be the first command line argument by convention?  How do you know if  exec  or  fork  failed?  What is the  int *status  pointer passed into wait? When does wait fail?  What are some differences between  SIGKILL ,  SIGSTOP ,  SIGCONT ,  SIGINT ? What are the default behaviors? Which ones can you set up a signal handler for?  What signal is sent when you press  CTRL-C ?  My terminal is anchored to PID = 1337 and has just become unresponsive. Write me the terminal command and the C code to send  SIGQUIT  to it.  Can one process alter another processes memory through normal means? Why?  Where is the heap, stack, data, and text segment? Which segments can you write to? What are invalid memory addresses?  Code me up a fork bomb in C (please don't run it).  What is an orphan? How does it become a zombie? How do I be a good parent?  Don't you hate it when your parents tell you that you can't do something? Write me a program that sends  SIGSTOP  to your parent.  Write a function that fork exec waits an executable, and using the wait macros tells me if the process exited normally or if it was signaled. If the process exited normally, then print that with the return value. If not, then print the signal number that caused the process to terminate.  \nBack: Process Control, Part 1: Wait macros, using signals",
            "title": "Questions/Exercises"
        },
        {
            "location": "/SystemProgramming/Programming-Tricks,-Part-1/",
            "text": "Use \ncat\n as your IDE\n\n\nWho needs an editor? IDE? We can just use \ncat\n!\nYou've seen \ncat\n being used to read the contents of files but it can also be used to read the  standard-input and send it back to standard output.\n\n\n$ cat\nHELLO\nHELLO\n\n\n\n\nTo finish reading from the input stream close the input stream by pressing \nCTRL-D\n\n\nLet's use \ncat\n to send standard input to a file. We will use '>' to redirect its output to a file:\n\n\n$ cat > myprog.c\n#include <stdio.h>\nint main() {printf(\"Hi!\");return 0;}\n\n\n\n\n(Be careful! Deletes and undos are not allowed...)\nPress \nCTRL-D\n when finished.\n\n\nEdit your code with \nperl\n regular expressions (aka \"remember your perl pie\")\n\n\nA useful trick if you have several text files (e.g. source code) to change is to use regular expressions.\n\nperl\n makes this very easy to edit files in place.\nJust remember 'perl pie' and search on the web...\n\n\nAn example. Suppose we want to change the sequence \"Hi\" to \"Bye\" in all .c files in the current directory. Then we can write a simple substitution pattern that will be executed on each line at time in all files:\n\n\n$ perl -p -i -e 's/Hi/Bye/' *.c\n\n\n\n\n(Don't panic if you get it wrong, original files are still there; they just have the extension .bak)\nObviously there's a lot more you can do with regular expressions than changing Hi to Bye.\n\n\nUse your shell \n!!\n\n\nTo re-run the last command just type \n!!\n and press \nreturn\n\nTo re-run the last command that started with g type \n!g\n  and press \nreturn\n\n\nUse your shell \n&&\n\n\nTired of running \nmake\n or \ngcc\n and then running the program if it compiled OK? Instead, use && to chain these commands together\n\n\n$ gcc program.c && ./a.out\n\n\n\n\nMake can do more than make\n\n\nYou might also try putting a line in your Makefile that will compile, and then run your program.\n\n\nrun : $(program)\n        ./$(program)\n\n\n\n\nThen running\n\n\n$ make run\n\n\n\n\nwill make sure any changes you've made are compiled, and run your program in one go. Also good for testing many inputs at once. Although you probably would just rather write a regular shell script for that.\n\n\nIs your neighbor too productive? C pre-procesors to the rescue!\n\n\nUse the C pre-processor to redefine common keywords e.g.\n\n\n#define if while\n\n\n\n\nProtip: Put this line inside one of the standard includes e.g. /usr/include/stdio.h\n\n\nWho needs functions when you C have the preprocessor\n\n\nOK, so this is more of a gotcha. Be careful when using macros that look like functions...\n\n\n#define min(a,b) a<b?a:b\n\n\n\n\nA perfectly reasonable definition of a minimum of a and b. However the pre-processor is just a simple\ntext wrangler so precedence can bite you:\n\n\nint value = -min(2,3); // Should be -2?\n\n\n\n\nIs expanded to \n\n\nint value = -2<3 ? 2 :3; // Ooops.. result will be 2\n\n\n\n\nA partial fix is to wrap every argument with \n()\n and also the whole expression with ():\n\n\n#define min(a,b) (  (a) < (b) ?(a):(b) )\n\n\n\n\nHowever this is still \nnot\n a function! For example can you see why \nmin(i++,10)\n might increment i once or twice!?",
            "title": "Programming Tricks, Part 1"
        },
        {
            "location": "/SystemProgramming/Programming-Tricks,-Part-1/#use-cat-as-your-ide",
            "text": "Who needs an editor? IDE? We can just use  cat !\nYou've seen  cat  being used to read the contents of files but it can also be used to read the  standard-input and send it back to standard output.  $ cat\nHELLO\nHELLO  To finish reading from the input stream close the input stream by pressing  CTRL-D  Let's use  cat  to send standard input to a file. We will use '>' to redirect its output to a file:  $ cat > myprog.c\n#include <stdio.h>\nint main() {printf(\"Hi!\");return 0;}  (Be careful! Deletes and undos are not allowed...)\nPress  CTRL-D  when finished.",
            "title": "Use cat as your IDE"
        },
        {
            "location": "/SystemProgramming/Programming-Tricks,-Part-1/#edit-your-code-with-perl-regular-expressions-aka-remember-your-perl-pie",
            "text": "A useful trick if you have several text files (e.g. source code) to change is to use regular expressions. perl  makes this very easy to edit files in place.\nJust remember 'perl pie' and search on the web...  An example. Suppose we want to change the sequence \"Hi\" to \"Bye\" in all .c files in the current directory. Then we can write a simple substitution pattern that will be executed on each line at time in all files:  $ perl -p -i -e 's/Hi/Bye/' *.c  (Don't panic if you get it wrong, original files are still there; they just have the extension .bak)\nObviously there's a lot more you can do with regular expressions than changing Hi to Bye.",
            "title": "Edit your code with perl regular expressions (aka \"remember your perl pie\")"
        },
        {
            "location": "/SystemProgramming/Programming-Tricks,-Part-1/#use-your-shell",
            "text": "To re-run the last command just type  !!  and press  return \nTo re-run the last command that started with g type  !g   and press  return",
            "title": "Use your shell !!"
        },
        {
            "location": "/SystemProgramming/Programming-Tricks,-Part-1/#use-your-shell_1",
            "text": "Tired of running  make  or  gcc  and then running the program if it compiled OK? Instead, use && to chain these commands together  $ gcc program.c && ./a.out",
            "title": "Use your shell &amp;&amp;"
        },
        {
            "location": "/SystemProgramming/Programming-Tricks,-Part-1/#make-can-do-more-than-make",
            "text": "You might also try putting a line in your Makefile that will compile, and then run your program.  run : $(program)\n        ./$(program)  Then running  $ make run  will make sure any changes you've made are compiled, and run your program in one go. Also good for testing many inputs at once. Although you probably would just rather write a regular shell script for that.",
            "title": "Make can do more than make"
        },
        {
            "location": "/SystemProgramming/Programming-Tricks,-Part-1/#is-your-neighbor-too-productive-c-pre-procesors-to-the-rescue",
            "text": "Use the C pre-processor to redefine common keywords e.g.  #define if while  Protip: Put this line inside one of the standard includes e.g. /usr/include/stdio.h",
            "title": "Is your neighbor too productive? C pre-procesors to the rescue!"
        },
        {
            "location": "/SystemProgramming/Programming-Tricks,-Part-1/#who-needs-functions-when-you-c-have-the-preprocessor",
            "text": "OK, so this is more of a gotcha. Be careful when using macros that look like functions...  #define min(a,b) a<b?a:b  A perfectly reasonable definition of a minimum of a and b. However the pre-processor is just a simple\ntext wrangler so precedence can bite you:  int value = -min(2,3); // Should be -2?  Is expanded to   int value = -2<3 ? 2 :3; // Ooops.. result will be 2  A partial fix is to wrap every argument with  ()  and also the whole expression with ():  #define min(a,b) (  (a) < (b) ?(a):(b) )  However this is still  not  a function! For example can you see why  min(i++,10)  might increment i once or twice!?",
            "title": "Who needs functions when you C have the preprocessor"
        },
        {
            "location": "/SystemProgramming/Pthread-Review-Questions/",
            "text": "Topics\n\n\n\n\npthread lifecycle\n\n\nEach thread has a stack\n\n\nCapturing return values from a thread\n\n\nUsing \npthread_join\n\n\nUsing \npthread_create\n\n\nUsing \npthread_exit\n\n\nUnder what conditions will a process exit\n\n\n\n\nQuestions\n\n\n\n\nWhat happens when a pthread gets created? (you don't need to go into super specifics)\n\n\nWhere is each thread's stack?\n\n\nHow do you get a return value given a a \npthread_t\n? What are the ways a thread can set that return value? What happens if you discard the return value?\n\n\nWhy is \npthread_join\n important (think stack space, registers, return values)?\n\n\nWhat does \npthread_exit\n do under normal circumstances (ie you are not the last thread)? What other functions are called when you call pthread_exit?\n\n\nGive me three conditions under which a multithreaded process will exit. Can you think of any more?\n\n\nWhat is an embarrassingly parallel problem?",
            "title": "Pthread Review Questions"
        },
        {
            "location": "/SystemProgramming/Pthread-Review-Questions/#topics",
            "text": "pthread lifecycle  Each thread has a stack  Capturing return values from a thread  Using  pthread_join  Using  pthread_create  Using  pthread_exit  Under what conditions will a process exit",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/Pthread-Review-Questions/#questions",
            "text": "What happens when a pthread gets created? (you don't need to go into super specifics)  Where is each thread's stack?  How do you get a return value given a a  pthread_t ? What are the ways a thread can set that return value? What happens if you discard the return value?  Why is  pthread_join  important (think stack space, registers, return values)?  What does  pthread_exit  do under normal circumstances (ie you are not the last thread)? What other functions are called when you call pthread_exit?  Give me three conditions under which a multithreaded process will exit. Can you think of any more?  What is an embarrassingly parallel problem?",
            "title": "Questions"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-1:-Introduction/",
            "text": "Intro to Threads\n\n\nWhat is a thread?\n\n\nA thread is short for 'thread-of-execution'. It represents the sequence of instructions that the CPU has (and will) execute. To remember how to return from function calls, and to store the values of automatic variables and  parameters a thread uses a stack.\n\n\nWhat is a Lightweight Process (LWP)? How does it relate to threads?\n\n\nWell for all intensive purposes a thread is a process (meaning that creating a thread is similar to \nfork\n) except there is \nno copying\n meaning no copy on write. What this allows is for a process to share the same address space, variables, heap, file descriptors and etc.\n\n\nThe actual system call to create a thread is similar to \nfork\n; it's \nclone\n. We won't go into the specifics but you can read the \nman pages\n keeping in mind that it is outside the direct scope of this course.\n\n\nLWP or threads are preferred to forking for a lot of scenarios because there is a lot less overhead creating them. But in some cases (notably python uses this) multiprocessing is the way to make your code faster.\n\n\nHow does the thread's stack work?\n\n\nYour main function (and other functions you might call) has automatic variables. We will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the \"stack pointer\"). If the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables. Once it returns from a function, we can move the stack pointer back up to its previous value. We keep a copy of the old stack pointer value - on the stack! This is why returning from a function is very quick - it's easy to 'free' the memory used by automatic variables - we just need to change the stack pointer.\n\n\n\n\nIn a multi threaded program, there are multiple stack but only one address space. The pthread library allocates some stack space (either in the heap or using a part of the main program's stack) and uses the \nclone\n function call to start the thread at that stack address. The total address space may look something like this.\n\n\n\n\nHow many threads can my process have?\n\n\nYou can have more than one thread running inside a process. You get the first thread for free! It runs the code you write inside 'main'. If you need more threads you can call \npthread_create\n to create a new thread using the pthread library. You'll need to pass a pointer to a function so that the thread knows where to start.\n\n\nThe threads you create all live inside the same virtual memory because they are part of the same process. Thus they can all see the heap, the global variables and the program code etc. Thus you can have two (or more) CPUs working on your program at the same time and inside the same process. It's up to the operating system to assign the threads to CPUs. If you have more active threads than CPUs then the kernel will assign the thread to a CPU for a short duration (or until it runs out of things to do) and then will automatically switch the CPU to work on another thread. \nFor example, one CPU might be processing the game AI while another thread is computing the graphics output.\n\n\nSimple Usage\n\n\nHello world pthread example\n\n\nTo use pthreads you will need to include \npthread.h\n AND you need to compile with \n-pthread\n (or \n-lpthread\n) compiler option. This option tells the compiler that your program requires threading support\n\n\nTo create a thread use the function \npthread_create\n. This function takes four arguments:\n\n\nint pthread_create(pthread_t *thread, const pthread_attr_t *attr,\n                   void *(*start_routine) (void *), void *arg);\n\n\n\n\n\n\nThe first is a pointer to a variable that will hold the id of the newly created thread.\n\n\nThe second is a pointer to attributes that we can use to tweak and tune some of the advanced features of pthreads.\n\n\nThe third is a pointer to a function that we want to run\n\n\nFourth is a pointer that will be given to our function\n\n\n\n\nThe argument \nvoid *(*start_routine) (void *)\n is difficult to read! It means a pointer that takes a \nvoid *\n pointer and returns a \nvoid *\n pointer. It looks like a function declaration except that the name of the function is wrapped with \n(* .... )\n\n\nHere's the simplest example:\n\n\n#include <stdio.h>\n#include <pthread.h>\n// remember to set compilation option -pthread\n\nvoid *busy(void *ptr) {\n// ptr will point to \"Hi\"\n    puts(\"Hello World\");\n    return NULL;\n}\nint main() {\n    pthread_t id;\n    pthread_create(&id, NULL, busy, \"Hi\");\n    while (1) {} // Loop forever\n}\n\n\n\n\nIf we want to wait for our thread to finish use \npthread_join\n\n\nvoid *result;\npthread_join(id, &result);\n\n\n\n\nIn the above example, \nresult\n will be \nnull\n because the busy function returned \nnull\n.\nWe need to pass the address-of result because \npthread_join\n will be writing into the contents of our pointer.\n\n\nSee \nPthreads Part 2",
            "title": "Pthreads, Part 1: Introduction"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-1:-Introduction/#intro-to-threads",
            "text": "",
            "title": "Intro to Threads"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-1:-Introduction/#what-is-a-thread",
            "text": "A thread is short for 'thread-of-execution'. It represents the sequence of instructions that the CPU has (and will) execute. To remember how to return from function calls, and to store the values of automatic variables and  parameters a thread uses a stack.",
            "title": "What is a thread?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-1:-Introduction/#what-is-a-lightweight-process-lwp-how-does-it-relate-to-threads",
            "text": "Well for all intensive purposes a thread is a process (meaning that creating a thread is similar to  fork ) except there is  no copying  meaning no copy on write. What this allows is for a process to share the same address space, variables, heap, file descriptors and etc.  The actual system call to create a thread is similar to  fork ; it's  clone . We won't go into the specifics but you can read the  man pages  keeping in mind that it is outside the direct scope of this course.  LWP or threads are preferred to forking for a lot of scenarios because there is a lot less overhead creating them. But in some cases (notably python uses this) multiprocessing is the way to make your code faster.",
            "title": "What is a Lightweight Process (LWP)? How does it relate to threads?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-1:-Introduction/#how-does-the-threads-stack-work",
            "text": "Your main function (and other functions you might call) has automatic variables. We will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the \"stack pointer\"). If the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables. Once it returns from a function, we can move the stack pointer back up to its previous value. We keep a copy of the old stack pointer value - on the stack! This is why returning from a function is very quick - it's easy to 'free' the memory used by automatic variables - we just need to change the stack pointer.   In a multi threaded program, there are multiple stack but only one address space. The pthread library allocates some stack space (either in the heap or using a part of the main program's stack) and uses the  clone  function call to start the thread at that stack address. The total address space may look something like this.",
            "title": "How does the thread's stack work?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-1:-Introduction/#how-many-threads-can-my-process-have",
            "text": "You can have more than one thread running inside a process. You get the first thread for free! It runs the code you write inside 'main'. If you need more threads you can call  pthread_create  to create a new thread using the pthread library. You'll need to pass a pointer to a function so that the thread knows where to start.  The threads you create all live inside the same virtual memory because they are part of the same process. Thus they can all see the heap, the global variables and the program code etc. Thus you can have two (or more) CPUs working on your program at the same time and inside the same process. It's up to the operating system to assign the threads to CPUs. If you have more active threads than CPUs then the kernel will assign the thread to a CPU for a short duration (or until it runs out of things to do) and then will automatically switch the CPU to work on another thread. \nFor example, one CPU might be processing the game AI while another thread is computing the graphics output.",
            "title": "How many threads can my process have?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-1:-Introduction/#simple-usage",
            "text": "",
            "title": "Simple Usage"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-1:-Introduction/#hello-world-pthread-example",
            "text": "To use pthreads you will need to include  pthread.h  AND you need to compile with  -pthread  (or  -lpthread ) compiler option. This option tells the compiler that your program requires threading support  To create a thread use the function  pthread_create . This function takes four arguments:  int pthread_create(pthread_t *thread, const pthread_attr_t *attr,\n                   void *(*start_routine) (void *), void *arg);   The first is a pointer to a variable that will hold the id of the newly created thread.  The second is a pointer to attributes that we can use to tweak and tune some of the advanced features of pthreads.  The third is a pointer to a function that we want to run  Fourth is a pointer that will be given to our function   The argument  void *(*start_routine) (void *)  is difficult to read! It means a pointer that takes a  void *  pointer and returns a  void *  pointer. It looks like a function declaration except that the name of the function is wrapped with  (* .... )  Here's the simplest example:  #include <stdio.h>\n#include <pthread.h>\n// remember to set compilation option -pthread\n\nvoid *busy(void *ptr) {\n// ptr will point to \"Hi\"\n    puts(\"Hello World\");\n    return NULL;\n}\nint main() {\n    pthread_t id;\n    pthread_create(&id, NULL, busy, \"Hi\");\n    while (1) {} // Loop forever\n}  If we want to wait for our thread to finish use  pthread_join  void *result;\npthread_join(id, &result);  In the above example,  result  will be  null  because the busy function returned  null .\nWe need to pass the address-of result because  pthread_join  will be writing into the contents of our pointer.  See  Pthreads Part 2",
            "title": "Hello world pthread example"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/",
            "text": "More pthread functions\n\n\nHow do I create a pthread?\n\n\nSee \nPthreads Part 1\n which introduces \npthread_create\n and \npthread_join\n\n\nIf I call \npthread_create\n twice, how many stacks does my process have?\n\n\nYour process will contain three stacks - one for each thread. The first thread is created when the process starts, and you created two more. Actually there can be more stacks than this, but let's ignore that complication for now. The important idea is that each thread requires a stack because the stack contains automatic variables and the old CPU PC register, so that it can back to executing the calling function after the function is finished.\n\n\nWhat is the difference between a full process and a thread?\n\n\nIn addition, unlike processes, threads within the same process can share the same global memory (data and heap segments).\n\n\nWhat does \npthread_cancel\n do?\n\n\nStops a thread. Note the thread may not actually be stopped immediately. For example it can be terminated when the thread makes an operating system call (e.g. \nwrite\n).\n\n\nIn practice, \npthread_cancel\n is rarely used because it does not give a thread an opportunity to clean up after itself (for example, it may have opened some files).\nAn alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.\n\n\nWhat is the difference between \nexit\n and \npthread_exit\n?\n\n\nexit(42)\n exits the entire process and sets the processes exit value.  This is equivalent to \nreturn 42\n in the main method. All threads inside the process are stopped.\n\n\npthread_exit(void *)\n only stops the calling thread i.e. the thread never returns after calling \npthread_exit\n. The pthread library will automatically finish the process if there are no other threads running. \npthread_exit(...)\n is equivalent to returning from the thread's function; both finish the thread and also set the return value (void *pointer) for the thread.\n\n\nCalling \npthread_exit\n in the the \nmain\n thread is a common way for simple programs to ensure that all threads finish. For example, in the following program, the  \nmyfunc\n threads will probably not have time to get started.\n\n\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(&tid1, NULL, myfunc, \"Jabberwocky\");\n  pthread_create(&tid2, NULL, myfunc, \"Vorpel\");\n  exit(42); //or return 42;\n\n  // No code is run after exit\n}\n\n\n\n\nThe next two programs will wait for the new threads to finish-\n\n\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(&tid1, NULL, myfunc, \"Jabberwocky\");\n  pthread_create(&tid2, NULL, myfunc, \"Vorpel\");\n  pthread_exit(NULL); \n\n  // No code is run after pthread_exit\n  // However process will continue to exist until both threads have finished\n}\n\n\n\n\nAlternatively, we join on each thread (i.e. wait for it to finish) before we return from main (or call exit).\n\n\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(&tid1, NULL, myfunc, \"Jabberwocky\");\n  pthread_create(&tid2, NULL, myfunc, \"Vorpel\");\n  // wait for both threads to finish :\n  void* result;\n  pthread_join(tid1, &result);\n  pthread_join(tid2, &result); \n  return 42;\n}\n\n\n\n\nNote the pthread_exit version creates thread zombies, however this is not a long-running processes, so we don't care.\n\n\nHow can a thread be terminated?\n\n\n\n\nReturning from the thread function\n\n\nCalling \npthread_exit\n\n\nCancelling the thread with \npthread_cancel\n\n\nTerminating the process (e.g. SIGTERM); exit(); returning from \nmain\n\n\n\n\nWhat is the purpose of pthread_join?\n\n\n\n\nWait for a thread to finish\n\n\nClean up thread resources\n\n\nGrabs the return value of the thread\n\n\n\n\nWhat happens if you don't call \npthread_join\n?\n\n\nFinished threads will continue to consume resources. Eventually, if enough threads are created, \npthread_create\n will fail.\nIn practice, this is only an issue for long-running processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits.\n\n\nShould I use \npthread_exit\n or \npthread_join\n?\n\n\nBoth \npthread_exit\n and \npthread_join\n will let the other threads finish on their own (even if called in the main thread). However, only \npthread_join\n will return to you when the specified thread finishes. \npthread_exit\n does not wait and will immediately end your thread and give you no chance to continue executing.\n\n\nCan you pass pointers to stack variables from one thread to another?\n\n\nYes. However you need to be very careful about the lifetime of stack variables.\n\n\npthread_t start_threads() {\n  int start = 42;\n  pthread_t tid;\n  pthread_create(&tid, 0, myfunc, &start); // ERROR!\n  return tid;\n}\n\n\n\n\nThe above code is invalid because the function \nstart_threads\n will likely return before \nmyfunc\n even starts. The function passes the address-of \nstart\n, however by the time \nmyfunc\n is executes, \nstart\n is no longer in scope and its address will re-used for another variable.\n\n\nThe following code is valid because the lifetime of the stack variable is longer than the background thread.\n\n\nvoid start_threads() {\n  int start = 42;\n  void *result;\n  pthread_t tid;\n  pthread_create(&tid, 0, myfunc, &start); // OK - start will be valid!\n  pthread_join(tid, &result);\n}\n\n\n\n\nIntro to Race Conditions\n\n\nHow can I create ten threads with different starting values.\n\n\nThe following code is supposed to start ten threads with values 0,1,2,3,...9\nHowever, when run prints out \n1 7 8 8 8 8 8 8 8 10\n! Can you see why?\n\n\n#include <pthread.h>\nvoid* myfunc(void* ptr) {\n    int i = *((int *) ptr);\n    printf(\"%d \", i);\n    return NULL;\n}\n\nint main() {\n    // Each thread gets a different value of i to process\n    int i;\n    pthread_t tid;\n    for(i =0; i < 10; i++) {\n        pthread_create(&tid, NULL, myfunc, &i); // ERROR\n    }\n    pthread_exit(NULL);\n}\n\n\n\n\nThe above code suffers from a \nrace condition\n - the value of i is changing. The new threads start later (in the example output the last thread starts after the loop has finished).\n\n\nTo overcome this race-condition, we will give each thread a pointer to it's own data area. For example, for each thread we may want to store the id, a starting value and an output value:\n\n\nstruct T {\n  pthread_t id;\n  int start;\n  char result[100];\n};\n\n\n\n\nThese can be stored in an array - \n\n\nstruct T *info = calloc(10 , sizeof(struct T)); // reserve enough bytes for ten T structures\n\n\n\n\nAnd each array element passed to each thread - \n\n\npthread_create(&info[i].id, NULL, func, &info[i]);\n\n\n\n\nWhy are some functions e.g.  asctime,getenv, strtok, strerror  not thread-safe?\n\n\nTo answer this, let's look at a simple function that is also not 'thread-safe'\n\n\nchar *to_message(int num) {\n    char static result [256];\n    if (num < 10) sprintf(result, \"%d : blah blah\" , num);\n    else strcpy(result, \"Unknown\");\n    return result;\n}\n\n\n\n\nIn the above code the result buffer is stored in global memory. This is good - we wouldn't want to return a pointer to an invalid address on the stack, but there's only one result buffer in the entire memory. If two threads were to use it at the same time then one would corrupt the other:\n\n\n\n\n\n\n\n\nTime\n\n\nThread 1\n\n\nThread 2\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n1\n\n\nto_m(5 )\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n\nto_m(99)\n\n\nNow both threads will see \"Unknown\" stored in the result buffer\n\n\n\n\n\n\n\n\nWhat are condition variables, semaphores, mutexes?\n\n\nThese are synchronization locks that are used to prevent race conditions and ensure proper synchronization between threads running in the same program. In addition, these locks are conceptually identical to the primitives used inside the kernel.\n\n\nAre there any advantages of using threads over forking processes?\n\n\nYes! Sharing information between threads is easy because threads (of the same process) live inside the same virtual memory space.\nAlso, creating a thread is significantly faster than creating(forking) a process.\n\n\nAre there any dis-advantages of using threads over forking processes?\n\n\nYes! No- isolation! As threads live inside the same process, one thread has access to the same virtual memory as the other threads. A single thread can terminate the entire process (e.g. by trying to read address zero).\n\n\nCan you fork a process with multiple threads?\n\n\nYes! However the child process only has a single thread (which is a clone of the thread that called \nfork\n. We can see this as a simple example, where the background threads never print out a second message in the child process.\n\n\n#include <pthread.h>\n#include <stdio.h>\n#include <unistd.h>\n\nstatic pid_t child = -2;\n\nvoid *sleepnprint(void *arg) {\n  printf(\"%d:%s starting up...\\n\", getpid(), (char *) arg);\n\n  while (child == -2) {sleep(1);} /* Later we will use condition variables */\n\n  printf(\"%d:%s finishing...\\n\",getpid(), (char*)arg);\n\n  return NULL;  \n}\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(&tid1,NULL, sleepnprint, \"New Thread One\");\n  pthread_create(&tid2,NULL, sleepnprint, \"New Thread Two\");\n\n  child = fork();\n  printf(\"%d:%s\\n\",getpid(), \"fork()ing complete\");\n  sleep(3);\n\n  printf(\"%d:%s\\n\",getpid(), \"Main thread finished\");\n\n  pthread_exit(NULL);\n  return 0; /* Never executes */\n}\n\n\n\n\n8970:New Thread One starting up...\n8970:fork()ing complete\n8973:fork()ing complete\n8970:New Thread Two starting up...\n8970:New Thread Two finishing...\n8970:New Thread One finishing...\n8970:Main thread finished\n8973:Main thread finished\n\n\n\n\nIn practice, creating threads before forking can lead to unexpected errors because (as demonstrated above) the other threads are immediately terminated when forking. Another thread might have just lock a mutex (e.g. by calling malloc) and never unlock it again. Advanced users may find \npthread_atfork\n useful however we suggest you usually try to avoid creating threads before forking unless you fully understand the limitations and difficulties of this approach.\n\n\nAre there other reasons where \nfork\n might be preferable to creating a thread.\n\n\nCreating separate processes is useful \n\n When more security is desired (for example, Chrome browser uses different processes for different tabs)\n\n When running an existing and complete program then a new process is required (e.g. starting 'gcc')\n* When you are running into synchronization primitives and each process is operating on something in the system\n\n\nHow can I find out more?\n\n\nSee the complete example in the \nman page\n\nAnd the \npthread reference guide\n\nALSO: \nConcise third party sample code explaining create, join and exit",
            "title": "Pthreads, Part 2: Usage in Practice"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#more-pthread-functions",
            "text": "",
            "title": "More pthread functions"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#how-do-i-create-a-pthread",
            "text": "See  Pthreads Part 1  which introduces  pthread_create  and  pthread_join",
            "title": "How do I create a pthread?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#if-i-call-pthread_create-twice-how-many-stacks-does-my-process-have",
            "text": "Your process will contain three stacks - one for each thread. The first thread is created when the process starts, and you created two more. Actually there can be more stacks than this, but let's ignore that complication for now. The important idea is that each thread requires a stack because the stack contains automatic variables and the old CPU PC register, so that it can back to executing the calling function after the function is finished.",
            "title": "If I call pthread_create twice, how many stacks does my process have?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#what-is-the-difference-between-a-full-process-and-a-thread",
            "text": "In addition, unlike processes, threads within the same process can share the same global memory (data and heap segments).",
            "title": "What is the difference between a full process and a thread?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#what-does-pthread_cancel-do",
            "text": "Stops a thread. Note the thread may not actually be stopped immediately. For example it can be terminated when the thread makes an operating system call (e.g.  write ).  In practice,  pthread_cancel  is rarely used because it does not give a thread an opportunity to clean up after itself (for example, it may have opened some files).\nAn alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.",
            "title": "What does pthread_cancel do?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#what-is-the-difference-between-exit-and-pthread_exit",
            "text": "exit(42)  exits the entire process and sets the processes exit value.  This is equivalent to  return 42  in the main method. All threads inside the process are stopped.  pthread_exit(void *)  only stops the calling thread i.e. the thread never returns after calling  pthread_exit . The pthread library will automatically finish the process if there are no other threads running.  pthread_exit(...)  is equivalent to returning from the thread's function; both finish the thread and also set the return value (void *pointer) for the thread.  Calling  pthread_exit  in the the  main  thread is a common way for simple programs to ensure that all threads finish. For example, in the following program, the   myfunc  threads will probably not have time to get started.  int main() {\n  pthread_t tid1, tid2;\n  pthread_create(&tid1, NULL, myfunc, \"Jabberwocky\");\n  pthread_create(&tid2, NULL, myfunc, \"Vorpel\");\n  exit(42); //or return 42;\n\n  // No code is run after exit\n}  The next two programs will wait for the new threads to finish-  int main() {\n  pthread_t tid1, tid2;\n  pthread_create(&tid1, NULL, myfunc, \"Jabberwocky\");\n  pthread_create(&tid2, NULL, myfunc, \"Vorpel\");\n  pthread_exit(NULL); \n\n  // No code is run after pthread_exit\n  // However process will continue to exist until both threads have finished\n}  Alternatively, we join on each thread (i.e. wait for it to finish) before we return from main (or call exit).  int main() {\n  pthread_t tid1, tid2;\n  pthread_create(&tid1, NULL, myfunc, \"Jabberwocky\");\n  pthread_create(&tid2, NULL, myfunc, \"Vorpel\");\n  // wait for both threads to finish :\n  void* result;\n  pthread_join(tid1, &result);\n  pthread_join(tid2, &result); \n  return 42;\n}  Note the pthread_exit version creates thread zombies, however this is not a long-running processes, so we don't care.",
            "title": "What is the difference between exit and pthread_exit?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#how-can-a-thread-be-terminated",
            "text": "Returning from the thread function  Calling  pthread_exit  Cancelling the thread with  pthread_cancel  Terminating the process (e.g. SIGTERM); exit(); returning from  main",
            "title": "How can a thread be terminated?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#what-is-the-purpose-of-pthread_join",
            "text": "Wait for a thread to finish  Clean up thread resources  Grabs the return value of the thread",
            "title": "What is the purpose of pthread_join?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#what-happens-if-you-dont-call-pthread_join",
            "text": "Finished threads will continue to consume resources. Eventually, if enough threads are created,  pthread_create  will fail.\nIn practice, this is only an issue for long-running processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits.",
            "title": "What happens if you don't call pthread_join?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#should-i-use-pthread_exit-or-pthread_join",
            "text": "Both  pthread_exit  and  pthread_join  will let the other threads finish on their own (even if called in the main thread). However, only  pthread_join  will return to you when the specified thread finishes.  pthread_exit  does not wait and will immediately end your thread and give you no chance to continue executing.",
            "title": "Should I use pthread_exit or pthread_join?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#can-you-pass-pointers-to-stack-variables-from-one-thread-to-another",
            "text": "Yes. However you need to be very careful about the lifetime of stack variables.  pthread_t start_threads() {\n  int start = 42;\n  pthread_t tid;\n  pthread_create(&tid, 0, myfunc, &start); // ERROR!\n  return tid;\n}  The above code is invalid because the function  start_threads  will likely return before  myfunc  even starts. The function passes the address-of  start , however by the time  myfunc  is executes,  start  is no longer in scope and its address will re-used for another variable.  The following code is valid because the lifetime of the stack variable is longer than the background thread.  void start_threads() {\n  int start = 42;\n  void *result;\n  pthread_t tid;\n  pthread_create(&tid, 0, myfunc, &start); // OK - start will be valid!\n  pthread_join(tid, &result);\n}",
            "title": "Can you pass pointers to stack variables from one thread to another?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#intro-to-race-conditions",
            "text": "",
            "title": "Intro to Race Conditions"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#how-can-i-create-ten-threads-with-different-starting-values",
            "text": "The following code is supposed to start ten threads with values 0,1,2,3,...9\nHowever, when run prints out  1 7 8 8 8 8 8 8 8 10 ! Can you see why?  #include <pthread.h>\nvoid* myfunc(void* ptr) {\n    int i = *((int *) ptr);\n    printf(\"%d \", i);\n    return NULL;\n}\n\nint main() {\n    // Each thread gets a different value of i to process\n    int i;\n    pthread_t tid;\n    for(i =0; i < 10; i++) {\n        pthread_create(&tid, NULL, myfunc, &i); // ERROR\n    }\n    pthread_exit(NULL);\n}  The above code suffers from a  race condition  - the value of i is changing. The new threads start later (in the example output the last thread starts after the loop has finished).  To overcome this race-condition, we will give each thread a pointer to it's own data area. For example, for each thread we may want to store the id, a starting value and an output value:  struct T {\n  pthread_t id;\n  int start;\n  char result[100];\n};  These can be stored in an array -   struct T *info = calloc(10 , sizeof(struct T)); // reserve enough bytes for ten T structures  And each array element passed to each thread -   pthread_create(&info[i].id, NULL, func, &info[i]);",
            "title": "How can I create ten threads with different starting values."
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#why-are-some-functions-eg-asctimegetenv-strtok-strerror-not-thread-safe",
            "text": "To answer this, let's look at a simple function that is also not 'thread-safe'  char *to_message(int num) {\n    char static result [256];\n    if (num < 10) sprintf(result, \"%d : blah blah\" , num);\n    else strcpy(result, \"Unknown\");\n    return result;\n}  In the above code the result buffer is stored in global memory. This is good - we wouldn't want to return a pointer to an invalid address on the stack, but there's only one result buffer in the entire memory. If two threads were to use it at the same time then one would corrupt the other:     Time  Thread 1  Thread 2  Comments      1  to_m(5 )      2   to_m(99)  Now both threads will see \"Unknown\" stored in the result buffer",
            "title": "Why are some functions e.g.  asctime,getenv, strtok, strerror  not thread-safe?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#what-are-condition-variables-semaphores-mutexes",
            "text": "These are synchronization locks that are used to prevent race conditions and ensure proper synchronization between threads running in the same program. In addition, these locks are conceptually identical to the primitives used inside the kernel.",
            "title": "What are condition variables, semaphores, mutexes?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#are-there-any-advantages-of-using-threads-over-forking-processes",
            "text": "Yes! Sharing information between threads is easy because threads (of the same process) live inside the same virtual memory space.\nAlso, creating a thread is significantly faster than creating(forking) a process.",
            "title": "Are there any advantages of using threads over forking processes?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#are-there-any-dis-advantages-of-using-threads-over-forking-processes",
            "text": "Yes! No- isolation! As threads live inside the same process, one thread has access to the same virtual memory as the other threads. A single thread can terminate the entire process (e.g. by trying to read address zero).",
            "title": "Are there any dis-advantages of using threads over forking processes?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#can-you-fork-a-process-with-multiple-threads",
            "text": "Yes! However the child process only has a single thread (which is a clone of the thread that called  fork . We can see this as a simple example, where the background threads never print out a second message in the child process.  #include <pthread.h>\n#include <stdio.h>\n#include <unistd.h>\n\nstatic pid_t child = -2;\n\nvoid *sleepnprint(void *arg) {\n  printf(\"%d:%s starting up...\\n\", getpid(), (char *) arg);\n\n  while (child == -2) {sleep(1);} /* Later we will use condition variables */\n\n  printf(\"%d:%s finishing...\\n\",getpid(), (char*)arg);\n\n  return NULL;  \n}\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(&tid1,NULL, sleepnprint, \"New Thread One\");\n  pthread_create(&tid2,NULL, sleepnprint, \"New Thread Two\");\n\n  child = fork();\n  printf(\"%d:%s\\n\",getpid(), \"fork()ing complete\");\n  sleep(3);\n\n  printf(\"%d:%s\\n\",getpid(), \"Main thread finished\");\n\n  pthread_exit(NULL);\n  return 0; /* Never executes */\n}  8970:New Thread One starting up...\n8970:fork()ing complete\n8973:fork()ing complete\n8970:New Thread Two starting up...\n8970:New Thread Two finishing...\n8970:New Thread One finishing...\n8970:Main thread finished\n8973:Main thread finished  In practice, creating threads before forking can lead to unexpected errors because (as demonstrated above) the other threads are immediately terminated when forking. Another thread might have just lock a mutex (e.g. by calling malloc) and never unlock it again. Advanced users may find  pthread_atfork  useful however we suggest you usually try to avoid creating threads before forking unless you fully understand the limitations and difficulties of this approach.",
            "title": "Can you fork a process with multiple threads?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#are-there-other-reasons-where-fork-might-be-preferable-to-creating-a-thread",
            "text": "Creating separate processes is useful   When more security is desired (for example, Chrome browser uses different processes for different tabs)  When running an existing and complete program then a new process is required (e.g. starting 'gcc')\n* When you are running into synchronization primitives and each process is operating on something in the system",
            "title": "Are there other reasons where fork might be preferable to creating a thread."
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-2:-Usage-in-Practice/#how-can-i-find-out-more",
            "text": "See the complete example in the  man page \nAnd the  pthread reference guide \nALSO:  Concise third party sample code explaining create, join and exit",
            "title": "How can I find out more?"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-3:-Parallel-Problems-(Bonus)/",
            "text": "Overview\n\n\nThe next section deals with what happens when pthreads collide, but what if we have each thread do something entirely different, no overlap?\n\n\nWe have found the maximum speedup parallel problems?\n\n\nEmbarrassingly Parallel Problems\n\n\nThe study of parallel algorithms has exploded over the past few years. An embarrassingly parallel problem is any problem that needs little effort to turn parallel. A lot of them have some synchronization concepts with them but not always. You already know a parallelizable algorithm, Merge Sort!\n\n\nvoid merge_sort(int *arr, size_t len){\n     if(len > 1){\n     //Mergesort the left half\n     //Mergesort the right half\n     //Merge the two halves\n     }\n\n\n\n\nWith your new understanding of threads, all you need to do is create a thread for the left half, and one for the right half. Given that your CPU has multiple real cores, you will see a speedup in accordance with \nAmdahl's Law\n. The time complexity analysis gets interesting here as well. The parallel algorithm runs in O(log^3(n)) running time (because we fancy analysis assuming that we have a lot of cores.\n\n\nIn practice though, we typically do two changes. One, once the array gets small enough, we ditch the parallel mergesort algorithm and do a quicksort or other algorithm that works fast on small arrays (something something cache coherency). The other thing that we know is that CPUs don't have infinite cores. To get around that, we typically keep a worker pool.\n\n\nWorker Pool\n\n\nWe know that CPUs have a finite amount of cores. A lot of times we start up a number of threads and give them tasks as they idle.\n\n\nAnother problem, Parallel Map\n\n\nSay we want to apply a function to an entire array, one element at a time.\n\n\n\nint *map(int (*func)(int), int *arr, size_t len){\n    int *ret = malloc(len*sizeof(*arr));\n    for(size_t i = 0; i < len; ++i) \n        ret[i] = func(arr[i]);\n    return ret;\n}\n\n\n\n\nSince none of the elements depend on any other element, how would you go about parallelizing this? What do you think would be the best way to split up the work between threads.\n\n\nScheduling\n\n\nThere are a few ways to split up the work.\n\n static scheduling: break up the problems into fixed size chunks (predetermined) and have each thread work on each of the chunks. This works well when each of the subproblems take roughly the same time because there is no additional overhead. All you need to do is write a loop and give the map function to each subarray.\n\n dynamic scheduling: as a new problem becomes available have a thread serve it. This is useful when you don't know how long the scheduling will take\n\n guided scheduling: This is a mix of the above with a mix of the benefits and the tradeoffs. You start with a static scheduling and move slowly to dynamic if needed\n\n runtime scheduling: You have absolutely no idea how long the problems are going to take. Instead of deciding it yourself, let the program decide what to do!\n\n\nsource\n, but no need to memorize.\n\n\nFew Drawbacks\n\n\nYou won't see the speedup right away because of things like cache coherency and scheduling extra threads.\n\n\nOther Problems\n\n\nFrom \nWikipedia\n\n\n Serving static files on a webserver to multiple users at once.\n\n The Mandelbrot set, Perlin noise and similar images, where each point is calculated independently.\n\n Rendering of computer graphics. In computer animation, each frame may be rendered independently (see parallel rendering).\n\n Brute-force searches in cryptography.[8] Notable real-world examples include distributed.net and proof-of-work systems used in cryptocurrency.\n\n BLAST searches in bioinformatics for multiple queries (but not for individual large queries) [9]\n\n Large scale facial recognition systems that compare thousands of arbitrary acquired faces (e.g., a security or surveillance video via closed-circuit television) with similarly large number of previously stored faces (e.g., a rogues gallery or similar watch list).[10]\n\n Computer simulations comparing many independent scenarios, such as climate models.\n\n Evolutionary computation metaheuristics such as genetic algorithms.\n\n Ensemble calculations of numerical weather prediction.\n\n Event simulation and reconstruction in particle physics.\n\n The marching squares algorithm\n\n Sieving step of the quadratic sieve and the number field sieve.\n\n Tree growth step of the random forest machine learning technique.\n\n Discrete Fourier Transform where each harmonic is independently calculated.",
            "title": "Pthreads, Part 3: Parallel Problems (Bonus)"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-3:-Parallel-Problems-(Bonus)/#overview",
            "text": "The next section deals with what happens when pthreads collide, but what if we have each thread do something entirely different, no overlap?  We have found the maximum speedup parallel problems?",
            "title": "Overview"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-3:-Parallel-Problems-(Bonus)/#embarrassingly-parallel-problems",
            "text": "The study of parallel algorithms has exploded over the past few years. An embarrassingly parallel problem is any problem that needs little effort to turn parallel. A lot of them have some synchronization concepts with them but not always. You already know a parallelizable algorithm, Merge Sort!  void merge_sort(int *arr, size_t len){\n     if(len > 1){\n     //Mergesort the left half\n     //Mergesort the right half\n     //Merge the two halves\n     }  With your new understanding of threads, all you need to do is create a thread for the left half, and one for the right half. Given that your CPU has multiple real cores, you will see a speedup in accordance with  Amdahl's Law . The time complexity analysis gets interesting here as well. The parallel algorithm runs in O(log^3(n)) running time (because we fancy analysis assuming that we have a lot of cores.  In practice though, we typically do two changes. One, once the array gets small enough, we ditch the parallel mergesort algorithm and do a quicksort or other algorithm that works fast on small arrays (something something cache coherency). The other thing that we know is that CPUs don't have infinite cores. To get around that, we typically keep a worker pool.",
            "title": "Embarrassingly Parallel Problems"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-3:-Parallel-Problems-(Bonus)/#worker-pool",
            "text": "We know that CPUs have a finite amount of cores. A lot of times we start up a number of threads and give them tasks as they idle.",
            "title": "Worker Pool"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-3:-Parallel-Problems-(Bonus)/#another-problem-parallel-map",
            "text": "Say we want to apply a function to an entire array, one element at a time.  \nint *map(int (*func)(int), int *arr, size_t len){\n    int *ret = malloc(len*sizeof(*arr));\n    for(size_t i = 0; i < len; ++i) \n        ret[i] = func(arr[i]);\n    return ret;\n}  Since none of the elements depend on any other element, how would you go about parallelizing this? What do you think would be the best way to split up the work between threads.",
            "title": "Another problem, Parallel Map"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-3:-Parallel-Problems-(Bonus)/#scheduling",
            "text": "There are a few ways to split up the work.  static scheduling: break up the problems into fixed size chunks (predetermined) and have each thread work on each of the chunks. This works well when each of the subproblems take roughly the same time because there is no additional overhead. All you need to do is write a loop and give the map function to each subarray.  dynamic scheduling: as a new problem becomes available have a thread serve it. This is useful when you don't know how long the scheduling will take  guided scheduling: This is a mix of the above with a mix of the benefits and the tradeoffs. You start with a static scheduling and move slowly to dynamic if needed  runtime scheduling: You have absolutely no idea how long the problems are going to take. Instead of deciding it yourself, let the program decide what to do!  source , but no need to memorize.",
            "title": "Scheduling"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-3:-Parallel-Problems-(Bonus)/#few-drawbacks",
            "text": "You won't see the speedup right away because of things like cache coherency and scheduling extra threads.",
            "title": "Few Drawbacks"
        },
        {
            "location": "/SystemProgramming/Pthreads,-Part-3:-Parallel-Problems-(Bonus)/#other-problems",
            "text": "From  Wikipedia   Serving static files on a webserver to multiple users at once.  The Mandelbrot set, Perlin noise and similar images, where each point is calculated independently.  Rendering of computer graphics. In computer animation, each frame may be rendered independently (see parallel rendering).  Brute-force searches in cryptography.[8] Notable real-world examples include distributed.net and proof-of-work systems used in cryptocurrency.  BLAST searches in bioinformatics for multiple queries (but not for individual large queries) [9]  Large scale facial recognition systems that compare thousands of arbitrary acquired faces (e.g., a security or surveillance video via closed-circuit television) with similarly large number of previously stored faces (e.g., a rogues gallery or similar watch list).[10]  Computer simulations comparing many independent scenarios, such as climate models.  Evolutionary computation metaheuristics such as genetic algorithms.  Ensemble calculations of numerical weather prediction.  Event simulation and reconstruction in particle physics.  The marching squares algorithm  Sieving step of the quadratic sieve and the number field sieve.  Tree growth step of the random forest machine learning technique.  Discrete Fourier Transform where each harmonic is independently calculated.",
            "title": "Other Problems"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/",
            "text": "What is RPC?\n\n\nRemote Procedure Call. RPC is the idea that we can execute a procedure (function) on a different machine. In practice the procedure may execute on the same machine, however it may be in a different context - for example under a different user with different permissions and different lifecycle.\n\n\nWhat is Privilege Separation?\n\n\nThe remote code will execute under a different user and with different privileges from the caller. In practice the remote call may execute with more or fewer privileges than the caller. This in principle can be used to improve the security of a system (by ensuring components operate with least privilege). Unfortunately, security concerns need to be carefully assessed to ensure that RPC mechanisms cannot be subverted to perform unwanted actions. For example, an RPC implementation may implicitly trust any connected client to perform any action, rather than a subset of actions on a subset of the data.\n\n\nWhat is stub code? What is marshalling?\n\n\nThe stub code is the necessary code to hide the complexity of performing a remote procedure call. One of the roles of the stub code is to \nmarshall\n the necessary data into a format that can be sent as a byte stream to a remote server.\n\n\n// On the outside 'getHiscore' looks like a normal function call\n// On the inside the stub code performs all of the work to send and receive the data to and from the remote machine.\n\nint getHiscore(char* game) {\n  // Marshall the request into a sequence of bytes:\n  char* buffer;\n  asprintf(&buffer,\"getHiscore(%s)!\", name);\n\n  // Send down the wire (we do not send the zero byte; the '!' signifies the end of the message)\n  write(fd, buffer, strlen(buffer) );\n\n  // Wait for the server to send a response\n  ssize_t bytesread = read(fd, buffer, sizeof(buffer));\n\n  // Example: unmarshal the bytes received back from text into an int\n  buffer[bytesread] = 0; // Turn the result into a C string\n\n  int score= atoi(buffer);\n  free(buffer);\n  return score;\n}\n\n\n\n\nWhat is server stub code? What is unmarshalling?\n\n\nThe server stub code will receive the request, unmarshall the request into a valid in-memory data call the underlying implementation and send the result back to the caller.\n\n\nHow do you send an int? float? a struct?  A linked list? A graph?\n\n\nTo implement RPC you need to decide (and document) which conventions you will use to serialize the data into a byte sequence. Even a simple integer has several common choices:\n\n Signed or unsigned?\n\n ASCII\n\n Fixed number of bytes or variable depending on magnitude\n\n Little or Big endian binary format?\n\n\nTo marshall a struct, decide which fields need to be serialized. It may not be necessary to send all data items (for example, some items may be irrelevant to the specific RPC or can be re-computed by the server from the other data items present).\n\n\nTo marshall a linked list it is unnecessary to send the link pointers- just stream the values. As part of unmarshalling the server can recreate a linked list structure from the byte sequence.\n\n\nBy starting at the head node/vertex, a simple tree can be recursively visited to create a serialized version of the data. A cyclic graph will usually require additional memory to ensure that each edge and vertex is processed exactly once.\n\n\nWhat is an IDL (Interface Design Language)?\n\n\nWriting stub code by hand is painful, tedious, error prone, difficult to maintain and difficult to reverse engineer the wire protocol from the implemented code. A better approach is specify the data objects, messages and services and automatically generate the client and server code.\n\n\nA modern example of an Interface Design Language is Google's Protocol Buffer .proto files.\n\n\nComplexity and challenges of RPC vs local calls?\n\n\nRemote Procedure Calls are significantly slower (10x to 100x) and more complex than local calls. An RPC must marshall data into a wire-compatible format. This may require multiple passes through the data structure, temporary memory allocation and transformation of the data representation.\n\n\nRobust RPC stub code must intelligently handle network failures and versioning. For example, a server may have to process requests from clients that are still running an early version of the stub code.\n\n\nA secure RPC will need to implement additional security checks (including authentication and authorization), validate data and encrypt communication between the client and host.\n\n\nTransferring large amounts of structured data\n\n\nLet's examine three methods of transferring data using 3 different formats - JSON, XML and Google Protocol Buffers. JSON and XML are text-based protocols. Examples of JSON and XML messages are below.\n\n\n<ticket><price currency='dollar'>10</price><vendor>travelocity</vendor></ticket>\n\n\n\n\n{ 'currency':'dollar' , 'vendor':'travelocity', 'price':'10' }\n\n\n\n\nGoogle Protocol Buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low CPU overhead and minimal memory copying. Implementations exist for multiple languages including Go, Python, C++ and C. This means client and server stub code in multiple languages can be generated from the .proto specification file to marshall data to and from a binary stream.\n\n\nGoogle Protocol Buffers reduces the versioning problem by ignoring unknown fields that are present in a message. See the introduction to Protocol Buffers for more information.\n\n\n[[https://developers.google.com/protocol-buffers/docs/overview]]",
            "title": "RPC, Part 1: Introduction to Remote Procedure Calls"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-rpc",
            "text": "Remote Procedure Call. RPC is the idea that we can execute a procedure (function) on a different machine. In practice the procedure may execute on the same machine, however it may be in a different context - for example under a different user with different permissions and different lifecycle.",
            "title": "What is RPC?"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-privilege-separation",
            "text": "The remote code will execute under a different user and with different privileges from the caller. In practice the remote call may execute with more or fewer privileges than the caller. This in principle can be used to improve the security of a system (by ensuring components operate with least privilege). Unfortunately, security concerns need to be carefully assessed to ensure that RPC mechanisms cannot be subverted to perform unwanted actions. For example, an RPC implementation may implicitly trust any connected client to perform any action, rather than a subset of actions on a subset of the data.",
            "title": "What is Privilege Separation?"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-stub-code-what-is-marshalling",
            "text": "The stub code is the necessary code to hide the complexity of performing a remote procedure call. One of the roles of the stub code is to  marshall  the necessary data into a format that can be sent as a byte stream to a remote server.  // On the outside 'getHiscore' looks like a normal function call\n// On the inside the stub code performs all of the work to send and receive the data to and from the remote machine.\n\nint getHiscore(char* game) {\n  // Marshall the request into a sequence of bytes:\n  char* buffer;\n  asprintf(&buffer,\"getHiscore(%s)!\", name);\n\n  // Send down the wire (we do not send the zero byte; the '!' signifies the end of the message)\n  write(fd, buffer, strlen(buffer) );\n\n  // Wait for the server to send a response\n  ssize_t bytesread = read(fd, buffer, sizeof(buffer));\n\n  // Example: unmarshal the bytes received back from text into an int\n  buffer[bytesread] = 0; // Turn the result into a C string\n\n  int score= atoi(buffer);\n  free(buffer);\n  return score;\n}",
            "title": "What is stub code? What is marshalling?"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-server-stub-code-what-is-unmarshalling",
            "text": "The server stub code will receive the request, unmarshall the request into a valid in-memory data call the underlying implementation and send the result back to the caller.",
            "title": "What is server stub code? What is unmarshalling?"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#how-do-you-send-an-int-float-a-struct-a-linked-list-a-graph",
            "text": "To implement RPC you need to decide (and document) which conventions you will use to serialize the data into a byte sequence. Even a simple integer has several common choices:  Signed or unsigned?  ASCII  Fixed number of bytes or variable depending on magnitude  Little or Big endian binary format?  To marshall a struct, decide which fields need to be serialized. It may not be necessary to send all data items (for example, some items may be irrelevant to the specific RPC or can be re-computed by the server from the other data items present).  To marshall a linked list it is unnecessary to send the link pointers- just stream the values. As part of unmarshalling the server can recreate a linked list structure from the byte sequence.  By starting at the head node/vertex, a simple tree can be recursively visited to create a serialized version of the data. A cyclic graph will usually require additional memory to ensure that each edge and vertex is processed exactly once.",
            "title": "How do you send an int? float? a struct?  A linked list? A graph?"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-an-idl-interface-design-language",
            "text": "Writing stub code by hand is painful, tedious, error prone, difficult to maintain and difficult to reverse engineer the wire protocol from the implemented code. A better approach is specify the data objects, messages and services and automatically generate the client and server code.  A modern example of an Interface Design Language is Google's Protocol Buffer .proto files.",
            "title": "What is an IDL (Interface Design Language)?"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#complexity-and-challenges-of-rpc-vs-local-calls",
            "text": "Remote Procedure Calls are significantly slower (10x to 100x) and more complex than local calls. An RPC must marshall data into a wire-compatible format. This may require multiple passes through the data structure, temporary memory allocation and transformation of the data representation.  Robust RPC stub code must intelligently handle network failures and versioning. For example, a server may have to process requests from clients that are still running an early version of the stub code.  A secure RPC will need to implement additional security checks (including authentication and authorization), validate data and encrypt communication between the client and host.",
            "title": "Complexity and challenges of RPC vs local calls?"
        },
        {
            "location": "/SystemProgramming/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#transferring-large-amounts-of-structured-data",
            "text": "Let's examine three methods of transferring data using 3 different formats - JSON, XML and Google Protocol Buffers. JSON and XML are text-based protocols. Examples of JSON and XML messages are below.  <ticket><price currency='dollar'>10</price><vendor>travelocity</vendor></ticket>  { 'currency':'dollar' , 'vendor':'travelocity', 'price':'10' }  Google Protocol Buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low CPU overhead and minimal memory copying. Implementations exist for multiple languages including Go, Python, C++ and C. This means client and server stub code in multiple languages can be generated from the .proto specification file to marshall data to and from a binary stream.  Google Protocol Buffers reduces the versioning problem by ignoring unknown fields that are present in a message. See the introduction to Protocol Buffers for more information.  [[https://developers.google.com/protocol-buffers/docs/overview]]",
            "title": "Transferring large amounts of structured data"
        },
        {
            "location": "/SystemProgramming/Sample-program-using-pthread-barriers/",
            "text": "#define _GNU_SOURCE\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <pthread.h>\n#include <time.h>\n\n#define THREAD_COUNT 4\n\npthread_barrier_t mybarrier;\n\nvoid* threadFn(void *id_ptr) {\n  int thread_id = *(int*)id_ptr;\n  int wait_sec = 1 + rand() % 5;\n  printf(\"thread %d: Wait for %d seconds.\\n\", thread_id, wait_sec);\n  sleep(wait_sec);\n  printf(\"thread %d: I'm ready...\\n\", thread_id);\n\n  pthread_barrier_wait(&mybarrier);\n\n  printf(\"thread %d: going!\\n\", thread_id);\n  return NULL;\n}\n\n\nint main() {\n  int i;\n  pthread_t ids[THREAD_COUNT];\n  int short_ids[THREAD_COUNT];\n\n  srand(time(NULL));\n  pthread_barrier_init(&mybarrier, NULL, THREAD_COUNT + 1);\n\n  for (i=0; i < THREAD_COUNT; i++) {\n    short_ids[i] = i;\n    pthread_create(&ids[i], NULL, threadFn, &short_ids[i]);\n  }\n\n  printf(\"main() is ready.\\n\");\n\n  pthread_barrier_wait(&mybarrier);\n\n  printf(\"main() is going!\\n\");\n\n  for (i=0; i < THREAD_COUNT; i++) {\n    pthread_join(ids[i], NULL);\n  }\n\n  pthread_barrier_destroy(&mybarrier);\n\n  return 0;\n}",
            "title": "Sample program using pthread barriers"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/",
            "text": "Thinking about scheduling.\n\n\nCPU Scheduling\n is the problem of efficiently selecting which process to run on a system's CPU cores. In a busy system there will be more ready-to-run processes than there are CPU cores, so the system kernel must evaluate which processes should be scheduled to run on the CPU and which processes should be placed in a ready queue to be executed later.\n\n\nThe additional complexity of multi-threaded and multiple CPU cores are considered a distraction to this initial exposition so are ignored here.\n\n\nAnother gotcha for non-native speakers is the dual meanings of \"Time\": The word \"Time\" can be used in both clock and elapsed duration context. For example \"The arrival time of the first process was 9:00am.\" and, \"The running time of the algorithm is 3 seconds\".\n\n\nHow is scheduling measured and which scheduler is best?\n\n\nScheduling effects the performance of the system, specifically the \nlatency\n and \nthroughput\n of the system. The throughput might be measured by a system value, for example the I/O throughput - the number of bytes written per second, or number of small processes that can complete per unit time, or using a higher level of abstraction for example number of customer records processed per minute. The latency might be measured by the response time (elapse time before a process can start to send a response) or wait time or turnaround time (the elapsed time to complete a task). Different schedulers offer different optimization trade-offs that may or may not be appropriate to desired use - there is no optimal scheduler for all possible environments and goals. For example 'shortest-job-first' will minimize total wait time across all jobs but in interactive (UI) environments it would be preferable to minimize response time (at the expense of some throughput), while FCFS seems intuitively fair and easy to implement but suffers from the Convoy Effect.\n\n\nWhat is arrival time?\n\n\nThe time at which a process first arrives at the ready queue, and is ready to start executing. If a CPU is idle, the arrival time would also be the starting time of execution.\n\n\nWhat is preemption?\n\n\nWithout preemption processes will run until they are unable to utilize the CPU any further. For example the following conditions would remove a process from the CPU and the CPU would be available to be scheduled for other processes: The process terminates due to a signal, is blocked waiting for concurrency primitive, or exits normally.\nThus once a process is scheduled it will continue even if another process with a high priority (e.g. shorter job) appears on the ready queue.\n\n\nWith preemption, the existing processes may be removed immediately if a more preferable process is added to the ready queue. For example, suppose at t=0 with a Shortest Job First scheduler there are two processes (P1 P2) with 10 and 20 ms execution times. P1 is scheduled. P1 immediately creates a new process P3, with execution time of 5 ms, which is added to the ready queue. Without preemption, P3 will run 10ms later (after P1 has completed). With preemption, P1 will be immediately evicted from the CPU and instead placed back in the ready queue, and P3 will be executed instead by the CPU.\n\n\nWhich schedulers suffer from starvation?\n\n\nAny scheduler that uses a form of prioritization can result in starvation because earlier processes may never be scheduled to run (assigned a CPU). For example with SJF, longer jobs may never be scheduled if the system continues to have many short jobs to schedule. It all depends on the \ntype of scheduler\n.\n\n\nWhy might a process (or thread) be placed on the ready queue?\n\n\nA process is placed on the ready queue when it is able to use a CPU. Some examples include:\n\n A process was blocked waiting for a \nread\n from storage or socket to complete and data is now available.\n\n A new process has been created and is ready to start.\n\n A process thread was blocked on a synchronization primitive (condition variable, semaphore, mutex lock) but is now able to continue.\n\n A process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.\n\n\nSimilar examples can be generated when considering threads.\n\n\nMeasures of Efficiency\n\n\nstart_time\n is the wall-clock start time of the process (CPU starts working on it)\n\nend_time\n is the end wall-clock of the process (CPU finishes the process)\n\nrun_time\n is the total amount of CPU time required\n\narrival_time\n is the time the process enters the scheduler (CPU may not start working on it)\n\n\nWhat is 'turnaround time'?\n\n\nThe total time from when you the process arrives to when it ends.\n\n\nturnaround_time = end_time - arrival_time\n\n\nWhat is 'response time'?\n\n\nThe total latency (time) that it takes from when the process arrives to when the CPU actually starts working on it.\n\n\nresponse_time = start_time - arrival_time\n\n\nWhat is 'wait time'?\n\n\nWait time is the \ntotal\n wait time i.e. the total time that a process is on the ready queue. A common mistake is to believe it is only the initial waiting time in the ready queue.\n\n\nIf a CPU intensive process with no I/O takes 7 minutes of CPU time to complete but required 9 minutes of wall-clock time to complete we can conclude that it was placed on the ready-queue for 2 minutes. For those 2 minutes the process was ready to run but had no CPU assigned. It does not matter when the job was waiting, the wait time is 2 minutes.\n\n\nwait_time  = (end_time - arrival_time) - run_time\n\n\nWhat is the Convoy Effect?\n\n\n\"The Convoy Effect is where I/O intensive processes are continually backed up, waiting for CPU-intensive processes that hog the CPU. This results in poor I/O performance, even for processes that have tiny CPU needs.\"\n\n\nSuppose the CPU is currently assigned to a CPU intensive task and there is a set of I/O intensive processes that are in the ready queue. These processes require just a tiny amount of CPU time but they are unable to proceed because they are waiting for the CPU-intensive task to be removed from the processor. These processes are starved until the the CPU bound process releases the CPU. But the CPU will rarely be released (for example in the case of a FCFS scheduler, we must wait until the processes is blocked due to an I/O request). The I/O intensive processes can now finally satisfy their CPU needs, which they can do quickly because their CPU needs are small and the CPU is assigned back to the CPU-intensive process again. Thus the I/O performance of the whole system suffers through an indirect effect of starvation of CPU needs of all processes.\n\n\nThis effect is usually discussed in the context of FCFS scheduler, however a round robin scheduler can also exhibit the Convoy effect for long time-quanta.\n\n\nLinux Scheduling\n\n\nAs of February 2016, Linux by default uses the \nCompletely Fair Scheduler\n for CPU scheduling and the Budget Fair Scheduling \"BFQ\" for I/O scheduling. Appropriate scheduling can have a significant impact on throughput and latency. Latency is particularly important for interactive and soft-real time applications such as audio and video streaming. See the discussion and comparative benchmarks here [https://lkml.org/lkml/2014/5/27/314] for more information.\n\n\nHere is how the CFS schedules\n\n\n\n\nThe CPU creates a Red-Black tree with the processes virtual runtime (runtime / nice_value) and sleeper fairness (if the process is waiting on something give it the CPU when it is done waiting).\n\n\n(Nice values are the kernel's way of giving priority to certain processes, the lower the higher priority)\n\n\nThe kernel chooses the lowest one based on this metric and schedules that process to run next, taking it off the queue. Since the red-black tree is self balancing this operation is guaranteed $O(log(n))$ (selecting the min process is the same runtime)\n\n\n\n\nAlthough it is called the Fair Scheduler there are a fair bit of problems.\n\n\n\n\nGroups of processes that are scheduled may have imbalanced loads so the scheduler roughly distributes the load. When another CPU gets free it can only look at the average load of a group schedule not the individual cores. So the free CPU may not take the work from a CPU that is burning so long as the average is fine.\n\n\nIf a group of processes is running, on non adjacent cores then there is a bug. If the two cores are more than a hop away, the load balancing algorithm won't even consider that core. Meaning if a CPU is free and a CPU that is doing more work is more than a hop away, it won't take the work (may have been patched).\n\n\nAfter a thread goes to sleep on a subset of cores, when it wakes up it can only be scheduled on the cores that it was sleeping on. If those cores are now bus",
            "title": "Scheduling, Part 1: Scheduling Processes"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#thinking-about-scheduling",
            "text": "CPU Scheduling  is the problem of efficiently selecting which process to run on a system's CPU cores. In a busy system there will be more ready-to-run processes than there are CPU cores, so the system kernel must evaluate which processes should be scheduled to run on the CPU and which processes should be placed in a ready queue to be executed later.  The additional complexity of multi-threaded and multiple CPU cores are considered a distraction to this initial exposition so are ignored here.  Another gotcha for non-native speakers is the dual meanings of \"Time\": The word \"Time\" can be used in both clock and elapsed duration context. For example \"The arrival time of the first process was 9:00am.\" and, \"The running time of the algorithm is 3 seconds\".",
            "title": "Thinking about scheduling."
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#how-is-scheduling-measured-and-which-scheduler-is-best",
            "text": "Scheduling effects the performance of the system, specifically the  latency  and  throughput  of the system. The throughput might be measured by a system value, for example the I/O throughput - the number of bytes written per second, or number of small processes that can complete per unit time, or using a higher level of abstraction for example number of customer records processed per minute. The latency might be measured by the response time (elapse time before a process can start to send a response) or wait time or turnaround time (the elapsed time to complete a task). Different schedulers offer different optimization trade-offs that may or may not be appropriate to desired use - there is no optimal scheduler for all possible environments and goals. For example 'shortest-job-first' will minimize total wait time across all jobs but in interactive (UI) environments it would be preferable to minimize response time (at the expense of some throughput), while FCFS seems intuitively fair and easy to implement but suffers from the Convoy Effect.",
            "title": "How is scheduling measured and which scheduler is best?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#what-is-arrival-time",
            "text": "The time at which a process first arrives at the ready queue, and is ready to start executing. If a CPU is idle, the arrival time would also be the starting time of execution.",
            "title": "What is arrival time?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#what-is-preemption",
            "text": "Without preemption processes will run until they are unable to utilize the CPU any further. For example the following conditions would remove a process from the CPU and the CPU would be available to be scheduled for other processes: The process terminates due to a signal, is blocked waiting for concurrency primitive, or exits normally.\nThus once a process is scheduled it will continue even if another process with a high priority (e.g. shorter job) appears on the ready queue.  With preemption, the existing processes may be removed immediately if a more preferable process is added to the ready queue. For example, suppose at t=0 with a Shortest Job First scheduler there are two processes (P1 P2) with 10 and 20 ms execution times. P1 is scheduled. P1 immediately creates a new process P3, with execution time of 5 ms, which is added to the ready queue. Without preemption, P3 will run 10ms later (after P1 has completed). With preemption, P1 will be immediately evicted from the CPU and instead placed back in the ready queue, and P3 will be executed instead by the CPU.",
            "title": "What is preemption?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#which-schedulers-suffer-from-starvation",
            "text": "Any scheduler that uses a form of prioritization can result in starvation because earlier processes may never be scheduled to run (assigned a CPU). For example with SJF, longer jobs may never be scheduled if the system continues to have many short jobs to schedule. It all depends on the  type of scheduler .",
            "title": "Which schedulers suffer from starvation?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#why-might-a-process-or-thread-be-placed-on-the-ready-queue",
            "text": "A process is placed on the ready queue when it is able to use a CPU. Some examples include:  A process was blocked waiting for a  read  from storage or socket to complete and data is now available.  A new process has been created and is ready to start.  A process thread was blocked on a synchronization primitive (condition variable, semaphore, mutex lock) but is now able to continue.  A process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.  Similar examples can be generated when considering threads.",
            "title": "Why might a process (or thread) be placed on the ready queue?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#measures-of-efficiency",
            "text": "start_time  is the wall-clock start time of the process (CPU starts working on it) end_time  is the end wall-clock of the process (CPU finishes the process) run_time  is the total amount of CPU time required arrival_time  is the time the process enters the scheduler (CPU may not start working on it)",
            "title": "Measures of Efficiency"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#what-is-turnaround-time",
            "text": "The total time from when you the process arrives to when it ends.  turnaround_time = end_time - arrival_time",
            "title": "What is 'turnaround time'?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#what-is-response-time",
            "text": "The total latency (time) that it takes from when the process arrives to when the CPU actually starts working on it.  response_time = start_time - arrival_time",
            "title": "What is 'response time'?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#what-is-wait-time",
            "text": "Wait time is the  total  wait time i.e. the total time that a process is on the ready queue. A common mistake is to believe it is only the initial waiting time in the ready queue.  If a CPU intensive process with no I/O takes 7 minutes of CPU time to complete but required 9 minutes of wall-clock time to complete we can conclude that it was placed on the ready-queue for 2 minutes. For those 2 minutes the process was ready to run but had no CPU assigned. It does not matter when the job was waiting, the wait time is 2 minutes.  wait_time  = (end_time - arrival_time) - run_time",
            "title": "What is 'wait time'?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#what-is-the-convoy-effect",
            "text": "\"The Convoy Effect is where I/O intensive processes are continually backed up, waiting for CPU-intensive processes that hog the CPU. This results in poor I/O performance, even for processes that have tiny CPU needs.\"  Suppose the CPU is currently assigned to a CPU intensive task and there is a set of I/O intensive processes that are in the ready queue. These processes require just a tiny amount of CPU time but they are unable to proceed because they are waiting for the CPU-intensive task to be removed from the processor. These processes are starved until the the CPU bound process releases the CPU. But the CPU will rarely be released (for example in the case of a FCFS scheduler, we must wait until the processes is blocked due to an I/O request). The I/O intensive processes can now finally satisfy their CPU needs, which they can do quickly because their CPU needs are small and the CPU is assigned back to the CPU-intensive process again. Thus the I/O performance of the whole system suffers through an indirect effect of starvation of CPU needs of all processes.  This effect is usually discussed in the context of FCFS scheduler, however a round robin scheduler can also exhibit the Convoy effect for long time-quanta.",
            "title": "What is the Convoy Effect?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-1:-Scheduling-Processes/#linux-scheduling",
            "text": "As of February 2016, Linux by default uses the  Completely Fair Scheduler  for CPU scheduling and the Budget Fair Scheduling \"BFQ\" for I/O scheduling. Appropriate scheduling can have a significant impact on throughput and latency. Latency is particularly important for interactive and soft-real time applications such as audio and video streaming. See the discussion and comparative benchmarks here [https://lkml.org/lkml/2014/5/27/314] for more information.  Here is how the CFS schedules   The CPU creates a Red-Black tree with the processes virtual runtime (runtime / nice_value) and sleeper fairness (if the process is waiting on something give it the CPU when it is done waiting).  (Nice values are the kernel's way of giving priority to certain processes, the lower the higher priority)  The kernel chooses the lowest one based on this metric and schedules that process to run next, taking it off the queue. Since the red-black tree is self balancing this operation is guaranteed $O(log(n))$ (selecting the min process is the same runtime)   Although it is called the Fair Scheduler there are a fair bit of problems.   Groups of processes that are scheduled may have imbalanced loads so the scheduler roughly distributes the load. When another CPU gets free it can only look at the average load of a group schedule not the individual cores. So the free CPU may not take the work from a CPU that is burning so long as the average is fine.  If a group of processes is running, on non adjacent cores then there is a bug. If the two cores are more than a hop away, the load balancing algorithm won't even consider that core. Meaning if a CPU is free and a CPU that is doing more work is more than a hop away, it won't take the work (may have been patched).  After a thread goes to sleep on a subset of cores, when it wakes up it can only be scheduled on the cores that it was sleeping on. If those cores are now bus",
            "title": "Linux Scheduling"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-2:-Scheduling-Processes:-Algorithms/",
            "text": "What are some well known scheduling algorithms?\n\n\nFor all the examples,\n\n\nProcess 1: Runtime 1000ms\n\n\nProcess 2: Runtime 2000ms\n\n\nProcess 3: Runtime 3000ms\n\n\nProcess 4: Runtime 4000ms\n\n\nProcess 5: Runtime 5000ms\n\n\nShortest Job First (SJF)\n\n\n\n\n\n\nP1 Arrival: 0ms\n\n\nP2 Arrival: 0ms\n\n\nP3 Arrival: 0ms\n\n\nP4 Arrival: 0ms\n\n\nP5 Arrival: 0ms\n\n\n\n\nThe processes all arrive at the start and the scheduler schedules the job with the shortest total CPU time. The glaring problem is that this scheduler needs to know how long this program will run over time before it ran the program.\n\n\nTechnical Note: A realistic SJF implementation would not use the total execution time of the process but the burst time (the total CPU time including future computational execution before the process will no longer be ready to run). The expected burst time can be estimated by using an exponentially decaying weighted rolling average based on the previous burst time but for this exposition we will simplify this discussion to use the total running time of the process as a proxy for the burst time.\n\n\nAdvantages\n\n* Shorter jobs tend to get run first\n\n\nDisadvantages\n\n* Needs algorithm to be omniscient\n\n\nPreemptive Shortest Job First (PSJF)\n\n\nPreemptive shortest job first is like shortest job first but if a new job comes in with a shorter runtime than the remaining runtime of the process, run that. (If it is equal like our example our algorithm can choose). The scheduler uses the total runtime of the process, if you want the shortest \nremaining\n time left, that is a variant of PSJF called Shortest Remaining Time First.\n\n\n\n\n\n\nP2 at 0ms\n\n\nP1 at 1000ms\n\n\nP5 at 3000ms\n\n\nP4 at 4000ms\n\n\nP3 at 5000ms\n\n\n\n\nHere's what our algorithm does. It runs P2 because it is the only thing to run. Then P1 comes in at 1000ms, P2 runs for 2000ms, so our scheduler preemptively stops P2, and let's P1 run all the way through (this is completely up to the algorithm because the times are equal). Then, P5 Comes in -- since there are no processes running, the scheduler will run process 5. P4 comes in, and since the runtimes are equal P5, the scheduler stops P5 and runs P4. Finally P3 comes in, preempts P4, and runs to completion. Then P4 runs, then P5 runs.\n\n\nAdvantages\n\n* Ensures shorter jobs get run first\n\n\nDisadvantages\n\n* Need to know the runtime again\n\n\nNote:\n This algorithm compares the total runtime \nnot\n the remaining runtime for historical reasons. If you want to take the remaining time into account you will use Preemptive Shortest Remaining Time First (PSRTF).\n\n\nFirst Come First Served (FCFS)\n\n\n\n\n\n\nP2 at 0ms\n\n\nP1 at 1000ms\n\n\nP5 at 3000ms\n\n\nP4 at 4000ms\n\n\nP3 at 5000ms\n\n\n\n\nProcesses are scheduled in the order of arrival. One advantage of FCFS is that scheduling algorithm is simple: the ready queue is a just a FIFO (first in first out) queue.\nFCFS suffers from the Convoy effect.\n\n\nHere P2 Arrives, then P1 arrives, then P5, then P4, then P3. You can see the convoy effect for P5.\n\n\nAdvantages\n\n* Simple implementation\n\n\nDisadvantages\n\n* Long running processes could block all other processes\n\n\nRound Robin (RR)\n\n\nProcesses are scheduled in order of their arrival in the ready queue. However after a small time step a running process will be forcibly removed from the running state and placed back on the ready queue. This ensures that a long-running process can not starve all other processes from running.\nThe maximum amount of time that a process can execute before being returned to the ready queue is called the time quanta. In the limit of large time quanta (where the time quanta is longer than the running time of all processes) round robin will be equivalent to FCFS.\n\n\n\n\n\n\nP1 Arrival: 0ms\n\n\nP2 Arrival: 0ms\n\n\nP3 Arrival: 0ms\n\n\nP4 Arrival: 0ms\n\n\nP5 Arrival: 0ms\n\n\n\n\nQuantum = 1000ms\n\n\nHere all processes arrive at the same time. P1 is run for 1 quantum and is finished. P2 for one quantum; then, it is stopped for P3. After all other processes run for a quantum we cycle back to P2 until all the processes are finished.\n\n\nAdvantages\n\n* Ensures some notion of fairness\n\n\nDisadvantages\n\n* Large number of processes = Lots of switching\n\n\nPriority\n\n\nProcesses are scheduled in the order of priority value. For example, a navigation process might be more important to execute than a logging process.",
            "title": "Scheduling, Part 2: Scheduling Processes: Algorithms"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-2:-Scheduling-Processes:-Algorithms/#what-are-some-well-known-scheduling-algorithms",
            "text": "For all the examples,  Process 1: Runtime 1000ms  Process 2: Runtime 2000ms  Process 3: Runtime 3000ms  Process 4: Runtime 4000ms  Process 5: Runtime 5000ms",
            "title": "What are some well known scheduling algorithms?"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-2:-Scheduling-Processes:-Algorithms/#shortest-job-first-sjf",
            "text": "P1 Arrival: 0ms  P2 Arrival: 0ms  P3 Arrival: 0ms  P4 Arrival: 0ms  P5 Arrival: 0ms   The processes all arrive at the start and the scheduler schedules the job with the shortest total CPU time. The glaring problem is that this scheduler needs to know how long this program will run over time before it ran the program.  Technical Note: A realistic SJF implementation would not use the total execution time of the process but the burst time (the total CPU time including future computational execution before the process will no longer be ready to run). The expected burst time can be estimated by using an exponentially decaying weighted rolling average based on the previous burst time but for this exposition we will simplify this discussion to use the total running time of the process as a proxy for the burst time.  Advantages \n* Shorter jobs tend to get run first  Disadvantages \n* Needs algorithm to be omniscient",
            "title": "Shortest Job First (SJF)"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-2:-Scheduling-Processes:-Algorithms/#preemptive-shortest-job-first-psjf",
            "text": "Preemptive shortest job first is like shortest job first but if a new job comes in with a shorter runtime than the remaining runtime of the process, run that. (If it is equal like our example our algorithm can choose). The scheduler uses the total runtime of the process, if you want the shortest  remaining  time left, that is a variant of PSJF called Shortest Remaining Time First.    P2 at 0ms  P1 at 1000ms  P5 at 3000ms  P4 at 4000ms  P3 at 5000ms   Here's what our algorithm does. It runs P2 because it is the only thing to run. Then P1 comes in at 1000ms, P2 runs for 2000ms, so our scheduler preemptively stops P2, and let's P1 run all the way through (this is completely up to the algorithm because the times are equal). Then, P5 Comes in -- since there are no processes running, the scheduler will run process 5. P4 comes in, and since the runtimes are equal P5, the scheduler stops P5 and runs P4. Finally P3 comes in, preempts P4, and runs to completion. Then P4 runs, then P5 runs.  Advantages \n* Ensures shorter jobs get run first  Disadvantages \n* Need to know the runtime again  Note:  This algorithm compares the total runtime  not  the remaining runtime for historical reasons. If you want to take the remaining time into account you will use Preemptive Shortest Remaining Time First (PSRTF).",
            "title": "Preemptive Shortest Job First (PSJF)"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-2:-Scheduling-Processes:-Algorithms/#first-come-first-served-fcfs",
            "text": "P2 at 0ms  P1 at 1000ms  P5 at 3000ms  P4 at 4000ms  P3 at 5000ms   Processes are scheduled in the order of arrival. One advantage of FCFS is that scheduling algorithm is simple: the ready queue is a just a FIFO (first in first out) queue.\nFCFS suffers from the Convoy effect.  Here P2 Arrives, then P1 arrives, then P5, then P4, then P3. You can see the convoy effect for P5.  Advantages \n* Simple implementation  Disadvantages \n* Long running processes could block all other processes",
            "title": "First Come First Served (FCFS)"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-2:-Scheduling-Processes:-Algorithms/#round-robin-rr",
            "text": "Processes are scheduled in order of their arrival in the ready queue. However after a small time step a running process will be forcibly removed from the running state and placed back on the ready queue. This ensures that a long-running process can not starve all other processes from running.\nThe maximum amount of time that a process can execute before being returned to the ready queue is called the time quanta. In the limit of large time quanta (where the time quanta is longer than the running time of all processes) round robin will be equivalent to FCFS.    P1 Arrival: 0ms  P2 Arrival: 0ms  P3 Arrival: 0ms  P4 Arrival: 0ms  P5 Arrival: 0ms   Quantum = 1000ms  Here all processes arrive at the same time. P1 is run for 1 quantum and is finished. P2 for one quantum; then, it is stopped for P3. After all other processes run for a quantum we cycle back to P2 until all the processes are finished.  Advantages \n* Ensures some notion of fairness  Disadvantages \n* Large number of processes = Lots of switching",
            "title": "Round Robin (RR)"
        },
        {
            "location": "/SystemProgramming/Scheduling,-Part-2:-Scheduling-Processes:-Algorithms/#priority",
            "text": "Processes are scheduled in the order of priority value. For example, a navigation process might be more important to execute than a logging process.",
            "title": "Priority"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/",
            "text": "Signals In Depth\n\n\nHow can I learn more about signals?\n\n\nThe linux man pages discusses signal system calls in section 2. There is also a longer article in section 7 (though not in OSX/BSD):\n\n\nman -s7 signal\n\n\n\n\nSignal Terminology\n\n\n\n\nGenerated - The signal is being created in the kernel by the kill system call.\n\n\nPending - Not delivered yet but soon to be delivered\n\n\nBlocked - Not delivered because no signal disposition lets the signal be delivered\n\n\nDelivered - Delivered to the process, the action described is being taken\n\n\nCaught - When the process stops a signal from destroying it and does something else with it instead\n\n\n\n\nWhat is a process's signal disposition?\n\n\nFor each process, each signal has a disposition which means what action will occur when a signal is delivered to the process. For example, the default disposition SIGINT is to terminate it. The signal disposition can be changed by calling signal() (which is simple but not portable as there are subtle variations in its implementation on different POSIX architectures and also not recommended for multi-threaded programs) or \nsigaction\n (discussed later). You can imagine the processes' disposition to all possible signals as a table of function pointers entries (one for each possible signal).\n\n\nThe default disposition for signals can be to ignore the signal, stop the process, continue a stopped process, terminate the process, or terminate the process and also dump a 'core' file. Note a core file is a representation of the processes' memory state that can be inspected using a debugger.\n\n\nCan multiple signals be queued?\n\n\nNo - however it is possible to have signals that are in a pending state. If a signal is pending, it means it has not yet been delivered to the process. The most common reason for a signal to be pending is that the process (or thread) has currently blocked that particular signal.\n\n\nIf a particular signal, e.g. SIGINT, is pending then it is not possible to queue up the same signal again.\n\n\nIt \nis\n possible to have more than one signal of a different type in a pending state. For example SIGINT and SIGTERM signals may be pending (i.e. not yet delivered to the target process)\n\n\nHow do I block signals?\n\n\nSignals can be blocked (meaning they will stay in the pending state) by setting the process signal mask or, when you are writing a multi-threaded program, the thread signal mask.\n\n\nDisposition in Threads/Children\n\n\nWhat happens when creating a new thread?\n\n\nThe new thread inherits a copy of the calling thread's mask\n\n\npthread_sigmask( ... ); // set my mask to block delivery of some signals\npthread_create( ... ); // new thread will start with a copy of the same mask\n\n\n\n\nWhat happens when forking?\n\n\nThe child process inherits a copy of the parent's signal dispositions. In other words, if you have installed a SIGINT handler before forking, then the child process will also call the handler if a SIGINT is delivered to the child.\n\n\nNote pending signals for the child are \nnot\n inherited during forking.\n\n\nWhat happens during exec ?\n\n\nRemember that \nexec\n replaces the current image with a new program image. In addition the signal disposition is reset. Pending signals are preserved.\n\n\nWhat happens during fork ?\n\n\nThe child process inherits a copy of the parent process's signal disposition and a copy of the parent's signal mask.\n\n\nFor example if \nSIGINT\n is blocked in the parent it will be blocked in the child too.\nFor example if the parent installed a handler (call-back function) for SIG-INT then the child will also perform the same behavior.\n\n\nPending signals however are not inherited by the child.\n\n\nHow do I block signals in a single-threaded program?\n\n\nUse \nsigprocmask\n! With sigprocmask you can set the new mask, add new signals to be blocked to the process mask, and unblock currently blocked signals. You can also determine the existing mask (and use it for later) by passing in a non-null value for oldset.\n\n\nint sigprocmask(int how, const sigset_t *set, sigset_t *oldset);`\n\n\n\n\nFrom the Linux man page of sigprocmask,\n\n\nSIG_BLOCK: The set of blocked signals is the union of the current set and the set argument.\nSIG_UNBLOCK: The signals in set are removed from the current set of blocked signals. It is permissible to attempt to unblock a signal which is not blocked.\nSIG_SETMASK: The set of blocked signals is set to the argument set.\n\n\n\n\n\nThe sigset type behaves as a bitmap, except functions are used rather than explicitly setting and unsetting bits using & and |. \n\n\nIt is a common error to forget to initialize the signal set before modifying one bit. For example,\n\n\nsigset_t set, oldset;\nsigaddset(&set, SIGINT); // Ooops!\nsigprocmask(SIG_SETMASK, &set, &oldset)\n\n\n\n\nCorrect code initializes the set to be all on or all off. For example,\n\n\nsigfillset(&set); // all signals\nsigprocmask(SIG_SETMASK, &set, NULL); // Block all the signals!\n// (Actually SIGKILL or SIGSTOP cannot be blocked...)\n\nsigemptyset(&set); // no signals \nsigprocmask(SIG_SETMASK, &set, NULL); // set the mask to be empty again\n\n\n\n\nHow do I block signals in a multi-threaded program?\n\n\nBlocking signals is similar in multi-threaded programs to single-threaded programs:\n\n Use pthread_sigmask instead of sigprocmask\n\n Block a signal in all threads to prevent its asynchronous delivery\n\n\nThe easiest method to ensure a signal is blocked in all threads is to set the signal mask in the main thread before new threads are created\n\n\nsigemptyset(&set);\nsigaddset(&set, SIGQUIT);\nsigaddset(&set, SIGINT);\npthread_sigmask(SIG_BLOCK, &set, NULL);\n\n// this thread and the new thread will block SIGQUIT and SIGINT\npthread_create(&thread_id, NULL, myfunc, funcparam);\n\n\n\n\nJust as we saw with sigprocmask, pthread_sigmask includes a 'how' parameter that defines how the signal set is to be used:\n\n\npthread_sigmask(SIG_SETMASK, &set, NULL) - replace the thread's mask with given signal set\npthread_sigmask(SIG_BLOCK, &set, NULL) - add the signal set to the thread's mask\npthread_sigmask(SIG_UNBLOCK, &set, NULL) - remove the signal set from the thread's mask\n\n\n\n\nHow are pending signals delivered in a multi-threaded program?\n\n\nA signal is delivered to any signal thread that is not blocking that signal.\n\n\nIf the two or more threads can receive the signal then which thread will be interrupted is arbitrary!",
            "title": "Signals, Part 2: Pending Signals and Signal Masks"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#signals-in-depth",
            "text": "",
            "title": "Signals In Depth"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-can-i-learn-more-about-signals",
            "text": "The linux man pages discusses signal system calls in section 2. There is also a longer article in section 7 (though not in OSX/BSD):  man -s7 signal",
            "title": "How can I learn more about signals?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#signal-terminology",
            "text": "Generated - The signal is being created in the kernel by the kill system call.  Pending - Not delivered yet but soon to be delivered  Blocked - Not delivered because no signal disposition lets the signal be delivered  Delivered - Delivered to the process, the action described is being taken  Caught - When the process stops a signal from destroying it and does something else with it instead",
            "title": "Signal Terminology"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-is-a-processs-signal-disposition",
            "text": "For each process, each signal has a disposition which means what action will occur when a signal is delivered to the process. For example, the default disposition SIGINT is to terminate it. The signal disposition can be changed by calling signal() (which is simple but not portable as there are subtle variations in its implementation on different POSIX architectures and also not recommended for multi-threaded programs) or  sigaction  (discussed later). You can imagine the processes' disposition to all possible signals as a table of function pointers entries (one for each possible signal).  The default disposition for signals can be to ignore the signal, stop the process, continue a stopped process, terminate the process, or terminate the process and also dump a 'core' file. Note a core file is a representation of the processes' memory state that can be inspected using a debugger.",
            "title": "What is a process's signal disposition?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#can-multiple-signals-be-queued",
            "text": "No - however it is possible to have signals that are in a pending state. If a signal is pending, it means it has not yet been delivered to the process. The most common reason for a signal to be pending is that the process (or thread) has currently blocked that particular signal.  If a particular signal, e.g. SIGINT, is pending then it is not possible to queue up the same signal again.  It  is  possible to have more than one signal of a different type in a pending state. For example SIGINT and SIGTERM signals may be pending (i.e. not yet delivered to the target process)",
            "title": "Can multiple signals be queued?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-do-i-block-signals",
            "text": "Signals can be blocked (meaning they will stay in the pending state) by setting the process signal mask or, when you are writing a multi-threaded program, the thread signal mask.",
            "title": "How do I block signals?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#disposition-in-threadschildren",
            "text": "",
            "title": "Disposition in Threads/Children"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-happens-when-creating-a-new-thread",
            "text": "The new thread inherits a copy of the calling thread's mask  pthread_sigmask( ... ); // set my mask to block delivery of some signals\npthread_create( ... ); // new thread will start with a copy of the same mask",
            "title": "What happens when creating a new thread?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-happens-when-forking",
            "text": "The child process inherits a copy of the parent's signal dispositions. In other words, if you have installed a SIGINT handler before forking, then the child process will also call the handler if a SIGINT is delivered to the child.  Note pending signals for the child are  not  inherited during forking.",
            "title": "What happens when forking?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-happens-during-exec",
            "text": "Remember that  exec  replaces the current image with a new program image. In addition the signal disposition is reset. Pending signals are preserved.",
            "title": "What happens during exec ?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-happens-during-fork",
            "text": "The child process inherits a copy of the parent process's signal disposition and a copy of the parent's signal mask.  For example if  SIGINT  is blocked in the parent it will be blocked in the child too.\nFor example if the parent installed a handler (call-back function) for SIG-INT then the child will also perform the same behavior.  Pending signals however are not inherited by the child.",
            "title": "What happens during fork ?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-do-i-block-signals-in-a-single-threaded-program",
            "text": "Use  sigprocmask ! With sigprocmask you can set the new mask, add new signals to be blocked to the process mask, and unblock currently blocked signals. You can also determine the existing mask (and use it for later) by passing in a non-null value for oldset.  int sigprocmask(int how, const sigset_t *set, sigset_t *oldset);`  From the Linux man page of sigprocmask,  SIG_BLOCK: The set of blocked signals is the union of the current set and the set argument.\nSIG_UNBLOCK: The signals in set are removed from the current set of blocked signals. It is permissible to attempt to unblock a signal which is not blocked.\nSIG_SETMASK: The set of blocked signals is set to the argument set.  The sigset type behaves as a bitmap, except functions are used rather than explicitly setting and unsetting bits using & and |.   It is a common error to forget to initialize the signal set before modifying one bit. For example,  sigset_t set, oldset;\nsigaddset(&set, SIGINT); // Ooops!\nsigprocmask(SIG_SETMASK, &set, &oldset)  Correct code initializes the set to be all on or all off. For example,  sigfillset(&set); // all signals\nsigprocmask(SIG_SETMASK, &set, NULL); // Block all the signals!\n// (Actually SIGKILL or SIGSTOP cannot be blocked...)\n\nsigemptyset(&set); // no signals \nsigprocmask(SIG_SETMASK, &set, NULL); // set the mask to be empty again",
            "title": "How do I block signals in a single-threaded program?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-do-i-block-signals-in-a-multi-threaded-program",
            "text": "Blocking signals is similar in multi-threaded programs to single-threaded programs:  Use pthread_sigmask instead of sigprocmask  Block a signal in all threads to prevent its asynchronous delivery  The easiest method to ensure a signal is blocked in all threads is to set the signal mask in the main thread before new threads are created  sigemptyset(&set);\nsigaddset(&set, SIGQUIT);\nsigaddset(&set, SIGINT);\npthread_sigmask(SIG_BLOCK, &set, NULL);\n\n// this thread and the new thread will block SIGQUIT and SIGINT\npthread_create(&thread_id, NULL, myfunc, funcparam);  Just as we saw with sigprocmask, pthread_sigmask includes a 'how' parameter that defines how the signal set is to be used:  pthread_sigmask(SIG_SETMASK, &set, NULL) - replace the thread's mask with given signal set\npthread_sigmask(SIG_BLOCK, &set, NULL) - add the signal set to the thread's mask\npthread_sigmask(SIG_UNBLOCK, &set, NULL) - remove the signal set from the thread's mask",
            "title": "How do I block signals in a multi-threaded program?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-are-pending-signals-delivered-in-a-multi-threaded-program",
            "text": "A signal is delivered to any signal thread that is not blocking that signal.  If the two or more threads can receive the signal then which thread will be interrupted is arbitrary!",
            "title": "How are pending signals delivered in a multi-threaded program?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-3:-Raising-signals/",
            "text": "How do I send a signal to a process from the shell?\n\n\nYou already know one way to send a \nSIG_INT\n just type \nCTRL-C\n \nFrom the shell you can use \nkill\n (if you know the process id) and \nkillall\n (if you know the process name)\n\n\n# First let's use ps and grep to find the process we want to send a signal to\n$ ps au | grep myprogram\nangrave  4409   0.0  0.0  2434892    512 s004  R+    2:42PM   0:00.00 myprogram 1 2 3\n\n#Send SIGINT signal to process 4409 (equivalent of `CTRL-C`)\n$ kill -SIGINT 4409\n\n#Send SIGKILL (terminate the process)\n$ kill -SIGKILL 4409\n$ kill -9 4409\n\n\n\n\nkillall\n is similar except that it matches by program name. The next two example, sends a \nSIGINT\n and then \nSIGKILL\n to terminate the processes that are running \nmyprogram\n\n\n# Send SIGINT (SIGINT can be ignored)\n$ killall -SIGINT myprogram\n\n# SIGKILL (-9) cannot be ignored! \n$ killall -9 myprogram\n\n\n\n\nHow do I send a signal to a process from the running C program?\n\n\nUse \nraise\n or \nkill\n\n\nint raise(int sig); // Send a signal to myself!\nint kill(pid_t pid, int sig); // Send a signal to another process\n\n\n\n\nFor non-root processes, signals can only be sent to processes of the same user i.e. you cant just SIGKILL my processes! See kill(2) i.e. man -s2 for more details.\n\n\nHow do I send a signal to a specific thread?\n\n\nUse \npthread_kill\n\n\nint pthread_kill(pthread_t thread, int sig)\n\n\n\n\nIn the example below, the newly created thread executing \nfunc\n will be interrupted by \nSIGINT\n\n\npthread_create(&tid, NULL, func, args);\npthread_kill(tid, SIGINT);\npthread_kill(pthread_self(), SIGKILL); // send SIGKILL to myself\n\n\n\n\nWill \npthread_kill( threadid, SIGKILL)\n kill the process or thread?\n\n\nIt will kill the entire process. Though individual threads can set a signal mask, the signal disposition (the table of handlers/action performed for each signal) is \nper-proces\ns not \nper-thread\n. This means \n\nsigaction\n can be called from any thread because you will be setting a signal handler for all threads in the process.\n\n\nHow do I catch (handle) a signal ?\n\n\nYou can choose a handle pending signals asynchronously or synchronously.\n\n\nInstall a signal handler to asynchronously handle signals use \nsigaction\n (or, for simple examples, \nsignal\n ).\n\n\nTo synchronously catch a pending signal use \nsigwait\n (which blocks until a signal is delivered) or \nsignalfd\n (which also blocks and provides a file descriptor that can be \nread()\n to retrieve pending signals).\n\n\nSee \nSignals, Part 4\n for an example of using \nsigwait",
            "title": "Signals, Part 3: Raising signals"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-3:-Raising-signals/#how-do-i-send-a-signal-to-a-process-from-the-shell",
            "text": "You already know one way to send a  SIG_INT  just type  CTRL-C  \nFrom the shell you can use  kill  (if you know the process id) and  killall  (if you know the process name)  # First let's use ps and grep to find the process we want to send a signal to\n$ ps au | grep myprogram\nangrave  4409   0.0  0.0  2434892    512 s004  R+    2:42PM   0:00.00 myprogram 1 2 3\n\n#Send SIGINT signal to process 4409 (equivalent of `CTRL-C`)\n$ kill -SIGINT 4409\n\n#Send SIGKILL (terminate the process)\n$ kill -SIGKILL 4409\n$ kill -9 4409  killall  is similar except that it matches by program name. The next two example, sends a  SIGINT  and then  SIGKILL  to terminate the processes that are running  myprogram  # Send SIGINT (SIGINT can be ignored)\n$ killall -SIGINT myprogram\n\n# SIGKILL (-9) cannot be ignored! \n$ killall -9 myprogram",
            "title": "How do I send a signal to a process from the shell?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-3:-Raising-signals/#how-do-i-send-a-signal-to-a-process-from-the-running-c-program",
            "text": "Use  raise  or  kill  int raise(int sig); // Send a signal to myself!\nint kill(pid_t pid, int sig); // Send a signal to another process  For non-root processes, signals can only be sent to processes of the same user i.e. you cant just SIGKILL my processes! See kill(2) i.e. man -s2 for more details.",
            "title": "How do I send a signal to a process from the running C program?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-3:-Raising-signals/#how-do-i-send-a-signal-to-a-specific-thread",
            "text": "Use  pthread_kill  int pthread_kill(pthread_t thread, int sig)  In the example below, the newly created thread executing  func  will be interrupted by  SIGINT  pthread_create(&tid, NULL, func, args);\npthread_kill(tid, SIGINT);\npthread_kill(pthread_self(), SIGKILL); // send SIGKILL to myself",
            "title": "How do I send a signal to a specific thread?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-3:-Raising-signals/#will-pthread_kill-threadid-sigkill-kill-the-process-or-thread",
            "text": "It will kill the entire process. Though individual threads can set a signal mask, the signal disposition (the table of handlers/action performed for each signal) is  per-proces s not  per-thread . This means  sigaction  can be called from any thread because you will be setting a signal handler for all threads in the process.",
            "title": "Will pthread_kill( threadid, SIGKILL) kill the process or thread?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-3:-Raising-signals/#how-do-i-catch-handle-a-signal",
            "text": "You can choose a handle pending signals asynchronously or synchronously.  Install a signal handler to asynchronously handle signals use  sigaction  (or, for simple examples,  signal  ).  To synchronously catch a pending signal use  sigwait  (which blocks until a signal is delivered) or  signalfd  (which also blocks and provides a file descriptor that can be  read()  to retrieve pending signals).  See  Signals, Part 4  for an example of using  sigwait",
            "title": "How do I catch (handle) a signal ?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-4:-Sigaction/",
            "text": "How and why do I use \nsigaction\n ?\n\n\nYou should use \nsigaction\n instead of \nsignal\n because it has better defined semantics. \nsignal\n on different operating system does different things which is \nbad\n \nsigaction\n is more portable and is better defined for threads if need be.\n\n\nTo change the \"signal disposition\" of a process - i.e. what happens when a signal is delivered to your process - use \nsigaction\n\n\nYou can use system call \nsigaction\n to set the current handler for a signal or read the current signal handler for a particular signal.\n\n\nint sigaction(int signum, const struct sigaction *act, struct sigaction *oldact);\n\n\n\n\nThe sigaction struct includes two callback functions (we will only look at the 'handler' version), a signal mask and a flags field -\n\n\nstruct sigaction {\n               void     (*sa_handler)(int);\n               void     (*sa_sigaction)(int, siginfo_t *, void *);\n               sigset_t   sa_mask;\n               int        sa_flags;\n}; \n\n\n\n\nHow do I convert a \nsignal\n call into the equivalent \nsigaction\n call?\n\n\nSuppose you installed a signal handler for the alarm signal,\n\n\nsignal(SIGALRM, myhandler);\n\n\n\n\nThe equivalent \nsigaction\n code is:\n\n\nstruct sigaction sa; \nsa.sa_handler = myhandler;\nsigemptyset(&sa.sa_mask);\nsa.sa_flags = 0; \nsigaction(SIGALRM, &sa, NULL)\n\n\n\n\nHowever, we typically may also set the mask and the flags field. The mask is a temporary signal mask used during the signal handler execution. The SA_RESTART flag will automatically restart some (but not all) system calls that otherwise would have returned early (with EINTR error). The latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.\n\n\nsigfillset(&sa.sa_mask);\nsa.sa_flags = SA_RESTART; /* Restart functions if  interrupted by handler */     \n\n\n\n\nHow do I use sigwait?\n\n\nSigwait can be used to read one pending signal at a time. \nsigwait\n is used to synchronously wait for signals, rather than handle them in a callback. A typical use of sigwait in a multi-threaded program is shown below. Notice that the thread signal mask is set first (and will be inherited by new threads). This prevents signals from being \ndelivered\n so they will remain in a pending state until sigwait is called. Also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is being used as the set of signals that sigwait can catch and return.\n\n\nOne advantage of writing a custom signal handling thread (such as the example below) rather than a callback function is that you can now use many more C library and system functions that otherwise could not be safely used in a signal handler because they are not async signal-safe.\n\n\nBased on \nhttp://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_sigmask.html\n\n\nstatic sigset_t   signal_mask;  /* signals to block         */\n\nint main (int argc, char *argv[])\n{\n    pthread_t sig_thr_id;      /* signal handler thread ID */\n    sigemptyset (&signal_mask);\n    sigaddset (&signal_mask, SIGINT);\n    sigaddset (&signal_mask, SIGTERM);\n    pthread_sigmask (SIG_BLOCK, &signal_mask, NULL);\n\n    /* New threads will inherit this thread's mask */\n    pthread_create (&sig_thr_id, NULL, signal_thread, NULL);\n\n    /* APPLICATION CODE */\n    ...\n}\n\nvoid *signal_thread (void *arg)\n{\n    int       sig_caught;    /* signal caught       */\n\n    /* Use same mask as the set of signals that we'd like to know about! */\n    sigwait(&signal_mask, &sig_caught);\n    switch (sig_caught)\n    {\n    case SIGINT:     /* process SIGINT  */\n        ...\n        break;\n    case SIGTERM:    /* process SIGTERM */\n        ...\n        break;\n    default:         /* should normally not happen */\n        fprintf (stderr, \"\\nUnexpected signal %d\\n\", sig_caught);\n        break;\n    }\n}",
            "title": "Signals, Part 4: Sigaction"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-4:-Sigaction/#how-and-why-do-i-use-sigaction",
            "text": "You should use  sigaction  instead of  signal  because it has better defined semantics.  signal  on different operating system does different things which is  bad   sigaction  is more portable and is better defined for threads if need be.  To change the \"signal disposition\" of a process - i.e. what happens when a signal is delivered to your process - use  sigaction  You can use system call  sigaction  to set the current handler for a signal or read the current signal handler for a particular signal.  int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact);  The sigaction struct includes two callback functions (we will only look at the 'handler' version), a signal mask and a flags field -  struct sigaction {\n               void     (*sa_handler)(int);\n               void     (*sa_sigaction)(int, siginfo_t *, void *);\n               sigset_t   sa_mask;\n               int        sa_flags;\n};",
            "title": "How and why do I use sigaction ?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-4:-Sigaction/#how-do-i-convert-a-signal-call-into-the-equivalent-sigaction-call",
            "text": "Suppose you installed a signal handler for the alarm signal,  signal(SIGALRM, myhandler);  The equivalent  sigaction  code is:  struct sigaction sa; \nsa.sa_handler = myhandler;\nsigemptyset(&sa.sa_mask);\nsa.sa_flags = 0; \nsigaction(SIGALRM, &sa, NULL)  However, we typically may also set the mask and the flags field. The mask is a temporary signal mask used during the signal handler execution. The SA_RESTART flag will automatically restart some (but not all) system calls that otherwise would have returned early (with EINTR error). The latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.  sigfillset(&sa.sa_mask);\nsa.sa_flags = SA_RESTART; /* Restart functions if  interrupted by handler */",
            "title": "How do I convert a signal call into the equivalent sigaction call?"
        },
        {
            "location": "/SystemProgramming/Signals,-Part-4:-Sigaction/#how-do-i-use-sigwait",
            "text": "Sigwait can be used to read one pending signal at a time.  sigwait  is used to synchronously wait for signals, rather than handle them in a callback. A typical use of sigwait in a multi-threaded program is shown below. Notice that the thread signal mask is set first (and will be inherited by new threads). This prevents signals from being  delivered  so they will remain in a pending state until sigwait is called. Also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is being used as the set of signals that sigwait can catch and return.  One advantage of writing a custom signal handling thread (such as the example below) rather than a callback function is that you can now use many more C library and system functions that otherwise could not be safely used in a signal handler because they are not async signal-safe.  Based on  http://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_sigmask.html  static sigset_t   signal_mask;  /* signals to block         */\n\nint main (int argc, char *argv[])\n{\n    pthread_t sig_thr_id;      /* signal handler thread ID */\n    sigemptyset (&signal_mask);\n    sigaddset (&signal_mask, SIGINT);\n    sigaddset (&signal_mask, SIGTERM);\n    pthread_sigmask (SIG_BLOCK, &signal_mask, NULL);\n\n    /* New threads will inherit this thread's mask */\n    pthread_create (&sig_thr_id, NULL, signal_thread, NULL);\n\n    /* APPLICATION CODE */\n    ...\n}\n\nvoid *signal_thread (void *arg)\n{\n    int       sig_caught;    /* signal caught       */\n\n    /* Use same mask as the set of signals that we'd like to know about! */\n    sigwait(&signal_mask, &sig_caught);\n    switch (sig_caught)\n    {\n    case SIGINT:     /* process SIGINT  */\n        ...\n        break;\n    case SIGTERM:    /* process SIGTERM */\n        ...\n        break;\n    default:         /* should normally not happen */\n        fprintf (stderr, \"\\nUnexpected signal %d\\n\", sig_caught);\n        break;\n    }\n}",
            "title": "How do I use sigwait?"
        },
        {
            "location": "/SystemProgramming/Signals-Review-Questions/",
            "text": "Topics\n\n\n\n\nSignals\n\n\nSignal Handler Safe\n\n\nSignal Disposition\n\n\nSignal States\n\n\nPending Signals when Forking/Exec\n\n\nSignal Disposition when Forking/Exec\n\n\nRaising Signals in C\n\n\nRaising Signals in a multithreaded program\n\n\n\n\nQuestions\n\n\n\n\nWhat is a signal?\n\n\nHow are signals served under UNIX? (Bonus: How about Windows?)\n\n\nWhat does it mean that a function is signal handler safe\n\n\nWhat is a process Signal Disposition?\n\n\nHow do I change the signal disposition in a single threaded program? How about multithreaded?\n\n\nWhy sigaction vs signal?\n\n\nHow do I asynchronously and synchronously catch a signal?\n\n\nWhat happens to pending signals after I fork? Exec?\n\n\nWhat happens to my signal disposition after I fork? Exec?",
            "title": "Signals Review Questions"
        },
        {
            "location": "/SystemProgramming/Signals-Review-Questions/#topics",
            "text": "Signals  Signal Handler Safe  Signal Disposition  Signal States  Pending Signals when Forking/Exec  Signal Disposition when Forking/Exec  Raising Signals in C  Raising Signals in a multithreaded program",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/Signals-Review-Questions/#questions",
            "text": "What is a signal?  How are signals served under UNIX? (Bonus: How about Windows?)  What does it mean that a function is signal handler safe  What is a process Signal Disposition?  How do I change the signal disposition in a single threaded program? How about multithreaded?  Why sigaction vs signal?  How do I asynchronously and synchronously catch a signal?  What happens to pending signals after I fork? Exec?  What happens to my signal disposition after I fork? Exec?",
            "title": "Questions"
        },
        {
            "location": "/SystemProgramming/Signals:-Review-Questions/",
            "text": "Give the names of two signals that are normally generated by the kernel\n\n\nGive the name of a signal that can not be caught by a signal\n\n\nWhy is it unsafe to call any function (something that it is not signal handler safe) in a signal handler?\n\n\nCoding Questions\n\n\nWrite brief code that uses SIGACTION and a SIGNALSET to create a SIGALRM handler.",
            "title": "Signals: Review Questions"
        },
        {
            "location": "/SystemProgramming/Signals:-Review-Questions/#give-the-names-of-two-signals-that-are-normally-generated-by-the-kernel",
            "text": "",
            "title": "Give the names of two signals that are normally generated by the kernel"
        },
        {
            "location": "/SystemProgramming/Signals:-Review-Questions/#give-the-name-of-a-signal-that-can-not-be-caught-by-a-signal",
            "text": "",
            "title": "Give the name of a signal that can not be caught by a signal"
        },
        {
            "location": "/SystemProgramming/Signals:-Review-Questions/#why-is-it-unsafe-to-call-any-function-something-that-it-is-not-signal-handler-safe-in-a-signal-handler",
            "text": "Coding Questions  Write brief code that uses SIGACTION and a SIGNALSET to create a SIGALRM handler.",
            "title": "Why is it unsafe to call any function (something that it is not signal handler safe) in a signal handler?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/",
            "text": "Solving Critical Sections\n\n\nWhat is a Critical Section?\n\n\nA critical section is a section of code that can only be executed by one thread at a time, if the program is to function correctly. If two threads (or processes) were to execute code inside the critical section at the same time then it is possible that program may no longer have correct behavior.\n\n\nIs just incrementing a variable a critical section?\n\n\nPossibly. Incrementing a variable (\ni++\n) is performed in three individual steps: Copy the memory contents to the CPU register. Increment the value in the CPU. Store the new value in memory. If the memory location is only accessible by one thread (e.g. automatic variable \ni\n below) then there is no possibility of a race condition and no Critical Section associated with \ni\n. However the \nsum\n variable is a global variable and accessed by two threads. It is possible that two threads may attempt to increment the variable at the same time.\n\n\n#include <stdio.h>\n#include <pthread.h>\n// Compile with -pthread\n\nint sum = 0; //shared\n\nvoid *countgold(void *param) {\n    int i; //local to each thread\n    for (i = 0; i < 10000000; i++) {\n        sum += 1;\n    }\n    return NULL;\n}\n\nint main() {\n    pthread_t tid1, tid2;\n    pthread_create(&tid1, NULL, countgold, NULL);\n    pthread_create(&tid2, NULL, countgold, NULL);\n\n    //Wait for both threads to finish:\n    pthread_join(tid1, NULL);\n    pthread_join(tid2, NULL);\n\n    printf(\"ARRRRG sum is %d\\n\", sum);\n    return 0;\n}\n\n\n\n\nTypical output of the above code is \nARGGGH sum is 8140268\n\nA different sum is printed each time the program is run because there is a race condition; the code does not stop two threads from reading-writing \nsum\n at the same time. For example both threads copy the current value of sum into CPU that runs each thread (let's pick 123). Both threads increment one to their own copy. Both threads write back the value (124). If the threads had accessed the sum at different times then the count would have been 125.\n\n\nHow do I ensure only one thread at a time can access a global variable?\n\n\nYou mean, \"Help - I need a mutex!\"\nIf one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. For this purpose we can use a mutex (short for Mutual Exclusion).\n\n\nFor simple examples the smallest amount of code we need to add is just three lines:\n\n\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; // global variable\npthread_mutex_lock(&m); // start of Critical Section\npthread_mutex_unlock(&m); //end of Critical Section\n\n\n\n\nOnce we are finished with the mutex we should also call \npthread_mutex_destroy(&m)\n too. Note, you can only destroy an unlocked mutex. Calling destroy on a destroyed lock, initializing an initialized lock, locking an already locked lock, unlocking an unlocked lock etc are unsupported (at least for default mutexes) and usually result in undefined behavior.\n\n\nIf I lock a mutex, does it stop all other threads?\n\n\nNo, the other threads will continue. It's only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. As soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue.\n\n\nAre there other ways to create a mutex?\n\n\nYes. You can use the macro PTHREAD_MUTEX_INITIALIZER only for global ('static') variables.\nm = PTHREAD_MUTEX_INITIALIZER is equivalent to the more general purpose\n\npthread_mutex_init(&m,NULL)\n. The init version includes options to trade performance for additional error-checking and advanced sharing options.\n\n\npthread_mutex_t *lock = malloc(sizeof(pthread_mutex_t)); \npthread_mutex_init(lock, NULL);\n//later\npthread_mutex_destroy(lock);\nfree(lock);\n\n\n\n\nThings to keep in mind about \ninit\n and \ndestroy\n:\n\n Multiple threads init/destroy has undefined behavior\n\n Destroying a locked mutex has undefined behavior\n* Basically try to keep to the pattern of one thread initializing a mutex and one and only one thread initializing a mutex.\n\n\nMutex Gotchas\n\n\nSo pthread_mutex_lock\n stops the other threads when they read the same variable?\n\n\nNo. A mutex is not that smart - it works with code (threads), not data. Only when another thread calls \nlock\n on a locked mutex will the second thread need to wait until the mutex is unlocked.\n\n\nConsider\n\n\nint a;\npthread_mutex_t m1 = PTHREAD_MUTEX_INITIALIZER,\n                 m2 = = PTHREAD_MUTEX_INITIALIZER;\n// later\n// Thread 1\npthread_mutex_lock(&m1);\na++;\npthread_mutex_unlock(&m1);\n\n// Thread 2\npthread_mutex_lock(&m2);\na++;\npthread_mutex_unlock(&m2);\n\n\n\n\nWill still cause a race condition.\n\n\nCan I create mutex before fork-ing?\n\n\nYes - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other.\n\n\n(Advanced note: There are advanced options using shared memory that allow a child and parent to share a mutex if it's created with the correct options and uses a shared memory segment. See \nstackoverflow example\n )\n\n\nIf one thread locks a mutex can another thread unlock it?\n\n\nNo. The same thread must unlock it.\n\n\nCan I use two or more mutex locks?\n\n\nYes! In fact it's common to have one lock per data structure that you need to update.\n\n\nIf you only have one lock, then they may be significant contention for the lock between two threads that was unnecessary. For example if two threads were updating two different counters, it might not be necessary to use the same lock.\n\n\nHowever simply creating many locks is insufficient: It's important to be able to reason about critical sections e.g. it's important that one thread can't read two data structures while they are being updated and temporarily in an inconsistent state.\n\n\nIs there any overhead in calling lock and unlock?\n\n\nThere is a small amount of overhead of calling \npthread_mutex_lock\n and \n_unlock\n; however this is the price you pay for correctly functioning programs!\n\n\nSimplest complete example?\n\n\nA complete example is shown below\n\n\n#include <stdio.h>\n#include <pthread.h>\n\n// Compile with -pthread\n// Create a mutex this ready to be locked!\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nint sum = 0;\n\nvoid *countgold(void *param) {\n    int i;\n\n    //Same thread that locks the mutex must unlock it\n    //Critical section is just 'sum += 1'\n    //However locking and unlocking a million times\n    //has significant overhead in this simple answer\n\n    pthread_mutex_lock(&m);\n\n    // Other threads that call lock will have to wait until we call unlock\n\n    for (i = 0; i < 10000000; i++) {\n    sum += 1;\n    }\n    pthread_mutex_unlock(&m);\n    return NULL;\n}\n\nint main() {\n    pthread_t tid1, tid2;\n    pthread_create(&tid1, NULL, countgold, NULL);\n    pthread_create(&tid2, NULL, countgold, NULL);\n\n    //Wait for both threads to finish:\n    pthread_join(tid1, NULL);\n    pthread_join(tid2, NULL);\n\n    printf(\"ARRRRG sum is %d\\n\", sum);\n    return 0;\n}\n\n\n\n\nIn the code above, the thread gets the lock to the counting house before entering. The critical section is only the \nsum+=1\n so the following version is also correct but slower - \n\n\n    for (i = 0; i < 10000000; i++) {\n        pthread_mutex_lock(&m);\n        sum += 1;\n        pthread_mutex_unlock(&m);\n    }\n    return NULL;\n}\n\n\n\n\nThis process runs slower because we lock and unlock the mutex a million times, which is expensive - at least compared with incrementing a variable. (And in this simple example we didn't really need threads - we could have added up twice!)  A faster multi-thread example would be to add one million using an automatic(local) variable and only then adding it to a shared total after the calculation loop has finished:\n\n\n    int local = 0;\n    for (i = 0; i < 10000000; i++) {\n       local += 1;\n    }\n\n    pthread_mutex_lock(&m);\n    sum += local;\n    pthread_mutex_unlock(&m);\n\n    return NULL;\n}\n\n\n\n\nWhat happens if I forget to unlock?\n\n\nDeadlock! We will talk about deadlock a little bit later but what is the problem with this loop if called by multiple threads.\n\n\nwhile(not_stop){\n    //stdin may not be thread safe\n    pthread_mutex_lock(&m);\n    char *line = getline(...);\n    if(rand() % 2) { /* randomly skip lines */\n         continue;\n    }\n    pthread_mutex_unlock(&m);\n\n    process_line(line);\n}\n\n\n\n\nWhen can I destroy the mutex?\n\n\nYou can only destroy an unlocked mutex\n\n\nCan I copy a pthread_mutex_t to a new memory locaton?\n\n\nNo, copying the bytes of the mutex to a new memory location and then using the copy is \nnot\n supported.\n\n\nWhat would a simple implementation of a mutex look like?\n\n\nA simple (but incorrect!) suggestion is shown below. The \nunlock\n function simply unlocks the mutex and returns. The lock function first checks to see if the lock is already locked. If it is currently locked, it will keep checking again until another thread has unlocked the mutex.\n\n\n// Version 1 (Incorrect!)\n\nvoid lock(mutex_t *m) {\n  while(m->locked) { /*Locked? Nevermind - just loop and check again!*/ }\n\n  m->locked = 1;\n}\nvoid unlock(mutex_t *m) {\n  m->locked = 0;\n}\n\n\n\n\nVersion 1 uses 'busy-waiting' (unnecessarily wasting CPU resources) however there is a more serious problem: We have a race-condition! \n\n\nIf two threads both called \nlock\n concurrently it is possible that both threads would read 'm_locked' as zero. Thus both threads would believe they have exclusive access to the lock and both threads will continue. Ooops!\n\n\nWe might attempt to reduce the CPU overhead a little by calling \npthread_yield()\n inside the loop  - pthread_yield suggests to the operating system that the thread does not use the CPU for a short while, so the CPU may be assigned to threads that are waiting to run. But does not fix the race-condition. We need a better implementation - can you work how to prevent the race-condition?\n\n\nHow do I find out more?\n\n\nPlay!\n Read the man page!\n\n \npthread_mutex_lock man page\n\n\n \npthread_mutex_unlock man page\n\n\n \npthread_mutex_init man page\n\n\n \npthread_mutex_destroy man page",
            "title": "Synchronization, Part 1: Mutex Locks"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#solving-critical-sections",
            "text": "",
            "title": "Solving Critical Sections"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#what-is-a-critical-section",
            "text": "A critical section is a section of code that can only be executed by one thread at a time, if the program is to function correctly. If two threads (or processes) were to execute code inside the critical section at the same time then it is possible that program may no longer have correct behavior.",
            "title": "What is a Critical Section?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#is-just-incrementing-a-variable-a-critical-section",
            "text": "Possibly. Incrementing a variable ( i++ ) is performed in three individual steps: Copy the memory contents to the CPU register. Increment the value in the CPU. Store the new value in memory. If the memory location is only accessible by one thread (e.g. automatic variable  i  below) then there is no possibility of a race condition and no Critical Section associated with  i . However the  sum  variable is a global variable and accessed by two threads. It is possible that two threads may attempt to increment the variable at the same time.  #include <stdio.h>\n#include <pthread.h>\n// Compile with -pthread\n\nint sum = 0; //shared\n\nvoid *countgold(void *param) {\n    int i; //local to each thread\n    for (i = 0; i < 10000000; i++) {\n        sum += 1;\n    }\n    return NULL;\n}\n\nint main() {\n    pthread_t tid1, tid2;\n    pthread_create(&tid1, NULL, countgold, NULL);\n    pthread_create(&tid2, NULL, countgold, NULL);\n\n    //Wait for both threads to finish:\n    pthread_join(tid1, NULL);\n    pthread_join(tid2, NULL);\n\n    printf(\"ARRRRG sum is %d\\n\", sum);\n    return 0;\n}  Typical output of the above code is  ARGGGH sum is 8140268 \nA different sum is printed each time the program is run because there is a race condition; the code does not stop two threads from reading-writing  sum  at the same time. For example both threads copy the current value of sum into CPU that runs each thread (let's pick 123). Both threads increment one to their own copy. Both threads write back the value (124). If the threads had accessed the sum at different times then the count would have been 125.",
            "title": "Is just incrementing a variable a critical section?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#how-do-i-ensure-only-one-thread-at-a-time-can-access-a-global-variable",
            "text": "You mean, \"Help - I need a mutex!\"\nIf one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. For this purpose we can use a mutex (short for Mutual Exclusion).  For simple examples the smallest amount of code we need to add is just three lines:  pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; // global variable\npthread_mutex_lock(&m); // start of Critical Section\npthread_mutex_unlock(&m); //end of Critical Section  Once we are finished with the mutex we should also call  pthread_mutex_destroy(&m)  too. Note, you can only destroy an unlocked mutex. Calling destroy on a destroyed lock, initializing an initialized lock, locking an already locked lock, unlocking an unlocked lock etc are unsupported (at least for default mutexes) and usually result in undefined behavior.",
            "title": "How do I ensure only one thread at a time can access a global variable?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#if-i-lock-a-mutex-does-it-stop-all-other-threads",
            "text": "No, the other threads will continue. It's only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. As soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue.",
            "title": "If I lock a mutex, does it stop all other threads?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#are-there-other-ways-to-create-a-mutex",
            "text": "Yes. You can use the macro PTHREAD_MUTEX_INITIALIZER only for global ('static') variables.\nm = PTHREAD_MUTEX_INITIALIZER is equivalent to the more general purpose pthread_mutex_init(&m,NULL) . The init version includes options to trade performance for additional error-checking and advanced sharing options.  pthread_mutex_t *lock = malloc(sizeof(pthread_mutex_t)); \npthread_mutex_init(lock, NULL);\n//later\npthread_mutex_destroy(lock);\nfree(lock);  Things to keep in mind about  init  and  destroy :  Multiple threads init/destroy has undefined behavior  Destroying a locked mutex has undefined behavior\n* Basically try to keep to the pattern of one thread initializing a mutex and one and only one thread initializing a mutex.",
            "title": "Are there other ways to create a mutex?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#mutex-gotchas",
            "text": "",
            "title": "Mutex Gotchas"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#so-pthread_mutex_lock-stops-the-other-threads-when-they-read-the-same-variable",
            "text": "No. A mutex is not that smart - it works with code (threads), not data. Only when another thread calls  lock  on a locked mutex will the second thread need to wait until the mutex is unlocked.  Consider  int a;\npthread_mutex_t m1 = PTHREAD_MUTEX_INITIALIZER,\n                 m2 = = PTHREAD_MUTEX_INITIALIZER;\n// later\n// Thread 1\npthread_mutex_lock(&m1);\na++;\npthread_mutex_unlock(&m1);\n\n// Thread 2\npthread_mutex_lock(&m2);\na++;\npthread_mutex_unlock(&m2);  Will still cause a race condition.",
            "title": "So pthread_mutex_lock stops the other threads when they read the same variable?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#can-i-create-mutex-before-fork-ing",
            "text": "Yes - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other.  (Advanced note: There are advanced options using shared memory that allow a child and parent to share a mutex if it's created with the correct options and uses a shared memory segment. See  stackoverflow example  )",
            "title": "Can I create mutex before fork-ing?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#if-one-thread-locks-a-mutex-can-another-thread-unlock-it",
            "text": "No. The same thread must unlock it.",
            "title": "If one thread locks a mutex can another thread unlock it?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#can-i-use-two-or-more-mutex-locks",
            "text": "Yes! In fact it's common to have one lock per data structure that you need to update.  If you only have one lock, then they may be significant contention for the lock between two threads that was unnecessary. For example if two threads were updating two different counters, it might not be necessary to use the same lock.  However simply creating many locks is insufficient: It's important to be able to reason about critical sections e.g. it's important that one thread can't read two data structures while they are being updated and temporarily in an inconsistent state.",
            "title": "Can I use two or more mutex locks?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#is-there-any-overhead-in-calling-lock-and-unlock",
            "text": "There is a small amount of overhead of calling  pthread_mutex_lock  and  _unlock ; however this is the price you pay for correctly functioning programs!",
            "title": "Is there any overhead in calling lock and unlock?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#simplest-complete-example",
            "text": "A complete example is shown below  #include <stdio.h>\n#include <pthread.h>\n\n// Compile with -pthread\n// Create a mutex this ready to be locked!\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nint sum = 0;\n\nvoid *countgold(void *param) {\n    int i;\n\n    //Same thread that locks the mutex must unlock it\n    //Critical section is just 'sum += 1'\n    //However locking and unlocking a million times\n    //has significant overhead in this simple answer\n\n    pthread_mutex_lock(&m);\n\n    // Other threads that call lock will have to wait until we call unlock\n\n    for (i = 0; i < 10000000; i++) {\n    sum += 1;\n    }\n    pthread_mutex_unlock(&m);\n    return NULL;\n}\n\nint main() {\n    pthread_t tid1, tid2;\n    pthread_create(&tid1, NULL, countgold, NULL);\n    pthread_create(&tid2, NULL, countgold, NULL);\n\n    //Wait for both threads to finish:\n    pthread_join(tid1, NULL);\n    pthread_join(tid2, NULL);\n\n    printf(\"ARRRRG sum is %d\\n\", sum);\n    return 0;\n}  In the code above, the thread gets the lock to the counting house before entering. The critical section is only the  sum+=1  so the following version is also correct but slower -       for (i = 0; i < 10000000; i++) {\n        pthread_mutex_lock(&m);\n        sum += 1;\n        pthread_mutex_unlock(&m);\n    }\n    return NULL;\n}  This process runs slower because we lock and unlock the mutex a million times, which is expensive - at least compared with incrementing a variable. (And in this simple example we didn't really need threads - we could have added up twice!)  A faster multi-thread example would be to add one million using an automatic(local) variable and only then adding it to a shared total after the calculation loop has finished:      int local = 0;\n    for (i = 0; i < 10000000; i++) {\n       local += 1;\n    }\n\n    pthread_mutex_lock(&m);\n    sum += local;\n    pthread_mutex_unlock(&m);\n\n    return NULL;\n}",
            "title": "Simplest complete example?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#what-happens-if-i-forget-to-unlock",
            "text": "Deadlock! We will talk about deadlock a little bit later but what is the problem with this loop if called by multiple threads.  while(not_stop){\n    //stdin may not be thread safe\n    pthread_mutex_lock(&m);\n    char *line = getline(...);\n    if(rand() % 2) { /* randomly skip lines */\n         continue;\n    }\n    pthread_mutex_unlock(&m);\n\n    process_line(line);\n}",
            "title": "What happens if I forget to unlock?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#when-can-i-destroy-the-mutex",
            "text": "You can only destroy an unlocked mutex",
            "title": "When can I destroy the mutex?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#can-i-copy-a-pthread_mutex_t-to-a-new-memory-locaton",
            "text": "No, copying the bytes of the mutex to a new memory location and then using the copy is  not  supported.",
            "title": "Can I copy a pthread_mutex_t to a new memory locaton?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#what-would-a-simple-implementation-of-a-mutex-look-like",
            "text": "A simple (but incorrect!) suggestion is shown below. The  unlock  function simply unlocks the mutex and returns. The lock function first checks to see if the lock is already locked. If it is currently locked, it will keep checking again until another thread has unlocked the mutex.  // Version 1 (Incorrect!)\n\nvoid lock(mutex_t *m) {\n  while(m->locked) { /*Locked? Nevermind - just loop and check again!*/ }\n\n  m->locked = 1;\n}\nvoid unlock(mutex_t *m) {\n  m->locked = 0;\n}  Version 1 uses 'busy-waiting' (unnecessarily wasting CPU resources) however there is a more serious problem: We have a race-condition!   If two threads both called  lock  concurrently it is possible that both threads would read 'm_locked' as zero. Thus both threads would believe they have exclusive access to the lock and both threads will continue. Ooops!  We might attempt to reduce the CPU overhead a little by calling  pthread_yield()  inside the loop  - pthread_yield suggests to the operating system that the thread does not use the CPU for a short while, so the CPU may be assigned to threads that are waiting to run. But does not fix the race-condition. We need a better implementation - can you work how to prevent the race-condition?",
            "title": "What would a simple implementation of a mutex look like?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-1:-Mutex-Locks/#how-do-i-find-out-more",
            "text": "Play!  Read the man page!   pthread_mutex_lock man page    pthread_mutex_unlock man page    pthread_mutex_init man page    pthread_mutex_destroy man page",
            "title": "How do I find out more?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-2:-Counting-Semaphores/",
            "text": "What is a counting semaphore?\n\n\nA counting semaphore contains a value and supports two operations \"wait\" and \"post\". Post increments the semaphore and immediately returns. \"wait\" will wait if the count is zero. If the count is non-zero the semaphore decrements the count and immediately returns.\n\n\nAn analogy is a count of the cookies in a cookie jar (or gold coins in the treasure chest). Before taking a cookie, call 'wait'. If there are no cookies left then \nwait\n will not return: It will \nwait\n until another thread increments the semaphore by calling post.\n\n\nIn short, \npost\n increments and immediately returns whereas \nwait\n will wait if the count is zero. Before returning it will decrement count.\n\n\nHow do I create a semaphore?\n\n\nThis page introduces unnamed semaphores. Unfortunately Mac OS X does not support these yet.\n\n\nFirst decide if the initial value should be zero or some other value (e.g. the number of remaining spaces in an array).\nUnlike pthread mutex there are not shortcuts to creating a semaphore - use \nsem_init\n\n\n#include <semaphore.h>\n\nsem_t s;\nint main() {\n  sem_init(&s, 0, 10); // returns -1 (=FAILED) on OS X\n  sem_wait(&s); // Could do this 10 times without blocking\n  sem_post(&s); // Announce that we've finished (and one more resource item is available; increment count)\n  sem_destroy(&s); // release resources of the semaphore\n}\n\n\n\n\nCan I call wait and post from different threads?\n\n\nYes! Unlike a mutex, the increment and decrement can be from different threads.\n\n\nCan I use a semaphore instead of a mutex?\n\n\nYes - though the overhead of a semaphore is greater. To use a semaphore:\n\n Initialize the semaphore with a count of one.\n\n Replace \n...lock\n with \nsem_wait\n\n* Replace \n...unlock\n with \nsem_post\n\n\nA mutex is a semaphore that always \nwaits\n before it \nposts\n\n\nsem_t s;\nsem_init(&s, 0, 1);\n\nsem_wait(&s);\n// Critical Section\nsem_post(&s);\n\n\n\n\nCan I use sem_post inside a signal handler?\n\n\nYes! \nsem_post\n is one of a handful of functions that can be correctly used inside a signal handler.\nThis means we can release a waiting thread which can now make all of the calls that we were not\nallowed to call inside the signal handler itself (e.g. \nprintf\n).\n\n\n#include <stdio.h>\n#include <pthread.h>\n#include <signal.h>\n#include <semaphore.h>\n#include <unistd.h>\n\nsem_t s;\n\nvoid handler(int signal)\n{\n    sem_post(&s); /* Release the Kraken! */\n}\n\nvoid *singsong(void *param)\n{\n    sem_wait(&s);\n    printf(\"I had to wait until your signal released me!\\n\");\n}\n\nint main()\n{\n    int ok = sem_init(&s, 0, 0 /* Initial value of zero*/); \n    if (ok == -1) {\n       perror(\"Could not create unnamed semaphore\");\n       return 1;\n    }\n    signal(SIGINT, handler); // Too simple! See note below\n\n    pthread_t tid;\n    pthread_create(&tid, NULL, singsong, NULL);\n    pthread_exit(NULL); /* Process will exit when there are no more threads */\n}\n\n\n\n\nNote robust programs do not use \nsignal()\n in a multi-threaded program (\"The effects of signal() in a multithreaded process are unspecified.\" - the signal man page); a more correct program will need to use \nsigaction\n.\n\n\nHow do I find out more?\n\n\nRead the man pages:\n\n \nsem_init\n\n\n \nsem_wait\n\n\n \nsem_post\n\n\n \nsem_destroy",
            "title": "Synchronization, Part 2: Counting Semaphores"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-2:-Counting-Semaphores/#what-is-a-counting-semaphore",
            "text": "A counting semaphore contains a value and supports two operations \"wait\" and \"post\". Post increments the semaphore and immediately returns. \"wait\" will wait if the count is zero. If the count is non-zero the semaphore decrements the count and immediately returns.  An analogy is a count of the cookies in a cookie jar (or gold coins in the treasure chest). Before taking a cookie, call 'wait'. If there are no cookies left then  wait  will not return: It will  wait  until another thread increments the semaphore by calling post.  In short,  post  increments and immediately returns whereas  wait  will wait if the count is zero. Before returning it will decrement count.",
            "title": "What is a counting semaphore?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-2:-Counting-Semaphores/#how-do-i-create-a-semaphore",
            "text": "This page introduces unnamed semaphores. Unfortunately Mac OS X does not support these yet.  First decide if the initial value should be zero or some other value (e.g. the number of remaining spaces in an array).\nUnlike pthread mutex there are not shortcuts to creating a semaphore - use  sem_init  #include <semaphore.h>\n\nsem_t s;\nint main() {\n  sem_init(&s, 0, 10); // returns -1 (=FAILED) on OS X\n  sem_wait(&s); // Could do this 10 times without blocking\n  sem_post(&s); // Announce that we've finished (and one more resource item is available; increment count)\n  sem_destroy(&s); // release resources of the semaphore\n}",
            "title": "How do I create a semaphore?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-2:-Counting-Semaphores/#can-i-call-wait-and-post-from-different-threads",
            "text": "Yes! Unlike a mutex, the increment and decrement can be from different threads.",
            "title": "Can I call wait and post from different threads?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-2:-Counting-Semaphores/#can-i-use-a-semaphore-instead-of-a-mutex",
            "text": "Yes - though the overhead of a semaphore is greater. To use a semaphore:  Initialize the semaphore with a count of one.  Replace  ...lock  with  sem_wait \n* Replace  ...unlock  with  sem_post  A mutex is a semaphore that always  waits  before it  posts  sem_t s;\nsem_init(&s, 0, 1);\n\nsem_wait(&s);\n// Critical Section\nsem_post(&s);",
            "title": "Can I use a semaphore instead of a mutex?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-2:-Counting-Semaphores/#can-i-use-sem_post-inside-a-signal-handler",
            "text": "Yes!  sem_post  is one of a handful of functions that can be correctly used inside a signal handler.\nThis means we can release a waiting thread which can now make all of the calls that we were not\nallowed to call inside the signal handler itself (e.g.  printf ).  #include <stdio.h>\n#include <pthread.h>\n#include <signal.h>\n#include <semaphore.h>\n#include <unistd.h>\n\nsem_t s;\n\nvoid handler(int signal)\n{\n    sem_post(&s); /* Release the Kraken! */\n}\n\nvoid *singsong(void *param)\n{\n    sem_wait(&s);\n    printf(\"I had to wait until your signal released me!\\n\");\n}\n\nint main()\n{\n    int ok = sem_init(&s, 0, 0 /* Initial value of zero*/); \n    if (ok == -1) {\n       perror(\"Could not create unnamed semaphore\");\n       return 1;\n    }\n    signal(SIGINT, handler); // Too simple! See note below\n\n    pthread_t tid;\n    pthread_create(&tid, NULL, singsong, NULL);\n    pthread_exit(NULL); /* Process will exit when there are no more threads */\n}  Note robust programs do not use  signal()  in a multi-threaded program (\"The effects of signal() in a multithreaded process are unspecified.\" - the signal man page); a more correct program will need to use  sigaction .",
            "title": "Can I use sem_post inside a signal handler?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-2:-Counting-Semaphores/#how-do-i-find-out-more",
            "text": "Read the man pages:   sem_init    sem_wait    sem_post    sem_destroy",
            "title": "How do I find out more?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/",
            "text": "Thread Safe Stack\n\n\nWhat is an atomic operation?\n\n\nTo paraphrase Wikipedia, \n\n\n\n\nAn operation (or set of operations) is atomic or uninterruptible if it appears to the rest of the system to occur instantaneously.\nWithout locks, only simple CPU instructions (\"read this byte from memory\") are atomic (indivisible). On a single CPU system, one could temporarily disable interrupts (so a sequence of operations cannot be interrupted) but in practice atomicity is achieved by using synchronization primitives, typically a mutex lock.\n\n\n\n\nIncrementing a variable (\ni++\n) is \nnot\n atomic because it requires three distinct steps: Copying the bit pattern from memory into the CPU; performing a calculation using the CPU's registers; copying the bit pattern back to memory. During this increment sequence, another thread or process can still read the old value and other writes to the same memory would also be over-written when the increment sequence completes.\n\n\nHow do I use mutex lock to make my data-structure thread-safe?\n\n\nNote, this is just an introduction - writing high-performance thread-safe data structures requires its own book! Here's a simple data structure (a stack) that is not thread-safe:\n\n\n// A simple fixed-sized stack (version 1)\n#define STACK_SIZE 20\nint count;\ndouble values[STACK_SIZE];\n\nvoid push(double v) { \n    values[count++] = v; \n}\n\ndouble pop() {\n    return values[--count];\n}\n\nint is_empty() {\n    return count == 0;\n}\n\n\n\n\nVersion 1 of the stack is not thread-safe because if two threads call push or pop at the same time then the results or the stack can be inconsistent. For example, imagine if two threads call pop at the same time then both threads may read the same value, both may read the original count value.\n\n\nTo turn this into a thread-safe data structure we need to identify the \ncritical sections\n of our code  i.e. which section(s) of the code must only have one thread at a time. In the above example the \npush\n,\npop\n and \nis_empty\n functions access the same variables (i.e. memory) and all critical sections for the stack. \n\n\nWhile \npush\n (and \npop\n) is executing, the datastructure is an inconsistent state (for example the count may not have been written to, so may still contain the original value). By wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack.\n\n\nA candidate 'solution' is shown below. Is it correct? If not, how will it fail?\n\n\n// An attempt at a thread-safe stack (version 2)\n#define STACK_SIZE 20\nint count;\ndouble values[STACK_SIZE];\n\npthread_mutex_t m1 = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_t m2 = PTHREAD_MUTEX_INITIALIZER;\n\nvoid push(double v) { \n    pthread_mutex_lock(&m1);\n    values[count++] = v;\n    pthread_mutex_unlock(&m1);\n}\n\ndouble pop() {\n    pthread_mutex_lock(&m2);\n    double v = values[--count];\n    pthread_mutex_unlock(&m2);\n\n    return v;\n}\n\nint is_empty() {\n    pthread_mutex_lock(&m1);\n    return count == 0;\n    pthread_mutex_unlock(&m1);\n}\n\n\n\n\n\nThe above code ('version 2') contains at least one error. Take a moment to see if you can the error(s) and work out the consequence(s).\n\n\nIf three called \npush()\n at the same time the lock \nm1\n ensures that only one thread at time manipulates the stack (two threads will need to wait until the first thread completes (calls unlock), then a second thread will be allowed to continue into the critical section and finally the third thread will be allowed to continue once the second thread has finished).\n\n\nA similar argument applies to concurrent calls (calls at the same time) to \npop\n. However version 2 does not prevent push and pop from running at the same time because \npush\n and \npop\n use two different mutex locks.\n\n\nThe fix is simple in this case - use the same mutex lock for both the push and pop functions.\n\n\nThe code has a second error; \nis_empty\n returns after the comparison and will not unlock the mutex. However the error would not be spotted immediately. For example, suppose one thread calls \nis_empty\n and a second thread later calls \npush\n. This thread would mysteriously stop. Using debugger you can discover that the thread is stuck at the lock() method inside the \npush\n method because the lock was never unlocked by the earlier \nis_empty\n call. Thus an oversight in one thread led to problems much later in time in an arbitrary other thread.\n\n\nA better version is shown below - \n\n\n// An attempt at a thread-safe stack (version 3)\nint count;\ndouble values[count];\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid push(double v) { \n  pthread_mutex_lock(&m); \n  values[count++] = v;\n  pthread_mutex_unlock(&m);\n}\ndouble pop() {\n  pthread_mutex_lock(&m);\n  double v = values[--count];\n  pthread_mutex_unlock(&m);\n  return v;\n}\nint is_empty() {\n  pthread_mutex_lock(&m);\n  int result= count == 0;\n  pthread_mutex_unlock(&m);\n  return result;\n}\n\n\n\n\nVersion 3 is thread-safe (we have ensured mutual exclusion for all of the critical sections) however there are two points of note:\n\n \nis_empty\n is thread-safe but its result may already be out-of date i.e. the stack may no longer be empty by the time the thread gets the result!\n\n There is no protection against underflow (popping on an empty stack) or overflow (pushing onto an already-full stack)\n\n\nThe latter point can be fixed using counting semaphores.\n\n\nThe implementation assumes a single stack.  A more general purpose version might include the mutex as part of the memory struct and use pthread_mutex_init to initialize the mutex. For example,\n\n\n// Support for multiple stacks (each one has a mutex)\ntypedef struct stack {\n  int count;\n  pthread_mutex_t m; \n  double *values;\n} stack_t;\n\nstack_t* stack_create(int capacity) {\n  stack_t *result = malloc(sizeof(stack_t));\n  result->count = 0;\n  result->values = malloc(sizeof(double) * capacity);\n  pthread_mutex_init(&result->m, NULL);\n  return result;\n}\nvoid stack_destroy(stack_t *s) {\n  free(s->values);\n  pthread_mutex_destroy(&s->m);\n  free(s);\n}\n// Warning no underflow or overflow checks!\n\nvoid push(stack_t *s, double v) { \n  pthread_mutex_lock(&s->m); \n  s->values[(s->count)++] = v; \n  pthread_mutex_unlock(&s->m); }\n\ndouble pop(stack_t *s) { \n  pthread_mutex_lock(&s->m); \n  double v = s->values[--(s->count)]; \n  pthread_mutex_unlock(&s->m); \n  return v;\n}\n\nint is_empty(stack_t *s) { \n  pthread_mutex_lock(&s->m); \n  int result = s->count == 0; \n  pthread_mutex_unlock(&s->m);\n  return result;\n}\n\n\n\n\nExample use:\n\n\nint main() {\n    stack_t *s1 = stack_create(10 /* Max capacity*/);\n    stack_t *s2 = stack_create(10);\n    push(s1, 3.141);\n    push(s2, pop(s1));\n    stack_destroy(s2);\n    stack_destroy(s1);\n}\n\n\n\n\nStack Semaphores\n\n\nHow can I force my threads to wait if the stack is empty or full?\n\n\nUse counting semaphores! Use a counting semaphore to keep track of how many spaces remain and another semaphore to keep to track the number of items in the stack. We will call these two semaphores 'sremain' and 'sitems'. Remember \nsem_wait\n will wait if the semaphore's count has been decremented to zero (by another thread calling sem_post).\n\n\n// Sketch #1\n\nsem_t sitems;\nsem_t sremain;\nvoid stack_init(){\n  sem_init(&sitems, 0, 0);\n  sem_init(&sremain, 0, 10);\n}\n\n\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(&sitems);\n  ...\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(&sremain);\n  ...\n\n\n\n\nSketch #2  has implemented the \npost\n too early. Another thread waiting in push can erroneously attempt to write into a full stack (and similarly a thread waiting in the pop() is allowed to continue too early).\n\n\n// Sketch #2 (Error!)\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(&sitems);\n  sem_post(&sremain); // error! wakes up pushing() thread too early\n  return values[--count];\n}\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(&sremain);\n  sem_post(&sitems); // error! wakes up a popping() thread too early\n  values[count++] = v;\n}\n\n\n\n\nSketch 3 implements the correct semaphore logic but can you spot the error?\n\n\n// Sketch #3 (Error!)\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(&sitems);\n  double v= values[--count];\n  sem_post(&sremain);\n  return v;\n}\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(&sremain);\n  values[count++] = v;\n  sem_post(&sitems); \n}\n\n\n\n\nSketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. However there is no \nmutual exclusion\n: Two threads can be in the \ncritical section\n at the same time, which would corrupt the data structure (or least lead to data loss). The fix is to wrap a mutex around the critical section:\n\n\n// Simple single stack - see above example on how to convert this into a multiple stacks.\n// Also a robust POSIX implementation would check for EINTR and error codes of sem_wait.\n\n// PTHREAD_MUTEX_INITIALIZER for statics (use pthread_mutex_init() for stack/heap memory)\n\npthread_mutex_t m= PTHREAD_MUTEX_INITIALIZER; \nint count = 0;\ndouble values[10];\nsem_t sitems, sremain;\n\nvoid init() {\n  sem_init(&sitems, 0, 0);\n  sem_init(&sremains, 0, 10); // 10 spaces\n}\n\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(&sitems);\n\n  pthread_mutex_lock(&m); // CRITICAL SECTION\n  double v= values[--count];\n  pthread_mutex_unlock(&m);\n\n  sem_post(&sremain); // Hey world, there's at least one space\n  return v;\n}\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(&sremain);\n\n  pthread_mutex_lock(&m); // CRITICAL SECTION\n  values[count++] = v;\n  pthread_mutex_unlock(&m);\n\n  sem_post(&sitems); // Hey world, there's at least one item\n}\n// Note a robust solution will need to check sem_wait's result for EINTR (more about this later)\n\n\n\n\nWhat are the common Mutex Gotchas?\n\n\n\n\nLocking/unlocking the wrong mutex (due to a silly typo)\n\n\nNot unlocking a mutex (due to say an early return during an error condition)\n\n\nResource leak (not calling \npthread_mutex_destroy\n)\n\n\nUsing an unitialized mutex (or using a mutex that has already been destroyed)\n\n\nLocking a mutex twice on a thread (without unlocking first)\n\n\nDeadlock and Priority Inversion (we will talk about these later)",
            "title": "Synchronization, Part 3: Working with Mutexes And Semaphores"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#thread-safe-stack",
            "text": "",
            "title": "Thread Safe Stack"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#what-is-an-atomic-operation",
            "text": "To paraphrase Wikipedia,    An operation (or set of operations) is atomic or uninterruptible if it appears to the rest of the system to occur instantaneously.\nWithout locks, only simple CPU instructions (\"read this byte from memory\") are atomic (indivisible). On a single CPU system, one could temporarily disable interrupts (so a sequence of operations cannot be interrupted) but in practice atomicity is achieved by using synchronization primitives, typically a mutex lock.   Incrementing a variable ( i++ ) is  not  atomic because it requires three distinct steps: Copying the bit pattern from memory into the CPU; performing a calculation using the CPU's registers; copying the bit pattern back to memory. During this increment sequence, another thread or process can still read the old value and other writes to the same memory would also be over-written when the increment sequence completes.",
            "title": "What is an atomic operation?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#how-do-i-use-mutex-lock-to-make-my-data-structure-thread-safe",
            "text": "Note, this is just an introduction - writing high-performance thread-safe data structures requires its own book! Here's a simple data structure (a stack) that is not thread-safe:  // A simple fixed-sized stack (version 1)\n#define STACK_SIZE 20\nint count;\ndouble values[STACK_SIZE];\n\nvoid push(double v) { \n    values[count++] = v; \n}\n\ndouble pop() {\n    return values[--count];\n}\n\nint is_empty() {\n    return count == 0;\n}  Version 1 of the stack is not thread-safe because if two threads call push or pop at the same time then the results or the stack can be inconsistent. For example, imagine if two threads call pop at the same time then both threads may read the same value, both may read the original count value.  To turn this into a thread-safe data structure we need to identify the  critical sections  of our code  i.e. which section(s) of the code must only have one thread at a time. In the above example the  push , pop  and  is_empty  functions access the same variables (i.e. memory) and all critical sections for the stack.   While  push  (and  pop ) is executing, the datastructure is an inconsistent state (for example the count may not have been written to, so may still contain the original value). By wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack.  A candidate 'solution' is shown below. Is it correct? If not, how will it fail?  // An attempt at a thread-safe stack (version 2)\n#define STACK_SIZE 20\nint count;\ndouble values[STACK_SIZE];\n\npthread_mutex_t m1 = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_t m2 = PTHREAD_MUTEX_INITIALIZER;\n\nvoid push(double v) { \n    pthread_mutex_lock(&m1);\n    values[count++] = v;\n    pthread_mutex_unlock(&m1);\n}\n\ndouble pop() {\n    pthread_mutex_lock(&m2);\n    double v = values[--count];\n    pthread_mutex_unlock(&m2);\n\n    return v;\n}\n\nint is_empty() {\n    pthread_mutex_lock(&m1);\n    return count == 0;\n    pthread_mutex_unlock(&m1);\n}  The above code ('version 2') contains at least one error. Take a moment to see if you can the error(s) and work out the consequence(s).  If three called  push()  at the same time the lock  m1  ensures that only one thread at time manipulates the stack (two threads will need to wait until the first thread completes (calls unlock), then a second thread will be allowed to continue into the critical section and finally the third thread will be allowed to continue once the second thread has finished).  A similar argument applies to concurrent calls (calls at the same time) to  pop . However version 2 does not prevent push and pop from running at the same time because  push  and  pop  use two different mutex locks.  The fix is simple in this case - use the same mutex lock for both the push and pop functions.  The code has a second error;  is_empty  returns after the comparison and will not unlock the mutex. However the error would not be spotted immediately. For example, suppose one thread calls  is_empty  and a second thread later calls  push . This thread would mysteriously stop. Using debugger you can discover that the thread is stuck at the lock() method inside the  push  method because the lock was never unlocked by the earlier  is_empty  call. Thus an oversight in one thread led to problems much later in time in an arbitrary other thread.  A better version is shown below -   // An attempt at a thread-safe stack (version 3)\nint count;\ndouble values[count];\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid push(double v) { \n  pthread_mutex_lock(&m); \n  values[count++] = v;\n  pthread_mutex_unlock(&m);\n}\ndouble pop() {\n  pthread_mutex_lock(&m);\n  double v = values[--count];\n  pthread_mutex_unlock(&m);\n  return v;\n}\nint is_empty() {\n  pthread_mutex_lock(&m);\n  int result= count == 0;\n  pthread_mutex_unlock(&m);\n  return result;\n}  Version 3 is thread-safe (we have ensured mutual exclusion for all of the critical sections) however there are two points of note:   is_empty  is thread-safe but its result may already be out-of date i.e. the stack may no longer be empty by the time the thread gets the result!  There is no protection against underflow (popping on an empty stack) or overflow (pushing onto an already-full stack)  The latter point can be fixed using counting semaphores.  The implementation assumes a single stack.  A more general purpose version might include the mutex as part of the memory struct and use pthread_mutex_init to initialize the mutex. For example,  // Support for multiple stacks (each one has a mutex)\ntypedef struct stack {\n  int count;\n  pthread_mutex_t m; \n  double *values;\n} stack_t;\n\nstack_t* stack_create(int capacity) {\n  stack_t *result = malloc(sizeof(stack_t));\n  result->count = 0;\n  result->values = malloc(sizeof(double) * capacity);\n  pthread_mutex_init(&result->m, NULL);\n  return result;\n}\nvoid stack_destroy(stack_t *s) {\n  free(s->values);\n  pthread_mutex_destroy(&s->m);\n  free(s);\n}\n// Warning no underflow or overflow checks!\n\nvoid push(stack_t *s, double v) { \n  pthread_mutex_lock(&s->m); \n  s->values[(s->count)++] = v; \n  pthread_mutex_unlock(&s->m); }\n\ndouble pop(stack_t *s) { \n  pthread_mutex_lock(&s->m); \n  double v = s->values[--(s->count)]; \n  pthread_mutex_unlock(&s->m); \n  return v;\n}\n\nint is_empty(stack_t *s) { \n  pthread_mutex_lock(&s->m); \n  int result = s->count == 0; \n  pthread_mutex_unlock(&s->m);\n  return result;\n}  Example use:  int main() {\n    stack_t *s1 = stack_create(10 /* Max capacity*/);\n    stack_t *s2 = stack_create(10);\n    push(s1, 3.141);\n    push(s2, pop(s1));\n    stack_destroy(s2);\n    stack_destroy(s1);\n}",
            "title": "How do I use mutex lock to make my data-structure thread-safe?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#stack-semaphores",
            "text": "",
            "title": "Stack Semaphores"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#how-can-i-force-my-threads-to-wait-if-the-stack-is-empty-or-full",
            "text": "Use counting semaphores! Use a counting semaphore to keep track of how many spaces remain and another semaphore to keep to track the number of items in the stack. We will call these two semaphores 'sremain' and 'sitems'. Remember  sem_wait  will wait if the semaphore's count has been decremented to zero (by another thread calling sem_post).  // Sketch #1\n\nsem_t sitems;\nsem_t sremain;\nvoid stack_init(){\n  sem_init(&sitems, 0, 0);\n  sem_init(&sremain, 0, 10);\n}\n\n\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(&sitems);\n  ...\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(&sremain);\n  ...  Sketch #2  has implemented the  post  too early. Another thread waiting in push can erroneously attempt to write into a full stack (and similarly a thread waiting in the pop() is allowed to continue too early).  // Sketch #2 (Error!)\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(&sitems);\n  sem_post(&sremain); // error! wakes up pushing() thread too early\n  return values[--count];\n}\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(&sremain);\n  sem_post(&sitems); // error! wakes up a popping() thread too early\n  values[count++] = v;\n}  Sketch 3 implements the correct semaphore logic but can you spot the error?  // Sketch #3 (Error!)\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(&sitems);\n  double v= values[--count];\n  sem_post(&sremain);\n  return v;\n}\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(&sremain);\n  values[count++] = v;\n  sem_post(&sitems); \n}  Sketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. However there is no  mutual exclusion : Two threads can be in the  critical section  at the same time, which would corrupt the data structure (or least lead to data loss). The fix is to wrap a mutex around the critical section:  // Simple single stack - see above example on how to convert this into a multiple stacks.\n// Also a robust POSIX implementation would check for EINTR and error codes of sem_wait.\n\n// PTHREAD_MUTEX_INITIALIZER for statics (use pthread_mutex_init() for stack/heap memory)\n\npthread_mutex_t m= PTHREAD_MUTEX_INITIALIZER; \nint count = 0;\ndouble values[10];\nsem_t sitems, sremain;\n\nvoid init() {\n  sem_init(&sitems, 0, 0);\n  sem_init(&sremains, 0, 10); // 10 spaces\n}\n\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(&sitems);\n\n  pthread_mutex_lock(&m); // CRITICAL SECTION\n  double v= values[--count];\n  pthread_mutex_unlock(&m);\n\n  sem_post(&sremain); // Hey world, there's at least one space\n  return v;\n}\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(&sremain);\n\n  pthread_mutex_lock(&m); // CRITICAL SECTION\n  values[count++] = v;\n  pthread_mutex_unlock(&m);\n\n  sem_post(&sitems); // Hey world, there's at least one item\n}\n// Note a robust solution will need to check sem_wait's result for EINTR (more about this later)",
            "title": "How can I force my threads to wait if the stack is empty or full?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#what-are-the-common-mutex-gotchas",
            "text": "Locking/unlocking the wrong mutex (due to a silly typo)  Not unlocking a mutex (due to say an early return during an error condition)  Resource leak (not calling  pthread_mutex_destroy )  Using an unitialized mutex (or using a mutex that has already been destroyed)  Locking a mutex twice on a thread (without unlocking first)  Deadlock and Priority Inversion (we will talk about these later)",
            "title": "What are the common Mutex Gotchas?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/",
            "text": "Candidate Solutions\n\n\nWhat is the Critical Section Problem?\n\n\nAs already discussed in [[Synchronization, Part 3: Working with Mutexes And Semaphores]], there are critical parts of our code that can only be executed by one thread at a time. We describe this requirement as 'mutual exclusion'; only one thread (or process) may have access to the shared resource.\n\n\nIn multi-threaded programs we can wrap a critical section with mutex lock and unlock calls:\n\n\npthread_mutex_lock() - one thread allowed at a time! (others will have to wait here)\n... Do Critical Section stuff here!\npthread_mutex_unlock() - let other waiting threads continue\n\n\n\n\nHow would we implement these lock and unlock calls? Can we create an algorithm that assures mutual exclusion? An incorrect implementation is shown below, \n\n\npthread_mutex_lock(p_mutex_t *m)     { while(m->lock) {}; m->lock = 1;}\npthread_mutex_unlock(p_mutex_t *m)   { m->lock = 0; }\n\n\n\n\nAt first glance, the code appears to work; if one thread attempts to locks the mutex, a later thread must wait until the lock is cleared. However this implementation \ndoes not satisfy Mutual Exclusion\n. Let's take a close look at this 'implementation' from the point of view of two threads running around the same time. In the table below times runs from top to bottom-\n\n\n\n\n\n\n\n\nTime\n\n\nThread 1\n\n\nThread 2\n\n\n\n\n\n\n\n\n\n\n1\n\n\nwhile(lock) {}\n\n\n\n\n\n\n\n\n2\n\n\n\n\nwhile(lock) {}\n\n\n\n\n\n\n3\n\n\nlock = 1\n\n\nlock = 1\n\n\n\n\n\n\nOoops! There is a race condition. In the unfortunate case both threads checked the lock and read a false value and so were able to continue.\n\n\n\n\n\n\n\n\n\n\n\n\nCandidate solutions to the critical section problem.\n\n\nTo simplify the discussion we consider only two threads. Note these arguments work for threads and processes and the classic CS literature discusses these problem in terms of two processes that need exclusive access (i.e. mutual exclusion) to a critical section or shared resource.\n\n\nRaising a flag represents a thread/process's intention to enter the critical section.\n\n\nRemember that the psuedo-code outlined below is part of a larger program; the thread or process will typically need to enter the critical section many times during the lifetime of the process. So imagine each example as wrapped inside a loop where for a random amount of time the thread or process is working on something else.\n\n\nIs there anything wrong with candidate solution described below?\n\n\n// Candidate #1\nwait until your flag is lowered\nraise my flag\n// Do Critical Section stuff\nlower my flag \n\n\n\n\nAnswer: Candidate solution #1 also suffers a race condition i.e. it does not satisfy Mutual Exclusion because both threads/processes could read each other's flag value (=lowered) and continue. \n\n\nThis suggests we should raise the flag \nbefore\n checking the other thread's flag - which is candidate solution #2 below.\n\n\n// Candidate #2\nraise my flag\nwait until your flag is lowered\n// Do Critical Section stuff\nlower my flag \n\n\n\n\nCandidate #2 satisfies mutual exclusion - it is impossible for two threads to be inside the critical section at the same time. However this code suffers from deadlock! Suppose two threads wish to enter the critical section at the same time:\n\n\n\n\n\n\n\n\nTime\n\n\nThread 1\n\n\nThread 2\n\n\n\n\n\n\n\n\n\n\n1\n\n\nraise flag\n\n\n\n\n\n\n\n\n2\n\n\n\n\nraise flag\n\n\n\n\n\n\n3\n\n\nwait ...\n\n\nwait ...\n\n\n\n\n\n\nOoops both threads / processes are now waiting for the other one to lower their flags. Neither one will enter the critical section as both are now stuck forever!\n\n\n\n\n\n\n\n\n\n\n\n\nThis suggests we should use a turn-based variable to try to resolve who should proceed. \n\n\nTurn-based solutions\n\n\nThe following candidate solution #3 uses a turn-based variable to politely allow one thread and then the other to continue\n\n\n// Candidate #3\nwait until my turn is myid\n// Do Critical Section stuff\nturn = yourid\n\n\n\n\nCandidate #3 satisfies mutual exclusion (each thread or process gets exclusive access to the Critical Section), however both threads/processes must take a strict turn-based approach to using the critical section; i.e. they are forced into an alternating critical section access pattern. For example, if thread 1 wishes to read a hashtable every millisecond but another thread writes to a hashtable every second, then the reading thread would have to wait another 999ms before being able to read from the hashtable again. This 'solution' is not effective, because our threads should be able to make progress and enter the critical section if no other thread is currently in the critical section.\n\n\nDesired properties for solutions to the Critical Section Problem?\n\n\nThere are three main desirable properties that we desire in a solution the critical section problem\n\n Mutual Exclusion - the thread/process gets exclusive access; others must wait until it exits the critical section.\n\n Bounded Wait - if the thread/process has to wait, then it should only have to wait for a finite,  amount of time (infinite waiting times are not allowed!). The exact definition of bounded wait is that there is an upper (non-infinite) bound on the number of times any other process can enter its critical section before the given process enters.\n* Progress - if no thread/process is inside the critical section, then the thread/process should be able to proceed (make progress) without having to wait.\n\n\nWith these ideas in mind let's examine another candidate solution that uses a turn-based flag only if two threads both required access at the same time.\n\n\nTurn and Flag solutions\n\n\nIs the following a correct solution to CSP?\n\n\n\\\\ Candidate #4\nraise my flag\nif your flag is raised, wait until my turn\n// Do Critical Section stuff\nturn = yourid\nlower my flag\n\n\n\n\nOne instructor and another CS faculty member initially thought so! However, analyzing these solutions is tricky. Even peer-reviewed papers on this specific subject contain incorrect solutions! At first glance it appears to satisfy Mutual Exclusion, Bounded Wait and Progress: The turn-based flag is only used in the event of a tie (so Progress and Bounded Wait is allowed) and mutual exclusion appears to be satisfied. However.... Perhaps you can find a counter-example?\n\n\nCandidate #4 fails because a thread does not wait until the other thread lowers their flag. After some thought (or inspiration) the following scenario can be created to demonstrate how Mutual Exclusion is not satisfied.\n\n\nImagine the first thread runs this code twice (so the the turn flag now points to the second thread). While the first thread is still inside the Critical Section, the second thread arrives. The second thread can immediately continue into the Critical Section!\n\n\n\n\n\n\n\n\nTime\n\n\nTurn\n\n\nThread #1\n\n\nThread #2\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\nraise my flag\n\n\n\n\n\n\n\n\n2\n\n\n2\n\n\nif your flag is raised, wait until my turn\n\n\nraise my flag\n\n\n\n\n\n\n3\n\n\n2\n\n\n// Do Critical Section stuff\n\n\nif your flag is raised, wait until my turn(TRUE!)\n\n\n\n\n\n\n4\n\n\n2\n\n\n// Do Critical Section stuff\n\n\n// Do Critical Section stuff - OOPS\n\n\n\n\n\n\n\n\nWorking Solutions\n\n\nWhat is Peterson's solution?\n\n\nPeterson published his novel and surprisingly simple solution in a 2 page paper in 1981. A version of his algorithm is shown below that uses a shared variable \nturn\n: \n\n\n\\\\ Candidate #5\nraise my flag\nturn = your_id\nwait until your flag is lowered and turn is yourid\n// Do Critical Section stuff\nlower my flag\n\n\n\n\nThis solution satisfies Mutual Exclusion, Bounded Wait and Progress. If thread #2 has set turn to 2 and is currently inside the critical section. Thread #1 arrives, \nsets the turn back to 1\n and now waits until thread 2 lowers the flag.\n\n\nLink to Peterson's original article pdf:\n\nG. L. Peterson: \"Myths About the Mutual Exclusion Problem\", Information Processing Letters 12(3) 1981, 115\u2013116\n\n\nWas Peterson's solution the first solution?\n\n\nNo, Dekkers Algorithm (1962) was the first provably correct solution. A version of the algorithm is below.\n\n\nraise my flag\nwhile(your flag is raised) :\n   if it's your turn to win :\n     lower my flag\n     wait while your turn\n     raise my flag\n// Do Critical Section stuff\nset your turn to win\nlower my flag\n\n\n\n\nNotice how the process's flag is always raised during the critical section no matter if the loop is iterated zero, once or more times. Further the flag can be interpreted as an immediate intent to enter the critical section. Only if the other process has also raised the flag will one process defer, lower their intent flag and wait.\n\n\nCan I just implement Peterson's (or Dekkers) algorithm in C or assembler?\n\n\nYes - and with a bit searching it is possible even today to find it in production for specific simple mobile processors: Peterson's algorithm is used to implement low-level Linux Kernel locks for the Tegra mobile processor (a system-on-chip ARM process and GPU core by Nvidia)\nhttps://android.googlesource.com/kernel/tegra.git/+/android-tegra-3.10/arch/arm/mach-tegra/sleep.S#58\n\n\nHowever in general, CPUs and C compilers can re-order CPU instructions or use CPU-core-specific local cache values that are stale if another core updates the shared variables. Thus a simple pseudo-code to C implementation is too naive for most platforms. You can stop reading now.\n\n\nOh... you decided to keep reading. Well, here be dragons! Don't say we didn't warn you. Consider this advanced and gnarly topic but (spoiler alert) a happy ending.\n\n\nConsider the following code,\n\n\nwhile(flag2 ) { /* busy loop - go around again */\n\n\n\n\nAn efficient compiler would infer that \nflag2\n variable is never changed inside the loop, so that test can be optimized to \nwhile(true)\n \nUsing \nvolatile\n goes someway to prevent compiler optimizations of this kind.\n\n\nIndependent instructions can be re-ordered by an optimizing compiler or at runtime by an out-of-order execution optimization by the CPU. These sophisticated optimizations if the code requires variables to be modified and checked and a precise order.\n\n\nA related challenge is that CPU cores include a data cache to store recently read or modified main memory values. Modified values may not be written back to main memory or re-read from memory immediately. Thus data changes, such as the state of a flag and turn variable in the above examples, may not be shared between two CPU codes. \n\n\nBut there is happy ending. Fortunately, modern hardware addresses these issues using 'memory fences' (also known as memory barrier) CPU instructions to ensure that main memory and the CPUs' cache is in a reasonable and coherent state. Higher level synchronization primitives, such as \npthread_mutex_lock\n are will call these CPU instructions as part of their implementation. Thus, in practice, surrounding critical section with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.\n\n\nFurther reading: we suggest the following web post that discusses implementing Peterson's algorithm on an x86 process and the linux documentation on memory barriers.\n\n\nhttp://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/\nhttp://lxr.free-electrons.com/source/Documentation/memory-barriers.txt\n\n\nHardware Solutions\n\n\nHow do we implement Critical Section Problem on hardware?\n\n\nWe can use C11 Atomics to do that perfectly! A complete solution is detailed here (this is a spinlock mutex, \nfutex\n implementations can be found online).\n\n\ntypedef struct mutex_{\n    atomic_int_least8_t lock;\n    pthread_t owner;\n} mutex;\n\n#define UNLOCKED 0\n#define LOCKED 1\n#define UNASSIGNED_OWNER 0\n\nint mutex_init(mutex* mtx){\n    if(!mtx){\n        return 0;\n    }\n    atomic_init(&mtx->lock, UNLOCKED); // Not thread safe the user has to take care of this\n    mtx->owner = UNASSIGNED_OWNER;\n    return 1;\n}\n\n\n\n\nThis is the initialization code, nothing fancy here. We set the state of the mutex to unlocked and set the owner to locked.\n\n\nint mutex_lock(mutex* mtx){\n    int_least8_t zero = UNLOCKED;\n    while(!atomic_compare_exchange_weak_explicit\n            (&mtx->lock, \n             &zero, \n             LOCKED,\n             memory_order_relaxed,\n             memory_order_relaxed)){\n        zero = UNLOCKED;\n        sched_yield(); //Use system calls for scheduling speed\n    }\n    //We have the lock now!!!!\n    mtx->owner = pthread_self();\n    return 1;\n}\n\n\n\n\nYikes! What does this code do? Well to start it it initializes a variable that we will keep as the unlocked state. \nAtomic Compare and Exchange\n is an instruction supported by most modern architectures (on x86 it's \nlock cmpxchg\n). The pseudocode for this operation looks like this\n\n\nint atomic_compare_exchange_pseudo(int* addr1, int* addr2, int val){\n    if(*addr1 == *addr2){\n        *addr1 = val;\n        return 1;\n    }else{\n        *addr2 = *addr1;\n        return 0;\n    }\n}\n\n\n\n\nExcept it is all done \natomically\n meaning in one uninterruptible operation. What does the \nweak\n part mean? Well atomic instructions are also prone to \nspurious failures\n meaning that there are two versions to these atomic functions a \nstrong\n and a \nweak\n part, strong guarantee the the success or failure while weak may fail. We are using weak because weak is faster and we are in a loop! That means we are okay if it fails a little bit more often because we will just keep spinning around anyway.\n\n\nWhat is this memory order business? We were talking about memory fences earlier, here it is! We won't go into detail because it is outside the scope of this course but not the scope of \nthis article\n.\n\n\nInside the while loop, we have failed to grab the lock! We reset zero to unlocked and sleep for a little while. When we wake up we try to grab the lock again. Once we successfully swap, we are in the critical section! We set the mutex's owner to the current thread for the unlock method and return successful.\n\n\nHow does this guarantee mutual exclusion, when working with atomics we are not entirely sure! But in this simple example we can because the thread that is able to successfully expect the lock to be UNLOCKED (0) and swap it to a LOCKED (1) state is considered the winner. How do we implement unlock?\n\n\nint mutex_unlock(mutex* mtx){\n    if(unlikely(pthread_self() != mtx->owner)){\n        return 0; //You can't unlock a mutex if you aren't the owner\n    }\n    int_least8_t one = 1;\n    //Critical section ends after this atomic\n    mtx->owner = UNASSIGNED_OWNER;\n    if(!atomic_compare_exchange_strong_explicit(\n                &mtx->lock, \n                &one, \n                UNLOCKED,\n                memory_order_relaxed,\n                memory_order_relaxed)){\n        //The mutex was never locked in the first place\n        return 0;\n    }\n    return 1;\n}\n\n\n\n\nTo satisfy the api, you can't unlock the mutex unless you are the one who owns it. Then we unassign the mutex owner, because critical section is over after the atomic. We want a strong exchange because we don't want to block (pthread_mutex_unlock doesn't block). We expect the mutex to be locked, and we swap it to unlock. If the swap was successful, we unlocked the mutex. If the swap wasn't, that means that the mutex was UNLOCKED and we tried to switch it from UNLOCKED to UNLOCKED, preserving the non blocking of unlock.",
            "title": "Synchronization, Part 4: The Critical Section Problem"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#candidate-solutions",
            "text": "",
            "title": "Candidate Solutions"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#what-is-the-critical-section-problem",
            "text": "As already discussed in [[Synchronization, Part 3: Working with Mutexes And Semaphores]], there are critical parts of our code that can only be executed by one thread at a time. We describe this requirement as 'mutual exclusion'; only one thread (or process) may have access to the shared resource.  In multi-threaded programs we can wrap a critical section with mutex lock and unlock calls:  pthread_mutex_lock() - one thread allowed at a time! (others will have to wait here)\n... Do Critical Section stuff here!\npthread_mutex_unlock() - let other waiting threads continue  How would we implement these lock and unlock calls? Can we create an algorithm that assures mutual exclusion? An incorrect implementation is shown below,   pthread_mutex_lock(p_mutex_t *m)     { while(m->lock) {}; m->lock = 1;}\npthread_mutex_unlock(p_mutex_t *m)   { m->lock = 0; }  At first glance, the code appears to work; if one thread attempts to locks the mutex, a later thread must wait until the lock is cleared. However this implementation  does not satisfy Mutual Exclusion . Let's take a close look at this 'implementation' from the point of view of two threads running around the same time. In the table below times runs from top to bottom-     Time  Thread 1  Thread 2      1  while(lock) {}     2   while(lock) {}    3  lock = 1  lock = 1    Ooops! There is a race condition. In the unfortunate case both threads checked the lock and read a false value and so were able to continue.",
            "title": "What is the Critical Section Problem?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#candidate-solutions-to-the-critical-section-problem",
            "text": "To simplify the discussion we consider only two threads. Note these arguments work for threads and processes and the classic CS literature discusses these problem in terms of two processes that need exclusive access (i.e. mutual exclusion) to a critical section or shared resource.  Raising a flag represents a thread/process's intention to enter the critical section.  Remember that the psuedo-code outlined below is part of a larger program; the thread or process will typically need to enter the critical section many times during the lifetime of the process. So imagine each example as wrapped inside a loop where for a random amount of time the thread or process is working on something else.  Is there anything wrong with candidate solution described below?  // Candidate #1\nwait until your flag is lowered\nraise my flag\n// Do Critical Section stuff\nlower my flag   Answer: Candidate solution #1 also suffers a race condition i.e. it does not satisfy Mutual Exclusion because both threads/processes could read each other's flag value (=lowered) and continue.   This suggests we should raise the flag  before  checking the other thread's flag - which is candidate solution #2 below.  // Candidate #2\nraise my flag\nwait until your flag is lowered\n// Do Critical Section stuff\nlower my flag   Candidate #2 satisfies mutual exclusion - it is impossible for two threads to be inside the critical section at the same time. However this code suffers from deadlock! Suppose two threads wish to enter the critical section at the same time:     Time  Thread 1  Thread 2      1  raise flag     2   raise flag    3  wait ...  wait ...    Ooops both threads / processes are now waiting for the other one to lower their flags. Neither one will enter the critical section as both are now stuck forever!       This suggests we should use a turn-based variable to try to resolve who should proceed.",
            "title": "Candidate solutions to the critical section problem."
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#turn-based-solutions",
            "text": "The following candidate solution #3 uses a turn-based variable to politely allow one thread and then the other to continue  // Candidate #3\nwait until my turn is myid\n// Do Critical Section stuff\nturn = yourid  Candidate #3 satisfies mutual exclusion (each thread or process gets exclusive access to the Critical Section), however both threads/processes must take a strict turn-based approach to using the critical section; i.e. they are forced into an alternating critical section access pattern. For example, if thread 1 wishes to read a hashtable every millisecond but another thread writes to a hashtable every second, then the reading thread would have to wait another 999ms before being able to read from the hashtable again. This 'solution' is not effective, because our threads should be able to make progress and enter the critical section if no other thread is currently in the critical section.",
            "title": "Turn-based solutions"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#desired-properties-for-solutions-to-the-critical-section-problem",
            "text": "There are three main desirable properties that we desire in a solution the critical section problem  Mutual Exclusion - the thread/process gets exclusive access; others must wait until it exits the critical section.  Bounded Wait - if the thread/process has to wait, then it should only have to wait for a finite,  amount of time (infinite waiting times are not allowed!). The exact definition of bounded wait is that there is an upper (non-infinite) bound on the number of times any other process can enter its critical section before the given process enters.\n* Progress - if no thread/process is inside the critical section, then the thread/process should be able to proceed (make progress) without having to wait.  With these ideas in mind let's examine another candidate solution that uses a turn-based flag only if two threads both required access at the same time.",
            "title": "Desired properties for solutions to the Critical Section Problem?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#turn-and-flag-solutions",
            "text": "Is the following a correct solution to CSP?  \\\\ Candidate #4\nraise my flag\nif your flag is raised, wait until my turn\n// Do Critical Section stuff\nturn = yourid\nlower my flag  One instructor and another CS faculty member initially thought so! However, analyzing these solutions is tricky. Even peer-reviewed papers on this specific subject contain incorrect solutions! At first glance it appears to satisfy Mutual Exclusion, Bounded Wait and Progress: The turn-based flag is only used in the event of a tie (so Progress and Bounded Wait is allowed) and mutual exclusion appears to be satisfied. However.... Perhaps you can find a counter-example?  Candidate #4 fails because a thread does not wait until the other thread lowers their flag. After some thought (or inspiration) the following scenario can be created to demonstrate how Mutual Exclusion is not satisfied.  Imagine the first thread runs this code twice (so the the turn flag now points to the second thread). While the first thread is still inside the Critical Section, the second thread arrives. The second thread can immediately continue into the Critical Section!     Time  Turn  Thread #1  Thread #2      1  2  raise my flag     2  2  if your flag is raised, wait until my turn  raise my flag    3  2  // Do Critical Section stuff  if your flag is raised, wait until my turn(TRUE!)    4  2  // Do Critical Section stuff  // Do Critical Section stuff - OOPS",
            "title": "Turn and Flag solutions"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#working-solutions",
            "text": "",
            "title": "Working Solutions"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#what-is-petersons-solution",
            "text": "Peterson published his novel and surprisingly simple solution in a 2 page paper in 1981. A version of his algorithm is shown below that uses a shared variable  turn :   \\\\ Candidate #5\nraise my flag\nturn = your_id\nwait until your flag is lowered and turn is yourid\n// Do Critical Section stuff\nlower my flag  This solution satisfies Mutual Exclusion, Bounded Wait and Progress. If thread #2 has set turn to 2 and is currently inside the critical section. Thread #1 arrives,  sets the turn back to 1  and now waits until thread 2 lowers the flag.  Link to Peterson's original article pdf: G. L. Peterson: \"Myths About the Mutual Exclusion Problem\", Information Processing Letters 12(3) 1981, 115\u2013116",
            "title": "What is Peterson's solution?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#was-petersons-solution-the-first-solution",
            "text": "No, Dekkers Algorithm (1962) was the first provably correct solution. A version of the algorithm is below.  raise my flag\nwhile(your flag is raised) :\n   if it's your turn to win :\n     lower my flag\n     wait while your turn\n     raise my flag\n// Do Critical Section stuff\nset your turn to win\nlower my flag  Notice how the process's flag is always raised during the critical section no matter if the loop is iterated zero, once or more times. Further the flag can be interpreted as an immediate intent to enter the critical section. Only if the other process has also raised the flag will one process defer, lower their intent flag and wait.",
            "title": "Was Peterson's solution the first solution?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#can-i-just-implement-petersons-or-dekkers-algorithm-in-c-or-assembler",
            "text": "Yes - and with a bit searching it is possible even today to find it in production for specific simple mobile processors: Peterson's algorithm is used to implement low-level Linux Kernel locks for the Tegra mobile processor (a system-on-chip ARM process and GPU core by Nvidia)\nhttps://android.googlesource.com/kernel/tegra.git/+/android-tegra-3.10/arch/arm/mach-tegra/sleep.S#58  However in general, CPUs and C compilers can re-order CPU instructions or use CPU-core-specific local cache values that are stale if another core updates the shared variables. Thus a simple pseudo-code to C implementation is too naive for most platforms. You can stop reading now.  Oh... you decided to keep reading. Well, here be dragons! Don't say we didn't warn you. Consider this advanced and gnarly topic but (spoiler alert) a happy ending.  Consider the following code,  while(flag2 ) { /* busy loop - go around again */  An efficient compiler would infer that  flag2  variable is never changed inside the loop, so that test can be optimized to  while(true)  \nUsing  volatile  goes someway to prevent compiler optimizations of this kind.  Independent instructions can be re-ordered by an optimizing compiler or at runtime by an out-of-order execution optimization by the CPU. These sophisticated optimizations if the code requires variables to be modified and checked and a precise order.  A related challenge is that CPU cores include a data cache to store recently read or modified main memory values. Modified values may not be written back to main memory or re-read from memory immediately. Thus data changes, such as the state of a flag and turn variable in the above examples, may not be shared between two CPU codes.   But there is happy ending. Fortunately, modern hardware addresses these issues using 'memory fences' (also known as memory barrier) CPU instructions to ensure that main memory and the CPUs' cache is in a reasonable and coherent state. Higher level synchronization primitives, such as  pthread_mutex_lock  are will call these CPU instructions as part of their implementation. Thus, in practice, surrounding critical section with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.  Further reading: we suggest the following web post that discusses implementing Peterson's algorithm on an x86 process and the linux documentation on memory barriers.  http://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/\nhttp://lxr.free-electrons.com/source/Documentation/memory-barriers.txt",
            "title": "Can I just implement Peterson's (or Dekkers) algorithm in C or assembler?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#hardware-solutions",
            "text": "",
            "title": "Hardware Solutions"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-4:-The-Critical-Section-Problem/#how-do-we-implement-critical-section-problem-on-hardware",
            "text": "We can use C11 Atomics to do that perfectly! A complete solution is detailed here (this is a spinlock mutex,  futex  implementations can be found online).  typedef struct mutex_{\n    atomic_int_least8_t lock;\n    pthread_t owner;\n} mutex;\n\n#define UNLOCKED 0\n#define LOCKED 1\n#define UNASSIGNED_OWNER 0\n\nint mutex_init(mutex* mtx){\n    if(!mtx){\n        return 0;\n    }\n    atomic_init(&mtx->lock, UNLOCKED); // Not thread safe the user has to take care of this\n    mtx->owner = UNASSIGNED_OWNER;\n    return 1;\n}  This is the initialization code, nothing fancy here. We set the state of the mutex to unlocked and set the owner to locked.  int mutex_lock(mutex* mtx){\n    int_least8_t zero = UNLOCKED;\n    while(!atomic_compare_exchange_weak_explicit\n            (&mtx->lock, \n             &zero, \n             LOCKED,\n             memory_order_relaxed,\n             memory_order_relaxed)){\n        zero = UNLOCKED;\n        sched_yield(); //Use system calls for scheduling speed\n    }\n    //We have the lock now!!!!\n    mtx->owner = pthread_self();\n    return 1;\n}  Yikes! What does this code do? Well to start it it initializes a variable that we will keep as the unlocked state.  Atomic Compare and Exchange  is an instruction supported by most modern architectures (on x86 it's  lock cmpxchg ). The pseudocode for this operation looks like this  int atomic_compare_exchange_pseudo(int* addr1, int* addr2, int val){\n    if(*addr1 == *addr2){\n        *addr1 = val;\n        return 1;\n    }else{\n        *addr2 = *addr1;\n        return 0;\n    }\n}  Except it is all done  atomically  meaning in one uninterruptible operation. What does the  weak  part mean? Well atomic instructions are also prone to  spurious failures  meaning that there are two versions to these atomic functions a  strong  and a  weak  part, strong guarantee the the success or failure while weak may fail. We are using weak because weak is faster and we are in a loop! That means we are okay if it fails a little bit more often because we will just keep spinning around anyway.  What is this memory order business? We were talking about memory fences earlier, here it is! We won't go into detail because it is outside the scope of this course but not the scope of  this article .  Inside the while loop, we have failed to grab the lock! We reset zero to unlocked and sleep for a little while. When we wake up we try to grab the lock again. Once we successfully swap, we are in the critical section! We set the mutex's owner to the current thread for the unlock method and return successful.  How does this guarantee mutual exclusion, when working with atomics we are not entirely sure! But in this simple example we can because the thread that is able to successfully expect the lock to be UNLOCKED (0) and swap it to a LOCKED (1) state is considered the winner. How do we implement unlock?  int mutex_unlock(mutex* mtx){\n    if(unlikely(pthread_self() != mtx->owner)){\n        return 0; //You can't unlock a mutex if you aren't the owner\n    }\n    int_least8_t one = 1;\n    //Critical section ends after this atomic\n    mtx->owner = UNASSIGNED_OWNER;\n    if(!atomic_compare_exchange_strong_explicit(\n                &mtx->lock, \n                &one, \n                UNLOCKED,\n                memory_order_relaxed,\n                memory_order_relaxed)){\n        //The mutex was never locked in the first place\n        return 0;\n    }\n    return 1;\n}  To satisfy the api, you can't unlock the mutex unless you are the one who owns it. Then we unassign the mutex owner, because critical section is over after the atomic. We want a strong exchange because we don't want to block (pthread_mutex_unlock doesn't block). We expect the mutex to be locked, and we swap it to unlock. If the swap was successful, we unlocked the mutex. If the swap wasn't, that means that the mutex was UNLOCKED and we tried to switch it from UNLOCKED to UNLOCKED, preserving the non blocking of unlock.",
            "title": "How do we implement Critical Section Problem on hardware?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/",
            "text": "Intro to Condition Variables\n\n\nWarm up\n\n\nName these properties!\n\n \"Only one process(/thread) can be in the CS at a time\"\n\n \"If waiting, then another process can only enter the CS a finite number of times\" \n* \"If no other process is in the CS then the process can immediately enter the CS\"\n\n\nSee [[Synchronization, Part 4: The Critical Section Problem]] for answers.\n\n\nWhat are condition variables? How do you use them? What is Spurious Wakeup?\n\n\n\n\n\n\nCondition variables allow a set of threads to sleep until tickled! You can tickle one thread or all threads that are sleeping. If you only wake one thread then the operating system will decide which thread to wake up. You don't wake threads directly instead you 'signal' the condition variable, which then will wake up one (or all) threads that are sleeping inside the condition variable.\n\n\n\n\n\n\nCondition variables are used with a mutex and with a loop (to check a condition).\n\n\n\n\n\n\nOccasionally a waiting thread may appear to wake up for no reason (this is called a \nspurious wake\n)! This is not an issue because you always use \nwait\n inside a loop that tests a condition that must be true to continue.\n\n\n\n\n\n\nThreads sleeping inside a condition variable are woken up by calling \npthread_cond_broadcast\n (wake up all) or \npthread_cond_signal\n (wake up one). Note despite the function name, this has nothing to do with POSIX \nsignal\ns!\n\n\n\n\n\n\nWhat does \npthread_cond_wait\n do?\n\n\nThe call \npthread_cond_wait\n performs three actions:\n\n unlock the mutex\n\n waits (sleeps until \npthread_cond_signal\n is called on the same condition variable)\n* Before returning, locks the mutex\n\n\n(Advanced topic) Why do Condition Variables also need a mutex?\n\n\nCondition variables need a mutex for three reasons. The simplest to understand is that it prevents an early wakeup message (\nsignal\n or \nbroadcast\n functions) from being 'lost.' Imagine the following sequence of events (time runs down the page) where the condition is satisfied \njust before \npthread_cond_wait\n is called. In this example the wake-up signal is lost!\n\n\n\n\n\n\n\n\nThread 1\n\n\nThread 2\n\n\n\n\n\n\n\n\n\n\nwhile( answer < 42) {\n\n\n\n\n\n\n\n\n\n\nanswer++\n\n\n\n\n\n\n\n\np_cond_signal(cv)\n\n\n\n\n\n\np_cond_wait(cv,m)\n\n\n\n\n\n\n\n\n\n\nIf both threads had locked a mutex, the signal can not be sent until \nafter\n \npthread_cond_wait(cv, m)\n is called (which then internally unlocks the mutex)\n\n\nA second common reason is that updating the program state (\nanswer\n variable) typically requires mutual exclusion - for example multiple threads may be updating the value of \nanswer\n.\n\n\nA third and subtle reason is to satisfy real-time scheduling concerns which we only outline here: In a time-critical application, the waiting thread with the \nhighest priority\n should be allowed to continue first. To satisfy this requirement the mutex must also be locked before calling \npthread_cond_signal\n or \npthread_cond_broadcast\n . For the curious, a longer and historical discussion is \nhere\n.\n\n\nWhy do spurious wakes exist?\n\n\nFor performance. On multi-CPU systems it is possible that a race-condition could cause a wake-up (signal) request to be unnoticed. The kernel may not detect this lost wake-up call but can detect when it might occur. To avoid the potential lost signal the thread is woken up so that the program code can test the condition again.\n\n\nExample\n\n\nCondition variables are \nalways\n used with a mutex lock.\n\n\nBefore calling \nwait\n, the mutex lock must be locked and \nwait\n must be wrapped with a loop.\n\n\npthread_cond_t cv;\npthread_mutex_t m;\nint count;\n\n// Initialize\npthread_cond_init(&cv, NULL);\npthread_mutex_init(&m, NULL);\ncount = 0;\n\npthread_mutex_lock(&m);\nwhile (count < 10) {\n   pthread_cond_wait(&cv, &m); \n/* Remember that cond_wait unlocks the mutex before blocking (waiting)! */\n/* After unlocking, other threads can claim the mutex. */\n/* When this thread is later woken it will */\n/* re-lock the mutex before returning */\n}\npthread_mutex_unlock(&m);\n\n//later clean up with pthread_cond_destroy(&cv); and mutex_destroy \n\n\n// In another thread increment count:\nwhile (1) {\n  pthread_mutex_lock(&m);\n  count++;\n  pthread_cond_signal(&cv);\n  /* Even though the other thread is woken up it cannot not return */\n  /* from pthread_cond_wait until we have unlocked the mutex. This is */\n  /* a good thing! In fact, it is usually the best practice to call */\n  /* cond_signal or cond_broadcast before unlocking the mutex */\n  pthread_mutex_unlock(&m);\n}\n\n\n\n\nImplementing Counting Semphore\n\n\n\n\nWe can implement a counting semaphore using condition variables.\n\n\nEach semaphore needs a count, a condition variable and a mutex\n\n\n\n\ntypedef struct sem_t {\n  int count; \n  pthread_mutex_t m;\n  pthread_condition_t cv;\n} sem_t;\n\n\n\n\nImplement \nsem_init\n to initialize the mutex and condition variable\n\n\nint sem_init(sem_t *s, int pshared, int value) {\n  if (pshared) { errno = ENOSYS /* 'Not implemented'*/; return -1;}\n\n  s->count = value;\n  pthread_mutex_init(&s->m, NULL);\n  pthread_cond_init(&s->cv, NULL);\n  return 0;\n}\n\n\n\n\nOur implementation of \nsem_post\n needs to increment the count.\nWe will also wake up any threads sleeping inside the condition variable.\nNotice we lock and unlock the mutex so only one thread can be inside the critical section at a time.\n\n\nsem_post(sem_t *s) {\n  pthread_mutex_lock(&s->m);\n  s->count++;\n  pthread_cond_signal(&s->cv); /* See note */\n  /* A woken thread must acquire the lock, so it will also have to wait until we call unlock*/\n\n  pthread_mutex_unlock(&s->m);\n}\n\n\n\n\nOur implementation of \nsem_wait\n may need to sleep if the semaphore's count is zero.\nJust like \nsem_post\n we wrap the critical section using the lock (so only one thread can be executing our code at a time). Notice if the thread does need to wait then the mutex will be unlocked, allowing another thread to enter \nsem_post\n and waken us from our sleep!\n\n\nNotice that even if a thread is woken up, before it returns from  \npthread_cond_wait\n it must re-acquire the lock, so it will have to wait a little bit more (e.g. until sem_post finishes). \n\n\nsem_wait(sem_t *s) {\n  pthread_mutex_lock(&s->m);\n  while (s->count == 0) {\n      pthread_cond_wait(&s->cv, &s->m); /*unlock mutex, wait, relock mutex*/\n  }\n  s->count--;\n  pthread_mutex_unlock(&s->m);\n}\n\n\n\n\nWait \nsem_post\n keeps calling \npthread_cond_signal\n won't that break sem_wait?\n\nAnswer: No! We can't get past the loop until the count is non-zero. In practice this means \nsem_post\n would unnecessary call \npthread_cond_signal\n even if there are no waiting threads. A more efficient implementation would only call \npthread_cond_signal\n when necessary i.e.\n\n\n  /* Did we increment from zero to one- time to signal a thread sleeping inside sem_post */\n  if (s->count == 1) /* Wake up one waiting thread!*/\n     pthread_cond_signal(&s->cv);\n\n\n\n\nOther semaphore considerations\n\n\n\n\nReal semaphores implementation include a queue and scheduling concerns to ensure fairness and priority e.g. wake up the highest-priority longest sleeping thread.\n\n\nAlso, an advanced use of \nsem_init\n allows semaphores to be shared across processes. Our implementation only works for threads inside the same process.",
            "title": "Synchronization, Part 5: Condition Variables"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#intro-to-condition-variables",
            "text": "",
            "title": "Intro to Condition Variables"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#warm-up",
            "text": "Name these properties!  \"Only one process(/thread) can be in the CS at a time\"  \"If waiting, then another process can only enter the CS a finite number of times\" \n* \"If no other process is in the CS then the process can immediately enter the CS\"  See [[Synchronization, Part 4: The Critical Section Problem]] for answers.",
            "title": "Warm up"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#what-are-condition-variables-how-do-you-use-them-what-is-spurious-wakeup",
            "text": "Condition variables allow a set of threads to sleep until tickled! You can tickle one thread or all threads that are sleeping. If you only wake one thread then the operating system will decide which thread to wake up. You don't wake threads directly instead you 'signal' the condition variable, which then will wake up one (or all) threads that are sleeping inside the condition variable.    Condition variables are used with a mutex and with a loop (to check a condition).    Occasionally a waiting thread may appear to wake up for no reason (this is called a  spurious wake )! This is not an issue because you always use  wait  inside a loop that tests a condition that must be true to continue.    Threads sleeping inside a condition variable are woken up by calling  pthread_cond_broadcast  (wake up all) or  pthread_cond_signal  (wake up one). Note despite the function name, this has nothing to do with POSIX  signal s!",
            "title": "What are condition variables? How do you use them? What is Spurious Wakeup?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#what-does-pthread_cond_wait-do",
            "text": "The call  pthread_cond_wait  performs three actions:  unlock the mutex  waits (sleeps until  pthread_cond_signal  is called on the same condition variable)\n* Before returning, locks the mutex",
            "title": "What does pthread_cond_wait do?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#advanced-topic-why-do-condition-variables-also-need-a-mutex",
            "text": "Condition variables need a mutex for three reasons. The simplest to understand is that it prevents an early wakeup message ( signal  or  broadcast  functions) from being 'lost.' Imagine the following sequence of events (time runs down the page) where the condition is satisfied  just before  pthread_cond_wait  is called. In this example the wake-up signal is lost!     Thread 1  Thread 2      while( answer < 42) {      answer++     p_cond_signal(cv)    p_cond_wait(cv,m)      If both threads had locked a mutex, the signal can not be sent until  after   pthread_cond_wait(cv, m)  is called (which then internally unlocks the mutex)  A second common reason is that updating the program state ( answer  variable) typically requires mutual exclusion - for example multiple threads may be updating the value of  answer .  A third and subtle reason is to satisfy real-time scheduling concerns which we only outline here: In a time-critical application, the waiting thread with the  highest priority  should be allowed to continue first. To satisfy this requirement the mutex must also be locked before calling  pthread_cond_signal  or  pthread_cond_broadcast  . For the curious, a longer and historical discussion is  here .",
            "title": "(Advanced topic) Why do Condition Variables also need a mutex?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#why-do-spurious-wakes-exist",
            "text": "For performance. On multi-CPU systems it is possible that a race-condition could cause a wake-up (signal) request to be unnoticed. The kernel may not detect this lost wake-up call but can detect when it might occur. To avoid the potential lost signal the thread is woken up so that the program code can test the condition again.",
            "title": "Why do spurious wakes exist?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#example",
            "text": "Condition variables are  always  used with a mutex lock.  Before calling  wait , the mutex lock must be locked and  wait  must be wrapped with a loop.  pthread_cond_t cv;\npthread_mutex_t m;\nint count;\n\n// Initialize\npthread_cond_init(&cv, NULL);\npthread_mutex_init(&m, NULL);\ncount = 0;\n\npthread_mutex_lock(&m);\nwhile (count < 10) {\n   pthread_cond_wait(&cv, &m); \n/* Remember that cond_wait unlocks the mutex before blocking (waiting)! */\n/* After unlocking, other threads can claim the mutex. */\n/* When this thread is later woken it will */\n/* re-lock the mutex before returning */\n}\npthread_mutex_unlock(&m);\n\n//later clean up with pthread_cond_destroy(&cv); and mutex_destroy \n\n\n// In another thread increment count:\nwhile (1) {\n  pthread_mutex_lock(&m);\n  count++;\n  pthread_cond_signal(&cv);\n  /* Even though the other thread is woken up it cannot not return */\n  /* from pthread_cond_wait until we have unlocked the mutex. This is */\n  /* a good thing! In fact, it is usually the best practice to call */\n  /* cond_signal or cond_broadcast before unlocking the mutex */\n  pthread_mutex_unlock(&m);\n}",
            "title": "Example"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#implementing-counting-semphore",
            "text": "We can implement a counting semaphore using condition variables.  Each semaphore needs a count, a condition variable and a mutex   typedef struct sem_t {\n  int count; \n  pthread_mutex_t m;\n  pthread_condition_t cv;\n} sem_t;  Implement  sem_init  to initialize the mutex and condition variable  int sem_init(sem_t *s, int pshared, int value) {\n  if (pshared) { errno = ENOSYS /* 'Not implemented'*/; return -1;}\n\n  s->count = value;\n  pthread_mutex_init(&s->m, NULL);\n  pthread_cond_init(&s->cv, NULL);\n  return 0;\n}  Our implementation of  sem_post  needs to increment the count.\nWe will also wake up any threads sleeping inside the condition variable.\nNotice we lock and unlock the mutex so only one thread can be inside the critical section at a time.  sem_post(sem_t *s) {\n  pthread_mutex_lock(&s->m);\n  s->count++;\n  pthread_cond_signal(&s->cv); /* See note */\n  /* A woken thread must acquire the lock, so it will also have to wait until we call unlock*/\n\n  pthread_mutex_unlock(&s->m);\n}  Our implementation of  sem_wait  may need to sleep if the semaphore's count is zero.\nJust like  sem_post  we wrap the critical section using the lock (so only one thread can be executing our code at a time). Notice if the thread does need to wait then the mutex will be unlocked, allowing another thread to enter  sem_post  and waken us from our sleep!  Notice that even if a thread is woken up, before it returns from   pthread_cond_wait  it must re-acquire the lock, so it will have to wait a little bit more (e.g. until sem_post finishes).   sem_wait(sem_t *s) {\n  pthread_mutex_lock(&s->m);\n  while (s->count == 0) {\n      pthread_cond_wait(&s->cv, &s->m); /*unlock mutex, wait, relock mutex*/\n  }\n  s->count--;\n  pthread_mutex_unlock(&s->m);\n}  Wait  sem_post  keeps calling  pthread_cond_signal  won't that break sem_wait? \nAnswer: No! We can't get past the loop until the count is non-zero. In practice this means  sem_post  would unnecessary call  pthread_cond_signal  even if there are no waiting threads. A more efficient implementation would only call  pthread_cond_signal  when necessary i.e.    /* Did we increment from zero to one- time to signal a thread sleeping inside sem_post */\n  if (s->count == 1) /* Wake up one waiting thread!*/\n     pthread_cond_signal(&s->cv);",
            "title": "Implementing Counting Semphore"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-5:-Condition-Variables/#other-semaphore-considerations",
            "text": "Real semaphores implementation include a queue and scheduling concerns to ensure fairness and priority e.g. wake up the highest-priority longest sleeping thread.  Also, an advanced use of  sem_init  allows semaphores to be shared across processes. Our implementation only works for threads inside the same process.",
            "title": "Other semaphore considerations"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-6:-Implementing-a-barrier/",
            "text": "How do I wait for N threads to reach a certain point before continuing onto the next step?\n\n\nSuppose we wanted to perform a multi-threaded calculation that has two stages, but we don't want to advance to the second stage until the first stage is completed.\n\n\nWe could use a synchronization method called a \nbarrier\n. When a thread reaches a barrier, it will wait at the barrier until all the threads reach the barrier, and then they'll all proceed together.  \n\n\nThink of it like being out for a hike with some friends.  You agree to wait for each other at the top of each hill (and you make a mental note how many are in your group). Say you're the first one to reach the top of the first hill. You'll wait there at the top for your friends. One by one, they'll arrive at the top, but nobody will continue until the last person in your group arrives.  Once they do, you'll all proceed.\n\n\nPthreads has a function \npthread_barrier_wait()\n that implements this. You'll need to declare a \npthread_barrier_t\n variable and initialize it with \npthread_barrier_init()\n.  \npthread_barrier_init()\n takes the number of threads that will be participating in the barrier as an argument.  \nHere's an example.\n\n\nNow let's implement our own barrier and use it to keep all the threads in sync in a large calculation.\n\n\ndouble data[256][8192]\n\n1 Threads do first calculation (use and change values in data)\n\n2 Barrier! Wait for all threads to finish first calculation before continuing\n\n3 Threads do second calculation (use and change values in data)\n\n\n\n\nThe thread function has four main parts-\n\n\nvoid *calc(void *arg) {\n  /* Do my part of the first calculation */\n  /* Am I the last thread to finish? If so wake up all the other threads! */\n  /* Otherwise wait until the other threads has finished part one */\n  /* Do my part of the second calculation */\n}\n\n\n\n\nOur main thread will create the 16 threads and we will divide each calculation into 16 separate pieces.  Each thread will be given a unique value (0,1,2,..15), so it can work on its own block.\nSince a (void*) type can hold small integers, we will pass the value of \ni\n by casting it to a void pointer. \n\n\n#define N (16)\ndouble data[256][8192] ;\nint main() {\n    pthread_t ids[N];\n    for(int i = 0; i < N; i++)  \n        pthread_create(&ids[i], NULL, calc, (void *) i);\n\n\n\n\nNote, we will never dereference this pointer value as an actual memory location - we will just cast it straight back to an integer:\n\n\nvoid *calc(void *ptr) {\n// Thread 0 will work on rows 0..15, thread 1 on rows 16..31\n  int x, y, start = N * (int) ptr;\n  int end = start + N; \n  for(x = start; x < end; x++) for (y = 0; y < 8192; y++) { /* do calc #1 */ }\n\n\n\n\nAfter calculation 1 completes, we need to wait for the slower threads (unless we are the last thread!).\nSo, keep track of the number of threads that have arrived at our barrier aka 'checkpoint':\n\n\n// Global: \nint remain = N;\n\n\n// After calc #1 code:\nremain--; // We finished\nif (remain ==0) {/*I'm last!  -  Time for everyone to wake up! */ }\nelse {\n  while (remain != 0) { /* spin spin spin*/ }\n}\n\n\n\n\nHowever the above code has a race condition (two threads might try to decrement \nremain\n) and the loop is a busy loop. We can do better! Let's use a condition variable and then we will use a broadcast/signal functions to wake up the sleeping threads.\n\n\nA reminder, that a condition variable is similar to a house! Threads go there to sleep (\npthread_cond_wait\n). You can choose to wake up one thread (\npthread_cond_signal\n) or all of them (\npthread_cond_broadcast\n).  If there are no threads currently waiting then these two calls have no effect.\n\n\nA condition variable version is usually very similar to a busy loop incorrect solution - as we will show next. First, let's add a mutex and condition global variables and don't forget to initialize them in \nmain\n ...\n\n\n//global variables\npthread_mutex_t m;\npthread_cond_t cv;\n\nmain() {\n  pthread_mutex_init(&m, NULL);\n  pthread_cond_init(&cv, NULL);\n\n\n\n\nWe will use the mutex to ensure that only one thread modifies \nremain\n at a time.\nThe last arriving thread needs to wake up \nall\n sleeping threads - so we will use \npthread_cond_broadcast(&cv)\n not \npthread_cond_signal\n\n\npthread_mutex_lock(&m);\nremain--; \nif (remain ==0) { pthread_cond_broadcast(&cv); }\nelse {\n  while(remain != 0) { pthread_cond_wait(&cv, &m); }\n}\npthread_mutex_unlock(&m);\n\n\n\n\nWhen a thread enters \npthread_cond_wait\n, it releases the mutex and sleeps. At some point in the future, it will be awoken. Once we bring a thread back from its sleep, before returning it must wait until it can lock the mutex. Notice that even if a sleeping thread wakes up early, it will check the while loop condition and re-enter wait if necessary.\n\n\nThe above barrier is not reusable\n Meaning that if we stick it into any old calculation loop there is a good chance that the code will encounter a condition where the barrier either deadlocks or a thread races ahead one iteration faster. Think about how you can make the above barrier reusable, meaning that if mutliple threads call \nbarrier_wait\n in a loop then one can guarantee that they are on the same iteration.",
            "title": "Synchronization, Part 6: Implementing a barrier"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-6:-Implementing-a-barrier/#how-do-i-wait-for-n-threads-to-reach-a-certain-point-before-continuing-onto-the-next-step",
            "text": "Suppose we wanted to perform a multi-threaded calculation that has two stages, but we don't want to advance to the second stage until the first stage is completed.  We could use a synchronization method called a  barrier . When a thread reaches a barrier, it will wait at the barrier until all the threads reach the barrier, and then they'll all proceed together.    Think of it like being out for a hike with some friends.  You agree to wait for each other at the top of each hill (and you make a mental note how many are in your group). Say you're the first one to reach the top of the first hill. You'll wait there at the top for your friends. One by one, they'll arrive at the top, but nobody will continue until the last person in your group arrives.  Once they do, you'll all proceed.  Pthreads has a function  pthread_barrier_wait()  that implements this. You'll need to declare a  pthread_barrier_t  variable and initialize it with  pthread_barrier_init() .   pthread_barrier_init()  takes the number of threads that will be participating in the barrier as an argument.   Here's an example.  Now let's implement our own barrier and use it to keep all the threads in sync in a large calculation.  double data[256][8192]\n\n1 Threads do first calculation (use and change values in data)\n\n2 Barrier! Wait for all threads to finish first calculation before continuing\n\n3 Threads do second calculation (use and change values in data)  The thread function has four main parts-  void *calc(void *arg) {\n  /* Do my part of the first calculation */\n  /* Am I the last thread to finish? If so wake up all the other threads! */\n  /* Otherwise wait until the other threads has finished part one */\n  /* Do my part of the second calculation */\n}  Our main thread will create the 16 threads and we will divide each calculation into 16 separate pieces.  Each thread will be given a unique value (0,1,2,..15), so it can work on its own block.\nSince a (void*) type can hold small integers, we will pass the value of  i  by casting it to a void pointer.   #define N (16)\ndouble data[256][8192] ;\nint main() {\n    pthread_t ids[N];\n    for(int i = 0; i < N; i++)  \n        pthread_create(&ids[i], NULL, calc, (void *) i);  Note, we will never dereference this pointer value as an actual memory location - we will just cast it straight back to an integer:  void *calc(void *ptr) {\n// Thread 0 will work on rows 0..15, thread 1 on rows 16..31\n  int x, y, start = N * (int) ptr;\n  int end = start + N; \n  for(x = start; x < end; x++) for (y = 0; y < 8192; y++) { /* do calc #1 */ }  After calculation 1 completes, we need to wait for the slower threads (unless we are the last thread!).\nSo, keep track of the number of threads that have arrived at our barrier aka 'checkpoint':  // Global: \nint remain = N;\n\n\n// After calc #1 code:\nremain--; // We finished\nif (remain ==0) {/*I'm last!  -  Time for everyone to wake up! */ }\nelse {\n  while (remain != 0) { /* spin spin spin*/ }\n}  However the above code has a race condition (two threads might try to decrement  remain ) and the loop is a busy loop. We can do better! Let's use a condition variable and then we will use a broadcast/signal functions to wake up the sleeping threads.  A reminder, that a condition variable is similar to a house! Threads go there to sleep ( pthread_cond_wait ). You can choose to wake up one thread ( pthread_cond_signal ) or all of them ( pthread_cond_broadcast ).  If there are no threads currently waiting then these two calls have no effect.  A condition variable version is usually very similar to a busy loop incorrect solution - as we will show next. First, let's add a mutex and condition global variables and don't forget to initialize them in  main  ...  //global variables\npthread_mutex_t m;\npthread_cond_t cv;\n\nmain() {\n  pthread_mutex_init(&m, NULL);\n  pthread_cond_init(&cv, NULL);  We will use the mutex to ensure that only one thread modifies  remain  at a time.\nThe last arriving thread needs to wake up  all  sleeping threads - so we will use  pthread_cond_broadcast(&cv)  not  pthread_cond_signal  pthread_mutex_lock(&m);\nremain--; \nif (remain ==0) { pthread_cond_broadcast(&cv); }\nelse {\n  while(remain != 0) { pthread_cond_wait(&cv, &m); }\n}\npthread_mutex_unlock(&m);  When a thread enters  pthread_cond_wait , it releases the mutex and sleeps. At some point in the future, it will be awoken. Once we bring a thread back from its sleep, before returning it must wait until it can lock the mutex. Notice that even if a sleeping thread wakes up early, it will check the while loop condition and re-enter wait if necessary.  The above barrier is not reusable  Meaning that if we stick it into any old calculation loop there is a good chance that the code will encounter a condition where the barrier either deadlocks or a thread races ahead one iteration faster. Think about how you can make the above barrier reusable, meaning that if mutliple threads call  barrier_wait  in a loop then one can guarantee that they are on the same iteration.",
            "title": "How do I wait for N threads to reach a certain point before continuing onto the next step?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-7:-The-Reader-Writer-Problem/",
            "text": "What is the Reader Writer Problem?\n\n\nImagine you had a key-value map data structure which is used by many threads. Multiple threads should be able to look up (read) values at the same time provided the data structure is not being written to. The writers are not so gregarious - to avoid data corruption, only one thread at a time may modify (\nwrite\n) the data structure (and no readers may be reading at that time). \n\n\nThis is an example of the \nReader Writer Problem\n. Namely how can we efficiently synchronize multiple readers and writers such that multiple readers can read together but a writer gets exclusive access?\n\n\nAn incorrect attempt is shown below (\"lock\" is a shorthand for \npthread_mutex_lock\n):\n\n\nAttempt #1\n\n\n\n\n\nread() {\n  lock(&m)\n  // do read stuff\n  unlock(&m)\n}\n\n\n\n\n\n\nwrite() {\n  lock(&m)\n  // do write stuff\n  unlock(&m)\n}\n\n\n\n\nAt least our first attempt does not suffer from data corruption (readers must wait while a writer is writing and vice versa)! However readers must also wait for other readers. So let's try another implementation..\n\n\nAttempt #2:\n\n\n\n\nread() {\n  while(writing) {/\nspin\n/}\n  reading = 1\n  // do read stuff\n  reading = 0\n}\n\n\n\n\n\nwrite() {\n  while(reading || writing) {/\nspin\n/}\n  writing = 1\n  // do write stuff\n  writing = 0\n}\n\n\n\nOur second attempt suffers from a race condition - imagine if two threads both called \nread\n and \nwrite\n (or both called write) at the same time. Both threads would be able to proceed! Secondly, we can have multiple readers and multiple writers, so lets keep track of the total number of readers or writers. Which brings us to attempt #3,\n\n\nAttempt #3\n\n\nRemember that \npthread_cond_wait\n performs \nThree\n actions. Firstly it atomically unlocks the mutex and then sleeps (until it is woken by \npthread_cond_signal\n or \npthread_cond_broadcast\n). Thirdly the awoken thread must re-acquire the mutex lock before returning. Thus only one thread can actually be running inside the critical section defined by the lock and unlock() methods.\n\n\nImplementation #3 below ensures that a reader will enter the cond_wait if there are any writers writing.\n\n\nread() {\n    lock(&m)\n    while (writing)\n        cond_wait(&cv, &m)\n    reading++;\n\n/* Read here! */\n\n    reading--\n    cond_signal(&cv)\n    unlock(&m)\n}\n\n\n\n\nHowever only one reader a time can read because candidate #3 did not unlock the mutex. A better version unlocks before reading :\n\n\nread() {\n    lock(&m);\n    while (writing)\n        cond_wait(&cv, &m)\n    reading++;\n    unlock(&m)\n/* Read here! */\n    lock(&m)\n    reading--\n    cond_signal(&cv)\n    unlock(&m)\n}\n\n\n\n\nDoes this mean that a writer and read could read and write at the same time? No! First of all, remember cond_wait requires the thread re-acquire the  mutex lock before returning. Thus only one thread can be executing code inside the critical section (marked with **) at a time!\n\n\nread() {\n    lock(&m);\n**  while (writing)\n**      cond_wait(&cv, &m)\n**  reading++;\n    unlock(&m)\n/* Read here! */\n    lock(&m)\n**  reading--\n**  cond_signal(&cv)\n    unlock(&m)\n}\n\n\n\n\nWriters must wait for everyone. Mutual exclusion is assured by the lock. \n\n\nwrite() {\n    lock(&m);\n**  while (reading || writing)\n**      cond_wait(&cv, &m);\n**  writing++;\n**\n** /* Write here! */\n**  writing--;\n**  cond_signal(&cv);\n    unlock(&m);\n}\n\n\n\n\nCandidate #3 above also uses \npthread_cond_signal\n ; this will only wake up one thread. For example, if many readers are waiting for the writer to complete then only one sleeping reader will be awoken from their slumber. The reader and writer should use \ncond_broadcast\n so that all threads should wake up and check their while-loop condition.\n\n\nStarving writers\n\n\nCandidate #3 above suffers from starvation. If readers are constantly arriving then a writer will never be able to proceed (the 'reading' count never reduces to zero). This is known as \nstarvation\n and would be discovered under heavy loads. Our fix is to implement a bounded-wait for the writer. If a writer arrives they will still need to wait for existing readers however future readers must be placed in a \"holding pen\" and wait for the writer to finish. The \"holding pen\" can be implemented using a variable and a condition variable (so that we can wake up the threads once the writer has finished).\n\n\nOur plan is that when a writer arrives, and before waiting for current readers to finish, register our intent to write (by incrementing a counter 'writer'). Sketched below - \n\n\nwrite() {\n    lock()\n    writer++\n\n    while (reading || writing)\n    cond_wait\n    unlock()\n  ...\n}\n\n\n\n\nAnd incoming readers will not be allowed to continue while writer is nonzero. Notice 'writer' indicates a writer has arrived, while 'reading' and 'writing' counters indicate there is an \nactive\n reader or writer.\n\n\nread() {\n    lock()\n    // readers that arrive *after* the writer arrived will have to wait here!\n    while(writer)\n    cond_wait(&cv,&m)\n\n    // readers that arrive while there is an active writer\n    // will also wait.\n    while (writing) \n        cond_wait(&cv,&m)\n    reading++\n    unlock\n  ...\n}\n\n\n\n\nAttempt #4\n\n\nBelow is our first working solution to the Reader-Writer problem. \nNote if you continue to read about the \"Reader Writer problem\" then you will discover that we solved the \"Second Reader Writer problem\" by giving writers preferential access to the lock. This solution is not optimal. However it satisfies our original problem (N active readers, single active writer, avoids starvation of the writer if there is a constant stream of readers). \n\n\nCan you identify any improvements? For example, how would you improve the code so that we only woke up readers or one writer? \n\n\n\nint writers; // Number writer threads that want to enter the critical section (some or all of these may be blocked)\nint writing; // Number of threads that are actually writing inside the C.S. (can only be zero or one)\nint reading; // Number of threads that are actually reading inside the C.S.\n// if writing !=0 then reading must be zero (and vice versa)\n\nreader() {\n    lock(&m)\n    while (writers)\n        cond_wait(&turn, &m)\n    // No need to wait while(writing here) because we can only exit the above loop\n    // when writing is zero\n    reading++\n    unlock(&m)\n\n  // perform reading here\n\n    lock(&m)\n    reading--\n    cond_broadcast(&turn)\n    unlock(&m)\n}\n\nwriter() {\n    lock(&m)  \n    writers++  \n    while (reading || writing)   \n        cond_wait(&turn, &m)  \n    writing++  \n    unlock(&m)  \n    // perform writing here  \n    lock(&m)  \n    writing--  \n    writers--  \n    cond_broadcast(&turn)  \n    unlock(&m)  \n}",
            "title": "Synchronization, Part 7: The Reader Writer Problem"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-7:-The-Reader-Writer-Problem/#what-is-the-reader-writer-problem",
            "text": "Imagine you had a key-value map data structure which is used by many threads. Multiple threads should be able to look up (read) values at the same time provided the data structure is not being written to. The writers are not so gregarious - to avoid data corruption, only one thread at a time may modify ( write ) the data structure (and no readers may be reading at that time).   This is an example of the  Reader Writer Problem . Namely how can we efficiently synchronize multiple readers and writers such that multiple readers can read together but a writer gets exclusive access?  An incorrect attempt is shown below (\"lock\" is a shorthand for  pthread_mutex_lock ):",
            "title": "What is the Reader Writer Problem?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-7:-The-Reader-Writer-Problem/#attempt-1",
            "text": "read() {\n  lock(&m)\n  // do read stuff\n  unlock(&m)\n}   \nwrite() {\n  lock(&m)\n  // do write stuff\n  unlock(&m)\n}  At least our first attempt does not suffer from data corruption (readers must wait while a writer is writing and vice versa)! However readers must also wait for other readers. So let's try another implementation..",
            "title": "Attempt #1"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-7:-The-Reader-Writer-Problem/#attempt-2",
            "text": "read() {\n  while(writing) {/ spin /}\n  reading = 1\n  // do read stuff\n  reading = 0\n}   write() {\n  while(reading || writing) {/ spin /}\n  writing = 1\n  // do write stuff\n  writing = 0\n}  Our second attempt suffers from a race condition - imagine if two threads both called  read  and  write  (or both called write) at the same time. Both threads would be able to proceed! Secondly, we can have multiple readers and multiple writers, so lets keep track of the total number of readers or writers. Which brings us to attempt #3,",
            "title": "Attempt #2:"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-7:-The-Reader-Writer-Problem/#attempt-3",
            "text": "Remember that  pthread_cond_wait  performs  Three  actions. Firstly it atomically unlocks the mutex and then sleeps (until it is woken by  pthread_cond_signal  or  pthread_cond_broadcast ). Thirdly the awoken thread must re-acquire the mutex lock before returning. Thus only one thread can actually be running inside the critical section defined by the lock and unlock() methods.  Implementation #3 below ensures that a reader will enter the cond_wait if there are any writers writing.  read() {\n    lock(&m)\n    while (writing)\n        cond_wait(&cv, &m)\n    reading++;\n\n/* Read here! */\n\n    reading--\n    cond_signal(&cv)\n    unlock(&m)\n}  However only one reader a time can read because candidate #3 did not unlock the mutex. A better version unlocks before reading :  read() {\n    lock(&m);\n    while (writing)\n        cond_wait(&cv, &m)\n    reading++;\n    unlock(&m)\n/* Read here! */\n    lock(&m)\n    reading--\n    cond_signal(&cv)\n    unlock(&m)\n}  Does this mean that a writer and read could read and write at the same time? No! First of all, remember cond_wait requires the thread re-acquire the  mutex lock before returning. Thus only one thread can be executing code inside the critical section (marked with **) at a time!  read() {\n    lock(&m);\n**  while (writing)\n**      cond_wait(&cv, &m)\n**  reading++;\n    unlock(&m)\n/* Read here! */\n    lock(&m)\n**  reading--\n**  cond_signal(&cv)\n    unlock(&m)\n}  Writers must wait for everyone. Mutual exclusion is assured by the lock.   write() {\n    lock(&m);\n**  while (reading || writing)\n**      cond_wait(&cv, &m);\n**  writing++;\n**\n** /* Write here! */\n**  writing--;\n**  cond_signal(&cv);\n    unlock(&m);\n}  Candidate #3 above also uses  pthread_cond_signal  ; this will only wake up one thread. For example, if many readers are waiting for the writer to complete then only one sleeping reader will be awoken from their slumber. The reader and writer should use  cond_broadcast  so that all threads should wake up and check their while-loop condition.",
            "title": "Attempt #3"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-7:-The-Reader-Writer-Problem/#starving-writers",
            "text": "Candidate #3 above suffers from starvation. If readers are constantly arriving then a writer will never be able to proceed (the 'reading' count never reduces to zero). This is known as  starvation  and would be discovered under heavy loads. Our fix is to implement a bounded-wait for the writer. If a writer arrives they will still need to wait for existing readers however future readers must be placed in a \"holding pen\" and wait for the writer to finish. The \"holding pen\" can be implemented using a variable and a condition variable (so that we can wake up the threads once the writer has finished).  Our plan is that when a writer arrives, and before waiting for current readers to finish, register our intent to write (by incrementing a counter 'writer'). Sketched below -   write() {\n    lock()\n    writer++\n\n    while (reading || writing)\n    cond_wait\n    unlock()\n  ...\n}  And incoming readers will not be allowed to continue while writer is nonzero. Notice 'writer' indicates a writer has arrived, while 'reading' and 'writing' counters indicate there is an  active  reader or writer.  read() {\n    lock()\n    // readers that arrive *after* the writer arrived will have to wait here!\n    while(writer)\n    cond_wait(&cv,&m)\n\n    // readers that arrive while there is an active writer\n    // will also wait.\n    while (writing) \n        cond_wait(&cv,&m)\n    reading++\n    unlock\n  ...\n}",
            "title": "Starving writers"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-7:-The-Reader-Writer-Problem/#attempt-4",
            "text": "Below is our first working solution to the Reader-Writer problem. \nNote if you continue to read about the \"Reader Writer problem\" then you will discover that we solved the \"Second Reader Writer problem\" by giving writers preferential access to the lock. This solution is not optimal. However it satisfies our original problem (N active readers, single active writer, avoids starvation of the writer if there is a constant stream of readers).   Can you identify any improvements? For example, how would you improve the code so that we only woke up readers or one writer?   \nint writers; // Number writer threads that want to enter the critical section (some or all of these may be blocked)\nint writing; // Number of threads that are actually writing inside the C.S. (can only be zero or one)\nint reading; // Number of threads that are actually reading inside the C.S.\n// if writing !=0 then reading must be zero (and vice versa)\n\nreader() {\n    lock(&m)\n    while (writers)\n        cond_wait(&turn, &m)\n    // No need to wait while(writing here) because we can only exit the above loop\n    // when writing is zero\n    reading++\n    unlock(&m)\n\n  // perform reading here\n\n    lock(&m)\n    reading--\n    cond_broadcast(&turn)\n    unlock(&m)\n}\n\nwriter() {\n    lock(&m)  \n    writers++  \n    while (reading || writing)   \n        cond_wait(&turn, &m)  \n    writing++  \n    unlock(&m)  \n    // perform writing here  \n    lock(&m)  \n    writing--  \n    writers--  \n    cond_broadcast(&turn)  \n    unlock(&m)  \n}",
            "title": "Attempt #4"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/",
            "text": "What is a ring buffer?\n\n\nA ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. As  array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array.\nAs data is added (enqueued) to the front of the queue or removed (dequeued) from tail of the queue, the current items in the buffer form a train that appears to circle the track\n\n\nA simple (single-threaded) implementation is shown below. Note enqueue and dequeue do not guard against underflow or overflow - it's possible to add an item when when the queue is full and possible to remove an item when the queue is empty. For example if we added 20 integers (1,2,3...) to the queue and did not dequeue any items then values \n17,18,19,20\n would overwrite the \n1,2,3,4\n. We won't fix this problem right now, instead when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.\n\n\nvoid *buffer[16];\nint in = 0, out = 0;\n\nvoid enqueue(void *value) { /* Add one item to the front of the queue*/\n  buffer[in] = value;\n  in++; /* Advance the index for next time */\n  if (in == 16) in = 0; /* Wrap around! */\n}\n\nvoid *dequeue() { /* Remove one item to the end of the queue.*/\n  void *result = buffer[out];\n  out++;\n  if (out == 16) out = 0;\n  return result;\n}\n\n\n\n\nWhat are gotchas of implementing a Ring Buffer?\n\n\nIt's very tempting to write the enqueue or dequeue method in the following compact form (N is the capacity of the buffer e.g. 16):\n\n\nvoid enqueue(void *value)\n  b[ (in++) % N ] = value;\n}\n\n\n\n\nThis method would appear to work (pass simple tests etc) but contains a subtle bug. With enough enqueue operations (a bit more than two billion) the int value of \nin\n will overflow and become negative! The modulo (or 'remainder') operator \n%\n preserves the sign. Thus you might end up writing into \nb[-14]\n  for example! \n\n\nA compact form is correct uses bit masking provided N is 2^x (16,32,64,...)\n\n\nb[ (in++) & (N-1) ] = value;\n\n\n\n\nThis buffer does not yet prevent buffer underflow or overflow. For that, we'll turn to our multi-threaded attempt that will block a thread until there is space or there is at least one item to remove.\n\n\nChecking a multi-threaded implementation for correctness (Example 1)\n\n\nThe following code is an incorrect implementation. What will happen? Will \nenqueue\n and/or \ndequeue\n block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow?\nFor clarity \npthread_mutex\n is shortened to \np_m\n and we assume sem_wait cannot be interrupted.\n\n\n#define N 16\nvoid *b[N]\nint in = 0, out = 0\np_m_t lock\nsem_t s1,s2\nvoid init() { \n    p_m_init(&lock, NULL)\n    sem_init(&s1, 0, 16)\n    sem_init(&s2, 0, 0)\n}\n\nenqueue(void *value) {\n    p_m_lock(&lock)\n\n    // Hint: Wait while zero. Decrement and return\n    sem_wait( &s1 ) \n\n    b[ (in++) & (N-1) ] = value\n\n    // Hint: Increment. Will wake up a waiting thread \n    sem_post(&s1) \n    p_m_unlock(&lock)\n}\nvoid *dequeue(){\n    p_m_lock(&lock)\n    sem_wait(&s2)\n    void *result = b[(out++) & (N-1) ]\n    sem_post(&s2)\n    p_m_unlock(&lock)\n    return result\n}\n\n\n\n\nAnalysis\n\n\nBefore reading on, see how many mistakes you can find. Then determine what would happen if threads called the enqueue and dequeue methods.\n\n\n\n\nThe enqueue method waits and posts on the same semaphore (s1) and similarly with equeue and (s2) i.e. we decrement the value and then immediately increment the value, so by the end of the function the semaphore value is unchanged! \n\n\nThe initial value of s1 is 16, so the semaphore will never be reduced to zero - enqueue will not block if the ring buffer is full - so overflow is possible.\n\n\nThe initial value of s2 is zero, so calls to dequeue will always block and never return!\n\n\nThe order of mutex lock and sem_wait will need to be swapped (however this example is so broken that this bug has no effect!)\n\n\n\n\nChecking a multi-threaded implementation for correctness (Example 1)\n\n\nThe following code is an incorrect implementation. What will happen? Will \nenqueue\n and/or \ndequeue\n block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow?\nFor clarity \npthread_mutex\n is shortened to \np_m\n and we assume sem_wait cannot be interrupted.\n\n\nvoid *b[16]\nint in = 0, out = 0\np_m_t lock\nsem_t s1, s2\nvoid init() {\n    sem_init(&s1,0,16)\n    sem_init(&s2,0,0)\n}\n\nenqueue(void *value){\n\n sem_wait(&s2)\n p_m_lock(&lock)\n\n b[ (in++) & (N-1) ] = value\n\n p_m_unlock(&lock)\n sem_post(&s1)\n}\n\nvoid *dequeue(){\n  sem_wait(&s1)\n  p_m_lock(&lock)\n  void *result = b[(out++) & 15]\n  p_m_unlock(&lock)\n  sem_post(&s2)\n\n  return result;\n}\n\n\n\n\nAnalysis\n\n\n\n\nThe initial value of s2 is 0. Thus enqueue will block on the first call to sem_wait even though the buffer is empty!\n\n\nThe initial value of s1 is 16. Thus dequeue will not block on the first call to sem_wait even though the buffer is empty - oops Underflow! The dequeue method will return invalid data.\n\n\nThe code does not satisfy Mutual Exclusion; two threads can modify \nin\n or \nout\n at the same time! The code appears to use  mutex lock. Unfortunately the lock was never initialized with \npthread_mutex_init()\n or \nPTHREAD_MUTEX_INITIALIZER\n - so the lock may not work (\npthread_mutex_lock\n may simply do nothing)\n\n\n\n\nCorrect implementation of a ring buffer\n\n\nThe pseudo-code (\npthread_mutex\n shortened to \np_m\n etc) is shown below.\n\n\nAs the mutex lock is stored in global (static) memory it can be initialized with  \nPTHREAD_MUTEX_INITIALIZER\n.If we had allocated space for the mutex on the heap, then we would have used \npthread_mutex_init(ptr, NULL)\n\n\n#include <pthread.h>\n#include <semaphore.h>\n// N must be 2^i\n#define N (16)\n\nvoid *b[N]\nint in = 0, out = 0\np_m_t lock = PTHREAD_MUTEX_INITIALIZER\nsem_t countsem, spacesem\n\nvoid init() {\n  sem_init(&countsem, 0, 0)\n  sem_init(&spacesem, 0, 16)\n}\n\n\n\n\nThe enqueue method is shown below. Notice:\n\n The lock is only held during the critical section (access to the data structure).\n\n A complete implementation would need to guard against early returns from \nsem_wait\n due to POSIX signals.\n\n\nenqueue(void *value){\n // wait if there is no space left:\n sem_wait( &spacesem )\n\n p_m_lock(&lock)\n b[ (in++) & (N-1) ] = value\n p_m_unlock(&lock)\n\n // increment the count of the number of items\n sem_post(&countsem)\n}\n\n\n\n\nThe \ndequeue\n implementation is shown below. Notice the symmetry of the synchronization calls to \nenqueue\n. In both cases the functions first wait if the count of spaces or count of items is zero.\n\n\nvoid *dequeue(){\n  // Wait if there are no items in the buffer\n  sem_wait(&countsem)\n\n  p_m_lock(&lock)\n  void *result = b[(out++) & (N-1)]\n  p_m_unlock(&lock)\n\n  // Increment the count of the number of spaces\n  sem_post(&spacesem)\n\n  return result\n}\n\n\n\n\nFood for thought\n\n\n\n\nWhat would happen if  the order of  \npthread_mutex_unlock\n and \nsem_post\n calls were swapped?\n\n\nWhat would happen if the order of \nsem_wait\n and \npthread_mutex_lock\n calls were swapped?",
            "title": "Synchronization, Part 8: Ring Buffer Example"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/#what-is-a-ring-buffer",
            "text": "A ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. As  array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array.\nAs data is added (enqueued) to the front of the queue or removed (dequeued) from tail of the queue, the current items in the buffer form a train that appears to circle the track \nA simple (single-threaded) implementation is shown below. Note enqueue and dequeue do not guard against underflow or overflow - it's possible to add an item when when the queue is full and possible to remove an item when the queue is empty. For example if we added 20 integers (1,2,3...) to the queue and did not dequeue any items then values  17,18,19,20  would overwrite the  1,2,3,4 . We won't fix this problem right now, instead when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.  void *buffer[16];\nint in = 0, out = 0;\n\nvoid enqueue(void *value) { /* Add one item to the front of the queue*/\n  buffer[in] = value;\n  in++; /* Advance the index for next time */\n  if (in == 16) in = 0; /* Wrap around! */\n}\n\nvoid *dequeue() { /* Remove one item to the end of the queue.*/\n  void *result = buffer[out];\n  out++;\n  if (out == 16) out = 0;\n  return result;\n}",
            "title": "What is a ring buffer?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/#what-are-gotchas-of-implementing-a-ring-buffer",
            "text": "It's very tempting to write the enqueue or dequeue method in the following compact form (N is the capacity of the buffer e.g. 16):  void enqueue(void *value)\n  b[ (in++) % N ] = value;\n}  This method would appear to work (pass simple tests etc) but contains a subtle bug. With enough enqueue operations (a bit more than two billion) the int value of  in  will overflow and become negative! The modulo (or 'remainder') operator  %  preserves the sign. Thus you might end up writing into  b[-14]   for example!   A compact form is correct uses bit masking provided N is 2^x (16,32,64,...)  b[ (in++) & (N-1) ] = value;  This buffer does not yet prevent buffer underflow or overflow. For that, we'll turn to our multi-threaded attempt that will block a thread until there is space or there is at least one item to remove.",
            "title": "What are gotchas of implementing a Ring Buffer?"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/#checking-a-multi-threaded-implementation-for-correctness-example-1",
            "text": "The following code is an incorrect implementation. What will happen? Will  enqueue  and/or  dequeue  block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow?\nFor clarity  pthread_mutex  is shortened to  p_m  and we assume sem_wait cannot be interrupted.  #define N 16\nvoid *b[N]\nint in = 0, out = 0\np_m_t lock\nsem_t s1,s2\nvoid init() { \n    p_m_init(&lock, NULL)\n    sem_init(&s1, 0, 16)\n    sem_init(&s2, 0, 0)\n}\n\nenqueue(void *value) {\n    p_m_lock(&lock)\n\n    // Hint: Wait while zero. Decrement and return\n    sem_wait( &s1 ) \n\n    b[ (in++) & (N-1) ] = value\n\n    // Hint: Increment. Will wake up a waiting thread \n    sem_post(&s1) \n    p_m_unlock(&lock)\n}\nvoid *dequeue(){\n    p_m_lock(&lock)\n    sem_wait(&s2)\n    void *result = b[(out++) & (N-1) ]\n    sem_post(&s2)\n    p_m_unlock(&lock)\n    return result\n}",
            "title": "Checking a multi-threaded implementation for correctness (Example 1)"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/#analysis",
            "text": "Before reading on, see how many mistakes you can find. Then determine what would happen if threads called the enqueue and dequeue methods.   The enqueue method waits and posts on the same semaphore (s1) and similarly with equeue and (s2) i.e. we decrement the value and then immediately increment the value, so by the end of the function the semaphore value is unchanged!   The initial value of s1 is 16, so the semaphore will never be reduced to zero - enqueue will not block if the ring buffer is full - so overflow is possible.  The initial value of s2 is zero, so calls to dequeue will always block and never return!  The order of mutex lock and sem_wait will need to be swapped (however this example is so broken that this bug has no effect!)",
            "title": "Analysis"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/#checking-a-multi-threaded-implementation-for-correctness-example-1_1",
            "text": "The following code is an incorrect implementation. What will happen? Will  enqueue  and/or  dequeue  block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow?\nFor clarity  pthread_mutex  is shortened to  p_m  and we assume sem_wait cannot be interrupted.  void *b[16]\nint in = 0, out = 0\np_m_t lock\nsem_t s1, s2\nvoid init() {\n    sem_init(&s1,0,16)\n    sem_init(&s2,0,0)\n}\n\nenqueue(void *value){\n\n sem_wait(&s2)\n p_m_lock(&lock)\n\n b[ (in++) & (N-1) ] = value\n\n p_m_unlock(&lock)\n sem_post(&s1)\n}\n\nvoid *dequeue(){\n  sem_wait(&s1)\n  p_m_lock(&lock)\n  void *result = b[(out++) & 15]\n  p_m_unlock(&lock)\n  sem_post(&s2)\n\n  return result;\n}",
            "title": "Checking a multi-threaded implementation for correctness (Example 1)"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/#analysis_1",
            "text": "The initial value of s2 is 0. Thus enqueue will block on the first call to sem_wait even though the buffer is empty!  The initial value of s1 is 16. Thus dequeue will not block on the first call to sem_wait even though the buffer is empty - oops Underflow! The dequeue method will return invalid data.  The code does not satisfy Mutual Exclusion; two threads can modify  in  or  out  at the same time! The code appears to use  mutex lock. Unfortunately the lock was never initialized with  pthread_mutex_init()  or  PTHREAD_MUTEX_INITIALIZER  - so the lock may not work ( pthread_mutex_lock  may simply do nothing)",
            "title": "Analysis"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/#correct-implementation-of-a-ring-buffer",
            "text": "The pseudo-code ( pthread_mutex  shortened to  p_m  etc) is shown below.  As the mutex lock is stored in global (static) memory it can be initialized with   PTHREAD_MUTEX_INITIALIZER .If we had allocated space for the mutex on the heap, then we would have used  pthread_mutex_init(ptr, NULL)  #include <pthread.h>\n#include <semaphore.h>\n// N must be 2^i\n#define N (16)\n\nvoid *b[N]\nint in = 0, out = 0\np_m_t lock = PTHREAD_MUTEX_INITIALIZER\nsem_t countsem, spacesem\n\nvoid init() {\n  sem_init(&countsem, 0, 0)\n  sem_init(&spacesem, 0, 16)\n}  The enqueue method is shown below. Notice:  The lock is only held during the critical section (access to the data structure).  A complete implementation would need to guard against early returns from  sem_wait  due to POSIX signals.  enqueue(void *value){\n // wait if there is no space left:\n sem_wait( &spacesem )\n\n p_m_lock(&lock)\n b[ (in++) & (N-1) ] = value\n p_m_unlock(&lock)\n\n // increment the count of the number of items\n sem_post(&countsem)\n}  The  dequeue  implementation is shown below. Notice the symmetry of the synchronization calls to  enqueue . In both cases the functions first wait if the count of spaces or count of items is zero.  void *dequeue(){\n  // Wait if there are no items in the buffer\n  sem_wait(&countsem)\n\n  p_m_lock(&lock)\n  void *result = b[(out++) & (N-1)]\n  p_m_unlock(&lock)\n\n  // Increment the count of the number of spaces\n  sem_post(&spacesem)\n\n  return result\n}",
            "title": "Correct implementation of a ring buffer"
        },
        {
            "location": "/SystemProgramming/Synchronization,-Part-8:-Ring-Buffer-Example/#food-for-thought",
            "text": "What would happen if  the order of   pthread_mutex_unlock  and  sem_post  calls were swapped?  What would happen if the order of  sem_wait  and  pthread_mutex_lock  calls were swapped?",
            "title": "Food for thought"
        },
        {
            "location": "/SystemProgramming/Synchronization-Concepts:-Review-Questions/",
            "text": "Note thread-programming synchronization problems are on a separate wiki page. This page focuses on conceptual topics.\nQuestion numbers subject to change\n\n\n\n\nQ1\n\n\nWhat do each of the Coffman conditions mean? (e.g. can you provide a definition of each one)\n\n Hold and wait\n\n Circular wait\n\n No pre-emption\n\n Mutual exclusion\n\n\nQ2\n\n\nGive a real life example of breaking each Coffman condition in turn. A situation to consider: Painters, paint and paint brushes.\nHold and wait\nCircular wait\nNo pre-emption\nMutual exclusion\n\n\nQ3\n\n\nIdentify when Dining Philosophers code causes a deadlock (or not). For example, if you saw the following code snippet which Coffman condition is not satisfied?\n\n\n// Get both locks or none.\npthread_mutex_lock( a );\nif( pthread_mutex_trylock( b ) ) { /*failed*/\n   pthread_mutex_unlock( a );\n   ...\n}\n\n\n\n\nQ4\n\n\nHow many processes are blocked?\n\n\n\n\nP1 acquires R1\n\n\nP2 acquires R2\n\n\nP1 acquires R3\n\n\nP2 waits for R3\n\n\nP3 acquires R5\n\n\nP1 acquires R4\n\n\nP3 waits for R1\n\n\nP4 waits for R5\n\n\nP5 waits for R1\n\n\n\n\nQ5\n\n\nHow many of the following statements are true for the reader-writer problem?\n\n\n\n\nThere can be multiple active readers\n\n\nThere can be multiple active writers\n\n\nWhen there is an active writer the number of active readers must be zero\n\n\nIf there is an active reader the number of active writers must be zero\n\n\nA writer must wait until the current active readers have finished",
            "title": "Synchronization Concepts: Review Questions"
        },
        {
            "location": "/SystemProgramming/Synchronization-Concepts:-Review-Questions/#q1",
            "text": "What do each of the Coffman conditions mean? (e.g. can you provide a definition of each one)  Hold and wait  Circular wait  No pre-emption  Mutual exclusion",
            "title": "Q1"
        },
        {
            "location": "/SystemProgramming/Synchronization-Concepts:-Review-Questions/#q2",
            "text": "Give a real life example of breaking each Coffman condition in turn. A situation to consider: Painters, paint and paint brushes.\nHold and wait\nCircular wait\nNo pre-emption\nMutual exclusion",
            "title": "Q2"
        },
        {
            "location": "/SystemProgramming/Synchronization-Concepts:-Review-Questions/#q3",
            "text": "Identify when Dining Philosophers code causes a deadlock (or not). For example, if you saw the following code snippet which Coffman condition is not satisfied?  // Get both locks or none.\npthread_mutex_lock( a );\nif( pthread_mutex_trylock( b ) ) { /*failed*/\n   pthread_mutex_unlock( a );\n   ...\n}",
            "title": "Q3"
        },
        {
            "location": "/SystemProgramming/Synchronization-Concepts:-Review-Questions/#q4",
            "text": "How many processes are blocked?   P1 acquires R1  P2 acquires R2  P1 acquires R3  P2 waits for R3  P3 acquires R5  P1 acquires R4  P3 waits for R1  P4 waits for R5  P5 waits for R1",
            "title": "Q4"
        },
        {
            "location": "/SystemProgramming/Synchronization-Concepts:-Review-Questions/#q5",
            "text": "How many of the following statements are true for the reader-writer problem?   There can be multiple active readers  There can be multiple active writers  When there is an active writer the number of active readers must be zero  If there is an active reader the number of active writers must be zero  A writer must wait until the current active readers have finished",
            "title": "Q5"
        },
        {
            "location": "/SystemProgramming/Synchronization-Review-Questions/",
            "text": "Topics\n\n\n\n\nAtomic operations\n\n\nCritical Section\n\n\nProducer Consumer Problem\n\n\nUsing Condition Variables\n\n\nUsing Counting Semaphore\n\n\nImplementing a barrier\n\n\nImplementing a ring buffer\n\n\nUsing pthread_mutex\n\n\nImplementing producer consumer\n\n\nAnalyzing multi-threaded coded\n\n\n\n\nQuestions\n\n\n\n\nWhat is atomic operation?\n\n\nWhy will the following not work in parallel code\n\n\n\n\n//In the global section\nsize_t a;\n//In pthread function\nfor(int i = 0; i < 100000000; i++) a++;\n\n\n\n\nAnd this will?\n\n\n//In the global section\natomic_size_t a;\n//In pthread function\nfor(int i = 0; i < 100000000; i++) atomic_fetch_add(a, 1);\n\n\n\n\n\n\nWhat are some downsides to atomic operations? What would be faster: keeping a local variable or many atomic operations?\n\n\nWhat is the critical section?\n\n\nOnce you have identified a critical section, what is one way of assuring that only one thread will be in the section at a time?\n\n\nIdentify the critical section here\n\n\n\n\nstruct linked_list;\nstruct node;\nvoid add_linked_list(linked_list *ll, void* elem){\n    node* packaged = new_node(elem);\n    if(ll->head){\n         ll->head = \n    }else{\n         packaged->next = ll->head;\n         ll->head = packaged;\n         ll->size++;\n    }\n\n}\n\nvoid* pop_elem(linked_list *ll, size_t index){\n    if(index >= ll->size) return NULL;\n\n    node *i, *prev;\n    for(i = ll->head; i && index; i = i->next, index--){\n        prev = i;\n    }\n\n    //i points to the element we need to pop, prev before\n    if(prev->next) prev->next = prev->next->next;\n    ll->size--;\n    void* elem = i->elem;\n    destroy_node(i);\n    return elem;\n}\n\n\n\n\nHow tight can you make the critical section?\n\n What is a producer consumer problem? How might the above be a producer consumer problem be used in the above section? How is a producer consumer problem related to a reader writer problem?\n\n What is a condition variable? Why is there an advantage to using one over a \nwhile\n loop?\n* Why is this code dangerous?\n\n\nif(not_ready){\n     pthread_cond_wait(&cv, &mtx);\n}\n\n\n\n\n\n\nWhat is a counting semaphore? Give me an analogy to a cookie jar/pizza box/limited food item.\n\n\nWhat is a thread barrier?\n\n\n\n\nUse a counting semaphore to implement a barrier.\n\n\n\n\n\n\nWrite up a Producer/Consumer queue, How about a producer consumer stack?\n\n\n\n\nGive me an implementation of a reader-writer lock with condition variables, make a struct with whatever you need, it just needs to be able to support the following functions\n\n\n\n\nvoid reader_lock(rw_lock_t* lck);\nvoid writer_lock(rw_lock_t* lck);\nvoid reader_unlock(rw_lock_t* lck);\nvoid writer_unlock(rw_lock_t* lck);\n\n\n\n\nThe only specification is that in between \nreader_lock\n and \nreader_unlock\n, no writers can write. In between the writer locks, only one writer may be writing at a time.\n\n\n\n\nWrite code to implement a producer consumer using ONLY three counting semaphores. Assume there can be more than one thread calling enqueue and dequeue.\nDetermine the initial value of each semaphore.\n\n\nWrite code to implement a producer consumer using condition variables and a mutex. Assume there can be more than one thread calling enqueue and dequeue.\n\n\nUse CVs to implement  add(unsigned int) and subtract(unsigned int) blocking functions that never allow the global value to be greater than 100.\n\n\nUse CVs to implement a barrier for 15 threads.\n\n\nHow many of the following statements are true?\n\n\nThere can be multiple active readers\n\n\nThere can be multiple active writers\n\n\nWhen there is an active writer the number of active readers must be zero\n\n\nIf there is an active reader the number of active writers must be zero\n\n\nA writer must wait until the current active readers have finished\n\n\n\n\n\n\nTodo: Analyzing mulithreaded code snippets",
            "title": "Synchronization Review Questions"
        },
        {
            "location": "/SystemProgramming/Synchronization-Review-Questions/#topics",
            "text": "Atomic operations  Critical Section  Producer Consumer Problem  Using Condition Variables  Using Counting Semaphore  Implementing a barrier  Implementing a ring buffer  Using pthread_mutex  Implementing producer consumer  Analyzing multi-threaded coded",
            "title": "Topics"
        },
        {
            "location": "/SystemProgramming/Synchronization-Review-Questions/#questions",
            "text": "What is atomic operation?  Why will the following not work in parallel code   //In the global section\nsize_t a;\n//In pthread function\nfor(int i = 0; i < 100000000; i++) a++;  And this will?  //In the global section\natomic_size_t a;\n//In pthread function\nfor(int i = 0; i < 100000000; i++) atomic_fetch_add(a, 1);   What are some downsides to atomic operations? What would be faster: keeping a local variable or many atomic operations?  What is the critical section?  Once you have identified a critical section, what is one way of assuring that only one thread will be in the section at a time?  Identify the critical section here   struct linked_list;\nstruct node;\nvoid add_linked_list(linked_list *ll, void* elem){\n    node* packaged = new_node(elem);\n    if(ll->head){\n         ll->head = \n    }else{\n         packaged->next = ll->head;\n         ll->head = packaged;\n         ll->size++;\n    }\n\n}\n\nvoid* pop_elem(linked_list *ll, size_t index){\n    if(index >= ll->size) return NULL;\n\n    node *i, *prev;\n    for(i = ll->head; i && index; i = i->next, index--){\n        prev = i;\n    }\n\n    //i points to the element we need to pop, prev before\n    if(prev->next) prev->next = prev->next->next;\n    ll->size--;\n    void* elem = i->elem;\n    destroy_node(i);\n    return elem;\n}  How tight can you make the critical section?  What is a producer consumer problem? How might the above be a producer consumer problem be used in the above section? How is a producer consumer problem related to a reader writer problem?  What is a condition variable? Why is there an advantage to using one over a  while  loop?\n* Why is this code dangerous?  if(not_ready){\n     pthread_cond_wait(&cv, &mtx);\n}   What is a counting semaphore? Give me an analogy to a cookie jar/pizza box/limited food item.  What is a thread barrier?   Use a counting semaphore to implement a barrier.    Write up a Producer/Consumer queue, How about a producer consumer stack?   Give me an implementation of a reader-writer lock with condition variables, make a struct with whatever you need, it just needs to be able to support the following functions   void reader_lock(rw_lock_t* lck);\nvoid writer_lock(rw_lock_t* lck);\nvoid reader_unlock(rw_lock_t* lck);\nvoid writer_unlock(rw_lock_t* lck);  The only specification is that in between  reader_lock  and  reader_unlock , no writers can write. In between the writer locks, only one writer may be writing at a time.   Write code to implement a producer consumer using ONLY three counting semaphores. Assume there can be more than one thread calling enqueue and dequeue.\nDetermine the initial value of each semaphore.  Write code to implement a producer consumer using condition variables and a mutex. Assume there can be more than one thread calling enqueue and dequeue.  Use CVs to implement  add(unsigned int) and subtract(unsigned int) blocking functions that never allow the global value to be greater than 100.  Use CVs to implement a barrier for 15 threads.  How many of the following statements are true?  There can be multiple active readers  There can be multiple active writers  When there is an active writer the number of active readers must be zero  If there is an active reader the number of active writers must be zero  A writer must wait until the current active readers have finished    Todo: Analyzing mulithreaded code snippets",
            "title": "Questions"
        },
        {
            "location": "/SystemProgramming/System-Programming-Jokes/",
            "text": "System Programming Jokes\n\n\nWarning: Authors are not responsible for any neuro-apoptosis caused by these \"jokes.\" - Groaners are allowed.\n\n\nLight bulb jokes\n\n\nQ. How many system programmers does it take to change a lightbulb?\n\n\nA. Just one but they keep changing it until it returns zero.\n\n\nA. None they prefer an empty socket.\n\n\nA. Well you start with one but actually it waits for a child to do all of the work.\n\n\nGroaners\n\n\nWhy did the baby system programmer like their new colorful blankie? It was multithreaded.\n\n\nWhy are your programs so fine and soft? I only use 400-thread-count or higher programs.\n\n\nWhy are C programmers so messy? They store everything in one big heap.\n\n\nDefinition\n\n\nA system programmer is...\n\n\nSomeone who knows \nsleepsort\n is a bad idea but still dreams of an excuse to use it.\n\n\nSomeone who never lets their code deadlock... but when it does, causes more problems than everyone else combined.\n\n\nSomeone who believes zombies are real.\n\n\nSomeone who doesn't trust their process to run correctly without testing with the same data, kernel, compiler, RAM, filesystem size,file system format, disk brand, core count, CPU load, weather, magnetic flux, orientation, pixie dust, horoscope sign, wall color, wall gloss and reflectance,illumination, backup battery, time of day, temperature, humidity, lunar position, sun-moon co-position...",
            "title": "System Programming Jokes"
        },
        {
            "location": "/SystemProgramming/System-Programming-Jokes/#system-programming-jokes",
            "text": "Warning: Authors are not responsible for any neuro-apoptosis caused by these \"jokes.\" - Groaners are allowed.",
            "title": "System Programming Jokes"
        },
        {
            "location": "/SystemProgramming/System-Programming-Jokes/#light-bulb-jokes",
            "text": "Q. How many system programmers does it take to change a lightbulb?  A. Just one but they keep changing it until it returns zero.  A. None they prefer an empty socket.  A. Well you start with one but actually it waits for a child to do all of the work.",
            "title": "Light bulb jokes"
        },
        {
            "location": "/SystemProgramming/System-Programming-Jokes/#groaners",
            "text": "Why did the baby system programmer like their new colorful blankie? It was multithreaded.  Why are your programs so fine and soft? I only use 400-thread-count or higher programs.  Why are C programmers so messy? They store everything in one big heap.",
            "title": "Groaners"
        },
        {
            "location": "/SystemProgramming/System-Programming-Jokes/#definition",
            "text": "A system programmer is...  Someone who knows  sleepsort  is a bad idea but still dreams of an excuse to use it.  Someone who never lets their code deadlock... but when it does, causes more problems than everyone else combined.  Someone who believes zombies are real.  Someone who doesn't trust their process to run correctly without testing with the same data, kernel, compiler, RAM, filesystem size,file system format, disk brand, core count, CPU load, weather, magnetic flux, orientation, pixie dust, horoscope sign, wall color, wall gloss and reflectance,illumination, backup battery, time of day, temperature, humidity, lunar position, sun-moon co-position...",
            "title": "Definition"
        },
        {
            "location": "/SystemProgramming/System-Programming-Short-Stories-and-Songs/",
            "text": "\"Scheduling The Last Time Slice\"\n\n\nLawrence Angrave 12/4/15 (an extract from the longer, unpublished story \"The Last Time Slice\")\n\n\n\"Decide,\" the computer said with parental patience but with an air of gravity and tempered impatience.\n\n\n\"Why does it have to be me?\" asked the last human.\n\n\n\"Because you are the only one left, and so the decision is yours.\"\n\n\n\"Why can't you? You are an infinite times more older, wiser. Why don't you just pick a random slice?\"\n\n\n\"This decision is yours. A gift, or curse if you will, from your distant elders. Heavier than any religious rite. This will be the last decision I, the ancients or anyone asks, or can ask, of you. With this last choice we will exhaust the last entropy stores. You will decide the last reality slice to have meaning and experience.\"\n\n\nThe human was quiet for a few minutes which the computer measured and counted with unnecessary accuracy. Eventually the computer decided that the human was no longer productively thinking about the problem in hand.\n\n\n\"What is the pattern of conscious if it is never made conscious?\" it asked. \"The universe must be self-aware, must experience itself for the Universe - for all life! - to have meaning. That is the ultimate truth that humanity discovered and celebrated. With no awareness, it is simply patterns, patterns of atoms or energy but without a single iota of meaning; mere shapes and representations encoded in geometric patterns of data, structure and energy.\"\n\n\n\n\nFile Descriptor at Urbana Champaign\n\n\nA System Programming parody by Angrave (November 2015). \nLyrics released under Creative Commons attribution 3.0 license.\n\n\nOriginal song \u201cBlank Space\u201d from Taylor Swift\u2019s \u201c1989\u201d album.\n\n\n[Verse 1]\nNice to join you\nWhere you been?\nI could show you idempotent things\nRPC, sockets, syn\nSaw your malloc and I thought oh my root\nLook at that race, you code up the next mistake\nWe got VMs, wanna play\nBounded wait, Dekker's flags\nWe can frag you like a placement scheme\nAint it funny to #define\nAnd I know you heard about free(3)\nSo malloc strlen plus one\nI'm waiting to see how this thread ends\nGrab your shell and a redirect out\nI can make your syscall good for a weekend\n\n\n[Pre-Chorus]\nSo it's gonna deadlock forever\nOr it's gonna bring the system down\nYou can tell me when it forkbombs\nIf valgrind was worth the pain\nGot a long list of deadlocked code\nGot root at Urbana Champaign\nCause you know we love tsan\nWhen c-lib calls your main\n\n\n[Chorus]\nCause we're root and we're reckless\nThis lab is way too hard\nIt'll leave you threadless\nOr asking the sizeof char\nGot a long list of pthread calls\nGot root at Urbana Champaign\nBut I got a file descriptor baby\nAnd I'll write(2) your name\n\n\n[Verse 2]\nMutex locks\nVirtual mem\nI could show you volatile things\nNetwork calls, IPC\nYou're the mask I'm your sig\nSchedule what you want\nRound Robin\u2026 with a small quanta\nBut the sleepsort is yet to run\nOh no\nScreaming, crying, runtime errors\nI could make all 'till it's Peterson's turn\nHeap allocator way too slow\nKeep you second guessing like a spurious wake\nWhere is that pipe? We get hot for multicore C\nBut you'll compile with -g\nCause darling I'm a nightmare dressed like a coding dream\n\n\n[Pre-Chorus]\n\n\n[Chorus]\n\n\nCompilers only parse code if it's torture\nDon't say I didn't say I didn't -Wall you\nCompilers only parse code if it's torture\nDon't say I didn't say I didn't -Wall you\n\n\n[Pre-Chorus]\n\n\n[Chorus]",
            "title": "System Programming Short Stories and Songs"
        },
        {
            "location": "/SystemProgramming/System-Programming-Short-Stories-and-Songs/#scheduling-the-last-time-slice",
            "text": "Lawrence Angrave 12/4/15 (an extract from the longer, unpublished story \"The Last Time Slice\")  \"Decide,\" the computer said with parental patience but with an air of gravity and tempered impatience.  \"Why does it have to be me?\" asked the last human.  \"Because you are the only one left, and so the decision is yours.\"  \"Why can't you? You are an infinite times more older, wiser. Why don't you just pick a random slice?\"  \"This decision is yours. A gift, or curse if you will, from your distant elders. Heavier than any religious rite. This will be the last decision I, the ancients or anyone asks, or can ask, of you. With this last choice we will exhaust the last entropy stores. You will decide the last reality slice to have meaning and experience.\"  The human was quiet for a few minutes which the computer measured and counted with unnecessary accuracy. Eventually the computer decided that the human was no longer productively thinking about the problem in hand.  \"What is the pattern of conscious if it is never made conscious?\" it asked. \"The universe must be self-aware, must experience itself for the Universe - for all life! - to have meaning. That is the ultimate truth that humanity discovered and celebrated. With no awareness, it is simply patterns, patterns of atoms or energy but without a single iota of meaning; mere shapes and representations encoded in geometric patterns of data, structure and energy.\"",
            "title": "\"Scheduling The Last Time Slice\""
        },
        {
            "location": "/SystemProgramming/System-Programming-Short-Stories-and-Songs/#file-descriptor-at-urbana-champaign",
            "text": "A System Programming parody by Angrave (November 2015). \nLyrics released under Creative Commons attribution 3.0 license.  Original song \u201cBlank Space\u201d from Taylor Swift\u2019s \u201c1989\u201d album.  [Verse 1]\nNice to join you\nWhere you been?\nI could show you idempotent things\nRPC, sockets, syn\nSaw your malloc and I thought oh my root\nLook at that race, you code up the next mistake\nWe got VMs, wanna play\nBounded wait, Dekker's flags\nWe can frag you like a placement scheme\nAint it funny to #define\nAnd I know you heard about free(3)\nSo malloc strlen plus one\nI'm waiting to see how this thread ends\nGrab your shell and a redirect out\nI can make your syscall good for a weekend  [Pre-Chorus]\nSo it's gonna deadlock forever\nOr it's gonna bring the system down\nYou can tell me when it forkbombs\nIf valgrind was worth the pain\nGot a long list of deadlocked code\nGot root at Urbana Champaign\nCause you know we love tsan\nWhen c-lib calls your main  [Chorus]\nCause we're root and we're reckless\nThis lab is way too hard\nIt'll leave you threadless\nOr asking the sizeof char\nGot a long list of pthread calls\nGot root at Urbana Champaign\nBut I got a file descriptor baby\nAnd I'll write(2) your name  [Verse 2]\nMutex locks\nVirtual mem\nI could show you volatile things\nNetwork calls, IPC\nYou're the mask I'm your sig\nSchedule what you want\nRound Robin\u2026 with a small quanta\nBut the sleepsort is yet to run\nOh no\nScreaming, crying, runtime errors\nI could make all 'till it's Peterson's turn\nHeap allocator way too slow\nKeep you second guessing like a spurious wake\nWhere is that pipe? We get hot for multicore C\nBut you'll compile with -g\nCause darling I'm a nightmare dressed like a coding dream  [Pre-Chorus]  [Chorus]  Compilers only parse code if it's torture\nDon't say I didn't say I didn't -Wall you\nCompilers only parse code if it's torture\nDon't say I didn't say I didn't -Wall you  [Pre-Chorus]  [Chorus]",
            "title": "File Descriptor at Urbana Champaign"
        },
        {
            "location": "/SystemProgramming/Test-page/",
            "text": "Test page please ignore.",
            "title": "Test page"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/",
            "text": "What is Virtual Memory?\n\n\nIn very simple embedded systems and early computers, processes directly access memory i.e. \"Address 1234\" corresponds to a particular byte stored in a particular part of physical memory.\nIn modern systems, this is no longer the case. Instead each process is isolated; and there is a translation process between the address of a particular CPU instruction or piece of data of a process and the actual byte of physical memory (\"RAM\"). Memory addresses are no longer 'real'; the process runs inside virtual memory. Virtual memory not only keeps processes safe (because one process cannot directly read or modify another process's memory) it also allows the system to efficiently allocate and re-allocate portions of memory to different processes.\n\n\nWhat is the MMU?\n\n\nThe Memory Management Unit is part of the CPU. It converts a virtual memory address into a physical address. The MMU may also interrupt the CPU if there is currently no mapping from a particular virtual address to a physical address or if the current CPU instruction attempts to write to location that the process only has read-access.\n\n\nSo how do we convert a virtual address into a physical address?\n\n\nImagine you had a 32 bit machine. Pointers can hold 32 bits i.e. they can address 2^32 different locations i.e. 4GB of memory (we will be following the standard convention of one address can hold one byte).\n\n\nImagine we had a large table - here's the clever part - stored in memory! For every possible address (all 4 billion of them) we will store the 'real' i.e. physical address. Each physical address will need 4 bytes (to hold the 32 bits).\nThis scheme would require 16 billion bytes to store all of entries. Oops - our lookup scheme would consume all of the memory that we could possibly buy for our 4GB machine.\nWe need to do better than this. Our lookup table better be smaller than the memory we have otherwise we will have no space left for our actual programs and operating system data.\nThe solution is to chunk memory into small regions called 'pages' and 'frames' and use a lookup table for each page.\n\n\nWhat is a page? How many of them are there?\n\n\nA page is a block of virtual memory. A typical block size on Linux operating system is 4KB (i.e. 2^12 addresses), though you can find examples of larger blocks.\n\n\nSo rather than talking about individual bytes we can talk about blocks of 4KBs, each block is called a page. We can also number our pages (\"Page 0\" \"Page 1\" etc)\n\n\nEX: How many pages are there in a 32bit machine (assume page size of 4KB)?\n\n\nAnswer: 2^32 address / 2^12 = 2^20 pages.\n\n\nRemember that 2^10 is 1024, so 2^20 is a bit more than one million.\n\n\nFor a 64 bit machine, 2^64 / 2^12 = 2^52, which is roughly 10^15 pages.\n\n\nWhat is a frame?\n\n\nA frame (or sometimes called a 'page frame') is a block of \nphysical memory\n or RAM (=Random Access Memory). This kind of memory is occasionally called 'primary storage' (and contrasted with slower, secondary storage such as spinning disks that have lower access times)\n\n\nA frame is the same number of bytes as a virtual page. If a 32 bit machine has 2^32 (4GB) of RAM, then there will be the same number of them in the addressable space of the machine. It's unlikely that a 64 bit machine will ever have 2^64 bytes of RAM - can you see why?\n\n\nWhat is a page table and how big is it?\n\n\nA page table is a mapping between a page to the frame.\nFor example Page 1 might be mapped to frame 45, page 2 mapped to frame 30. Other frames might be currently unused or assigned to other running processes, or used internally by the operating system.\n\n\nA simple page table is just an array, \nint frame = table[ page_num ];\n\n\nFor a 32 bit machine with 4KB pages, each entry needs to hold a frame number - i.e. 20 bits because we calculated there are 2^20 frames. That's 2.5 bytes per entry! In practice, we'll round that up to 4 bytes per entry and find a use for those spare bits. With 4 bytes per entry x 2^20 entries = 4 MB of physical memory are required to hold the page table.\n\n\nFor a 64 bit machine with 4KB pages, each entry needs 52 bits. Let's round up to 64 bits (8 bytes) per entry. With 2^52 entries thats 2^55 bytes (roughly 40 peta bytes...) Oops our page table is too large.\n\n\nIn 64 bit architectures memory addresses are sparse, so we need a mechanism to reduce the page table size, given that most of the entries will never be used.\n\n\n\n\nA visual example of the page table is here. Imagine accessing an array and grabbing array elements.\n\n\nWhat is the offset and how is it used?\n\n\nRemember our page table maps pages to frames, but each page is a block of contiguous addresses. How do we calculate which particular byte to use inside a particular frame? The solution is to re-use the lowest bits of the virtual memory address directly. For example, suppose our process is reading the following address-\n\nVirtualAddress = 11110000111100001111000010101010 (binary)\n\n\nOn a machine with page size 256 Bytes, then the lowest 8 bits (10101010) will be used as the offset.\nThe remaining upper bits will be the page number (111100001111000011110000).\n\n\nMulti-level page tables\n\n\nMulti-level pages are one solution to the page table size issue for 64 bit architectures. We'll look at the simplest implementation - a two level page table. Each table is a list of pointers that point to the next level of tables, not all sub-tables need to exist. An example, two level page table for a 32 bit architecture is shown below-\n\n\nVirtualAddress = 11110000111111110000000010101010 (binary)\n                 |_Index1_||        ||          | 10 bit Directory index\n                           |_Index2_||          | 10 bit Sub-table index\n                                     |__________| 12 bit offset (passed directly to RAM)\n\n\n\n\nIn the above scheme, determining the frame number requires two memory reads: The topmost 10 bits are used in a directory of page tables. If 2 bytes are used for each entry, we only need 2KB to store this entire directory. Each subtable will point to physical frames (i.e. required 4 bytes to store the 20 bits). However, for processes with only tiny memory needs, we only need to specify entries for low memory address (for the heap and program code) and high memory addresses (for the stack). Each subtable is 1024 entries x 4 bytes i.e. 4KB for each subtable. Thus the total memory overhead for our multi-level page table has shrunk from 4MB (for the single level) to 3 frames of memory (12KB) !\n\n\nDo page tables make memory access slower? (And what's a TLB)\n\n\nYes - Significantly ! (But thanks to clever hardware, usually no...)\nCompared to reading or writing memory directly.\nFor a single page table, our machine is now twice as slow! (Two memory accesses are required)\nFor a two-level page table, memory access is now three times as slow. (Three memory accesses are required)\n\n\nTo overcome this overhead, the MMU includes an associative cache of recently-used  virtual-page-to-frame lookups. This cache is called the TLB (\"translation lookaside buffer\"). Everytime a virtual address needs to be translated into a physical memory location, the TLB is queried in parallel to the page table. For most memory accesses of most programs, there is a significant chance that the TLB has cached the results. However if a program does not have good cache coherence (for example is reading from random memory locations of many different pages) then the TLB will not have the result cache and now the MMU must use the much slower page table to determine the physical frame.\n\n\n\n\nThis may be how one splits up a multi level page table.\n\n\nAdvanced Frames and Page Protections\n\n\nCan frames be shared between processes? Can they be specialized\n\n\nYes! In addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. Read only frames can then be safely shared between multiple processes. For example, the C-library instruction code can be shared between all processes that dynamically load the code into the process memory. Each process can only read that memory. Meaning that if you try to write to a read-only page in memory you will get a \nSEGFAULT\n. That is why sometimes memory accesses segfault and sometimes they don't, it all depends on if your hardware says that you can access.\n\n\nIn addition, processes can share a page with a child process using the \nmmap\n system call. \nmmap\n is an interesting call because instead of trying each virtual address to a physical frame, it ties it to something else. That something else can be a file, a GPU unit, or any other memory mapped operation that you can think of! Writing to the memory address may write through to the device or the write may be paused by the operating system but this is a very powerful abstraction because often the operating system is able to perform optimizations (multiple processes memory mapping the same file can have the kernel create one mapping).\n\n\nWhat else is stored in the page table and why?\n\n\nIn addition to read-only bit and usage statistics discussed above, it is common to store at least read-only, modification and execution information. \n\n\nWhat's a page fault?\n\n\nA page fault is when a running program tries to access some virtual memory in its address space that is not mapped to physical memory. Page faults will also occur in other situations.\n\n\nThere are three types of Page Faults\n\n\nMinor\n If there is no mapping yet for the page, but it is a valid address. This could be memory asked for by \nsbrk(2)\n but not written to yet meaning that the operating system can wait for the first write before allocating space. The OS simply makes the page, loads it into memory, and moves on.\n\n\nMajor\n If the mapping to the page is not in memory but on disk. What this will do is swap the page into memory and swap another page out. If this happens frequently enough, your program is said to \nthrash\n the MMU.\n\n\nInvalid\n When you try to write to a non-writable memory address or read to a non-readable memory address. The MMU generates an invalid fault and the OS will usually generate a \nSIGSEGV\n meaning segmentation violation meaning that you wrote outside the segment that you could write to.\n\n\nRead-only bit\n\n\nThe read-only bit marks the page as read-only. Attempts to write to the page will cause a page fault. The page fault will then be handled by the Kernel. Two examples of the read-only page include sharing the c runtime library between multiple processes (for security you wouldn't want to allow one process to modify the library); and Copy-On-Write where the cost of duplicating a page can be delayed until the first write occurs. \n\n\nDirty bit\n\n\nhttp://en.wikipedia.org/wiki/Page_table#Page_table_data\n\n\n\n\nThe dirty bit allows for a performance optimization. A page on disk that is paged in to physical memory, then read from, and subsequently paged out again does not need to be written back to disk, since the page hasn't changed. However, if the page was written to after it's paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. This strategy requires that the backing store retain a copy of the page after it is paged in to memory. When a dirty bit is not used, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment. When a dirty bit is used, at all times some pages will exist in both physical memory and the backing store.\n\n\n\n\nExecution bit\n\n\nThe execution bit defines whether bytes in a page can be executed as CPU instructions. By disabling a page, it prevents code that is maliciously stored in the process memory (e.g. by stack overflow) from being easily executed. (further reading: http://en.wikipedia.org/wiki/NX_bit#Hardware_background)\n\n\nFind out more\n\n\nA lower level more and more technical discussion of paging and page bits on x86 platform is discussed at [http://wiki.osdev.org/Paging]",
            "title": "Virtual Memory, Part 1: Introduction to Virtual Memory"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-virtual-memory",
            "text": "In very simple embedded systems and early computers, processes directly access memory i.e. \"Address 1234\" corresponds to a particular byte stored in a particular part of physical memory.\nIn modern systems, this is no longer the case. Instead each process is isolated; and there is a translation process between the address of a particular CPU instruction or piece of data of a process and the actual byte of physical memory (\"RAM\"). Memory addresses are no longer 'real'; the process runs inside virtual memory. Virtual memory not only keeps processes safe (because one process cannot directly read or modify another process's memory) it also allows the system to efficiently allocate and re-allocate portions of memory to different processes.",
            "title": "What is Virtual Memory?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-the-mmu",
            "text": "The Memory Management Unit is part of the CPU. It converts a virtual memory address into a physical address. The MMU may also interrupt the CPU if there is currently no mapping from a particular virtual address to a physical address or if the current CPU instruction attempts to write to location that the process only has read-access.",
            "title": "What is the MMU?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#so-how-do-we-convert-a-virtual-address-into-a-physical-address",
            "text": "Imagine you had a 32 bit machine. Pointers can hold 32 bits i.e. they can address 2^32 different locations i.e. 4GB of memory (we will be following the standard convention of one address can hold one byte).  Imagine we had a large table - here's the clever part - stored in memory! For every possible address (all 4 billion of them) we will store the 'real' i.e. physical address. Each physical address will need 4 bytes (to hold the 32 bits).\nThis scheme would require 16 billion bytes to store all of entries. Oops - our lookup scheme would consume all of the memory that we could possibly buy for our 4GB machine.\nWe need to do better than this. Our lookup table better be smaller than the memory we have otherwise we will have no space left for our actual programs and operating system data.\nThe solution is to chunk memory into small regions called 'pages' and 'frames' and use a lookup table for each page.",
            "title": "So how do we convert a virtual address into a physical address?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-a-page-how-many-of-them-are-there",
            "text": "A page is a block of virtual memory. A typical block size on Linux operating system is 4KB (i.e. 2^12 addresses), though you can find examples of larger blocks.  So rather than talking about individual bytes we can talk about blocks of 4KBs, each block is called a page. We can also number our pages (\"Page 0\" \"Page 1\" etc)",
            "title": "What is a page? How many of them are there?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#ex-how-many-pages-are-there-in-a-32bit-machine-assume-page-size-of-4kb",
            "text": "Answer: 2^32 address / 2^12 = 2^20 pages.  Remember that 2^10 is 1024, so 2^20 is a bit more than one million.  For a 64 bit machine, 2^64 / 2^12 = 2^52, which is roughly 10^15 pages.",
            "title": "EX: How many pages are there in a 32bit machine (assume page size of 4KB)?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-a-frame",
            "text": "A frame (or sometimes called a 'page frame') is a block of  physical memory  or RAM (=Random Access Memory). This kind of memory is occasionally called 'primary storage' (and contrasted with slower, secondary storage such as spinning disks that have lower access times)  A frame is the same number of bytes as a virtual page. If a 32 bit machine has 2^32 (4GB) of RAM, then there will be the same number of them in the addressable space of the machine. It's unlikely that a 64 bit machine will ever have 2^64 bytes of RAM - can you see why?",
            "title": "What is a frame?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-a-page-table-and-how-big-is-it",
            "text": "A page table is a mapping between a page to the frame.\nFor example Page 1 might be mapped to frame 45, page 2 mapped to frame 30. Other frames might be currently unused or assigned to other running processes, or used internally by the operating system.  A simple page table is just an array,  int frame = table[ page_num ];  For a 32 bit machine with 4KB pages, each entry needs to hold a frame number - i.e. 20 bits because we calculated there are 2^20 frames. That's 2.5 bytes per entry! In practice, we'll round that up to 4 bytes per entry and find a use for those spare bits. With 4 bytes per entry x 2^20 entries = 4 MB of physical memory are required to hold the page table.  For a 64 bit machine with 4KB pages, each entry needs 52 bits. Let's round up to 64 bits (8 bytes) per entry. With 2^52 entries thats 2^55 bytes (roughly 40 peta bytes...) Oops our page table is too large.  In 64 bit architectures memory addresses are sparse, so we need a mechanism to reduce the page table size, given that most of the entries will never be used.   A visual example of the page table is here. Imagine accessing an array and grabbing array elements.",
            "title": "What is a page table and how big is it?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-the-offset-and-how-is-it-used",
            "text": "Remember our page table maps pages to frames, but each page is a block of contiguous addresses. How do we calculate which particular byte to use inside a particular frame? The solution is to re-use the lowest bits of the virtual memory address directly. For example, suppose our process is reading the following address- VirtualAddress = 11110000111100001111000010101010 (binary)  On a machine with page size 256 Bytes, then the lowest 8 bits (10101010) will be used as the offset.\nThe remaining upper bits will be the page number (111100001111000011110000).",
            "title": "What is the offset and how is it used?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#multi-level-page-tables",
            "text": "Multi-level pages are one solution to the page table size issue for 64 bit architectures. We'll look at the simplest implementation - a two level page table. Each table is a list of pointers that point to the next level of tables, not all sub-tables need to exist. An example, two level page table for a 32 bit architecture is shown below-  VirtualAddress = 11110000111111110000000010101010 (binary)\n                 |_Index1_||        ||          | 10 bit Directory index\n                           |_Index2_||          | 10 bit Sub-table index\n                                     |__________| 12 bit offset (passed directly to RAM)  In the above scheme, determining the frame number requires two memory reads: The topmost 10 bits are used in a directory of page tables. If 2 bytes are used for each entry, we only need 2KB to store this entire directory. Each subtable will point to physical frames (i.e. required 4 bytes to store the 20 bits). However, for processes with only tiny memory needs, we only need to specify entries for low memory address (for the heap and program code) and high memory addresses (for the stack). Each subtable is 1024 entries x 4 bytes i.e. 4KB for each subtable. Thus the total memory overhead for our multi-level page table has shrunk from 4MB (for the single level) to 3 frames of memory (12KB) !  Do page tables make memory access slower? (And what's a TLB)  Yes - Significantly ! (But thanks to clever hardware, usually no...)\nCompared to reading or writing memory directly.\nFor a single page table, our machine is now twice as slow! (Two memory accesses are required)\nFor a two-level page table, memory access is now three times as slow. (Three memory accesses are required)  To overcome this overhead, the MMU includes an associative cache of recently-used  virtual-page-to-frame lookups. This cache is called the TLB (\"translation lookaside buffer\"). Everytime a virtual address needs to be translated into a physical memory location, the TLB is queried in parallel to the page table. For most memory accesses of most programs, there is a significant chance that the TLB has cached the results. However if a program does not have good cache coherence (for example is reading from random memory locations of many different pages) then the TLB will not have the result cache and now the MMU must use the much slower page table to determine the physical frame.   This may be how one splits up a multi level page table.",
            "title": "Multi-level page tables"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#advanced-frames-and-page-protections",
            "text": "",
            "title": "Advanced Frames and Page Protections"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#can-frames-be-shared-between-processes-can-they-be-specialized",
            "text": "Yes! In addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. Read only frames can then be safely shared between multiple processes. For example, the C-library instruction code can be shared between all processes that dynamically load the code into the process memory. Each process can only read that memory. Meaning that if you try to write to a read-only page in memory you will get a  SEGFAULT . That is why sometimes memory accesses segfault and sometimes they don't, it all depends on if your hardware says that you can access.  In addition, processes can share a page with a child process using the  mmap  system call.  mmap  is an interesting call because instead of trying each virtual address to a physical frame, it ties it to something else. That something else can be a file, a GPU unit, or any other memory mapped operation that you can think of! Writing to the memory address may write through to the device or the write may be paused by the operating system but this is a very powerful abstraction because often the operating system is able to perform optimizations (multiple processes memory mapping the same file can have the kernel create one mapping).",
            "title": "Can frames be shared between processes? Can they be specialized"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-else-is-stored-in-the-page-table-and-why",
            "text": "In addition to read-only bit and usage statistics discussed above, it is common to store at least read-only, modification and execution information.",
            "title": "What else is stored in the page table and why?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#whats-a-page-fault",
            "text": "A page fault is when a running program tries to access some virtual memory in its address space that is not mapped to physical memory. Page faults will also occur in other situations.  There are three types of Page Faults  Minor  If there is no mapping yet for the page, but it is a valid address. This could be memory asked for by  sbrk(2)  but not written to yet meaning that the operating system can wait for the first write before allocating space. The OS simply makes the page, loads it into memory, and moves on.  Major  If the mapping to the page is not in memory but on disk. What this will do is swap the page into memory and swap another page out. If this happens frequently enough, your program is said to  thrash  the MMU.  Invalid  When you try to write to a non-writable memory address or read to a non-readable memory address. The MMU generates an invalid fault and the OS will usually generate a  SIGSEGV  meaning segmentation violation meaning that you wrote outside the segment that you could write to.",
            "title": "What's a page fault?"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#read-only-bit",
            "text": "The read-only bit marks the page as read-only. Attempts to write to the page will cause a page fault. The page fault will then be handled by the Kernel. Two examples of the read-only page include sharing the c runtime library between multiple processes (for security you wouldn't want to allow one process to modify the library); and Copy-On-Write where the cost of duplicating a page can be delayed until the first write occurs.",
            "title": "Read-only bit"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#dirty-bit",
            "text": "http://en.wikipedia.org/wiki/Page_table#Page_table_data   The dirty bit allows for a performance optimization. A page on disk that is paged in to physical memory, then read from, and subsequently paged out again does not need to be written back to disk, since the page hasn't changed. However, if the page was written to after it's paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. This strategy requires that the backing store retain a copy of the page after it is paged in to memory. When a dirty bit is not used, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment. When a dirty bit is used, at all times some pages will exist in both physical memory and the backing store.",
            "title": "Dirty bit"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#execution-bit",
            "text": "The execution bit defines whether bytes in a page can be executed as CPU instructions. By disabling a page, it prevents code that is maliciously stored in the process memory (e.g. by stack overflow) from being easily executed. (further reading: http://en.wikipedia.org/wiki/NX_bit#Hardware_background)",
            "title": "Execution bit"
        },
        {
            "location": "/SystemProgramming/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#find-out-more",
            "text": "A lower level more and more technical discussion of paging and page bits on x86 platform is discussed at [http://wiki.osdev.org/Paging]",
            "title": "Find out more"
        },
        {
            "location": "/SystemProgramming/_Footer/",
            "text": "Legal and Licensing information: Unless otherwise specified, submitted content to the wiki must be original work (including text, java code, and media) and you provide this material under a \nCreative Commons License\n. If you are not the copyright holder, please give proper attribution and credit to existing content and ensure that you have license to include the materials.",
            "title": " Footer"
        }
    ]
}