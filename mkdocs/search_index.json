{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Angrave's crowd-sourced System Programming wiki-book!\nThis wiki is being built by students and faculty from the University of Illinois and is a crowd-source authoring experiment by Lawrence Angrave from CS @ Illinois.\n\n\nRather than requiring an existing paper-based book this semester, we will build our own set of resources here.\n\n\nExam Practice Questions\n\n\nWarning these are good practice but not comprehensive. The CS241 final assumes you fully understand and can apply all topics of the course. Questions will focus mostly but not entirely on topics that you have used in the lab and programming assignments.\n\n\n\n\n[[Exam Topics]]\n\n\n[[C Programming: Review Questions]]\n\n\n[[Multi-threaded Programming: Review Questions]]\n\n\n[[Synchronization Concepts: Review Questions]]\n\n\n[[Memory: Review Questions]]\n\n\n[[Pipe: Review Questions]]\n\n\n[[Filesystem: Review Questions]]\n\n\n[[Networking: Review Questions]]\n\n\n[[Signals: Review Questions]] (todo)\n\n\n\n\nWeek 1 - 2\n\n\n\n\n[[HW0]]\n\n\n[[C Programming, Part 1: Introduction]]\n\n\n[[C Programming, Part 2: Text Input And Output]]\n\n\n[[Informal Glossary of basic terms|#Informal Glossary]]\n\n\n[[#Piazza: When And How to Ask For Help]]\n\n\n[[C Programming, Part 3: Common Gotchas]]\n\n\n\n\nWeek 3\n\n\n\n\n[[Forking, Part 1: Introduction]]\n\n\n[[Forking, Part 2: Fork, Exec, Wait]]\n\n\n[[Process Control, Part 1: Wait macros, using signals]]\n\n\n\n\nWeek 4\n\n\n\n\n[[Memory, Part 1: Heap Memory Introduction]]\n\n\n[[Memory, Part 2: Implementing a Memory Allocator]]\n\n\n[[Memory, Part 3: Smashing the Stack Example]]\n\n\n[[Pthreads, Part 1: Introduction]]\n\n\n\n\nWeek 5\n\n\n\n\n[[Pthreads, Part 2: Usage in Practice]]\n\n\n[[Synchronization, Part 1: Mutex Locks]]\n\n\n[[Synchronization, Part 2: Counting Semaphores]]\n\n\n\n\nWeek 6\n\n\n\n\n[[Synchronization, Part 3: Working with Mutexes And Semaphores]]\n\n\n[[Synchronization, Part 4: The Critical Section Problem]]\n\n\n[[Synchronization, Part 5: Condition Variables]]\n\n\n\n\nWeek 7\n\n\n\n\n[[Synchronization, Part 6: Implementing a barrier]]\n\n\n[[Synchronization, Part 7: The Reader Writer Problem]]\n\n\n[[Synchronization, Part 8: Ring Buffer Example]]\n\n\n[[Synchronization, Part 9: The Reader Writer Problem (part 2)]]\n\n\n\n\nWeek 8\n\n\n\n\n[[Deadlock, Part 1: Resource Allocation Graph]]\n\n\n[[Deadlock, Part 2: Deadlock Conditions]]\n\n\n\n\nWeek 9\n\n\n\n\nTodo Analysis of Dining Philosophers (for now see the discussion section handout)\n\n Breaking Circular Wait. Using a global mutex to break hold-and-wait\n\n Beware of starvation if all philosophers hold their left chopstick and try+release their right chopstick\n\n\n[[Virtual Memory, Part 1: Introduction to Virtual Memory]]\n\n\n[[Pipes, Part 1: Introduction to pipes]]\n\n\n[[Pipes, Part 2: Pipe programming secrets]]\n\n\n[[Files, Part 1: Working with files]]\n\n\n\n\nWeek 10\n\n\n\n\n[[ POSIX, Part 1: Error handling]]\n\n\n[[ Networking, Part 1: Introduction]]\n\n\n[[ Networking, Part 2: Using getaddrinfo ]]\n\n\n[[ Networking, Part 3: Building a simple TCP Client ]]\n\n\n[[ Programming Tricks, Part 1 ]]\n\n\n\n\nWeek 11\n\n\n\n\n[[ Networking, Part 4: Building a simple TCP Server ]]\n\n\n[[ Networking, Part 5: Reusing ports ]]\n\n\n[[ Scheduling, Part 1: Scheduling Processes ]]\n\n\n[[ Networking, Part 6: Creating a UDP server ]]\n\n\n[[ Nonblocking I O, select(), and epoll ]]\n\n\n\n\nWeek 12\n\n\n\n\n[[ File System, Part 1: Introduction ]]\n\n\n[[ File System, Part 2: Files are inodes (everything else is just data...) ]]\n\n\n[[ File System, Part 3: Permissions ]]\n\n\n\n\nWeek 13\n\n\n\n\n[[ File System, Part 4: Working with directories ]]\n\n\n[[ File System, Part 5: Virtual file systems ]]\n\n\n[[ File System, Part 6: Memory mapped files and Shared memory ]]\n\n\n[[ File System, Part 7: Scalable and Reliable Filesystems ]]\n\n\n[[ File System, Part 8: Removing preinstalled malware from an Android device ]]\n\n\n\n\nWeek 14\n\n\n\n\n[[ Signals, Part 2: Pending Signals and Signal Masks ]]\n\n\n[[ Signals, Part 3: Raising signals ]]\n\n\n[[ Signals, Part 4: Sigaction ]]\n\n\n[[ Nonblocking I O, select(), and epoll ]]\n\n\n\n\nWeek 15\n\n\n\n\n[[ RPC, Part 1: Introduction to Remote Procedure Calls ]]\n\n\n[[ File System, Part 8: Disk blocks example ]]\n\n\n\n\nOther content\n\n\n\n\n[[C Programming, Part 4: Debugging]]\n\n\n[[#Example Markdown]]\n\n\n[[ System Programming Short Stories and Songs ]]", 
            "title": "Home"
        }, 
        {
            "location": "/#exam-practice-questions", 
            "text": "Warning these are good practice but not comprehensive. The CS241 final assumes you fully understand and can apply all topics of the course. Questions will focus mostly but not entirely on topics that you have used in the lab and programming assignments.   [[Exam Topics]]  [[C Programming: Review Questions]]  [[Multi-threaded Programming: Review Questions]]  [[Synchronization Concepts: Review Questions]]  [[Memory: Review Questions]]  [[Pipe: Review Questions]]  [[Filesystem: Review Questions]]  [[Networking: Review Questions]]  [[Signals: Review Questions]] (todo)", 
            "title": "Exam Practice Questions"
        }, 
        {
            "location": "/#week-1-2", 
            "text": "[[HW0]]  [[C Programming, Part 1: Introduction]]  [[C Programming, Part 2: Text Input And Output]]  [[Informal Glossary of basic terms|#Informal Glossary]]  [[#Piazza: When And How to Ask For Help]]  [[C Programming, Part 3: Common Gotchas]]", 
            "title": "Week 1 - 2"
        }, 
        {
            "location": "/#week-3", 
            "text": "[[Forking, Part 1: Introduction]]  [[Forking, Part 2: Fork, Exec, Wait]]  [[Process Control, Part 1: Wait macros, using signals]]", 
            "title": "Week 3"
        }, 
        {
            "location": "/#week-4", 
            "text": "[[Memory, Part 1: Heap Memory Introduction]]  [[Memory, Part 2: Implementing a Memory Allocator]]  [[Memory, Part 3: Smashing the Stack Example]]  [[Pthreads, Part 1: Introduction]]", 
            "title": "Week 4"
        }, 
        {
            "location": "/#week-5", 
            "text": "[[Pthreads, Part 2: Usage in Practice]]  [[Synchronization, Part 1: Mutex Locks]]  [[Synchronization, Part 2: Counting Semaphores]]", 
            "title": "Week 5"
        }, 
        {
            "location": "/#week-6", 
            "text": "[[Synchronization, Part 3: Working with Mutexes And Semaphores]]  [[Synchronization, Part 4: The Critical Section Problem]]  [[Synchronization, Part 5: Condition Variables]]", 
            "title": "Week 6"
        }, 
        {
            "location": "/#week-7", 
            "text": "[[Synchronization, Part 6: Implementing a barrier]]  [[Synchronization, Part 7: The Reader Writer Problem]]  [[Synchronization, Part 8: Ring Buffer Example]]  [[Synchronization, Part 9: The Reader Writer Problem (part 2)]]", 
            "title": "Week 7"
        }, 
        {
            "location": "/#week-8", 
            "text": "[[Deadlock, Part 1: Resource Allocation Graph]]  [[Deadlock, Part 2: Deadlock Conditions]]", 
            "title": "Week 8"
        }, 
        {
            "location": "/#week-9", 
            "text": "Todo Analysis of Dining Philosophers (for now see the discussion section handout)  Breaking Circular Wait. Using a global mutex to break hold-and-wait  Beware of starvation if all philosophers hold their left chopstick and try+release their right chopstick  [[Virtual Memory, Part 1: Introduction to Virtual Memory]]  [[Pipes, Part 1: Introduction to pipes]]  [[Pipes, Part 2: Pipe programming secrets]]  [[Files, Part 1: Working with files]]", 
            "title": "Week 9"
        }, 
        {
            "location": "/#week-10", 
            "text": "[[ POSIX, Part 1: Error handling]]  [[ Networking, Part 1: Introduction]]  [[ Networking, Part 2: Using getaddrinfo ]]  [[ Networking, Part 3: Building a simple TCP Client ]]  [[ Programming Tricks, Part 1 ]]", 
            "title": "Week 10"
        }, 
        {
            "location": "/#week-11", 
            "text": "[[ Networking, Part 4: Building a simple TCP Server ]]  [[ Networking, Part 5: Reusing ports ]]  [[ Scheduling, Part 1: Scheduling Processes ]]  [[ Networking, Part 6: Creating a UDP server ]]  [[ Nonblocking I O, select(), and epoll ]]", 
            "title": "Week 11"
        }, 
        {
            "location": "/#week-12", 
            "text": "[[ File System, Part 1: Introduction ]]  [[ File System, Part 2: Files are inodes (everything else is just data...) ]]  [[ File System, Part 3: Permissions ]]", 
            "title": "Week 12"
        }, 
        {
            "location": "/#week-13", 
            "text": "[[ File System, Part 4: Working with directories ]]  [[ File System, Part 5: Virtual file systems ]]  [[ File System, Part 6: Memory mapped files and Shared memory ]]  [[ File System, Part 7: Scalable and Reliable Filesystems ]]  [[ File System, Part 8: Removing preinstalled malware from an Android device ]]", 
            "title": "Week 13"
        }, 
        {
            "location": "/#week-14", 
            "text": "[[ Signals, Part 2: Pending Signals and Signal Masks ]]  [[ Signals, Part 3: Raising signals ]]  [[ Signals, Part 4: Sigaction ]]  [[ Nonblocking I O, select(), and epoll ]]", 
            "title": "Week 14"
        }, 
        {
            "location": "/#week-15", 
            "text": "[[ RPC, Part 1: Introduction to Remote Procedure Calls ]]  [[ File System, Part 8: Disk blocks example ]]", 
            "title": "Week 15"
        }, 
        {
            "location": "/#other-content", 
            "text": "[[C Programming, Part 4: Debugging]]  [[#Example Markdown]]  [[ System Programming Short Stories and Songs ]]", 
            "title": "Other content"
        }, 
        {
            "location": "/#Example Markdown/", 
            "text": "Example\n\n\nSubsection\n\n\nHere's some code,\n\n\nint main() {\n  int a = 4;\n  a = a + 1;\n  return a;\n}", 
            "title": "#Example Markdown"
        }, 
        {
            "location": "/#Example Markdown/#example", 
            "text": "", 
            "title": "Example"
        }, 
        {
            "location": "/#Example Markdown/#subsection", 
            "text": "Here's some code,  int main() {\n  int a = 4;\n  a = a + 1;\n  return a;\n}", 
            "title": "Subsection"
        }, 
        {
            "location": "/#Informal-Glossary/", 
            "text": "What is the kernel?\n\n\nThe kernel is central part of the operating system that manages processes, resources (including memory) and hardware input-output devices. User programs interact with the kernel by making system calls.\n\n\nLearn more:\n[[http://en.wikipedia.org/wiki/Kernel_%28operating_system%29]]\n\n\nWhat is a process?\n\n\nA process is an instance of a program that is running on a machine. There can be multiple processes of the same program. For example, you and I might both be running 'cat' or 'gnuchess'\n\n\nA process contains the program code and modifiable state information such as variables, signals, open file descriptors for files, network connections and other system resources which are stored inside the process's memory. An operating system also stores meta-information about the process which is used by the system to manage and monitor the process's activity and resource use.\n\n\nLearn more:\n[[http://en.wikipedia.org/wiki/Process_%28computing%29]]\n\n\nWhat is virtual memory?\n\n\nProcesses running on your smart phone and laptop use virtual memory: Each process is isolated from other processes and appears to get complete access to all possible memory addresses! In reality only a small fraction of the process's address space is mapped to physical memory and the actual amount of physical memory allocated to a process can change over time and be paged out to disk, re-mapped and securely shared with other processes. Virtual memory provides significant benefits including strong process isolation (security), resource and performance benefits (simplified and efficient physical memory use) that we will discuss later. \n\n\nLearn more:\n[[http://en.wikipedia.org/wiki/Virtual_memory]]", 
            "title": "#Informal Glossary"
        }, 
        {
            "location": "/#Informal-Glossary/#what-is-the-kernel", 
            "text": "The kernel is central part of the operating system that manages processes, resources (including memory) and hardware input-output devices. User programs interact with the kernel by making system calls.  Learn more:\n[[http://en.wikipedia.org/wiki/Kernel_%28operating_system%29]]", 
            "title": "What is the kernel?"
        }, 
        {
            "location": "/#Informal-Glossary/#what-is-a-process", 
            "text": "A process is an instance of a program that is running on a machine. There can be multiple processes of the same program. For example, you and I might both be running 'cat' or 'gnuchess'  A process contains the program code and modifiable state information such as variables, signals, open file descriptors for files, network connections and other system resources which are stored inside the process's memory. An operating system also stores meta-information about the process which is used by the system to manage and monitor the process's activity and resource use.  Learn more:\n[[http://en.wikipedia.org/wiki/Process_%28computing%29]]", 
            "title": "What is a process?"
        }, 
        {
            "location": "/#Informal-Glossary/#what-is-virtual-memory", 
            "text": "Processes running on your smart phone and laptop use virtual memory: Each process is isolated from other processes and appears to get complete access to all possible memory addresses! In reality only a small fraction of the process's address space is mapped to physical memory and the actual amount of physical memory allocated to a process can change over time and be paged out to disk, re-mapped and securely shared with other processes. Virtual memory provides significant benefits including strong process isolation (security), resource and performance benefits (simplified and efficient physical memory use) that we will discuss later.   Learn more:\n[[http://en.wikipedia.org/wiki/Virtual_memory]]", 
            "title": "What is virtual memory?"
        }, 
        {
            "location": "/#Piazza: When And How to Ask For Help/", 
            "text": "Purpose\n\n\nTAs and student assistants get a ton of questions. Some are well-researched, and some...are not. This is a handy guide that'll help you move away from the latter and towards the former. (Oh, and did I mention that this is an easy way to score points with your internship managers?)\n\n\nAsk yourself...\n\n\n\n\nAm I running on EWS?\n\n\nDid I check the man pages?\n\n\nHave I searched for similar questions/followups on Piazza?\n\n\nHave I read the MP/DS specification completely?\n\n\nHave I watched all of the videos?\n\n\nDid I Google the error message\n (and a few permutations thereof if necessary)?\n\n\nDid I try commenting out, printlining, and/or stepping through parts of the code bit by bit to find out precisely where the error occurs?\n\n\nDid I commit my code to SVN in case the TAs need more context?\n\n\nDid I include the console/GDB/Valgrind output \nAND\n code surrounding the bug in my Piazza post?\n\n\nHave I fixed other segmentation faults not related to the issue I'm having?\n\n\nAm I following good programming practice? (i.e. encapsulation, functions to limit repetition, etc)", 
            "title": "#Piazza: When And How to Ask For Help"
        }, 
        {
            "location": "/#Piazza: When And How to Ask For Help/#purpose", 
            "text": "TAs and student assistants get a ton of questions. Some are well-researched, and some...are not. This is a handy guide that'll help you move away from the latter and towards the former. (Oh, and did I mention that this is an easy way to score points with your internship managers?)", 
            "title": "Purpose"
        }, 
        {
            "location": "/#Piazza: When And How to Ask For Help/#ask-yourself", 
            "text": "Am I running on EWS?  Did I check the man pages?  Have I searched for similar questions/followups on Piazza?  Have I read the MP/DS specification completely?  Have I watched all of the videos?  Did I Google the error message  (and a few permutations thereof if necessary)?  Did I try commenting out, printlining, and/or stepping through parts of the code bit by bit to find out precisely where the error occurs?  Did I commit my code to SVN in case the TAs need more context?  Did I include the console/GDB/Valgrind output  AND  code surrounding the bug in my Piazza post?  Have I fixed other segmentation faults not related to the issue I'm having?  Am I following good programming practice? (i.e. encapsulation, functions to limit repetition, etc)", 
            "title": "Ask yourself..."
        }, 
        {
            "location": "/C Programming, Part 4: Debugging/", 
            "text": "The Hitchhiker's Guide to Debugging C Programs\n\n\nFeel free to add anything that you found helpful in debugging C programs including but not limited to, debugger usage, recognizing common error types, gotchas, and effective googling tips.\n\n\n\n\n\n\nMake your code modular using helper functions. If there is a repeated task (getting the pointers to contiguous blocks in MP2 for example), make them helper functions. And make sure each function does one thing very well, so that you don't have to debug twice.\n\n\n\n\n\n\nUse assertions to make sure your code works up to a certain point -- and importantly, to make sure you don't break it later. For example, if your data structure is a doubly linked list, you can do something like, assert(node-\nsize == node-\nnext-\nprev-\nsize) to assert that the next node has a pointer to the current node. You can also check the pointer is pointing to an expected range of memory address, not null, -\nsize is reasonable etc.\nThe NDEBUG macro will disable all assertions, so don't forget to set that once you finish debugging. http://www.cplusplus.com/reference/cassert/assert/\n\n\n\n\n\n\nGDB\n\n\nIntroduction: http://www.cs.cmu.edu/~gilpin/tutorial/\n\n\nSetting breakpoints programmatically\n\n\nA very useful trick when debugging complex C programs with GDB is setting breakpoints in the source code.\n\n\nint main() {\n    int val = 1;\n    val = 42;\n    asm(\nint $3\n); // set a breakpoint here\n    val = 7;\n}\n\n\n\n\n$ gcc main.c -g -o main \n ./main\n(gdb) r\n[...]\nProgram received signal SIGTRAP, Trace/breakpoint trap.\nmain () at main.c:6\n6       val = 7;\n(gdb) p val\n$1 = 42\n\n\n\n\nChecking memory content\n\n\nhttp://www.delorie.com/gnu/docs/gdb/gdb_56.html\n\n\nFor example,\n\n\nint main() {\n    char bad_string[3] = {'C', 'a', 't'};\n    printf(\n%s\n, bad_string);\n}\n\n\n\n\n$ gcc main.c -g -o main \n ./main\n$ Cat ZVQ\ufffd\u007f $\n\n\n\n\n(gdb) l\n1   #include \nstdio.h\n\n2   int main() {\n3       char bad_string[3] = {'C', 'a', 't'};\n4       printf(\n%s\n, bad_string);\n5   }\n(gdb) b 4\nBreakpoint 1 at 0x100000f57: file main.c, line 4.\n(gdb) r\n[...]\nBreakpoint 1, main () at main.c:4\n4       printf(\n%s\n, bad_string);\n(gdb) x/16xb bad_string\n0x7fff5fbff9cd: 0x63    0x61    0x74    0xe0    0xf9    0xbf    0x5f    0xff\n0x7fff5fbff9d5: 0x7f    0x00    0x00    0xfd    0xb5    0x23    0x89    0xff\n\n\n\n\nHere, by using the \nx\n command with parameters \n16xb\n, we can see that starting at memory address \n0x7fff5fbff9c\n (value of \nbad_string\n), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.\n\n\n0x43 0x61 0x74 0xe0 0xf9 0xbf 0x5f 0xff 0x7f 0x00", 
            "title": "C Programming, Part 4: Debugging"
        }, 
        {
            "location": "/C Programming, Part 4: Debugging/#the-hitchhikers-guide-to-debugging-c-programs", 
            "text": "Feel free to add anything that you found helpful in debugging C programs including but not limited to, debugger usage, recognizing common error types, gotchas, and effective googling tips.    Make your code modular using helper functions. If there is a repeated task (getting the pointers to contiguous blocks in MP2 for example), make them helper functions. And make sure each function does one thing very well, so that you don't have to debug twice.    Use assertions to make sure your code works up to a certain point -- and importantly, to make sure you don't break it later. For example, if your data structure is a doubly linked list, you can do something like, assert(node- size == node- next- prev- size) to assert that the next node has a pointer to the current node. You can also check the pointer is pointing to an expected range of memory address, not null, - size is reasonable etc.\nThe NDEBUG macro will disable all assertions, so don't forget to set that once you finish debugging. http://www.cplusplus.com/reference/cassert/assert/", 
            "title": "The Hitchhiker's Guide to Debugging C Programs"
        }, 
        {
            "location": "/C Programming, Part 4: Debugging/#gdb", 
            "text": "Introduction: http://www.cs.cmu.edu/~gilpin/tutorial/", 
            "title": "GDB"
        }, 
        {
            "location": "/C Programming, Part 4: Debugging/#setting-breakpoints-programmatically", 
            "text": "A very useful trick when debugging complex C programs with GDB is setting breakpoints in the source code.  int main() {\n    int val = 1;\n    val = 42;\n    asm( int $3 ); // set a breakpoint here\n    val = 7;\n}  $ gcc main.c -g -o main   ./main\n(gdb) r\n[...]\nProgram received signal SIGTRAP, Trace/breakpoint trap.\nmain () at main.c:6\n6       val = 7;\n(gdb) p val\n$1 = 42", 
            "title": "Setting breakpoints programmatically"
        }, 
        {
            "location": "/C Programming, Part 4: Debugging/#checking-memory-content", 
            "text": "http://www.delorie.com/gnu/docs/gdb/gdb_56.html  For example,  int main() {\n    char bad_string[3] = {'C', 'a', 't'};\n    printf( %s , bad_string);\n}  $ gcc main.c -g -o main   ./main\n$ Cat ZVQ\ufffd\u007f $  (gdb) l\n1   #include  stdio.h \n2   int main() {\n3       char bad_string[3] = {'C', 'a', 't'};\n4       printf( %s , bad_string);\n5   }\n(gdb) b 4\nBreakpoint 1 at 0x100000f57: file main.c, line 4.\n(gdb) r\n[...]\nBreakpoint 1, main () at main.c:4\n4       printf( %s , bad_string);\n(gdb) x/16xb bad_string\n0x7fff5fbff9cd: 0x63    0x61    0x74    0xe0    0xf9    0xbf    0x5f    0xff\n0x7fff5fbff9d5: 0x7f    0x00    0x00    0xfd    0xb5    0x23    0x89    0xff  Here, by using the  x  command with parameters  16xb , we can see that starting at memory address  0x7fff5fbff9c  (value of  bad_string ), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.  0x43 0x61 0x74 0xe0 0xf9 0xbf 0x5f 0xff 0x7f 0x00", 
            "title": "Checking memory content"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/", 
            "text": "Want a quick introduction to C?\n\n\n\n\nKeep reading for the quick crash-course to C Programming below\n\n\nThen see the [[C Gotchas wiki page|C Programming, Part 3: Common Gotchas]].\n\n\nAnd learn about [[text I/O|C Programming, Part 2: Text Input And Output]].\n\n\nKick back relax with \nLawrence's intro videos\n\n\n^ and the same link includes a virtual machine-in-a-browser you can play with\n\n\nOr current CS241 students may wish to review some old CS241 slides \nCS241 2013 Slides\n (requires logon)\n\n\n\n\nExternal resources\n\n\n\n\nC for C++/Java Programmers\n\n\nC Tutorial by Brian Kernighan\n\n\nc faq\n\n\nC Bootcamp\n\n\nC/C++ function reference\n\n\ngdb (Gnu debugger) tutorial\n Tip: run gdb with the \"-tui\" command line argument to get a full screen version of the debugger.\n\n\nAdd your favorite resources here\n\n\n\n\nCrash course intro to C\n\n\nWarning new page\n Please fix typos and formatting mistakes for me and add useful links too.*\n\n\nHow do you write a complete hello world program in C?\n\n\n#include \nstdio.h\n\nint main(void) { \n    printf(\nHello World\\n\n);\n    return 0; \n}\n\n\n\n\nWhy do we use '\n#include stdio.h\n'?\n\n\nWe're lazy! We don't want to declare the \nprintf\n function. It's already done for us inside the file '\nstdio.h\n'. The #include includes the text of the file as part of our file to be compiled.\n\n\nHow are C strings represented?\n\n\nAs characters in memory.  The end of the string includes a NULL (0) byte. So \"ABC\" requires four(4) bytes. The only way to find out the length of a C string is to keep reading memory until you find the NULL byte. C characters are always exactly one byte each.\n\n\nWhen you write a string literal \n\"ABC\"\n in an expression the string literal evaluates to a char pointer (char *), which points to the first byte/char of the string.  This means \nptr\n in the example below will hold the memory address of the first character in the string.\n\n\nchar *ptr = \nABC\n\n\n\n\n\nHow do you declare a pointer?\n\n\nA pointer refers to a memory address. The type of the pointer is useful - it tells the compiler how many bytes need to be read/written.\n\n\nint *ptr1;\nchar *ptr2;\n\n\n\n\nHow do you use a pointer to read/write some memory?\n\n\nif 'p' is a pointer then use \"*p\" to write to the memory address(es) pointed to by p.\n\n\n*ptr = 0; // Writes some memory.\n\n\n\n\nThe number of bytes written depends on the pointer type.\n\n\nWhat is pointer arithmetic?\n\n\nYou can add an integer to a pointer. However the pointer type is used to determine how much to increment the pointer. For char pointers this is trivial because characters are always one byte:\n\n\nchar *ptr = \nHello\n; // ptr holds the memory location of 'H'\nptr += 2; //ptr now points to the first'l'\n\n\n\n\nIf an int is 4 bytes then ptr+1 points to 4 bytes after whatever ptr is pointing at.\n\n\nchar *ptr = \nABCDEFGH\n;\nint *bna = (int *) ptr;\nbna +=1; // Would cause iterate by one integer space (i.e 4 bytes on some systems)\nptr = (char *) bna;\nprintf(\n%s\n, ptr);\n/* Notice how only 'EFGH' is printed. Why is that? Well as mentioned above, when performing 'bna+=1' we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte)*/\nreturn 0;\n\n\n\n\nBecause pointer arithmetic in C is always automatically scaled by the size of the type that is pointed to, you can't perform pointer arithmetic on void pointers.\n\n\nWhat is a void pointer?\n\n\nA pointer without a type (very similar to a void variable). You can think of this as a raw pointer, or just a memory address. You cannot directly read or write to it because the void type does not have a size.\n\n\nThis is often used when either a datatype you're dealing with is unknown or when you're interfacing C code with other programming languages.\n\n\nDoes \nprintf\n call write or does write call \nprintf\n?\n\n\nprintf\n calls \nwrite\n. \nprintf\n includes an internal buffer so, to increase performance \nprintf\n may not call \nwrite\n everytime you call \nprintf\n. \nprintf\n is a C library function. \nwrite\n is a system call and as we know system calls are expensive. On the other hand \nprintf\n uses a buffer which suits our needs better at that point\n\n\nHow do you print out pointer values? integers? strings?\n\n\nUse format specifiers \"%p\" for pointers, \"%d\" for integers and \"%s\" for Strings.\nA full list of all of the format specifiers is found \nhere\n\n\nExample of integer:\n\n\nint num1 = 10;\nprintf(\n%d\n, num1); //prints num1\n\n\n\n\nExample of integer pointer:\n\n\nint *ptr = (int *) malloc(sizeof(int));\n*ptr = 10;\nprintf(\n%p\\n\n, ptr); //prints the address pointed to by the pointer\nprintf(\n%p\\n\n, \nptr); /*prints the address of pointer -- extremely useful\nwhen dealing with double pointers*/\nprintf(\n%d\n, *ptr); //prints the integer content of ptr\nreturn 0;\n\n\n\n\nExample of string:\n\n\nchar *str = (char *) malloc(256 * sizeof(char));\nstrcpy(str, \nHello there!\n);\nprintf(\n%p\\n\n, str); // print the address in the heap\nprintf(\n%s\n, str);\nreturn 0;\n\n\n\n\nStrings as Pointers \n Arrays @ BU\n\n\nHow would you make standard out be saved to a file?\n\n\nSimplest way: run your program and use shell redirection\ne.g.\n\n\n./program \n output.txt\n\n#To read the contents of the file,\ncat output.txt\n\n\n\n\nMore complicated way: close(1) and then use open to re-open the file descriptor.\nSee [[http://cs-education.github.io/sys/#chapter/0/section/3/activity/0]]\n\n\nWhat's the difference between a pointer and an array? Give an example of something you can do with one but not the other.\n\n\nchar ary[] = \nHello\n;\nchar *ptr = \nHello\n;\n\n\n\n\nExample \n\n\nThe array name points to the first byte of the array. Both \nary\n and \nptr\n can be printed out:\n\n\nchar ary[] = \nHello\n;\nchar *ptr = \nHello\n;\n// Print out address and contents\nprintf(\n%p : %s\\n\n, ary, ary);\nprintf(\n%p : %s\\n\n, ptr, ptr);\n\n\n\n\nThe array is mutable, so we can change its contents (be careful not to write bytes beyond the end of the array though). Fortunately 'World' is no longer than 'Hello\"\n\n\nIn this case, the char pointer \nptr\n points to some read only memory (where the statically allocated string literal is stored), so we cannot change those contents.\n\n\nstrcpy(ary, \nWorld\n); // OK\nstrcpy(ptr, \nWorld\n); // NOT OK - Segmentation fault (crashes)\n\n\n\n\n\nWe can, however, unlike the array, we change \nptr\n to point to another piece of memory,\n\n\nptr = \nWorld\n; // OK!\nptr = ary; // OK!\nary = (..anything..) ; // WONT COMPILE\n// ary is doomed to always refer to the original array.\nprintf(\n%p : %s\\n\n, ptr, ptr);\nstrcpy(ptr, \nWorld\n); // OK because now ptr is pointing to mutable memory (the array)\n\n\n\n\nWhat to take away from this is that pointers * can point to any type of memory while C arrays [] can only point to memory on the stack. In a more common case, pointers will point to heap memory in which case the memory referred to by the pointer CAN be modified.\n\n\nsizeof()\n returns the number of bytes. So using above code, what is sizeof(ary) and sizeof(ptr)?\n\n\nsizeof(ary)\n: \nary\n is an array. Returns the number of bytes required for the entire array (5 chars + zero byte = 6 bytes)\n\nsizeof(ptr)\n: Same as sizeof(char *). Returns the number bytes required for a pointer (e.g. 4 or 8 for a 32 bit or 64 bit machine)\n\n\nWhich of the following code is incorrect or correct and why?\n\n\nint* f1(int *p) {\n    *p = 42;\n    return p;\n} // This code is correct;\n\n\n\n\nchar* f2() {\n    char p[] = \nHello\n;\n    return p;\n} // Incorrect!\n\n\n\n\nExplanation: An array p is created on the stack for the correct size to hold H,e,l,l,o, and a null byte i.e. (6) bytes. This array is stored on the stack and is invalid after we return from f2.\n\n\nchar* f3() {\n    char *p = \nHello\n;\n    return p;\n} // OK\n\n\n\n\nExplanation: p is a pointer. It holds the address of the string constant. The string constant is unchanged and valid even after f3 returns.\n\n\nchar* f4() {\n    static char p[] = \nHello\n;\n    return p;\n} // OK\n\n\n\n\nExplanation: The array is static meaning it exists for the lifetime of the process (static variables are not on the heap or the stack).\n\n\nHow do you look up information C library calls and system calls?\n\n\nUse the man pages. Note the man pages are organized into sections. Section 2 = System calls. Section 3 = C libraries.\nWeb: Google \"man7 open\"\nshell: man -S2 open  or man -S3 printf\n\n\nHow do you allocate memory on the heap?\n\n\nUse malloc. There's also realloc and calloc.\nTypically used with sizeof. e.g. enough space to hold 10 integers\n\n\nint *space = malloc(sizeof(int) * 10);\n\n\n\n\nWhat's wrong with this string copy code?\n\n\nvoid mystrcpy(char*dest, char* src) { \n  // void means no return value   \n  while( *src ) { dest = src; src ++; dest++; }  \n}\n\n\n\n\nIn the above code it simply changes the dest pointer to point to source string. Also the nul bytes is not copied. Here's a better version - \n\n\n  while( *src ) { *dest = *src; src ++; dest++; } \n  *dest = *src;\n\n\n\n\nNote it's also usual to see the following kind of implementation, which does everything inside the expression test, including copying the nul byte.\n\n\n  while( (*dest++ = *src++ )) {};\n\n\n\n\nHow do you write a strdup replacement?\n\n\n// Use strlen+1 to find the zero byte... \nchar* mystrdup(char*source) {\n   char *p = (char *) malloc ( strlen(source)+1 );\n   strcpy(p,source);\n   return p;\n}\n\n\n\n\nHow do you unallocate memory on the heap?\n\n\nUse free!\n\n\nint *n = (int *) malloc(sizeof(int));\n*n = 10;\n//Do some work\nfree(n);\n\n\n\n\nWhat is double free error? How can you avoid? What is a dangling pointer? How do you avoid?\n\n\nA double free error is when you accidentally attempt to free the same allocation twice.\n\n\nint *p = malloc(sizeof(int));\nfree(p);\n\n*p = 123; // Oops! - Dangling pointer! Writing to memory we don't own anymore\n\nfree(p); // Oops! - Double free!\n\n\n\n\nThe fix is firstly to write correct programs! Secondly, it's good programming hygiene to reset pointers\nonce the memory has been freed. This ensures the pointer cant be used incorrectly without the program crashing.\n\n\nFix:\n\n\np = NULL; // Now you can't use this pointer by mistake\n\n\n\n\nWhat is an example of buffer overflow?\n\n\nFamous example: Heart Bleed (performed a memcpy into a buffer that was of insufficient size).\nSimple example: implement a strcpy and forget to add one to strlen, when determining the size of the memory required.\n\n\nWhat is 'typedef' and how do you use it?\n\n\nDeclares an alias for a type. Often used with structs to reduce the visual clutter of having to write 'struct' as part of the type.\n\n\ntypedef float real; \nreal gravity = 10;\n// Also typedef gives us an abstraction over the underlying type used. \n// For example in the future we only need to change this typedef if we\n// wanted our physics library to use doubles instead of floats.\n\ntypedef struct link link_t; \n//With structs, include the keyword 'struct' as part of the original types", 
            "title": "C Programming, Part 1: Introduction"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#want-a-quick-introduction-to-c", 
            "text": "Keep reading for the quick crash-course to C Programming below  Then see the [[C Gotchas wiki page|C Programming, Part 3: Common Gotchas]].  And learn about [[text I/O|C Programming, Part 2: Text Input And Output]].  Kick back relax with  Lawrence's intro videos  ^ and the same link includes a virtual machine-in-a-browser you can play with  Or current CS241 students may wish to review some old CS241 slides  CS241 2013 Slides  (requires logon)", 
            "title": "Want a quick introduction to C?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#external-resources", 
            "text": "C for C++/Java Programmers  C Tutorial by Brian Kernighan  c faq  C Bootcamp  C/C++ function reference  gdb (Gnu debugger) tutorial  Tip: run gdb with the \"-tui\" command line argument to get a full screen version of the debugger.  Add your favorite resources here", 
            "title": "External resources"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#crash-course-intro-to-c", 
            "text": "Warning new page  Please fix typos and formatting mistakes for me and add useful links too.*", 
            "title": "Crash course intro to C"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-do-you-write-a-complete-hello-world-program-in-c", 
            "text": "#include  stdio.h \nint main(void) { \n    printf( Hello World\\n );\n    return 0; \n}", 
            "title": "How do you write a complete hello world program in C?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#why-do-we-use-include-stdioh", 
            "text": "We're lazy! We don't want to declare the  printf  function. It's already done for us inside the file ' stdio.h '. The #include includes the text of the file as part of our file to be compiled.", 
            "title": "Why do we use '#include stdio.h'?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-are-c-strings-represented", 
            "text": "As characters in memory.  The end of the string includes a NULL (0) byte. So \"ABC\" requires four(4) bytes. The only way to find out the length of a C string is to keep reading memory until you find the NULL byte. C characters are always exactly one byte each.  When you write a string literal  \"ABC\"  in an expression the string literal evaluates to a char pointer (char *), which points to the first byte/char of the string.  This means  ptr  in the example below will hold the memory address of the first character in the string.  char *ptr =  ABC", 
            "title": "How are C strings represented?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-do-you-declare-a-pointer", 
            "text": "A pointer refers to a memory address. The type of the pointer is useful - it tells the compiler how many bytes need to be read/written.  int *ptr1;\nchar *ptr2;", 
            "title": "How do you declare a pointer?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-do-you-use-a-pointer-to-readwrite-some-memory", 
            "text": "if 'p' is a pointer then use \"*p\" to write to the memory address(es) pointed to by p.  *ptr = 0; // Writes some memory.  The number of bytes written depends on the pointer type.", 
            "title": "How do you use a pointer to read/write some memory?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#what-is-pointer-arithmetic", 
            "text": "You can add an integer to a pointer. However the pointer type is used to determine how much to increment the pointer. For char pointers this is trivial because characters are always one byte:  char *ptr =  Hello ; // ptr holds the memory location of 'H'\nptr += 2; //ptr now points to the first'l'  If an int is 4 bytes then ptr+1 points to 4 bytes after whatever ptr is pointing at.  char *ptr =  ABCDEFGH ;\nint *bna = (int *) ptr;\nbna +=1; // Would cause iterate by one integer space (i.e 4 bytes on some systems)\nptr = (char *) bna;\nprintf( %s , ptr);\n/* Notice how only 'EFGH' is printed. Why is that? Well as mentioned above, when performing 'bna+=1' we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte)*/\nreturn 0;  Because pointer arithmetic in C is always automatically scaled by the size of the type that is pointed to, you can't perform pointer arithmetic on void pointers.", 
            "title": "What is pointer arithmetic?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#what-is-a-void-pointer", 
            "text": "A pointer without a type (very similar to a void variable). You can think of this as a raw pointer, or just a memory address. You cannot directly read or write to it because the void type does not have a size.  This is often used when either a datatype you're dealing with is unknown or when you're interfacing C code with other programming languages.", 
            "title": "What is a void pointer?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#does-printf-call-write-or-does-write-call-printf", 
            "text": "printf  calls  write .  printf  includes an internal buffer so, to increase performance  printf  may not call  write  everytime you call  printf .  printf  is a C library function.  write  is a system call and as we know system calls are expensive. On the other hand  printf  uses a buffer which suits our needs better at that point", 
            "title": "Does printf call write or does write call printf?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-do-you-print-out-pointer-values-integers-strings", 
            "text": "Use format specifiers \"%p\" for pointers, \"%d\" for integers and \"%s\" for Strings.\nA full list of all of the format specifiers is found  here  Example of integer:  int num1 = 10;\nprintf( %d , num1); //prints num1  Example of integer pointer:  int *ptr = (int *) malloc(sizeof(int));\n*ptr = 10;\nprintf( %p\\n , ptr); //prints the address pointed to by the pointer\nprintf( %p\\n ,  ptr); /*prints the address of pointer -- extremely useful\nwhen dealing with double pointers*/\nprintf( %d , *ptr); //prints the integer content of ptr\nreturn 0;  Example of string:  char *str = (char *) malloc(256 * sizeof(char));\nstrcpy(str,  Hello there! );\nprintf( %p\\n , str); // print the address in the heap\nprintf( %s , str);\nreturn 0;  Strings as Pointers   Arrays @ BU", 
            "title": "How do you print out pointer values? integers? strings?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-would-you-make-standard-out-be-saved-to-a-file", 
            "text": "Simplest way: run your program and use shell redirection\ne.g.  ./program   output.txt\n\n#To read the contents of the file,\ncat output.txt  More complicated way: close(1) and then use open to re-open the file descriptor.\nSee [[http://cs-education.github.io/sys/#chapter/0/section/3/activity/0]]", 
            "title": "How would you make standard out be saved to a file?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#whats-the-difference-between-a-pointer-and-an-array-give-an-example-of-something-you-can-do-with-one-but-not-the-other", 
            "text": "char ary[] =  Hello ;\nchar *ptr =  Hello ;  Example   The array name points to the first byte of the array. Both  ary  and  ptr  can be printed out:  char ary[] =  Hello ;\nchar *ptr =  Hello ;\n// Print out address and contents\nprintf( %p : %s\\n , ary, ary);\nprintf( %p : %s\\n , ptr, ptr);  The array is mutable, so we can change its contents (be careful not to write bytes beyond the end of the array though). Fortunately 'World' is no longer than 'Hello\"  In this case, the char pointer  ptr  points to some read only memory (where the statically allocated string literal is stored), so we cannot change those contents.  strcpy(ary,  World ); // OK\nstrcpy(ptr,  World ); // NOT OK - Segmentation fault (crashes)  We can, however, unlike the array, we change  ptr  to point to another piece of memory,  ptr =  World ; // OK!\nptr = ary; // OK!\nary = (..anything..) ; // WONT COMPILE\n// ary is doomed to always refer to the original array.\nprintf( %p : %s\\n , ptr, ptr);\nstrcpy(ptr,  World ); // OK because now ptr is pointing to mutable memory (the array)  What to take away from this is that pointers * can point to any type of memory while C arrays [] can only point to memory on the stack. In a more common case, pointers will point to heap memory in which case the memory referred to by the pointer CAN be modified.", 
            "title": "What's the difference between a pointer and an array? Give an example of something you can do with one but not the other."
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#sizeof-returns-the-number-of-bytes-so-using-above-code-what-is-sizeofary-and-sizeofptr", 
            "text": "sizeof(ary) :  ary  is an array. Returns the number of bytes required for the entire array (5 chars + zero byte = 6 bytes) sizeof(ptr) : Same as sizeof(char *). Returns the number bytes required for a pointer (e.g. 4 or 8 for a 32 bit or 64 bit machine)", 
            "title": "sizeof() returns the number of bytes. So using above code, what is sizeof(ary) and sizeof(ptr)?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#which-of-the-following-code-is-incorrect-or-correct-and-why", 
            "text": "int* f1(int *p) {\n    *p = 42;\n    return p;\n} // This code is correct;  char* f2() {\n    char p[] =  Hello ;\n    return p;\n} // Incorrect!  Explanation: An array p is created on the stack for the correct size to hold H,e,l,l,o, and a null byte i.e. (6) bytes. This array is stored on the stack and is invalid after we return from f2.  char* f3() {\n    char *p =  Hello ;\n    return p;\n} // OK  Explanation: p is a pointer. It holds the address of the string constant. The string constant is unchanged and valid even after f3 returns.  char* f4() {\n    static char p[] =  Hello ;\n    return p;\n} // OK  Explanation: The array is static meaning it exists for the lifetime of the process (static variables are not on the heap or the stack).", 
            "title": "Which of the following code is incorrect or correct and why?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-do-you-look-up-information-c-library-calls-and-system-calls", 
            "text": "Use the man pages. Note the man pages are organized into sections. Section 2 = System calls. Section 3 = C libraries.\nWeb: Google \"man7 open\"\nshell: man -S2 open  or man -S3 printf", 
            "title": "How do you look up information C library calls and system calls?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-do-you-allocate-memory-on-the-heap", 
            "text": "Use malloc. There's also realloc and calloc.\nTypically used with sizeof. e.g. enough space to hold 10 integers  int *space = malloc(sizeof(int) * 10);", 
            "title": "How do you allocate memory on the heap?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#whats-wrong-with-this-string-copy-code", 
            "text": "void mystrcpy(char*dest, char* src) { \n  // void means no return value   \n  while( *src ) { dest = src; src ++; dest++; }  \n}  In the above code it simply changes the dest pointer to point to source string. Also the nul bytes is not copied. Here's a better version -     while( *src ) { *dest = *src; src ++; dest++; } \n  *dest = *src;  Note it's also usual to see the following kind of implementation, which does everything inside the expression test, including copying the nul byte.    while( (*dest++ = *src++ )) {};", 
            "title": "What's wrong with this string copy code?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-do-you-write-a-strdup-replacement", 
            "text": "// Use strlen+1 to find the zero byte... \nchar* mystrdup(char*source) {\n   char *p = (char *) malloc ( strlen(source)+1 );\n   strcpy(p,source);\n   return p;\n}", 
            "title": "How do you write a strdup replacement?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#how-do-you-unallocate-memory-on-the-heap", 
            "text": "Use free!  int *n = (int *) malloc(sizeof(int));\n*n = 10;\n//Do some work\nfree(n);", 
            "title": "How do you unallocate memory on the heap?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#what-is-double-free-error-how-can-you-avoid-what-is-a-dangling-pointer-how-do-you-avoid", 
            "text": "A double free error is when you accidentally attempt to free the same allocation twice.  int *p = malloc(sizeof(int));\nfree(p);\n\n*p = 123; // Oops! - Dangling pointer! Writing to memory we don't own anymore\n\nfree(p); // Oops! - Double free!  The fix is firstly to write correct programs! Secondly, it's good programming hygiene to reset pointers\nonce the memory has been freed. This ensures the pointer cant be used incorrectly without the program crashing.  Fix:  p = NULL; // Now you can't use this pointer by mistake", 
            "title": "What is double free error? How can you avoid? What is a dangling pointer? How do you avoid?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#what-is-an-example-of-buffer-overflow", 
            "text": "Famous example: Heart Bleed (performed a memcpy into a buffer that was of insufficient size).\nSimple example: implement a strcpy and forget to add one to strlen, when determining the size of the memory required.", 
            "title": "What is an example of buffer overflow?"
        }, 
        {
            "location": "/C-Programming,-Part-1:-Introduction/#what-is-typedef-and-how-do-you-use-it", 
            "text": "Declares an alias for a type. Often used with structs to reduce the visual clutter of having to write 'struct' as part of the type.  typedef float real; \nreal gravity = 10;\n// Also typedef gives us an abstraction over the underlying type used. \n// For example in the future we only need to change this typedef if we\n// wanted our physics library to use doubles instead of floats.\n\ntypedef struct link link_t; \n//With structs, include the keyword 'struct' as part of the original types", 
            "title": "What is 'typedef' and how do you use it?"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/", 
            "text": "How do I print strings, ints, chars to the standard output stream?\n\n\nUse \nprintf\n. The first parameter is a format string that includes placeholders for the data to be printed. Common format specifiers are \n%s\n treat the argument as a c string pointer, keep printing all characters until the NULL-character is reached; \n%d\n print the argument as an integer; \n%p\n print the argument as a memory address. \n\n\nA simple example is shown below:\n\n\nchar *name = ... ; int score = ...;\nprintf(\nHello %s, your result is %d\\n\n, name, score);\nprintf(\nDebug: The string and int are stored at: %p and %p\\n\n, name, \nscore );\n// name already is a char pointer and points to the start of the array. \n// We need \n to get the address of the int variable\n\n\n\n\nBy default, for performance, \nprintf\n does not actually write anything out (by calling write) until its buffer is full or a newline is printed. \n\n\nHow else can I print strings and single characters?\n\n\nUse \nputs( name );\n and \nputchar( c )\n  where name is a pointer to a C string and c is just a \nchar\n\n\nHow do I print to other file streams?\n\n\nUse \nfprintf( _file_ , \"Hello %s, score: %d\", name, score);\n\nWhere _file_ is either predefined 'stdout' 'stderr' or a FILE pointer that was returned by \nfopen\n or \nfdopen\n\n\nHow do I print data into a C string?\n\n\nUse \nsprintf\n or better \nsnprintf\n.\n\n\nchar result[200];\nint len = snprintf(result, sizeof(result), \n%s:%d\n, name, score);\n\n\n\n\nsnprintf returns the number of characters written excluding the terminating byte. In the above example this would be a maximum of 199.\n\n\nHow do I parse input using \nscanf\n into parameters?\n\n\nUse \nscanf\n (or \nfscanf\n or \nsscanf\n) to get input from the default input stream, an arbitrary file stream or a C string respectively.\nIt's a good idea to check the return value to see how many items were parsed.\n\nscanf\n functions require valid pointers. It's a common source of error to pass in an incorrect pointer value. For example,\n\n\nint *data = (int *) malloc(sizeof(int));\nchar *line = \nv 10\n;\nchar type;\n// Good practice: Check scanf parsed the line and read two values:\nint ok = 2 == sscanf(line, \n%c %d\n, \ntype, \ndata); // pointer error\n\n\n\n\nWe wanted to write the character value into c and the integer value into the malloc'd memory.\nHowever we passed the address of the data pointer, not what the pointer is pointing to! So \nsscanf\n will change the pointer itself. i.e. the pointer will now point to address 10 so this code will later fail e.g. when free(data) is called.\n\n\nHow do I stop scanf from causing a buffer overflow?\n\n\nThe following code assumes the scanf won't read more than 10 characters (including the terminating byte) into the buffer.\n\n\nchar buffer[10];\nscanf(\n%s\n,buffer);\n\n\n\n\nYou can include an optional integer to specify how many characters EXCLUDING the terminating byte:\n\n\nchar buffer[10];\nscanf(\n%9s\n, buffer); // reads upto 9 charactes from input (leave room for the 10th byte to be the terminating byte)\n\n\n\n\nWhy is \ngets\n dangerous? What should I use instead?\n\n\nThe following code is vulnerable to buffer overflow. It assumes or trusts that the input line will be no more than 10 characters, including the terminating byte.\n\n\nchar buf[10];\ngets(buf); // Remember the array name means the first byte of the array\n\n\n\n\ngets\n is deprecated in C99 standard and has been removed from the latest C standard (C11). Programs should use \nfgets\n or \ngetline\n instead. \n\n\nWhere each have the following structure respectively:\n\n\nchar *fgets (char *str, int num, FILE *stream); \n\nssize_t getline(char **lineptr, size_t *n, FILE *stream);\n\n\n\n\nHere's a simple, safe way to read a single line. Lines longer than 9 characters will be truncated:\n\n\nchar buffer[10];\nchar *result = fgets(buffer, sizeof(buffer), stdin);\n\n\n\n\nThe result is NULL if there was an error or the end of the file is reached.\nNote, unlike \ngets\n,  \nfgets\n copies the newline into the buffer, which you may want to discard-\n\n\nif (!result) { return; /* no data - don't read the buffer contents */}\n\nint i = strlen(buffer) - 1;\nif (buffer[i] == '\\n') \n    buffer[i] = '\\0';\n\n\n\n\nHow do I use \ngetline\n?\n\n\nOne of the advantages of \ngetline\n is that will automatically (re-) allocate a buffer on the heap of sufficient size.\n\n\n// ssize_t getline(char **lineptr, size_t *n, FILE *stream);\n\n /* set buffer and size to 0; they will be changed by getline */\nchar *buffer = NULL;\nsize_t size = 0;\n\nssize_t chars = getline(\nbuffer, \nsize, stdin);\n\n// Discard newline character if it is present,\nif (chars \n 0 \n buffer[chars-1] == '\\n') \n    buffer[chars-1] = '\\0';\n\n// Read another line.\n// The existing buffer will be re-used, or, if necessary,\n// It will be `free`'d and a new larger buffer will `malloc`'d\nchars = getline(\nbuffer, \nsize, stdin);\n\n// Later... don't forget to free the buffer!\nfree(buffer);", 
            "title": "C Programming, Part 2: Text Input And Output"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-print-strings-ints-chars-to-the-standard-output-stream", 
            "text": "Use  printf . The first parameter is a format string that includes placeholders for the data to be printed. Common format specifiers are  %s  treat the argument as a c string pointer, keep printing all characters until the NULL-character is reached;  %d  print the argument as an integer;  %p  print the argument as a memory address.   A simple example is shown below:  char *name = ... ; int score = ...;\nprintf( Hello %s, your result is %d\\n , name, score);\nprintf( Debug: The string and int are stored at: %p and %p\\n , name,  score );\n// name already is a char pointer and points to the start of the array. \n// We need   to get the address of the int variable  By default, for performance,  printf  does not actually write anything out (by calling write) until its buffer is full or a newline is printed.", 
            "title": "How do I print strings, ints, chars to the standard output stream?"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/#how-else-can-i-print-strings-and-single-characters", 
            "text": "Use  puts( name );  and  putchar( c )   where name is a pointer to a C string and c is just a  char", 
            "title": "How else can I print strings and single characters?"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-print-to-other-file-streams", 
            "text": "Use  fprintf( _file_ , \"Hello %s, score: %d\", name, score); \nWhere _file_ is either predefined 'stdout' 'stderr' or a FILE pointer that was returned by  fopen  or  fdopen", 
            "title": "How do I print to other file streams?"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-print-data-into-a-c-string", 
            "text": "Use  sprintf  or better  snprintf .  char result[200];\nint len = snprintf(result, sizeof(result),  %s:%d , name, score);  snprintf returns the number of characters written excluding the terminating byte. In the above example this would be a maximum of 199.", 
            "title": "How do I print data into a C string?"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-parse-input-using-scanf-into-parameters", 
            "text": "Use  scanf  (or  fscanf  or  sscanf ) to get input from the default input stream, an arbitrary file stream or a C string respectively.\nIt's a good idea to check the return value to see how many items were parsed. scanf  functions require valid pointers. It's a common source of error to pass in an incorrect pointer value. For example,  int *data = (int *) malloc(sizeof(int));\nchar *line =  v 10 ;\nchar type;\n// Good practice: Check scanf parsed the line and read two values:\nint ok = 2 == sscanf(line,  %c %d ,  type,  data); // pointer error  We wanted to write the character value into c and the integer value into the malloc'd memory.\nHowever we passed the address of the data pointer, not what the pointer is pointing to! So  sscanf  will change the pointer itself. i.e. the pointer will now point to address 10 so this code will later fail e.g. when free(data) is called.", 
            "title": "How do I parse input using scanf into parameters?"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-stop-scanf-from-causing-a-buffer-overflow", 
            "text": "The following code assumes the scanf won't read more than 10 characters (including the terminating byte) into the buffer.  char buffer[10];\nscanf( %s ,buffer);  You can include an optional integer to specify how many characters EXCLUDING the terminating byte:  char buffer[10];\nscanf( %9s , buffer); // reads upto 9 charactes from input (leave room for the 10th byte to be the terminating byte)", 
            "title": "How do I stop scanf from causing a buffer overflow?"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/#why-is-gets-dangerous-what-should-i-use-instead", 
            "text": "The following code is vulnerable to buffer overflow. It assumes or trusts that the input line will be no more than 10 characters, including the terminating byte.  char buf[10];\ngets(buf); // Remember the array name means the first byte of the array  gets  is deprecated in C99 standard and has been removed from the latest C standard (C11). Programs should use  fgets  or  getline  instead.   Where each have the following structure respectively:  char *fgets (char *str, int num, FILE *stream); \n\nssize_t getline(char **lineptr, size_t *n, FILE *stream);  Here's a simple, safe way to read a single line. Lines longer than 9 characters will be truncated:  char buffer[10];\nchar *result = fgets(buffer, sizeof(buffer), stdin);  The result is NULL if there was an error or the end of the file is reached.\nNote, unlike  gets ,   fgets  copies the newline into the buffer, which you may want to discard-  if (!result) { return; /* no data - don't read the buffer contents */}\n\nint i = strlen(buffer) - 1;\nif (buffer[i] == '\\n') \n    buffer[i] = '\\0';", 
            "title": "Why is gets dangerous? What should I use instead?"
        }, 
        {
            "location": "/C-Programming,-Part-2:-Text-Input-And-Output/#how-do-i-use-getline", 
            "text": "One of the advantages of  getline  is that will automatically (re-) allocate a buffer on the heap of sufficient size.  // ssize_t getline(char **lineptr, size_t *n, FILE *stream);\n\n /* set buffer and size to 0; they will be changed by getline */\nchar *buffer = NULL;\nsize_t size = 0;\n\nssize_t chars = getline( buffer,  size, stdin);\n\n// Discard newline character if it is present,\nif (chars   0   buffer[chars-1] == '\\n') \n    buffer[chars-1] = '\\0';\n\n// Read another line.\n// The existing buffer will be re-used, or, if necessary,\n// It will be `free`'d and a new larger buffer will `malloc`'d\nchars = getline( buffer,  size, stdin);\n\n// Later... don't forget to free the buffer!\nfree(buffer);", 
            "title": "How do I use getline?"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/", 
            "text": "What common mistakes do C programmers make?\n\n\nMemory mistakes\n\n\nString constants are constant\n\n\nchar array[] = \nHi!\n; // array contains a mutable copy \nstrcpy(array, \nOK\n);\n\nchar *ptr = \nCan't change me\n; // ptr points to some immutable memory\nstrcpy(ptr, \nWill not work\n);\n\n\n\n\nString literals are character arrays stored in the code segment of the program, which is immutable. Two string literals may share the same space in memory. An example follows:\n\n\nchar * str1 = \nBrandon Chong is the best TA\n;\nchar * str2 = \nBrandon Chong is the best TA\n;\n\n\n\n\nThe strings pointed to by \nstr1\n and \nstr2\n may actually reside in the same location in memory.\n\n\nChar arrays, however, contain the literal value which has been copied from the code segment into either the stack or static memory. These following char arrays do not reside in the same place in memory.\n\n\nchar arr1[] = \nBrandon Chong didn't write this\n;\nchar arr2[] = \nBrandon Chong didn't write this\n;\n\n\n\n\nBuffer overflow/ underflow\n\n\n#define N (10)\nint i = N, array[N];\nfor( ; i \n= 0; i--) array[i] = i;\n\n\n\n\nC does not check that pointers are valid. The above example writes into \narray[10]\n which is outside the array bounds. This can cause memory corruption because that memory location is probably being used for something else.\nIn practice, this can be harder to spot because the overflow/underflow may occur in a library call e.g.\n\n\ngets(array); // Let's hope the input is shorter than my array!\n\n\n\n\nReturning pointers to automatic variables\n\n\nint *f() {\n    int result = 42;\n    static int imok;\n    return \nimok; // OK - static variables are not on the stack\n    return \nresult; // Not OK\n}\n\n\n\n\nAutomatic variables are bound to stack memory only for the lifetime of the function.\nAfter the function returns it is an error to continue to use the memory.\n\n\nInsufficient memory allocation\n\n\nstruct User {\n   char name[100];\n};\ntypedef struct User user_t;\n\nuser_t *user = (user_t *) malloc(sizeof(user));\n\n\n\n\nIn the above example, we needed to allocate enough bytes for the struct. Instead we allocated enough bytes to hold a pointer. Once we start using the user pointer we will corrupt memory. Correct code is shown below.\n\n\nstruct User {\n   char name[100];\n};\ntypedef struct User user_t;\n\nuser_t * user = (user_t *) malloc(sizeof(user_t));\n\n\n\n\nStrings require \nstrlen(s)+1\n bytes\n\n\nEvery string must have a null byte after the last characters. To store the string \n\"Hi\"\n it takes 3 bytes: \n[H] [i] [\\0]\n.\n\n\n  char *strdup(const char *s) {  /* return a copy of 'input' */\n    char *copy;\n    copy = malloc(sizeof(char*));     /* nope! this allocates space for a pointer, not a string */\n    copy = malloc(strlen(input));     /* Almost...but what about the null terminator? */\n    copy = malloc(strlen(input) + 1); /* That's right. */\n    strcpy(copy, input);   /* strcpy will provide the null terminator */\n    return copy;\n}\n\n\n\n\nUsing uninitialized variables\n\n\nint myfunction() {\n  int x;\n  int y = x + 2;\n...\n\n\n\n\nAutomatic variables hold garbage (whatever bit pattern happened to be in memory). It is an error to assume that it will always be initialized to zero.\n\n\nAssuming Uninitialized memory will be zeroed\n\n\nvoid myfunct() {\n   char array[10];\n   char *p = malloc(10);\n\n\n\n\nAutomatic (temporary variables) are not automatically initialized to zero.\nHeap allocations using malloc are not automatically initialized to zero.\n\n\nDouble-free\n\n\n  char *p = malloc(10);\n  free(p);\n//  .. later ...\n  free(p); \n\n\n\n\nIt is an error to free the same block of memory twice.\n\n\nDangling pointers\n\n\n  char *p = malloc(10);\n  strcpy(p, \nHello\n);\n  free(p);\n//  .. later ...\n  strcpy(p,\nWorld\n); \n\n\n\n\nPointers to freed memory should not be used. A defensive programming practice is to set pointers to null as soon as the memory is freed.\n\n\nIt is a good idea to turn free into the following snippet that automatically sets the freed variable to null right after:(vim - ultisnips)  \n\n\nsnippet free \nfree(something)\n b\nfree(${1});\n$1 = NULL;\n${2}\nendsnippet\n\n\n\n\nLogic and Program flow mistakes\n\n\nForgetting break\n\n\nint flag = 1; // Will print all three lines.\nswitch(flag) {\n  case 1: printf(\nI'm printed\\n\n);\n  case 2: printf(\nMe too\\n\n);\n  case 3: printf(\nMe three\\n\n);\n}\n\n\n\n\nCase statements without a break will just continue onto the code of the next case statement. Correct code is show bellow. The break for the last statements is unnecessary because there are no more cases to be executed after the last one. However if more are added, it can cause some bugs.\n\n\nint flag = 1; // Will print all three lines.\nswitch(flag) {\n  case 1: \n    printf(\nI'm printed\\n\n);\n    break;\n  case 2: \n    printf(\nMe too\\n\n);\n    break;\n  case 3: \n    printf(\nMe three\\n\n);\n    break; //unnecessary\n}\n\n\n\n\nEqual vs. equality\n\n\nint answer = 3; // Will print out the answer.\nif (answer = 42) { printf(\nI've solved the answer! It's %d\n, answer);}\n\n\n\n\nUndeclared or incorrectly prototyped functions\n\n\ntime_t start = time();\n\n\n\n\nThe system function 'time' actually takes a parameter (a pointer to some memory that can receive the time_t structure). The compiler did not catch this error because the programmer did not provide a valid function prototype by including \ntime.h\n\n\nExtra Semicolons\n\n\nfor(int i = 0; i \n 5; i++) ; printf(\nI'm printed once\n);\nwhile(x \n 10); x++ ; // X is never incremented\n\n\n\n\nHowever, the following code is perfectly OK.\n\n\nfor(int i = 0; i \n 5; i++){\n    printf(\n%d\\n\n, i);;;;;;;;;;;;;\n}\n\n\n\n\nIt is OK to have this kind of code, because the C language uses semicolons (;) to separate statements. If there is no statement in between semicolons, then there is nothing to do and the compiler moves on to the next statement\n\n\nOther Gotchas\n\n\nC Preprocessor macros and side-effects\n\n\n#define min(a,b) ((a)\n(b) ? (a) : (b))\nint x = 4;\nif(min(x++, 100)) printf(\n%d is six\n, x);\n\n\n\n\nMacros are simple text substitution so the above example expands to \nx++ \n 100 ? x++ : 100\n (parenthesis omitted for clarity)\n\n\nC Preprocessor macros and precedence\n\n\n#define min(a,b) a\nb ? a : b\nint x = 99;\nint r = 10 + min(99, 100); // r is 100!\n\n\n\n\nMacros are simple text substitution so the above example expands to \n10 + 99 \n 100 ? 99 : 100\n\n\nAssignments in Conditions\n\n\nint a = 0;\nif (a = 1) {\n    printf(\nWhat is a?\\n\n);\n}\n\n\n\n\nNotice the second line--\na = 1\n vs. \na == 1\n. What happens here? The assignment operator in C returns the value on the right. So in this case, \nif (a = 1)\n evaluates to \nif (1)\n.", 
            "title": "C Programming, Part 3: Common Gotchas"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#memory-mistakes", 
            "text": "", 
            "title": "Memory mistakes"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#string-constants-are-constant", 
            "text": "char array[] =  Hi! ; // array contains a mutable copy \nstrcpy(array,  OK );\n\nchar *ptr =  Can't change me ; // ptr points to some immutable memory\nstrcpy(ptr,  Will not work );  String literals are character arrays stored in the code segment of the program, which is immutable. Two string literals may share the same space in memory. An example follows:  char * str1 =  Brandon Chong is the best TA ;\nchar * str2 =  Brandon Chong is the best TA ;  The strings pointed to by  str1  and  str2  may actually reside in the same location in memory.  Char arrays, however, contain the literal value which has been copied from the code segment into either the stack or static memory. These following char arrays do not reside in the same place in memory.  char arr1[] =  Brandon Chong didn't write this ;\nchar arr2[] =  Brandon Chong didn't write this ;", 
            "title": "String constants are constant"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#buffer-overflow-underflow", 
            "text": "#define N (10)\nint i = N, array[N];\nfor( ; i  = 0; i--) array[i] = i;  C does not check that pointers are valid. The above example writes into  array[10]  which is outside the array bounds. This can cause memory corruption because that memory location is probably being used for something else.\nIn practice, this can be harder to spot because the overflow/underflow may occur in a library call e.g.  gets(array); // Let's hope the input is shorter than my array!", 
            "title": "Buffer overflow/ underflow"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#returning-pointers-to-automatic-variables", 
            "text": "int *f() {\n    int result = 42;\n    static int imok;\n    return  imok; // OK - static variables are not on the stack\n    return  result; // Not OK\n}  Automatic variables are bound to stack memory only for the lifetime of the function.\nAfter the function returns it is an error to continue to use the memory.", 
            "title": "Returning pointers to automatic variables"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#insufficient-memory-allocation", 
            "text": "struct User {\n   char name[100];\n};\ntypedef struct User user_t;\n\nuser_t *user = (user_t *) malloc(sizeof(user));  In the above example, we needed to allocate enough bytes for the struct. Instead we allocated enough bytes to hold a pointer. Once we start using the user pointer we will corrupt memory. Correct code is shown below.  struct User {\n   char name[100];\n};\ntypedef struct User user_t;\n\nuser_t * user = (user_t *) malloc(sizeof(user_t));", 
            "title": "Insufficient memory allocation"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#strings-require-strlens1-bytes", 
            "text": "Every string must have a null byte after the last characters. To store the string  \"Hi\"  it takes 3 bytes:  [H] [i] [\\0] .    char *strdup(const char *s) {  /* return a copy of 'input' */\n    char *copy;\n    copy = malloc(sizeof(char*));     /* nope! this allocates space for a pointer, not a string */\n    copy = malloc(strlen(input));     /* Almost...but what about the null terminator? */\n    copy = malloc(strlen(input) + 1); /* That's right. */\n    strcpy(copy, input);   /* strcpy will provide the null terminator */\n    return copy;\n}", 
            "title": "Strings require strlen(s)+1 bytes"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#using-uninitialized-variables", 
            "text": "int myfunction() {\n  int x;\n  int y = x + 2;\n...  Automatic variables hold garbage (whatever bit pattern happened to be in memory). It is an error to assume that it will always be initialized to zero.", 
            "title": "Using uninitialized variables"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#assuming-uninitialized-memory-will-be-zeroed", 
            "text": "void myfunct() {\n   char array[10];\n   char *p = malloc(10);  Automatic (temporary variables) are not automatically initialized to zero.\nHeap allocations using malloc are not automatically initialized to zero.", 
            "title": "Assuming Uninitialized memory will be zeroed"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#double-free", 
            "text": "char *p = malloc(10);\n  free(p);\n//  .. later ...\n  free(p);   It is an error to free the same block of memory twice.", 
            "title": "Double-free"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#dangling-pointers", 
            "text": "char *p = malloc(10);\n  strcpy(p,  Hello );\n  free(p);\n//  .. later ...\n  strcpy(p, World );   Pointers to freed memory should not be used. A defensive programming practice is to set pointers to null as soon as the memory is freed.  It is a good idea to turn free into the following snippet that automatically sets the freed variable to null right after:(vim - ultisnips)    snippet free  free(something)  b\nfree(${1});\n$1 = NULL;\n${2}\nendsnippet", 
            "title": "Dangling pointers"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#logic-and-program-flow-mistakes", 
            "text": "", 
            "title": "Logic and Program flow mistakes"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#forgetting-break", 
            "text": "int flag = 1; // Will print all three lines.\nswitch(flag) {\n  case 1: printf( I'm printed\\n );\n  case 2: printf( Me too\\n );\n  case 3: printf( Me three\\n );\n}  Case statements without a break will just continue onto the code of the next case statement. Correct code is show bellow. The break for the last statements is unnecessary because there are no more cases to be executed after the last one. However if more are added, it can cause some bugs.  int flag = 1; // Will print all three lines.\nswitch(flag) {\n  case 1: \n    printf( I'm printed\\n );\n    break;\n  case 2: \n    printf( Me too\\n );\n    break;\n  case 3: \n    printf( Me three\\n );\n    break; //unnecessary\n}", 
            "title": "Forgetting break"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#equal-vs-equality", 
            "text": "int answer = 3; // Will print out the answer.\nif (answer = 42) { printf( I've solved the answer! It's %d , answer);}", 
            "title": "Equal vs. equality"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#undeclared-or-incorrectly-prototyped-functions", 
            "text": "time_t start = time();  The system function 'time' actually takes a parameter (a pointer to some memory that can receive the time_t structure). The compiler did not catch this error because the programmer did not provide a valid function prototype by including  time.h", 
            "title": "Undeclared or incorrectly prototyped functions"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#extra-semicolons", 
            "text": "for(int i = 0; i   5; i++) ; printf( I'm printed once );\nwhile(x   10); x++ ; // X is never incremented  However, the following code is perfectly OK.  for(int i = 0; i   5; i++){\n    printf( %d\\n , i);;;;;;;;;;;;;\n}  It is OK to have this kind of code, because the C language uses semicolons (;) to separate statements. If there is no statement in between semicolons, then there is nothing to do and the compiler moves on to the next statement", 
            "title": "Extra Semicolons"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#other-gotchas", 
            "text": "", 
            "title": "Other Gotchas"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#c-preprocessor-macros-and-side-effects", 
            "text": "#define min(a,b) ((a) (b) ? (a) : (b))\nint x = 4;\nif(min(x++, 100)) printf( %d is six , x);  Macros are simple text substitution so the above example expands to  x++   100 ? x++ : 100  (parenthesis omitted for clarity)", 
            "title": "C Preprocessor macros and side-effects"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#c-preprocessor-macros-and-precedence", 
            "text": "#define min(a,b) a b ? a : b\nint x = 99;\nint r = 10 + min(99, 100); // r is 100!  Macros are simple text substitution so the above example expands to  10 + 99   100 ? 99 : 100", 
            "title": "C Preprocessor macros and precedence"
        }, 
        {
            "location": "/C-Programming,-Part-3:-Common-Gotchas/#assignments-in-conditions", 
            "text": "int a = 0;\nif (a = 1) {\n    printf( What is a?\\n );\n}  Notice the second line-- a = 1  vs.  a == 1 . What happens here? The assignment operator in C returns the value on the right. So in this case,  if (a = 1)  evaluates to  if (1) .", 
            "title": "Assignments in Conditions"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/", 
            "text": "Warning - question numbers subject to change\n\n\nMemory and Strings\n\n\nQ1.1\n\n\nIn the example below, which variables are guaranteed to print the value of zero?\n\n\nint a;\nstatic int b;\n\nvoid func() {\n   static int c;\n   int d;\n   printf(\n%d %d %d %d\\n\n,a,b,c,d);\n}\n\n\n\n\nQ 1.2\n\n\nIn the example below, which variables are guaranteed to print the value of zero?\n\n\nvoid func() {\n   int* ptr1 = malloc( sizeof(int) );\n   int* ptr2 = realloc(NULL, sizeof(int) );\n   int* ptr3 = calloc( 1, sizeof(int) );\n   int* ptr4 = calloc( sizeof(int) , 1);\n\n   printf(\n%d %d %d %d\\n\n,*ptr1,*ptr2,*ptr3,*ptr);\n}\n\n\n\n\nQ 1.3\n\n\nExplain the error in the following attempt to copy a string.\n\n\nchar* copy(char*src) {\n char*result = malloc( strlen(src) ); \n strcpy(result, src); \n return result;\n}\n\n\n\n\nQ 1.4\n\n\nWhy does the following attempt to copy a string sometimes work and sometimes fail?\n\n\nchar* copy(char*src) {\n char*result = malloc( strlen(src) +1 ); \n strcat(result, src); \n return result;\n}\n\n\n\n\nQ 1.4\n\n\nExplain the two errors in the following code that attempts to copy a string.\n\n\nchar* copy(char*src) {\n char result[sizeof(src)]; \n strcpy(result, src); \n return result;\n}\n\n\n\n\nQ 1.5\n\n\nWhich of the following is legal?\n\n\nchar a[] = \nHello\n; strcpy(a, \nWorld\n);\nchar b[] = \nHello\n; strcpy(b, \nWorld12345\n, b);\nchar* c = \nHello\n; strcpy(c, \nWorld\n);\n\n\n\n\nQ 1.6\n\n\nComplete the function pointer typedef to declare a pointer to a function that takes a void\n argument and returns a void\n. Name your type 'pthread_callback'\n\n\ntypedef ______________________;\n\n\n\n\nQ 1.7\n\n\nIn addition to the function arguments what else is stored on a thread's stack?\n\n\nQ 1.8\n\n\nImplement a version of \nchar* strcat(char*dest, const char*src)\n using only \nstrcpy\n  \nstrlen\n and pointer arithmetic\n\n\nchar* mystrcat(char*dest, const char*src) {\n\n  ? Use strcpy strlen here\n\n  return dest;\n}\n\n\n\n\nQ 1.9\n\n\nImplement version of size_t strlen(const char*) using a loop and no function calls.\n\n\nsize_t mystrlen(const char*s) {\n\n}\n\n\n\n\nQ 1.10\n\n\nIdentify the three bugs in the following implementation of \nstrcpy\n.\n\n\nchar* strcpy(const char* dest, const char* src) {\n  while(*src) { *dest++ = *src++; }\n  return dest;\n}\n\n\n\n\nPrinting\n\n\nQ 2.1\n\n\nSpot the error!\n\n\nfprintf(stdout, \nYou scored 100%\n);\n\n\n\n\nFormatting and Printing to a file\n\n\nQ 3.1\n\n\nComplete the following code to print to a file. Print the name, a comma and the score to the file 'result.txt'\n\n\nchar* name = .....;\nint score = ......\nFILE *f = fopen(\nresult.txt\n,_____);\nif(f) {\n    _____\n}\nfclose(f);\n\n\n\n\nPrinting to a string\n\n\nQ 4.1\n\n\nHow would you print the values of variables a,mesg,val and ptr to a string? Print a as an integer, mesg as C string, val as a double val and ptr as a hexadecimal pointer. You may assume the mesg points to a short C string(\n50 characters).\nBonus: How would you make this code more robust or able to cope with?\n\n\nchar* toString(int a, char*mesg, double val, void* ptr) {\n   char* result = malloc( strlen(mesg) + 50);\n    _____\n   return result;\n}\n\n\n\n\nInput parsing\n\n\nQ 5.1\n\n\nWhy should you check the return value of sscanf and scanf?\n\n\nQ 5.2\n\n\nWhy is 'gets' dangerous?\n\n\nQ 5.3\n\n\nWrite a complete program that uses \ngetline\n. Ensure your program has no memory leaks.\n\n\nHeap memory\n\n\nWhen would you use calloc not malloc? \nWhen would realloc be useful?\n\n\n(Todo - move this question to another page)\nWhat mistake did the programmer make in the following code? Is it possible to fix it i) using heap memory? ii) using global (static) memory?\n\n\nstatic int id;\n\nchar* next_ticket() {\n  id ++;\n  char result[20];\n  sprintf(result,\n%d\n,id);\n  return result;\n}", 
            "title": "C Programming: Review Questions"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#warning-question-numbers-subject-to-change", 
            "text": "", 
            "title": "Warning - question numbers subject to change"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#memory-and-strings", 
            "text": "", 
            "title": "Memory and Strings"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q11", 
            "text": "In the example below, which variables are guaranteed to print the value of zero?  int a;\nstatic int b;\n\nvoid func() {\n   static int c;\n   int d;\n   printf( %d %d %d %d\\n ,a,b,c,d);\n}", 
            "title": "Q1.1"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-12", 
            "text": "In the example below, which variables are guaranteed to print the value of zero?  void func() {\n   int* ptr1 = malloc( sizeof(int) );\n   int* ptr2 = realloc(NULL, sizeof(int) );\n   int* ptr3 = calloc( 1, sizeof(int) );\n   int* ptr4 = calloc( sizeof(int) , 1);\n\n   printf( %d %d %d %d\\n ,*ptr1,*ptr2,*ptr3,*ptr);\n}", 
            "title": "Q 1.2"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-13", 
            "text": "Explain the error in the following attempt to copy a string.  char* copy(char*src) {\n char*result = malloc( strlen(src) ); \n strcpy(result, src); \n return result;\n}", 
            "title": "Q 1.3"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-14", 
            "text": "Why does the following attempt to copy a string sometimes work and sometimes fail?  char* copy(char*src) {\n char*result = malloc( strlen(src) +1 ); \n strcat(result, src); \n return result;\n}", 
            "title": "Q 1.4"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-14_1", 
            "text": "Explain the two errors in the following code that attempts to copy a string.  char* copy(char*src) {\n char result[sizeof(src)]; \n strcpy(result, src); \n return result;\n}", 
            "title": "Q 1.4"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-15", 
            "text": "Which of the following is legal?  char a[] =  Hello ; strcpy(a,  World );\nchar b[] =  Hello ; strcpy(b,  World12345 , b);\nchar* c =  Hello ; strcpy(c,  World );", 
            "title": "Q 1.5"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-16", 
            "text": "Complete the function pointer typedef to declare a pointer to a function that takes a void  argument and returns a void . Name your type 'pthread_callback'  typedef ______________________;", 
            "title": "Q 1.6"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-17", 
            "text": "In addition to the function arguments what else is stored on a thread's stack?", 
            "title": "Q 1.7"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-18", 
            "text": "Implement a version of  char* strcat(char*dest, const char*src)  using only  strcpy    strlen  and pointer arithmetic  char* mystrcat(char*dest, const char*src) {\n\n  ? Use strcpy strlen here\n\n  return dest;\n}", 
            "title": "Q 1.8"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-19", 
            "text": "Implement version of size_t strlen(const char*) using a loop and no function calls.  size_t mystrlen(const char*s) {\n\n}", 
            "title": "Q 1.9"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-110", 
            "text": "Identify the three bugs in the following implementation of  strcpy .  char* strcpy(const char* dest, const char* src) {\n  while(*src) { *dest++ = *src++; }\n  return dest;\n}", 
            "title": "Q 1.10"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#printing", 
            "text": "", 
            "title": "Printing"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-21", 
            "text": "Spot the error!  fprintf(stdout,  You scored 100% );", 
            "title": "Q 2.1"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#formatting-and-printing-to-a-file", 
            "text": "", 
            "title": "Formatting and Printing to a file"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-31", 
            "text": "Complete the following code to print to a file. Print the name, a comma and the score to the file 'result.txt'  char* name = .....;\nint score = ......\nFILE *f = fopen( result.txt ,_____);\nif(f) {\n    _____\n}\nfclose(f);", 
            "title": "Q 3.1"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#printing-to-a-string", 
            "text": "", 
            "title": "Printing to a string"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-41", 
            "text": "How would you print the values of variables a,mesg,val and ptr to a string? Print a as an integer, mesg as C string, val as a double val and ptr as a hexadecimal pointer. You may assume the mesg points to a short C string( 50 characters).\nBonus: How would you make this code more robust or able to cope with?  char* toString(int a, char*mesg, double val, void* ptr) {\n   char* result = malloc( strlen(mesg) + 50);\n    _____\n   return result;\n}", 
            "title": "Q 4.1"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#input-parsing", 
            "text": "", 
            "title": "Input parsing"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-51", 
            "text": "Why should you check the return value of sscanf and scanf?", 
            "title": "Q 5.1"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-52", 
            "text": "Why is 'gets' dangerous?", 
            "title": "Q 5.2"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#q-53", 
            "text": "Write a complete program that uses  getline . Ensure your program has no memory leaks.", 
            "title": "Q 5.3"
        }, 
        {
            "location": "/C-Programming:-Review-Questions/#heap-memory", 
            "text": "When would you use calloc not malloc? \nWhen would realloc be useful?  (Todo - move this question to another page)\nWhat mistake did the programmer make in the following code? Is it possible to fix it i) using heap memory? ii) using global (static) memory?  static int id;\n\nchar* next_ticket() {\n  id ++;\n  char result[20];\n  sprintf(result, %d ,id);\n  return result;\n}", 
            "title": "Heap memory"
        }, 
        {
            "location": "/Deadlock,-Part-1:-Resource-Allocation-Graph/", 
            "text": "What is a Resource Allocation Graph?\n\n\nA resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. It is very powerful and simple tool to illustrate how interacting  processes can deadlock. If a process is \nusing\n a resource, an arrow is drawn from the resource node to the process node. If a process is \nrequesting\n a resource, an arrow is drawn from the process node to the resource node.\n\n\nIf there is a cycle in the Resource Allocation Graph then the processes will deadlock. For example, if process 1 holds resource A, process 2 holds resource B and process 1 is waiting for B and process 2 is waiting for A, then process 1 and 2 process will be deadlocked.\n\n\nHere's another example, that shows Processes 1 and 2 acquiring resources 1 and 2 while process 3 is waiting to acquire both resources. In this example there is no deadlock because there is no circular dependency.\n\n\n\n\nTodo: More complicated example", 
            "title": "Deadlock, Part 1: Resource Allocation Graph"
        }, 
        {
            "location": "/Deadlock,-Part-1:-Resource-Allocation-Graph/#what-is-a-resource-allocation-graph", 
            "text": "A resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. It is very powerful and simple tool to illustrate how interacting  processes can deadlock. If a process is  using  a resource, an arrow is drawn from the resource node to the process node. If a process is  requesting  a resource, an arrow is drawn from the process node to the resource node.  If there is a cycle in the Resource Allocation Graph then the processes will deadlock. For example, if process 1 holds resource A, process 2 holds resource B and process 1 is waiting for B and process 2 is waiting for A, then process 1 and 2 process will be deadlocked.  Here's another example, that shows Processes 1 and 2 acquiring resources 1 and 2 while process 3 is waiting to acquire both resources. In this example there is no deadlock because there is no circular dependency.   Todo: More complicated example", 
            "title": "What is a Resource Allocation Graph?"
        }, 
        {
            "location": "/Deadlock,-Part-2:-Deadlock-Conditions/", 
            "text": "Coffman conditions\n\n\nThere are four \nnecessary\n and \nsufficient\n conditions for deadlock. These are known as the Coffman conditions.\n\n\n\n\nMutual Exclusion\n\n\nCircular Wait\n\n\nHold and Wait\n\n\nNo pre-emption\n\n\n\n\nAll of these conditions are required for deadlock, so let's discuss each one in turn. First the easy ones-\n\n Mutual Exclusion: The resource cannot be shared\n\n Circular Wait: There exists a cycle in the Resource Allocation Graph. There exists a set of processes {P1,P2,...} such that P1 is waiting for resources held by P2, which is waiting for P3,..., which is waiting for P1.\n\n Hold and Wait: A process acquires an incomplete set of resources and holds onto them while waiting for the other resources.\n\n No pre-emption: Once a process has acquired a resource, the resource cannot be taken away from a process and the process will not voluntarily give up a resource.\n\n\nBreaking the Coffman Conditions\n\n\nTwo students need a pen and paper:\n\n The students share a pen and paper. Deadlock is avoided because Mutual Exclusion was not required.\n\n The students both agree to grab the pen before grabbing the paper. Deadlock is avoided because there cannot be a circular wait.\n\n The students grab both the pen and paper in one operation (\"Get both or get none\"). Deadlock is avoided because there is no \nHold and Wait\n\n\n The students are friends and will ask each other to give up a held resource. Deadlock is avoided because pre-emption is allowed.\n\n\nLivelock\n\n\nLivelock is \nnot\n deadlock-\n\n\nConsider the following 'solution'\n* The students will put down one held resource if they are unable to pick up the other resource within 10 seconds. This solution avoids deadlock however it may suffer from livelock.\n\n\nLivelock occurs when a process continues to execute but is unable to make progress.\nIn practice Livelock may occur because the programmer has taken steps to avoid deadlock. In the above example, in a busy system, the student will continually release the first resource because they are never able to obtain the second resource. The system is not deadlock (the student process is still executing) however it's not making any progress either.\n\n\nDining Philosophers\n\n\nThe Dining Philosophers problem is a classic synchronization problem. Imagine I invite N (let's say 5) philosophers to a meal. We will sit them at a table with 5 chopsticks (one between each philosopher). A philosopher alternates between wanting to eat or think. To eat the philosopher must pick up the two chopsticks either side of their position (the original problem required each philosopher to have two forks). However these chopsticks are shared with his neighbor.\n\n\n\n\nIs it possible to design an efficient solution such that all philosophers get to eat? Or, will some philosophers starve, never obtaining a second chopstick? Or will all of them deadlock? For example, imagine each guest picks up the chopstick on their left and then waits for the chopstick on their right to be free. Oops - our philosophers have deadlocked!", 
            "title": "Deadlock, Part 2: Deadlock Conditions"
        }, 
        {
            "location": "/Deadlock,-Part-2:-Deadlock-Conditions/#coffman-conditions", 
            "text": "There are four  necessary  and  sufficient  conditions for deadlock. These are known as the Coffman conditions.   Mutual Exclusion  Circular Wait  Hold and Wait  No pre-emption   All of these conditions are required for deadlock, so let's discuss each one in turn. First the easy ones-  Mutual Exclusion: The resource cannot be shared  Circular Wait: There exists a cycle in the Resource Allocation Graph. There exists a set of processes {P1,P2,...} such that P1 is waiting for resources held by P2, which is waiting for P3,..., which is waiting for P1.  Hold and Wait: A process acquires an incomplete set of resources and holds onto them while waiting for the other resources.  No pre-emption: Once a process has acquired a resource, the resource cannot be taken away from a process and the process will not voluntarily give up a resource.", 
            "title": "Coffman conditions"
        }, 
        {
            "location": "/Deadlock,-Part-2:-Deadlock-Conditions/#breaking-the-coffman-conditions", 
            "text": "Two students need a pen and paper:  The students share a pen and paper. Deadlock is avoided because Mutual Exclusion was not required.  The students both agree to grab the pen before grabbing the paper. Deadlock is avoided because there cannot be a circular wait.  The students grab both the pen and paper in one operation (\"Get both or get none\"). Deadlock is avoided because there is no  Hold and Wait   The students are friends and will ask each other to give up a held resource. Deadlock is avoided because pre-emption is allowed.", 
            "title": "Breaking the Coffman Conditions"
        }, 
        {
            "location": "/Deadlock,-Part-2:-Deadlock-Conditions/#livelock", 
            "text": "Livelock is  not  deadlock-  Consider the following 'solution'\n* The students will put down one held resource if they are unable to pick up the other resource within 10 seconds. This solution avoids deadlock however it may suffer from livelock.  Livelock occurs when a process continues to execute but is unable to make progress.\nIn practice Livelock may occur because the programmer has taken steps to avoid deadlock. In the above example, in a busy system, the student will continually release the first resource because they are never able to obtain the second resource. The system is not deadlock (the student process is still executing) however it's not making any progress either.", 
            "title": "Livelock"
        }, 
        {
            "location": "/Deadlock,-Part-2:-Deadlock-Conditions/#dining-philosophers", 
            "text": "The Dining Philosophers problem is a classic synchronization problem. Imagine I invite N (let's say 5) philosophers to a meal. We will sit them at a table with 5 chopsticks (one between each philosopher). A philosopher alternates between wanting to eat or think. To eat the philosopher must pick up the two chopsticks either side of their position (the original problem required each philosopher to have two forks). However these chopsticks are shared with his neighbor.   Is it possible to design an efficient solution such that all philosophers get to eat? Or, will some philosophers starve, never obtaining a second chopstick? Or will all of them deadlock? For example, imagine each guest picks up the chopstick on their left and then waits for the chopstick on their right to be free. Oops - our philosophers have deadlocked!", 
            "title": "Dining Philosophers"
        }, 
        {
            "location": "/Exam-Topics/", 
            "text": "The final exam will likely include multiple choice questions that test your mastery of the following.\n\n\n`\nCSP (critical section problems)\nHTTP\nSIGINT\nTCP\nTLB\nVirtual Memory\narrays\nbarrier\nc strings\nchmod\nclient/server\ncoffman conditions\ncondition variables\ncontext switch\ndeadlock\ndining philosophers\nepoll\nexit\nfile I/O\nfile system representation\nfork/exec/wait\nfprintf\nfree\nheap allocator\nheap/stack\ninode vs name\nmalloc\nmkfifo\nmmap\nmutexes\nnetwork ports\nopen/close\noperating system terms\npage fault\npage tables\npipes\npointer arithmetic\npointers\nprinting (printf)\nproducer/consumer\nprogress/mutex\nrace conditions\nread/write\nreader/writer\nresource allocation graphs\nring buffer\nscanf \nbuffering\nscheduling\nselect\nsemaphores\nsignals\nsizeof\nstat\nstderr/stdout\nsymlinks\nthread control (_create, _join, _exit)\nvariable initializers\nvariable scope\nvm thrashing\nwait macros\nwrite/read with errno, EINTR and partial data", 
            "title": "Exam Topics"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/", 
            "text": "Design a file system! What are your design goals?\n\n\nThe design of a file system is difficult problem because there many high-level design goals that we'd like to satisfy. An incomplete list of ideal goals include:\n\n\n\n\nReliable and robust (even with hardware failures or incomplete writes dues to power loss)\n\n\nAccess (security) controls\n\n\nAccounting and quotas\n\n\nIndexing and search\n\n\nVersioning and backup capabilities\n\n\nEncryption\n\n\nAutomatic compression\n\n\nHigh performance (e.g. Caching in-memory)\n\n\nEfficient use of storage de-duplication\n\n\n\n\nNot all filesystems natively support all of these goals. For example, many filesystems do not automatically compress rarely-used files\n\n\nWhat are \n.\n, \n..\n, and \n...\n?\n\n\n.\n represents the current directory  \n\n\n..\n represents the parent directory  \n\n\n...\n is NOT a valid representation of any directory (this not the grandparent directory)\n\n\nWhat are absolute and relative paths?\n\n\nAbsolute paths are paths that start from the 'root node' of your directory tree. Relative paths are paths that start from your current position in the tree.\n\n\nWhat are some examples of relative and absolute paths?\n\n\nIf you start in your home directory (\"~\" for short), then \nDesktop/cs241\n would be a relative path. Its absolute path counterpart might be something like \n/Users/[yourname]/Desktop/cs241\n.\n\n\nHow do I simplify \na/b/../c/./\n?\n\n\nRemember that \n..\n means 'parent folder' and that \n.\n means 'current folder'.\n\n\nExample: \na/b/../c/./\n\n- Step 1: \ncd a\n (in a)\n- Step 2: \ncd b\n (in a/b)\n- Step 3: \ncd ..\n (in a, because .. represents 'parent folder')\n- Step 4: \ncd c\n (in a/c)\n- Step 5: \ncd .\n (in a/c, because . represents 'current folder')\n\n\nThus, this path can be simplified to \na/c\n.\n\n\nWhy make disk blocks the same size as memory pages?\n\n\nTo support virtual memory, so we can page stuff in and out of memory.\n\n\nWhat information do we want to store for each file?\n\n\n\n\nFilename\n\n\nFile size\n\n\nTime created, last modified, last accessed\n\n\nPermissions\n\n\nFilepath\n\n\nChecksum\n\n\nFile data (inode)\n\n\n\n\nWhat are the traditional permissions: user \u2013 group \u2013 other permissions for a file?\n\n\nSome common file permissions include:\n* 755: \nrwx r-x r-x\n\n\nuser: \nrwx\n, group: \nr-x\n, others: \nr-x\n\n\nUser can read, write and execute. Group and others can only read and execute.\n* 644: \nrw- r-- r--\n\n\nuser: \nrw-\n, group: \nr--\n, others: \nr--\n\n\nUser can read and write. Group and others can only read.\n\n\nWhat are the the 3 permission bits for a regular file for each role?\n\n\n\n\nRead (most significant bit)  \n\n\nWrite (2nd bit)  \n\n\nExecute (least significant bit)\n\n\n\n\nWhat do \"644\" \"755\" mean?\n\n\nThese are examples of permissions in octal format (base 8). Each octal digit corresponds to a different role (user, group, world).\n\n\nWe can read permissions in octal format as follows:\n\n\n 644 - R/W user permissions, R group permissions, R world permissions\n\n\n 755 - R/W/X user permissions, R/X group permissions, R/X world permissions\n\n\nWhat is an inode? Which of the above items is stored in the inode?\n\n\nFrom \nWikipedia\n:\n\n\n\n\nIn a Unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be one of various things including a file or a directory. Each inode stores the attributes and disk block location(s) of the filesystem object's data. Filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).\n\n\n\n\nHow does inode store the file contents?\n\n\n\n\nImage source: http://en.wikipedia.org/wiki/Ext2  \n\n\n\n\n\"All problems in computer science can be solved by another level of indirection.\" - David Wheeler\n\n\n\n\nHow many pointers can you store in each indirection table?\n\n\nAs a worked example, suppose we divide the disk into 4KB blocks and we want to address up to 2^32 blocks.\n\n\nThe maximum disk size is 4KB *2^32 = 16TB  (remember 2^10 = 1024)\n\n\nA disk block can store 4KB / 4B (each pointer needs to be 32 bits) = 1024 pointers. Each pointer refers to a 4KB disk block - so you can refer up to 1024*4KB = 4MB of data\n\n\nFor the same disk configuration, a double indirect block stores 1024 pointers to 1024 indirection tables. Thus a double-indirect block can refer up to 1024 * 4MB = 4GB of data.\n\n\nSimilarly, a triple indirect block can refer up to 4TB of data.\n\n\nGo to File System: Part 2", 
            "title": "File System, Part 1: Introduction"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#design-a-file-system-what-are-your-design-goals", 
            "text": "The design of a file system is difficult problem because there many high-level design goals that we'd like to satisfy. An incomplete list of ideal goals include:   Reliable and robust (even with hardware failures or incomplete writes dues to power loss)  Access (security) controls  Accounting and quotas  Indexing and search  Versioning and backup capabilities  Encryption  Automatic compression  High performance (e.g. Caching in-memory)  Efficient use of storage de-duplication   Not all filesystems natively support all of these goals. For example, many filesystems do not automatically compress rarely-used files", 
            "title": "Design a file system! What are your design goals?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#what-are-and", 
            "text": ".  represents the current directory    ..  represents the parent directory    ...  is NOT a valid representation of any directory (this not the grandparent directory)", 
            "title": "What are ., .., and ...?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#what-are-absolute-and-relative-paths", 
            "text": "Absolute paths are paths that start from the 'root node' of your directory tree. Relative paths are paths that start from your current position in the tree.", 
            "title": "What are absolute and relative paths?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#what-are-some-examples-of-relative-and-absolute-paths", 
            "text": "If you start in your home directory (\"~\" for short), then  Desktop/cs241  would be a relative path. Its absolute path counterpart might be something like  /Users/[yourname]/Desktop/cs241 .", 
            "title": "What are some examples of relative and absolute paths?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#how-do-i-simplify-abc", 
            "text": "Remember that  ..  means 'parent folder' and that  .  means 'current folder'.  Example:  a/b/../c/./ \n- Step 1:  cd a  (in a)\n- Step 2:  cd b  (in a/b)\n- Step 3:  cd ..  (in a, because .. represents 'parent folder')\n- Step 4:  cd c  (in a/c)\n- Step 5:  cd .  (in a/c, because . represents 'current folder')  Thus, this path can be simplified to  a/c .", 
            "title": "How do I simplify a/b/../c/./?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#why-make-disk-blocks-the-same-size-as-memory-pages", 
            "text": "To support virtual memory, so we can page stuff in and out of memory.", 
            "title": "Why make disk blocks the same size as memory pages?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#what-information-do-we-want-to-store-for-each-file", 
            "text": "Filename  File size  Time created, last modified, last accessed  Permissions  Filepath  Checksum  File data (inode)", 
            "title": "What information do we want to store for each file?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#what-are-the-traditional-permissions-user-group-other-permissions-for-a-file", 
            "text": "Some common file permissions include:\n* 755:  rwx r-x r-x  user:  rwx , group:  r-x , others:  r-x  User can read, write and execute. Group and others can only read and execute.\n* 644:  rw- r-- r--  user:  rw- , group:  r-- , others:  r--  User can read and write. Group and others can only read.", 
            "title": "What are the traditional permissions: user \u2013 group \u2013 other permissions for a file?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#what-are-the-the-3-permission-bits-for-a-regular-file-for-each-role", 
            "text": "Read (most significant bit)    Write (2nd bit)    Execute (least significant bit)", 
            "title": "What are the the 3 permission bits for a regular file for each role?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#what-do-644-755-mean", 
            "text": "These are examples of permissions in octal format (base 8). Each octal digit corresponds to a different role (user, group, world).  We can read permissions in octal format as follows:   644 - R/W user permissions, R group permissions, R world permissions   755 - R/W/X user permissions, R/X group permissions, R/X world permissions", 
            "title": "What do \"644\" \"755\" mean?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#what-is-an-inode-which-of-the-above-items-is-stored-in-the-inode", 
            "text": "From  Wikipedia :   In a Unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be one of various things including a file or a directory. Each inode stores the attributes and disk block location(s) of the filesystem object's data. Filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).", 
            "title": "What is an inode? Which of the above items is stored in the inode?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#how-does-inode-store-the-file-contents", 
            "text": "Image source: http://en.wikipedia.org/wiki/Ext2     \"All problems in computer science can be solved by another level of indirection.\" - David Wheeler", 
            "title": "How does inode store the file contents?"
        }, 
        {
            "location": "/File-System,-Part-1:-Introduction/#how-many-pointers-can-you-store-in-each-indirection-table", 
            "text": "As a worked example, suppose we divide the disk into 4KB blocks and we want to address up to 2^32 blocks.  The maximum disk size is 4KB *2^32 = 16TB  (remember 2^10 = 1024)  A disk block can store 4KB / 4B (each pointer needs to be 32 bits) = 1024 pointers. Each pointer refers to a 4KB disk block - so you can refer up to 1024*4KB = 4MB of data  For the same disk configuration, a double indirect block stores 1024 pointers to 1024 indirection tables. Thus a double-indirect block can refer up to 1024 * 4MB = 4GB of data.  Similarly, a triple indirect block can refer up to 4TB of data.  Go to File System: Part 2", 
            "title": "How many pointers can you store in each indirection table?"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/", 
            "text": "Big idea: Forget names of files: The 'inode' is the file.  \n\n\nIt is common to think of the file name as the 'actual' file. It's not! Instead consider the inode as the file. The inode holds the meta-information (last accessed, ownership, size) and points to the disk blocks used to hold the file contents.\n\n\nSo... How do we implement a directory?\n\n\nA directory is just a mapping of names to inode numbers.\nPOSIX provides a small set of functions to read the filename and inode number for each entry (see below)\n\n\nHow can I find the inode number of a file?\n\n\nFrom a shell, use \nls\n with the \n-i\n option\n\n\n$ ls -i\n12983989 dirlist.c      12984068 sandwhich.c\n\n\n\n\nFrom C, call one of the stat functions (introduced below).\n\n\nHow do I find out meta-information about a file (or directory)?\n\n\nUse the stat calls. For example, to find out when my 'notes.txt' file was last accessed -\n\n\n   struct stat s;\n   stat(\nnotes.txt\n, \n s);\n   printf(\nLast accessed %s\n, ctime(s.st_atime));\n\n\n\n\nThere are actually three versions of \nstat\n;\n\n\n       int stat(const char *path, struct stat *buf);\n       int fstat(int fd, struct stat *buf);\n       int lstat(const char *path, struct stat *buf);\n\n\n\n\nFor example you can use \nfstat\n to find out the meta-information about a file if you already have an file descriptor associated with that file\n\n\n   FILE *file = fopen(\nnotes.txt\n, \nr\n);\n   int fd = fileno(file); /* Just for fun - extract the file descriptor from a C FILE struct */\n   struct stat s;\n   fstat(fd, \n s);\n   printf(\nLast accessed %s\n, ctime(s.st_atime));\n\n\n\n\nThe third call 'lstat' we will discuss when we introduce symbolic links.\n\n\nIn addition to access,creation, and modified times, the stat structure includes the inode number, length of the file and owner information.\n\n\nstruct stat {\n               dev_t     st_dev;     /* ID of device containing file */\n               ino_t     st_ino;     /* inode number */\n               mode_t    st_mode;    /* protection */\n               nlink_t   st_nlink;   /* number of hard links */\n               uid_t     st_uid;     /* user ID of owner */\n               gid_t     st_gid;     /* group ID of owner */\n               dev_t     st_rdev;    /* device ID (if special file) */\n               off_t     st_size;    /* total size, in bytes */\n               blksize_t st_blksize; /* blocksize for file system I/O */\n               blkcnt_t  st_blocks;  /* number of 512B blocks allocated */\n               time_t    st_atime;   /* time of last access */\n               time_t    st_mtime;   /* time of last modification */\n               time_t    st_ctime;   /* time of last status change */\n           };\n\n\n\n\nHow do I list the contents of a directory ?\n\n\nLet's write our own version of 'ls' to list the contents of a directory.\n\n\n#include \nstdio.h\n\n#include \ndirent.h\n\n#include \nstdlib.h\n\nint main(int argc, char **argv) {\n    if(argc == 1) {\n        printf(\nUsage: %s [directory]\\n\n, *argv);\n        exit(0);\n    }\n    struct dirent *dp;\n    DIR *dirp = opendir(argv[1]);\n    while ((dp = readdir(dirp)) != NULL) {\n        puts(dp-\nd_name);\n    }\n\n    closedir(dirp);\n    return 0;\n}\n\n\n\n\nHow do I read the contents of a directory?\n\n\nAns: Use opendir readdir closedir\nFor example, here's a very simple implementation of 'ls' to list the contents of a directory.\n\n\n#include \nstdio.h\n\n#include \ndirent.h\n\n#include \nstdlib.h\n\nint main(int argc, char **argv) {\n    if(argc ==1) {\n        printf(\nUsage: %s [directory]\\n\n, *argv);\n        exit(0);\n    }\n    struct dirent *dp;\n    DIR *dirp = opendir(argv[1]);\n    while ((dp = readdir(dirp)) != NULL) {\n        printf(\n%s %lu\\n\n, dp-\n d_name, (unsigned long)dp-\n d_ino );\n    }\n\n    closedir(dirp);\n    return 0;\n}\n\n\n\n\nNote: after a call to fork(), either (XOR) the parent or the child can use readdir(), rewinddir() or seekdir(). If both the parent and the child use the above, behavior is undefined.\n\n\nHow do I check to see if a file is in the current directory?\n\n\nFor example, to see if a particular directory includes a file (or filename) 'name', we might write the following code. (Hint: Can you spot the bug?)\n\n\nint exists(char *directory, char *name)  {\n    struct dirent *dp;\n    DIR *dirp = opendir(directory);\n    while ((dp = readdir(dirp)) != NULL) {\n        puts(dp-\nd_name);\n        if (!strcmp(dp-\nd_name, name)) {\n        return 1; /* Found */\n        }\n    }\n    closedir(dirp);\n    return 0; /* Not Found */\n}\n\n\n\n\nThe above code has a subtle bug: It leaks resources! If a matching filename is found then 'closedir' is never called as part of the early return. Any file descriptors opened, and any memory allocated, by opendir are never released. This means eventually the process will run out of resources and an \nopen\n or \nopendir\n call will fail.\n\n\nThe fix is to ensure we free up resources in every possible code-path. In the above code this means calling \nclosedir\n before \nreturn 1\n. Forgetting to release resources is a common C programming bug because there is no support in the C lanaguage to ensure resources are always released with all codepaths.\n\n\nAn aside - A System programming pattern to clean up resources - goto considered useful!\n\n\nNote If C supported exception handling this discussion would be unnecessary.\nImagine your function required several temporary resources that need to be freed before returning.\nHow can we write readable code that correctly frees resources under all code paths? Some system programs use \ngoto\n to jump forward into the clean up code, using the following pattern:\n\n\nint f() {\n   Acquire resource r1\n   if(...) goto clean_up_r1\n   Acquire resource r2\n   if(...) goto clean_up_r2\n\n   perform work\nclean_up_r2:\n   clean up r2\nclean_up_r1:\n   clean up r1\n   return result\n}\n\n\n\n\nWhether this is a good thing or not has led to long rigorous debates that have generally helped system programmers stay warm during the cold winter months. Are there alternatives? Yes! For example using conditional logic, breaking out of do-while loops and writing secondary functions that perform the innermost work. However all choices are problematic and cumbersome as we are attempting to shoe-horn in exception handling in a language that has no inbuilt support for it.\n\n\nWhat are the gotcha's of using readdir? For example to recursively search directories?\n\n\nThere are two main gotchas and one consideration:\nThe \nreaddir\n function returns \".\" (current directory) and \"..\" (parent directory). If you are looking for sub-directories, you need to explicitly exclude these directories.\n\n\nFor many applications it's reasonable to check the current directory first before recursively searching sub-directories. This can be achieved by storing the results in a linked list, or resetting the directory struct to restart from the beginning.\n\n\nOne final note of caution: \nreaddir\n is not thread-safe! For multi-threaded searches use \nreaddir_r\n which requires the caller to pass in the address of an existing dirent struct.\n\n\nSee the man page of readdir for more details.\n\n\nHow I do determine if a directory entry is a directory?\n\n\nAns: Use \nS_ISDIR\n to check the mode bits stored in the stat structure\n\n\nAnd to check if a file is regular file use \nS_ISREG\n,\n\n\n   struct stat s;\n   if (0 == stat(name, \ns)) {\n      printf(\n%s \n, name);\n      if (S_ISDIR( s.st_mode)) puts(\nis a directory\n);\n      if (S_ISREG( s.st_mode)) puts(\nis a regular file\n);\n   } else {\n      perror(\nstat failed - are you sure I can read this file's meta data?\n);\n   }\n\n\n\n\nGo to File System, Part 3", 
            "title": "File System, Part 2: Files are inodes (everything else is just data...)"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#so-how-do-we-implement-a-directory", 
            "text": "A directory is just a mapping of names to inode numbers.\nPOSIX provides a small set of functions to read the filename and inode number for each entry (see below)", 
            "title": "So... How do we implement a directory?"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-can-i-find-the-inode-number-of-a-file", 
            "text": "From a shell, use  ls  with the  -i  option  $ ls -i\n12983989 dirlist.c      12984068 sandwhich.c  From C, call one of the stat functions (introduced below).", 
            "title": "How can I find the inode number of a file?"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-find-out-meta-information-about-a-file-or-directory", 
            "text": "Use the stat calls. For example, to find out when my 'notes.txt' file was last accessed -     struct stat s;\n   stat( notes.txt ,   s);\n   printf( Last accessed %s , ctime(s.st_atime));  There are actually three versions of  stat ;         int stat(const char *path, struct stat *buf);\n       int fstat(int fd, struct stat *buf);\n       int lstat(const char *path, struct stat *buf);  For example you can use  fstat  to find out the meta-information about a file if you already have an file descriptor associated with that file     FILE *file = fopen( notes.txt ,  r );\n   int fd = fileno(file); /* Just for fun - extract the file descriptor from a C FILE struct */\n   struct stat s;\n   fstat(fd,   s);\n   printf( Last accessed %s , ctime(s.st_atime));  The third call 'lstat' we will discuss when we introduce symbolic links.  In addition to access,creation, and modified times, the stat structure includes the inode number, length of the file and owner information.  struct stat {\n               dev_t     st_dev;     /* ID of device containing file */\n               ino_t     st_ino;     /* inode number */\n               mode_t    st_mode;    /* protection */\n               nlink_t   st_nlink;   /* number of hard links */\n               uid_t     st_uid;     /* user ID of owner */\n               gid_t     st_gid;     /* group ID of owner */\n               dev_t     st_rdev;    /* device ID (if special file) */\n               off_t     st_size;    /* total size, in bytes */\n               blksize_t st_blksize; /* blocksize for file system I/O */\n               blkcnt_t  st_blocks;  /* number of 512B blocks allocated */\n               time_t    st_atime;   /* time of last access */\n               time_t    st_mtime;   /* time of last modification */\n               time_t    st_ctime;   /* time of last status change */\n           };", 
            "title": "How do I find out meta-information about a file (or directory)?"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-list-the-contents-of-a-directory", 
            "text": "Let's write our own version of 'ls' to list the contents of a directory.  #include  stdio.h \n#include  dirent.h \n#include  stdlib.h \nint main(int argc, char **argv) {\n    if(argc == 1) {\n        printf( Usage: %s [directory]\\n , *argv);\n        exit(0);\n    }\n    struct dirent *dp;\n    DIR *dirp = opendir(argv[1]);\n    while ((dp = readdir(dirp)) != NULL) {\n        puts(dp- d_name);\n    }\n\n    closedir(dirp);\n    return 0;\n}", 
            "title": "How do I list the contents of a directory ?"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-read-the-contents-of-a-directory", 
            "text": "Ans: Use opendir readdir closedir\nFor example, here's a very simple implementation of 'ls' to list the contents of a directory.  #include  stdio.h \n#include  dirent.h \n#include  stdlib.h \nint main(int argc, char **argv) {\n    if(argc ==1) {\n        printf( Usage: %s [directory]\\n , *argv);\n        exit(0);\n    }\n    struct dirent *dp;\n    DIR *dirp = opendir(argv[1]);\n    while ((dp = readdir(dirp)) != NULL) {\n        printf( %s %lu\\n , dp-  d_name, (unsigned long)dp-  d_ino );\n    }\n\n    closedir(dirp);\n    return 0;\n}  Note: after a call to fork(), either (XOR) the parent or the child can use readdir(), rewinddir() or seekdir(). If both the parent and the child use the above, behavior is undefined.", 
            "title": "How do I read the contents of a directory?"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-do-i-check-to-see-if-a-file-is-in-the-current-directory", 
            "text": "For example, to see if a particular directory includes a file (or filename) 'name', we might write the following code. (Hint: Can you spot the bug?)  int exists(char *directory, char *name)  {\n    struct dirent *dp;\n    DIR *dirp = opendir(directory);\n    while ((dp = readdir(dirp)) != NULL) {\n        puts(dp- d_name);\n        if (!strcmp(dp- d_name, name)) {\n        return 1; /* Found */\n        }\n    }\n    closedir(dirp);\n    return 0; /* Not Found */\n}  The above code has a subtle bug: It leaks resources! If a matching filename is found then 'closedir' is never called as part of the early return. Any file descriptors opened, and any memory allocated, by opendir are never released. This means eventually the process will run out of resources and an  open  or  opendir  call will fail.  The fix is to ensure we free up resources in every possible code-path. In the above code this means calling  closedir  before  return 1 . Forgetting to release resources is a common C programming bug because there is no support in the C lanaguage to ensure resources are always released with all codepaths.", 
            "title": "How do I check to see if a file is in the current directory?"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#an-aside-a-system-programming-pattern-to-clean-up-resources-goto-considered-useful", 
            "text": "Note If C supported exception handling this discussion would be unnecessary.\nImagine your function required several temporary resources that need to be freed before returning.\nHow can we write readable code that correctly frees resources under all code paths? Some system programs use  goto  to jump forward into the clean up code, using the following pattern:  int f() {\n   Acquire resource r1\n   if(...) goto clean_up_r1\n   Acquire resource r2\n   if(...) goto clean_up_r2\n\n   perform work\nclean_up_r2:\n   clean up r2\nclean_up_r1:\n   clean up r1\n   return result\n}  Whether this is a good thing or not has led to long rigorous debates that have generally helped system programmers stay warm during the cold winter months. Are there alternatives? Yes! For example using conditional logic, breaking out of do-while loops and writing secondary functions that perform the innermost work. However all choices are problematic and cumbersome as we are attempting to shoe-horn in exception handling in a language that has no inbuilt support for it.", 
            "title": "An aside - A System programming pattern to clean up resources - goto considered useful!"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#what-are-the-gotchas-of-using-readdir-for-example-to-recursively-search-directories", 
            "text": "There are two main gotchas and one consideration:\nThe  readdir  function returns \".\" (current directory) and \"..\" (parent directory). If you are looking for sub-directories, you need to explicitly exclude these directories.  For many applications it's reasonable to check the current directory first before recursively searching sub-directories. This can be achieved by storing the results in a linked list, or resetting the directory struct to restart from the beginning.  One final note of caution:  readdir  is not thread-safe! For multi-threaded searches use  readdir_r  which requires the caller to pass in the address of an existing dirent struct.  See the man page of readdir for more details.", 
            "title": "What are the gotcha's of using readdir? For example to recursively search directories?"
        }, 
        {
            "location": "/File-System,-Part-2:-Files-are-inodes-(everything-else-is-just-data...)/#how-i-do-determine-if-a-directory-entry-is-a-directory", 
            "text": "Ans: Use  S_ISDIR  to check the mode bits stored in the stat structure  And to check if a file is regular file use  S_ISREG ,     struct stat s;\n   if (0 == stat(name,  s)) {\n      printf( %s  , name);\n      if (S_ISDIR( s.st_mode)) puts( is a directory );\n      if (S_ISREG( s.st_mode)) puts( is a regular file );\n   } else {\n      perror( stat failed - are you sure I can read this file's meta data? );\n   }  Go to File System, Part 3", 
            "title": "How I do determine if a directory entry is a directory?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/", 
            "text": "Does a directory have an inode too?\n\n\nYes! Though a better way to think about this, is that a directory (like a file) \nis\n an inode (with some data - the directory name and inode contents). It just happens to be a special kind of inode.\n\n\nFrom \nWikipedia\n:\n\n\n\n\nUnix directories are lists of association structures, each of which contains one filename and one inode number.\n\n\n\n\nRemember, inodes don't contain filenames--only other file metadata.\n\n\nHow can I have the same file appear in two different places in my file system?\n\n\nFirst remember that a file name != the file. Think of the inode as 'the file' and a directory as just a list of names with each name mapped to an inode number. Some of those inodes may be regular file inodes, others may be directory inodes.\n\n\nIf we already have a file on a file system we can create another link to the same inode using the 'ln' command\n\n\n$ ln file1.txt blip.txt\n\n\n\n\nHowever blip.txt \nis\n the same file; if I edit blip I'm editing the same file as 'file1.txt!'\nWe can prove this by showing that both file names refer to the same inode:\n\n\n$ ls -i file1.txt blip.txt\n134235 file1.txt\n134235 blip.txt\n\n\n\n\nThese kinds of links (aka directory entries) are called 'hard links'\n\n\nThe equivalent C call is \nlink\n\n\nlink(const char *path1, const char *path2);\n\nlink(\nfile1.txt\n, \nblip.txt\n);\n\n\n\n\nFor simplicity the above examples made hard links inside the same directory however hard links can be created anywhere inside the same filesystem.\n\n\nWhat happens when I \nrm\n (remove) a file?\n\n\nWhen you remove a file (using \nrm\n or \nunlink\n) you are removing an inode reference from a directory.\nHowever the inode may still be referenced from other directories. In order to determine if the contents of the file are still required, each inode keeps a reference count that is updated whenever a new link is created or destroyed.\n\n\nCase study: Back up software that minimizes file duplication\n\n\nAn example use of hard-links is to efficiently create multiple archives of a file system at different points in time. Once the archive area has a copy of a particular file, then future archives can re-use these archive files rather than creating a duplicate file. Apple's \"Time Machine\" software does this.\n\n\nCan I create hard links to directories as well as regular files?\n\n\nNo. Well yes. Not really... Actually you didn't really want to do this, did you?\nThe POSIX standard says no you may not! The \nln\n command will only allow root to do this and only if you provide the \n-d\n option. However even root may not be able to perform this because most filesystems prevent it! \n\n\nWhy?\nThe integrity of the file system assumes the directory structure (excluding softlinks which we will talk about later) is a non-cyclic tree that is reachable from the root directory. It becomes expensive to enforce or verify this constraint if directory linking is allowed. Breaking these assumptions can cause file integrity tools to not be able to repair the file system. Recursive searches potentially never terminate and directories can have more than one parent but \"..\" can only refer to a single parent. All in all, a bad idea.\n\n\nHow do I change the permissions on a file?\n\n\nUse \nchmod\n  (short for \"change the file mode bits\")\n\n\nThere is a system call, \nint chmod(const char *path, mode_t mode);\n but we will concentrate on the shell command. There's two common ways to use \nchmod\n ; with an octal value or with a symbolic string:\n\n\n$ chmod 644 file1\n$ chmod 755 file2\n$ chmod 700 file3\n$ chmod ugo-w file4\n$ chmod o-rx file4\n\n\n\n\nThe base-8 ('octal') digits describe the permissions for each role: The user who owns the file, the group and everyone else. The octal number is the sum of three values given to the three types of permission: read(4), write(2), execute(1)\n\n\nExample: chmod 755 myfile\n\n r + w + x = digit\n\n user has 4+2+1, full permission\n\n group has 4+0+1, read and execute permission\n\n all users have 4+0+1, read and execute permission\n\n\nHow do I read the permission string from ls?\n\n\nUse `ls -l'. \nNote that the permissions will output in the format 'drwxrwxrwx'. The first character indicates the type of file type. \nPossible values for the first character:\n\n (-) regular file\n\n (d) directory\n\n (c) character device file\\\n\n (l) symbolic link\n\n (p) pipe\n\n (b) block device\n* (s) socket\n\n\nWhat is sudo?\n\n\nUse \nsudo\n to become the admin on the machine.\ne.g. Normally (unless explicitly specified in the '/etc/fstab' file, you need root access to mount a filesystem). \nsudo\n can be used to temporarily run a command as root (provided the user has sudo privileges)\n\n\n$ sudo mount /dev/sda2 /stuff/mydisk\n$ sudo adduser fred\n\n\n\n\nHow do I change ownership of a file?\n\n\nUse \nchown username filename\n\n\nHow do I set permissions from code?\n\n\nchmod(const char *path, mode_t mode);\n\n\nWhy are some files 'setuid'? What does this mean?\n\n\nThe set-user-ID-on-execution bit changes the user associated with the process when the file is run. This is typically used for commands that need to run as root but are executed by non-root users. An example of this is \nsudo\n\n\nThe set-group-ID-on-execution changes the group under which the process is run.\n\n\nWhy are they useful?\n\n\nThe most common usecase is so that the user can have root(admin) access for the duration of the program.\n\n\nWhat permissions does sudo run as ?\n\n\n$ ls -l /usr/bin/sudo\n-r-s--x--x  1 root  wheel  327920 Oct 24 09:04 /usr/bin/sudo\n\n\n\n\nThe 's' bit means execute and set-uid; the effective userid of the process will be different from the parent process. In this example it will be root\n\n\nWhat's the difference between getuid() and geteuid()?\n\n\n\n\ngetuid\n returns the real user id (zero if logged in as root)\n\n\ngeteuid\n returns the effective userid (zero if acting as root, e.g. due to the setuid flag set on a program)\n\n\n\n\nHow do I ensure only privileged users can run my code?\n\n\n\n\nCheck the effective permissions of the user by calling \ngeteuid()\n. A return value of zero means the program is running effectively as root.\n\n\n\n\nGo to File System: Part 4", 
            "title": "File System, Part 3: Permissions"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#does-a-directory-have-an-inode-too", 
            "text": "Yes! Though a better way to think about this, is that a directory (like a file)  is  an inode (with some data - the directory name and inode contents). It just happens to be a special kind of inode.  From  Wikipedia :   Unix directories are lists of association structures, each of which contains one filename and one inode number.   Remember, inodes don't contain filenames--only other file metadata.", 
            "title": "Does a directory have an inode too?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#how-can-i-have-the-same-file-appear-in-two-different-places-in-my-file-system", 
            "text": "First remember that a file name != the file. Think of the inode as 'the file' and a directory as just a list of names with each name mapped to an inode number. Some of those inodes may be regular file inodes, others may be directory inodes.  If we already have a file on a file system we can create another link to the same inode using the 'ln' command  $ ln file1.txt blip.txt  However blip.txt  is  the same file; if I edit blip I'm editing the same file as 'file1.txt!'\nWe can prove this by showing that both file names refer to the same inode:  $ ls -i file1.txt blip.txt\n134235 file1.txt\n134235 blip.txt  These kinds of links (aka directory entries) are called 'hard links'  The equivalent C call is  link  link(const char *path1, const char *path2);\n\nlink( file1.txt ,  blip.txt );  For simplicity the above examples made hard links inside the same directory however hard links can be created anywhere inside the same filesystem.", 
            "title": "How can I have the same file appear in two different places in my file system?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#what-happens-when-i-rm-remove-a-file", 
            "text": "When you remove a file (using  rm  or  unlink ) you are removing an inode reference from a directory.\nHowever the inode may still be referenced from other directories. In order to determine if the contents of the file are still required, each inode keeps a reference count that is updated whenever a new link is created or destroyed.", 
            "title": "What happens when I rm (remove) a file?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#case-study-back-up-software-that-minimizes-file-duplication", 
            "text": "An example use of hard-links is to efficiently create multiple archives of a file system at different points in time. Once the archive area has a copy of a particular file, then future archives can re-use these archive files rather than creating a duplicate file. Apple's \"Time Machine\" software does this.", 
            "title": "Case study: Back up software that minimizes file duplication"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#can-i-create-hard-links-to-directories-as-well-as-regular-files", 
            "text": "No. Well yes. Not really... Actually you didn't really want to do this, did you?\nThe POSIX standard says no you may not! The  ln  command will only allow root to do this and only if you provide the  -d  option. However even root may not be able to perform this because most filesystems prevent it!   Why?\nThe integrity of the file system assumes the directory structure (excluding softlinks which we will talk about later) is a non-cyclic tree that is reachable from the root directory. It becomes expensive to enforce or verify this constraint if directory linking is allowed. Breaking these assumptions can cause file integrity tools to not be able to repair the file system. Recursive searches potentially never terminate and directories can have more than one parent but \"..\" can only refer to a single parent. All in all, a bad idea.", 
            "title": "Can I create hard links to directories as well as regular files?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#how-do-i-change-the-permissions-on-a-file", 
            "text": "Use  chmod   (short for \"change the file mode bits\")  There is a system call,  int chmod(const char *path, mode_t mode);  but we will concentrate on the shell command. There's two common ways to use  chmod  ; with an octal value or with a symbolic string:  $ chmod 644 file1\n$ chmod 755 file2\n$ chmod 700 file3\n$ chmod ugo-w file4\n$ chmod o-rx file4  The base-8 ('octal') digits describe the permissions for each role: The user who owns the file, the group and everyone else. The octal number is the sum of three values given to the three types of permission: read(4), write(2), execute(1)  Example: chmod 755 myfile  r + w + x = digit  user has 4+2+1, full permission  group has 4+0+1, read and execute permission  all users have 4+0+1, read and execute permission", 
            "title": "How do I change the permissions on a file?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#how-do-i-read-the-permission-string-from-ls", 
            "text": "Use `ls -l'. \nNote that the permissions will output in the format 'drwxrwxrwx'. The first character indicates the type of file type. \nPossible values for the first character:  (-) regular file  (d) directory  (c) character device file\\  (l) symbolic link  (p) pipe  (b) block device\n* (s) socket", 
            "title": "How do I read the permission string from ls?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#what-is-sudo", 
            "text": "Use  sudo  to become the admin on the machine.\ne.g. Normally (unless explicitly specified in the '/etc/fstab' file, you need root access to mount a filesystem).  sudo  can be used to temporarily run a command as root (provided the user has sudo privileges)  $ sudo mount /dev/sda2 /stuff/mydisk\n$ sudo adduser fred", 
            "title": "What is sudo?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#how-do-i-change-ownership-of-a-file", 
            "text": "Use  chown username filename", 
            "title": "How do I change ownership of a file?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#how-do-i-set-permissions-from-code", 
            "text": "chmod(const char *path, mode_t mode);", 
            "title": "How do I set permissions from code?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#why-are-some-files-setuid-what-does-this-mean", 
            "text": "The set-user-ID-on-execution bit changes the user associated with the process when the file is run. This is typically used for commands that need to run as root but are executed by non-root users. An example of this is  sudo  The set-group-ID-on-execution changes the group under which the process is run.", 
            "title": "Why are some files 'setuid'? What does this mean?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#why-are-they-useful", 
            "text": "The most common usecase is so that the user can have root(admin) access for the duration of the program.", 
            "title": "Why are they useful?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#what-permissions-does-sudo-run-as", 
            "text": "$ ls -l /usr/bin/sudo\n-r-s--x--x  1 root  wheel  327920 Oct 24 09:04 /usr/bin/sudo  The 's' bit means execute and set-uid; the effective userid of the process will be different from the parent process. In this example it will be root", 
            "title": "What permissions does sudo run as ?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#whats-the-difference-between-getuid-and-geteuid", 
            "text": "getuid  returns the real user id (zero if logged in as root)  geteuid  returns the effective userid (zero if acting as root, e.g. due to the setuid flag set on a program)", 
            "title": "What's the difference between getuid() and geteuid()?"
        }, 
        {
            "location": "/File-System,-Part-3:-Permissions/#how-do-i-ensure-only-privileged-users-can-run-my-code", 
            "text": "Check the effective permissions of the user by calling  geteuid() . A return value of zero means the program is running effectively as root.   Go to File System: Part 4", 
            "title": "How do I ensure only privileged users can run my code?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/", 
            "text": "How do I find out if file (an inode) is a regular file or directory?\n\n\nUse the \nS_ISDIR\n macro to check the mode bits in the stat struct:\n\n\nstruct stat s;\nstat(\n/tmp\n, \ns);\nif (S_ISDIR(s.st_mode)) { ... \n\n\n\n\nNote, later we will write robust code to verify that the stat call succeeds (returns 0); if the \nstat\n call fails, we should assume the stat struct content is arbitrary.\n\n\nHow do I recurse into subdirectories?\n\n\nFirst a puzzle - how many bugs can you find in the following code?\n\n\nvoid dirlist(char *path) {\n\n  struct dirent *dp;\n  DIR *dirp = opendir(path);\n  while ((dp = readdir(dirp)) != NULL) {\n     char newpath[strlen(path) + strlen(dp-\nd_name) + 1];\n     sprintf(newpath,\n%s/%s\n, newpath, dp-\nd_name);\n     printf(\n%s\\n\n, dp-\nd_name);\n     dirlist(newpath);\n  }\n}\n\nint main(int argc, char **argv) { dirlist(argv[1]); return 0; }\n\n\n\n\nDid you find all 5 bugs?\n\n\n// Check opendir result (perhaps user gave us a path that can not be opened as a directory\nif (!dirp) { perror(\nCould not open directory\n); return; }\n// +2 as we need space for the / and the terminating 0\nchar newpath[strlen(path) + strlen(dp-\nd_name) + 2]; \n// Correct parameter\nsprintf(newpath,\n%s/%s\n, path, dp-\nd_name); \n// Perform stat test (and verify) before recursing\nif (0 == stat(newpath,\ns) \n S_ISDIR(s.st_mode)) dirlist(newpath)\n// Resource leak: the directory file handle is not closed after the while loop\nclosedir(dirp);\n\n\n\n\nWhat are symbolic links? How do they work? How do I make one?\n\n\nsymlink(const char *target, const char *symlink);\n\n\n\n\nTo create a symbolic link in the shell use \nln -s\n\n\nTo read the contents of the link as just a file use \nreadlink\n\n\n$ readlink myfile.txt\n../../dir1/notes.txt\n\n\n\n\nTo read the meta-(stat) information of a symbolic link use \nlstat\n not \nstat\n\n\nstruct stat s;\nstat(\nmyfile.txt\n, \ns1); // stat info about  the notes.txt file\nlstat(\nmyfile.txt\n, \ns2); // stat info about the symbolic link\n\n\n\n\nAdvantages of symbolic links\n\n\n\n\nCan refer to a files that don't exist yet\n\n\nUnlike hard links, can refer to directories as well as regular files\n\n\nCan refer to files (and directories) that exist outside of the current file system\n\n\n\n\nMain disadvantage: Slower than regular files and directories. When the links contents are read, they must be interpreted as a new path to the target file.\n\n\nWhat is \n/dev/null\n and when is it used?\n\n\nThe file \n/dev/null\n is a great place to store bits that you never need to read!\nBytes sent to \n/dev/null/\n are never stored - they are simply discarded. A common use of \n/dev/null\n is to discard standard output. For example,\n\n\n$ ls . \n/dev/null\n\n\n\n\nWhy would I want to set a directory's sticky bit?\n\n\nWhen a directory's sticky bit is set only the file's owner, the directory's owner, and the root user can rename (or delete) the file. This is useful when multiple users have write access to a common directory.\n\n\nA common use of the sticky bit is for the shared and writable \n/tmp\n directory.\n\n\nWhy do shell and script programs start with \n#!/usr/bin/env python\n ?\n\n\nAns: For portability!\nWhile it is possible to write the fully qualified path to a python or perl interpreter, this approach is not portable because you may have installed python in a different directory.\n\n\nTo overcome this use the \nenv\n utility is used to find and execute the program on the user's path.\nThe env utility itself has historically been stored in \n/usr/bin\n - and it must be specified with an absolute path.\n\n\nHow do I make 'hidden' files i.e. not listed by \"ls\"? How do I list them?\n\n\nEasy! Create files (or directories) that start with a \".\" - then (by default) they are not displayed by standard tools and utilities.\n\n\nThis is often used to hide configuration files inside the user's home directory.\nFor example \nssh\n stores its preferences inside a directory called \n.sshd\n\n\nTo list all files including the normally hidden entries use \nls\n with  \n-a\n option \n\n\n$ ls -a\n.           a.c         myls\n..          a.out           other.txt\n.secret \n\n\n\n\nWhat happens if I turn off the execute bit on directories?\n\n\nThe execute bit for a directory is used to control whether the directory contents is listable.\n\n\n$ chmod ugo-x dir1\n$ ls -l\ndrw-r--r--   3 angrave  staff   102 Nov 10 11:22 dir1\n\n\n\n\nHowever when attempting to list the contents of the directory,\n\n\n$ ls dir1\nls: dir1: Permission denied\n\n\n\n\nIn other words, the directory itself is discoverable but its contents cannot be listed.\n\n\nWhat is file globbing (and who does it)?\n\n\nBefore executing the program the shell expands parameters into matching filenames. For example, if the current directory has three filenames that start with my ( my1.txt mytext.txt myomy), then\n\n\n$ echo my*\n\n\n\n\nExpands to \n\n\n$ echo my1.txt mytext.txt myomy\n\n\n\n\nThis is known as file globbing and is processed before the command is executed.\nie the command's parameters are identical to manually typing every matching filename.\n\n\nCreating secure directories\n\n\nSuppose you created your own directory in /tmp and then set the permissions so that only you can use the directory (see below). Is this secure? \n\n\n$ mkdir /tmp/mystuff\n$ chmod 700 /tmp/mystuff\n\n\n\n\nThere is a window of opportunity between when the directory is created and when it's permissions are changed. This leads to several vulnerabilities that are based on a race condition (where an attacker modifies the directory in some way before the privileges are removed). Some examples include:\n\n\nAnother user replaces \nmystuff\n with a hardlink to an existing file or directory owned by the second user, then they would be able to read and control the contents of the \nmystuff\n directory. Oh no - our secrets are no longer secret!\n\n\nHowever in this specific example the \n/tmp\n directory has the sticky bit set, so other users may not delete the \nmystuff\n directory, and the simple attack scenario described above is impossible. This does not mean that creating the directory and then later making the directory private is secure! A better version is to atomically create the directory with the correct permissions from its inception - \n\n\n$ mkdir -m 700 /tmp/mystuff\n\n\n\n\nHow do I automatically create parent directories?\n\n\n$ mkdir -p d1/d2/d3\n\n\n\n\nWill automatically create d1 and d2 if they don't exist.\n\n\nMy default umask 022; what does this mean?\n\n\nThe umask \nsubtracts\n (reduces) permission bits from 777 and is used when new files and new directories are created by open,mkdir etc. Thus \n022\n (octal) means that group and other privileges will not include the writable bit . Each process (including the shell) has a current umask value. When forking, the child inherits the parent's umask value.\n\n\nFor example, by setting the umask to 077 in the shell, ensures that future file and directory creation will only be accessible to the current user,\n\n\n$ umask 077\n$ mkdir secretdir\n\n\n\n\nAs a code example, suppose a new file is created with \nopen()\n and mode bits \n666\n (write and read bits for user,group and other):\n\n\nopen(\nmyfile\n, O_CREAT, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);\n\n\n\n\nIf umask is octal 022, then the permissions of the created file will be 0666 \n ~022\nie.\n\n\n           S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH\n\n\n\n\nHow can I copy bytes from one file to another?\n\n\nUse  the versatile \ndd\n command. For example, the following command copies 1mb of data from the file \n/dev/urandom\n to the file \n/dev/null\n. The data is copied as 1024 blocks of blocksize 1024 bytes.\n\n\n$ dd if=/dev/urandom of=/dev/null bs=1k count=1024\n\n\n\n\nBoth the input and output files in the example above are virtual - they don't exist on a disk. This means the speed of the transfer is unaffected by hardware power. Instead they are part of the \ndev\n filesystem, which is virtual filesystem provided by the kernel.\nThe virtual file \n/dev/urandom\n provides an infinite stream of random bytes, while the virtal file \n/dev/null\n ignores all bytes written to it. A common use of \n/dev/null\n is to discard the output of a command,\n\n\n$ myverboseexecutable \n /dev/null\n\n\n\n\nAnother commonly used /dev virtual file is \n/dev/zero\n which provides an infinite stream of zero bytes.\nFor example, we can benchmark the operating system performance of reading stream zero bytes in the kernel into a process memory and writing the bytes back to the kernel without any disk I/O. Note the throughput (~20GB/s) is strongly dependent on blocksize. For small block sizes the overhead of additional \nread\n and \nwrite\n system calls will  dominate.\n\n\n$ dd if=/dev/zero of=/dev/null bs=1M count=1024\n1024+0 records in\n1024+0 records out\n1073741824 bytes (1.1 GB) copied, 0.0539153 s, 19.9 GB/s\n\n\n\n\nWhat happens when I touch a file?\n\n\nThe \ntouch\n executable creates file if it does not exist and also updates the file's last modified time to be the current time. For example, we can make a new private file with the current time:\n\n\n$ umask 077       # all future new files will maskout all r,w,x bits for group and other access\n$ touch file123   # create a file if it does not exist, and update its modified time\n$ stat file123\n  File: `file123'\n  Size: 0           Blocks: 0          IO Block: 65536  regular empty file\nDevice: 21h/33d Inode: 226148      Links: 1\nAccess: (0600/-rw-------)  Uid: (395606/ angrave)   Gid: (61019/     ews)\nAccess: 2014-11-12 13:42:06.000000000 -0600\nModify: 2014-11-12 13:42:06.001787000 -0600\nChange: 2014-11-12 13:42:06.001787000 -0600\n\n\n\n\nAn example use of touch is to force make to recompile a file that is unchanged after modifying the compiler options inside the makefile. Remeber that make is 'lazy' - it will compare the modified time of the source file with the corresponding output file to see if the file needs to be recompiled\n\n\n$ touch myprogram.c   # force my source file to be recompiled\n$ make\n\n\n\n\nGo to File System: Part 5", 
            "title": "File System, Part 4: Working with directories"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#how-do-i-find-out-if-file-an-inode-is-a-regular-file-or-directory", 
            "text": "Use the  S_ISDIR  macro to check the mode bits in the stat struct:  struct stat s;\nstat( /tmp ,  s);\nif (S_ISDIR(s.st_mode)) { ...   Note, later we will write robust code to verify that the stat call succeeds (returns 0); if the  stat  call fails, we should assume the stat struct content is arbitrary.", 
            "title": "How do I find out if file (an inode) is a regular file or directory?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#how-do-i-recurse-into-subdirectories", 
            "text": "First a puzzle - how many bugs can you find in the following code?  void dirlist(char *path) {\n\n  struct dirent *dp;\n  DIR *dirp = opendir(path);\n  while ((dp = readdir(dirp)) != NULL) {\n     char newpath[strlen(path) + strlen(dp- d_name) + 1];\n     sprintf(newpath, %s/%s , newpath, dp- d_name);\n     printf( %s\\n , dp- d_name);\n     dirlist(newpath);\n  }\n}\n\nint main(int argc, char **argv) { dirlist(argv[1]); return 0; }  Did you find all 5 bugs?  // Check opendir result (perhaps user gave us a path that can not be opened as a directory\nif (!dirp) { perror( Could not open directory ); return; }\n// +2 as we need space for the / and the terminating 0\nchar newpath[strlen(path) + strlen(dp- d_name) + 2]; \n// Correct parameter\nsprintf(newpath, %s/%s , path, dp- d_name); \n// Perform stat test (and verify) before recursing\nif (0 == stat(newpath, s)   S_ISDIR(s.st_mode)) dirlist(newpath)\n// Resource leak: the directory file handle is not closed after the while loop\nclosedir(dirp);", 
            "title": "How do I recurse into subdirectories?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#what-are-symbolic-links-how-do-they-work-how-do-i-make-one", 
            "text": "symlink(const char *target, const char *symlink);  To create a symbolic link in the shell use  ln -s  To read the contents of the link as just a file use  readlink  $ readlink myfile.txt\n../../dir1/notes.txt  To read the meta-(stat) information of a symbolic link use  lstat  not  stat  struct stat s;\nstat( myfile.txt ,  s1); // stat info about  the notes.txt file\nlstat( myfile.txt ,  s2); // stat info about the symbolic link", 
            "title": "What are symbolic links? How do they work? How do I make one?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#advantages-of-symbolic-links", 
            "text": "Can refer to a files that don't exist yet  Unlike hard links, can refer to directories as well as regular files  Can refer to files (and directories) that exist outside of the current file system   Main disadvantage: Slower than regular files and directories. When the links contents are read, they must be interpreted as a new path to the target file.", 
            "title": "Advantages of symbolic links"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#what-is-devnull-and-when-is-it-used", 
            "text": "The file  /dev/null  is a great place to store bits that you never need to read!\nBytes sent to  /dev/null/  are never stored - they are simply discarded. A common use of  /dev/null  is to discard standard output. For example,  $ ls .  /dev/null", 
            "title": "What is /dev/null and when is it used?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#why-would-i-want-to-set-a-directorys-sticky-bit", 
            "text": "When a directory's sticky bit is set only the file's owner, the directory's owner, and the root user can rename (or delete) the file. This is useful when multiple users have write access to a common directory.  A common use of the sticky bit is for the shared and writable  /tmp  directory.", 
            "title": "Why would I want to set a directory's sticky bit?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#why-do-shell-and-script-programs-start-with-usrbinenv-python", 
            "text": "Ans: For portability!\nWhile it is possible to write the fully qualified path to a python or perl interpreter, this approach is not portable because you may have installed python in a different directory.  To overcome this use the  env  utility is used to find and execute the program on the user's path.\nThe env utility itself has historically been stored in  /usr/bin  - and it must be specified with an absolute path.", 
            "title": "Why do shell and script programs start with #!/usr/bin/env python ?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#how-do-i-make-hidden-files-ie-not-listed-by-ls-how-do-i-list-them", 
            "text": "Easy! Create files (or directories) that start with a \".\" - then (by default) they are not displayed by standard tools and utilities.  This is often used to hide configuration files inside the user's home directory.\nFor example  ssh  stores its preferences inside a directory called  .sshd  To list all files including the normally hidden entries use  ls  with   -a  option   $ ls -a\n.           a.c         myls\n..          a.out           other.txt\n.secret", 
            "title": "How do I make 'hidden' files i.e. not listed by \"ls\"? How do I list them?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#what-happens-if-i-turn-off-the-execute-bit-on-directories", 
            "text": "The execute bit for a directory is used to control whether the directory contents is listable.  $ chmod ugo-x dir1\n$ ls -l\ndrw-r--r--   3 angrave  staff   102 Nov 10 11:22 dir1  However when attempting to list the contents of the directory,  $ ls dir1\nls: dir1: Permission denied  In other words, the directory itself is discoverable but its contents cannot be listed.", 
            "title": "What happens if I turn off the execute bit on directories?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#what-is-file-globbing-and-who-does-it", 
            "text": "Before executing the program the shell expands parameters into matching filenames. For example, if the current directory has three filenames that start with my ( my1.txt mytext.txt myomy), then  $ echo my*  Expands to   $ echo my1.txt mytext.txt myomy  This is known as file globbing and is processed before the command is executed.\nie the command's parameters are identical to manually typing every matching filename.", 
            "title": "What is file globbing (and who does it)?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#creating-secure-directories", 
            "text": "Suppose you created your own directory in /tmp and then set the permissions so that only you can use the directory (see below). Is this secure?   $ mkdir /tmp/mystuff\n$ chmod 700 /tmp/mystuff  There is a window of opportunity between when the directory is created and when it's permissions are changed. This leads to several vulnerabilities that are based on a race condition (where an attacker modifies the directory in some way before the privileges are removed). Some examples include:  Another user replaces  mystuff  with a hardlink to an existing file or directory owned by the second user, then they would be able to read and control the contents of the  mystuff  directory. Oh no - our secrets are no longer secret!  However in this specific example the  /tmp  directory has the sticky bit set, so other users may not delete the  mystuff  directory, and the simple attack scenario described above is impossible. This does not mean that creating the directory and then later making the directory private is secure! A better version is to atomically create the directory with the correct permissions from its inception -   $ mkdir -m 700 /tmp/mystuff", 
            "title": "Creating secure directories"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#how-do-i-automatically-create-parent-directories", 
            "text": "$ mkdir -p d1/d2/d3  Will automatically create d1 and d2 if they don't exist.", 
            "title": "How do I automatically create parent directories?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#my-default-umask-022-what-does-this-mean", 
            "text": "The umask  subtracts  (reduces) permission bits from 777 and is used when new files and new directories are created by open,mkdir etc. Thus  022  (octal) means that group and other privileges will not include the writable bit . Each process (including the shell) has a current umask value. When forking, the child inherits the parent's umask value.  For example, by setting the umask to 077 in the shell, ensures that future file and directory creation will only be accessible to the current user,  $ umask 077\n$ mkdir secretdir  As a code example, suppose a new file is created with  open()  and mode bits  666  (write and read bits for user,group and other):  open( myfile , O_CREAT, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH);  If umask is octal 022, then the permissions of the created file will be 0666   ~022\nie.             S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH", 
            "title": "My default umask 022; what does this mean?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#how-can-i-copy-bytes-from-one-file-to-another", 
            "text": "Use  the versatile  dd  command. For example, the following command copies 1mb of data from the file  /dev/urandom  to the file  /dev/null . The data is copied as 1024 blocks of blocksize 1024 bytes.  $ dd if=/dev/urandom of=/dev/null bs=1k count=1024  Both the input and output files in the example above are virtual - they don't exist on a disk. This means the speed of the transfer is unaffected by hardware power. Instead they are part of the  dev  filesystem, which is virtual filesystem provided by the kernel.\nThe virtual file  /dev/urandom  provides an infinite stream of random bytes, while the virtal file  /dev/null  ignores all bytes written to it. A common use of  /dev/null  is to discard the output of a command,  $ myverboseexecutable   /dev/null  Another commonly used /dev virtual file is  /dev/zero  which provides an infinite stream of zero bytes.\nFor example, we can benchmark the operating system performance of reading stream zero bytes in the kernel into a process memory and writing the bytes back to the kernel without any disk I/O. Note the throughput (~20GB/s) is strongly dependent on blocksize. For small block sizes the overhead of additional  read  and  write  system calls will  dominate.  $ dd if=/dev/zero of=/dev/null bs=1M count=1024\n1024+0 records in\n1024+0 records out\n1073741824 bytes (1.1 GB) copied, 0.0539153 s, 19.9 GB/s", 
            "title": "How can I copy bytes from one file to another?"
        }, 
        {
            "location": "/File-System,-Part-4:-Working-with-directories/#what-happens-when-i-touch-a-file", 
            "text": "The  touch  executable creates file if it does not exist and also updates the file's last modified time to be the current time. For example, we can make a new private file with the current time:  $ umask 077       # all future new files will maskout all r,w,x bits for group and other access\n$ touch file123   # create a file if it does not exist, and update its modified time\n$ stat file123\n  File: `file123'\n  Size: 0           Blocks: 0          IO Block: 65536  regular empty file\nDevice: 21h/33d Inode: 226148      Links: 1\nAccess: (0600/-rw-------)  Uid: (395606/ angrave)   Gid: (61019/     ews)\nAccess: 2014-11-12 13:42:06.000000000 -0600\nModify: 2014-11-12 13:42:06.001787000 -0600\nChange: 2014-11-12 13:42:06.001787000 -0600  An example use of touch is to force make to recompile a file that is unchanged after modifying the compiler options inside the makefile. Remeber that make is 'lazy' - it will compare the modified time of the source file with the corresponding output file to see if the file needs to be recompiled  $ touch myprogram.c   # force my source file to be recompiled\n$ make  Go to File System: Part 5", 
            "title": "What happens when I touch a file?"
        }, 
        {
            "location": "/File-System,-Part-5:-Virtual-file-systems/", 
            "text": "Virtual file systems\n\n\nPOSIX systems, such as Linux and Mac OSX (which is based on BSD) include several virtual filesystems that are mounted (available) as part of the file-system. Files inside these virtual filesystems do not exist on the disk; they are generated dynamically by the kernel when a process requests a directory listing.\nLinux provides 3 main virtual filesystems\n\n\n/dev  - A list of physical and virtual devices (for example network card, cdrom, random number generator)\n/proc - A list of resources used by each process and (by tradition) set of system information\n/sys - An organized list of internal kernel entities\n\n\n\n\nHow do I find out what filesystems are currently available (mounted)?\n\n\nUse \nmount\n\nUsing mount without any options generates a list (one filesystem per line) of mounted filesystems including networked, virtual and local (spinning disk / SSD-based) filesystems. Here is a typical output of mount\n\n\n$ mount\n/dev/mapper/cs241--server_sys-root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext=\nsystem_u:object_r:tmpfs_t:s0\n)\n/dev/sda1 on /boot type ext3 (rw)\n/dev/mapper/cs241--server_sys-srv on /srv type ext4 (rw)\n/dev/mapper/cs241--server_sys-tmp on /tmp type ext4 (rw)\n/dev/mapper/cs241--server_sys-var on /var type ext4 (rw)rw,bind)\n/srv/software/Mathematica-8.0 on /software/Mathematica-8.0 type none (rw,bind)\nengr-ews-homes.engr.illinois.edu:/fs1-homes/angrave/linux on /home/angrave type nfs (rw,soft,intr,tcp,noacl,acregmin=30,vers=3,sec=sys,sloppy,addr=128.174.252.102)\n\n\n\n\nNotice that each line includes the filesystem type source of the filesystem and mount point.\nTo reduce this output we can pipe it into \ngrep\n and only see lines that match a regular expression. \n\n\nmount | grep proc  # only see lines that contain 'proc'\nproc on /proc type proc (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n\n\n\n\nTodo\n\n\n$ sudo mount /dev/cdrom /media/cdrom\n$ mount\n$ mount | grep proc\n\n\n\n\nExamples of virtual files in /proc:\n\n\n$ cat /proc/sys/kernel/random/entropy_avail\n$ hexdump /dev/random\n$ hexdump /dev/urandom\n\n\n\n\nDifferences between random and urandom?\n\n\n/dev/random is a file which contains pseudorandom number generator where the entropy is determined from environmental noise. Random will block/wait until enough entropy is collected from the environment. \n\n\n/dev/urandom is like random, but differs in the fact that it allows for repetition (lower entropy threshold), thus wont block.\n\n\n$ cat /proc/meminfo\n$ cat /proc/cpuinfo\n$ cat /proc/cpuinfo | grep bogomips\n\n$ cat /proc/meminfo | grep Swap\n\n$ cd /proc/self\n$ echo $$; cd /proc/12345; cat maps\n\n\n\n\nHow do I mount a disk image?\n\n\nSuppose you had downloaded a bootable linux disk image...\n\n\nwget http://cosmos.cites.illinois.edu/pub/archlinux/iso/2015.04.01/archlinux-2015.04.01-dual.iso\n\n\n\n\nBefore putting the filesystem on a CD, we can mount the file as a filesystem and explore its contents. Note, mount requires root access, so let's run it using sudo\n\n\n$ mkdir arch\n$ sudo mount -o loop archlinux-2015.04.01-dual.iso ./arch\n$ cd arch\n\n\n\n\nBefore the mount command, the arch directory is new and obviously empty. After mounting, the contents of \narch/\n will be drawn from the files and directories stored in the filesystem stored inside the \narchlinux-2014.11.01-dual.iso\n file.\nThe \nloop\n option is required because we want to mount a regular file not a block device such as a physical disk. \n\n\nThe loop option wraps the original file as a block device - in this example we will find out below that the file system is provided under \n/dev/loop0\n : We can check the filesystem type and mount options by running the mount command without any parameters. We will pipe the output into \ngrep\n so that we only see the relevant output line(s) that contain 'arch'\n\n\n$ mount | grep arch\n/home/demo/archlinux-2014.11.01-dual.iso on /home/demo/arch type iso9660 (rw,loop=/dev/loop0)\n\n\n\n\nThe iso9660 filesystem is a read-only filesystem originally designed for optical storage media (i.e. CDRoms). Attempting to change the contents of the filesystem will fail\n\n\n$ touch arch/nocando\ntouch: cannot touch `/home/demo/arch/nocando': Read-only file system\n\n\n\n\nGo to File System: Part 6", 
            "title": "File System, Part 5: Virtual file systems"
        }, 
        {
            "location": "/File-System,-Part-5:-Virtual-file-systems/#virtual-file-systems", 
            "text": "POSIX systems, such as Linux and Mac OSX (which is based on BSD) include several virtual filesystems that are mounted (available) as part of the file-system. Files inside these virtual filesystems do not exist on the disk; they are generated dynamically by the kernel when a process requests a directory listing.\nLinux provides 3 main virtual filesystems  /dev  - A list of physical and virtual devices (for example network card, cdrom, random number generator)\n/proc - A list of resources used by each process and (by tradition) set of system information\n/sys - An organized list of internal kernel entities", 
            "title": "Virtual file systems"
        }, 
        {
            "location": "/File-System,-Part-5:-Virtual-file-systems/#how-do-i-find-out-what-filesystems-are-currently-available-mounted", 
            "text": "Use  mount \nUsing mount without any options generates a list (one filesystem per line) of mounted filesystems including networked, virtual and local (spinning disk / SSD-based) filesystems. Here is a typical output of mount  $ mount\n/dev/mapper/cs241--server_sys-root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext= system_u:object_r:tmpfs_t:s0 )\n/dev/sda1 on /boot type ext3 (rw)\n/dev/mapper/cs241--server_sys-srv on /srv type ext4 (rw)\n/dev/mapper/cs241--server_sys-tmp on /tmp type ext4 (rw)\n/dev/mapper/cs241--server_sys-var on /var type ext4 (rw)rw,bind)\n/srv/software/Mathematica-8.0 on /software/Mathematica-8.0 type none (rw,bind)\nengr-ews-homes.engr.illinois.edu:/fs1-homes/angrave/linux on /home/angrave type nfs (rw,soft,intr,tcp,noacl,acregmin=30,vers=3,sec=sys,sloppy,addr=128.174.252.102)  Notice that each line includes the filesystem type source of the filesystem and mount point.\nTo reduce this output we can pipe it into  grep  and only see lines that match a regular expression.   mount | grep proc  # only see lines that contain 'proc'\nproc on /proc type proc (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)", 
            "title": "How do I find out what filesystems are currently available (mounted)?"
        }, 
        {
            "location": "/File-System,-Part-5:-Virtual-file-systems/#todo", 
            "text": "$ sudo mount /dev/cdrom /media/cdrom\n$ mount\n$ mount | grep proc  Examples of virtual files in /proc:  $ cat /proc/sys/kernel/random/entropy_avail\n$ hexdump /dev/random\n$ hexdump /dev/urandom", 
            "title": "Todo"
        }, 
        {
            "location": "/File-System,-Part-5:-Virtual-file-systems/#differences-between-random-and-urandom", 
            "text": "/dev/random is a file which contains pseudorandom number generator where the entropy is determined from environmental noise. Random will block/wait until enough entropy is collected from the environment.   /dev/urandom is like random, but differs in the fact that it allows for repetition (lower entropy threshold), thus wont block.  $ cat /proc/meminfo\n$ cat /proc/cpuinfo\n$ cat /proc/cpuinfo | grep bogomips\n\n$ cat /proc/meminfo | grep Swap\n\n$ cd /proc/self\n$ echo $$; cd /proc/12345; cat maps", 
            "title": "Differences between random and urandom?"
        }, 
        {
            "location": "/File-System,-Part-5:-Virtual-file-systems/#how-do-i-mount-a-disk-image", 
            "text": "Suppose you had downloaded a bootable linux disk image...  wget http://cosmos.cites.illinois.edu/pub/archlinux/iso/2015.04.01/archlinux-2015.04.01-dual.iso  Before putting the filesystem on a CD, we can mount the file as a filesystem and explore its contents. Note, mount requires root access, so let's run it using sudo  $ mkdir arch\n$ sudo mount -o loop archlinux-2015.04.01-dual.iso ./arch\n$ cd arch  Before the mount command, the arch directory is new and obviously empty. After mounting, the contents of  arch/  will be drawn from the files and directories stored in the filesystem stored inside the  archlinux-2014.11.01-dual.iso  file.\nThe  loop  option is required because we want to mount a regular file not a block device such as a physical disk.   The loop option wraps the original file as a block device - in this example we will find out below that the file system is provided under  /dev/loop0  : We can check the filesystem type and mount options by running the mount command without any parameters. We will pipe the output into  grep  so that we only see the relevant output line(s) that contain 'arch'  $ mount | grep arch\n/home/demo/archlinux-2014.11.01-dual.iso on /home/demo/arch type iso9660 (rw,loop=/dev/loop0)  The iso9660 filesystem is a read-only filesystem originally designed for optical storage media (i.e. CDRoms). Attempting to change the contents of the filesystem will fail  $ touch arch/nocando\ntouch: cannot touch `/home/demo/arch/nocando': Read-only file system  Go to File System: Part 6", 
            "title": "How do I mount a disk image?"
        }, 
        {
            "location": "/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/", 
            "text": "How does the operating system load my process and libraries into memory?\n\n\nBy mapping the files' contents into the address-space of the process.\nIf many programs only need read-access to the same file (e.g. /bin/bash, the C library) then the same physical memory can be shared between multiple processes.\n\n\nThe same mechanism can be used by programs to directly map files into memory\n\n\nHow do I map a file into memory?\n\n\nA simple program to map a file into memory is shown below. The key points to notice are:\n\n mmap requires a filedescriptor, so we need to \nopen\n the file first\n\n We seek to our desired size and write one byte to ensure that the file is sufficient length\n* When finished call munmap to unmap the file from memory.\n\n\nThis example also shows the preprocessor constants \"\nLINE\n\" and \"\nFILE\n\" that hold the current line number and filename of the file currently being compiled.\n\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nsys/types.h\n\n#include \nsys/stat.h\n\n#include \nsys/mman.h\n\n#include \nfcntl.h\n\n#include \nunistd.h\n\n#include \nerrno.h\n\n#include \nstring.h\n\n\n\nint fail(char *filename, int linenumber) { \n  fprintf(stderr, \n%s:%d %s\\n\n, filename, linenumber, strerror(errno)); \n  exit(1);\n  return 0; /*Make compiler happy */\n}\n#define QUIT fail(__FILE__, __LINE__ )\n\nint main() {\n  // We want a file big enough to hold 10 integers  \n  int size = sizeof(int) * 10;\n\n  int fd = open(\ndata\n, O_RDWR | O_CREAT | O_TRUNC, 0600); //6 = read+write for me!\n\n  lseek(fd, size, SEEK_SET);\n  write(fd, \nA\n, 1);\n\n  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n  printf(\nMapped at %p\\n\n, addr);\n  if (addr == (void*) -1 ) QUIT;\n\n  int *array = addr;\n  array[0] = 0x12345678;\n  array[1] = 0xdeadc0de;\n\n  munmap(addr,size);\n  return 0;\n\n}\n\n\n\n\nThe contents of our binary file can be listed using hexdump\n\n\n$ hexdump data\n0000000 78 56 34 12 de c0 ad de 00 00 00 00 00 00 00 00\n0000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n0000020 00 00 00 00 00 00 00 00 41   \n\n\n\n\nThe careful reader may notice that our integers were written in least-significant-byte format (because that is the endianess of the CPU) and that we allocated a file that is one byte too many!\n\n\nThe \nPROT_READ | PROT_WRITE\n options specify the virtual memory protection. The option \nPROT_EXEC\n (not used here) can be set to allow CPU execution of instructions in memory (e.g. this would be useful if you mapped an executable or library).\n\n\nWhat are the advantages of memory mapping a file\n\n\nFor many applications the main advantages are:\n\nSimplified coding - the file data is immediately available. No need to parse the incoming data and store it in new memory structures.\n\nSharing of files - memory mapped files are particularly efficient when the same data is shared between multiple processes.\n\n\nNote for simple sequential processing memory mapped files are not necessarily faster than standard 'stream-based' approaches of \nread\n / fscanf etc. \n\n\nHow do I share memory between a parent and child process?\n\n\nEasy -  Use \nmmap\n without a file - just specify the MAP_ANONYMOUS and MAP_SHARED options!\n\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nsys/types.h\n\n#include \nsys/stat.h\n\n#include \nsys/mman.h\n /* mmap() is defined in this header */\n#include \nfcntl.h\n\n#include \nunistd.h\n\n#include \nerrno.h\n\n#include \nstring.h\n\n\nint main() {\n\n  int size = 100 * sizeof(int);  \n  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n  printf(\nMapped at %p\\n\n, addr);\n\n  int *shared = addr;\n  pid_t mychild = fork();\n  if (mychild \n 0) {\n    shared[0] = 10;\n    shared[1] = 20;\n  } else {\n    sleep(1); // We will talk about synchronization later\n    printf(\n%d\\n\n, shared[1] + shared[0]);\n  }\n\n  munmap(addr,size);\n  return 0;\n}\n\n\n\n\nCan I use shared memory for IPC ?\n\n\nYes! As a simple example you could reserve just a few bytes and change the value in shared memory when you want the child process to quit.\nSharing memory is a very efficient form of inter-process communication because there is no copying overhead - the two processes literally share the same \nphysical\n frame of memory.\n\n\nGo to File System: Part 7", 
            "title": "File System, Part 6: Memory mapped files and Shared memory"
        }, 
        {
            "location": "/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#how-does-the-operating-system-load-my-process-and-libraries-into-memory", 
            "text": "By mapping the files' contents into the address-space of the process.\nIf many programs only need read-access to the same file (e.g. /bin/bash, the C library) then the same physical memory can be shared between multiple processes.  The same mechanism can be used by programs to directly map files into memory", 
            "title": "How does the operating system load my process and libraries into memory?"
        }, 
        {
            "location": "/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#how-do-i-map-a-file-into-memory", 
            "text": "A simple program to map a file into memory is shown below. The key points to notice are:  mmap requires a filedescriptor, so we need to  open  the file first  We seek to our desired size and write one byte to ensure that the file is sufficient length\n* When finished call munmap to unmap the file from memory.  This example also shows the preprocessor constants \" LINE \" and \" FILE \" that hold the current line number and filename of the file currently being compiled.  #include  stdio.h \n#include  stdlib.h \n#include  sys/types.h \n#include  sys/stat.h \n#include  sys/mman.h \n#include  fcntl.h \n#include  unistd.h \n#include  errno.h \n#include  string.h \n\n\nint fail(char *filename, int linenumber) { \n  fprintf(stderr,  %s:%d %s\\n , filename, linenumber, strerror(errno)); \n  exit(1);\n  return 0; /*Make compiler happy */\n}\n#define QUIT fail(__FILE__, __LINE__ )\n\nint main() {\n  // We want a file big enough to hold 10 integers  \n  int size = sizeof(int) * 10;\n\n  int fd = open( data , O_RDWR | O_CREAT | O_TRUNC, 0600); //6 = read+write for me!\n\n  lseek(fd, size, SEEK_SET);\n  write(fd,  A , 1);\n\n  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\n  printf( Mapped at %p\\n , addr);\n  if (addr == (void*) -1 ) QUIT;\n\n  int *array = addr;\n  array[0] = 0x12345678;\n  array[1] = 0xdeadc0de;\n\n  munmap(addr,size);\n  return 0;\n\n}  The contents of our binary file can be listed using hexdump  $ hexdump data\n0000000 78 56 34 12 de c0 ad de 00 00 00 00 00 00 00 00\n0000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n0000020 00 00 00 00 00 00 00 00 41     The careful reader may notice that our integers were written in least-significant-byte format (because that is the endianess of the CPU) and that we allocated a file that is one byte too many!  The  PROT_READ | PROT_WRITE  options specify the virtual memory protection. The option  PROT_EXEC  (not used here) can be set to allow CPU execution of instructions in memory (e.g. this would be useful if you mapped an executable or library).", 
            "title": "How do I map a file into memory?"
        }, 
        {
            "location": "/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#what-are-the-advantages-of-memory-mapping-a-file", 
            "text": "For many applications the main advantages are: \nSimplified coding - the file data is immediately available. No need to parse the incoming data and store it in new memory structures. \nSharing of files - memory mapped files are particularly efficient when the same data is shared between multiple processes.  Note for simple sequential processing memory mapped files are not necessarily faster than standard 'stream-based' approaches of  read  / fscanf etc.", 
            "title": "What are the advantages of memory mapping a file"
        }, 
        {
            "location": "/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#how-do-i-share-memory-between-a-parent-and-child-process", 
            "text": "Easy -  Use  mmap  without a file - just specify the MAP_ANONYMOUS and MAP_SHARED options!  #include  stdio.h \n#include  stdlib.h \n#include  sys/types.h \n#include  sys/stat.h \n#include  sys/mman.h  /* mmap() is defined in this header */\n#include  fcntl.h \n#include  unistd.h \n#include  errno.h \n#include  string.h \n\nint main() {\n\n  int size = 100 * sizeof(int);  \n  void *addr = mmap(0, size, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\n  printf( Mapped at %p\\n , addr);\n\n  int *shared = addr;\n  pid_t mychild = fork();\n  if (mychild   0) {\n    shared[0] = 10;\n    shared[1] = 20;\n  } else {\n    sleep(1); // We will talk about synchronization later\n    printf( %d\\n , shared[1] + shared[0]);\n  }\n\n  munmap(addr,size);\n  return 0;\n}", 
            "title": "How do I share memory between a parent and child process?"
        }, 
        {
            "location": "/File-System,-Part-6:-Memory-mapped-files-and-Shared-memory/#can-i-use-shared-memory-for-ipc", 
            "text": "Yes! As a simple example you could reserve just a few bytes and change the value in shared memory when you want the child process to quit.\nSharing memory is a very efficient form of inter-process communication because there is no copying overhead - the two processes literally share the same  physical  frame of memory.  Go to File System: Part 7", 
            "title": "Can I use shared memory for IPC ?"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/", 
            "text": "Note These notes are not quite complete. Please also see\n\n\nhttps://subversion.ews.illinois.edu/svn/fa15-cs241/_shared/lecture_handouts/CS241-28-filesystems-5.pptx\n\n\nHow and why does the kernel cache the filesystem?\n\n\nMost filesystems cache significant amounts of disk data in physical memory.\nLinux, in this respect, is particularly extreme: All unused memory is used as a giant disk cache.\n\n\nThe disk cache can have significant impact on overall system performance because disk I/O is slow. This is especially true for random access requests on spinning disks where the disk read-write latency is dominated by the seek time required to move the read-write disk head to the correct position.\n\n\nFor efficiency, the kernel caches recently used disk blocks. \nFor writing we have to choose a trade-off between performance and reliability: Disk writes can also be cached (\"Write-back cache\") where modified disk blocks are stored in memory until evicted. Alternatively a 'write-through cache' policy can be employed where disk writes are sent immediately to the disk. The latter is safer (as filesystem modifications are quickly stored to persistent media) but slower than a write-back cache; If writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block.\n\n\nNote this is a simplified description because solid state drives (SSDs) can be used as a secondary write-back cache.\n\n\nBoth solid state disks (SSD) and spinning disks have improved performance when reading or writing sequential data. Thus operating system can often use a read-ahead strategy to amortize the read-request costs (e.g. time cost for a spinning disk) and request several contiguous disk blocks per request. By issuing an I/O request for the next disk block before the user application requires the next disk block, the apparent disk I/O latency can be reduced.\n\n\nMy data is important! Can I force the disk writes to be saved to the physical media and wait for it to complete?\n\n\nYes (almost). Call \nsync\n to request that a filesystem changes be written (flushed) to disk.\nHowever, not all operating systems honor this request and even if the data is evicted from the kernel buffers the disk firmware use an internal on-disk cache or may not yet have finished changing the physical media. \n\n\nNote you can also request that all changes associated with a particular file descriptor are flushed to disk using \nfsync(int fd)\n\n\nHow likely is a disk failure?\n\n\nDisk failures are measured using \"Mean-Time-Failure\". For large arrays, the mean failure time can be surprising short. For example if the MTTF(single disk) = 30,000 hours, then the MTTF(100 disks)= 30000/100=300 hours  ie about 12 days!\n\n\nHow do I protect my data from disk failure?\n\n\nEasy! Store the data twice! This is the main principle of a \"RAID-1\" disk array. By duplicating the writes to a disk with writes to another (backup disk) there are exactly two copies of the data. If one disk fails, the other disk serves as the only copy until it can be re-cloned. Reading data is faster (since data can be requested from either disk) but writes are potentially twice as slow (now two write commands need to be issued for every disk block write) and, compared to using a single disk, the cost of storage per byte has doubled.\n\n\nWhat is RAID-3?\n\n\nRAID-3 uses parity codes instead of mirroring the data. For each N-bits written we will write one extra bit, the 'Parity bit' that ensures the total number of 1s written is even.  The parity bit is written to an additional disk. If any one disk (including the parity disk) is lost, then its contents can still be computed using the contents of the other disks.\n\n\nOne disadvantage of RAID-3 is that whenever a disk block is written, the parity block will always be written too.\n\n\nHow secure is RAID-3 to data-loss?\n\n\nA single disk failure will not result in data loss (because there is sufficient data to rebuild the array from the remaining disks). Data-loss will occur when a two disks are unusable because there is no longer sufficient data to rebuild the array. We can calculate the probability of a two disk failure based on the repair time which includes not just the time to insert a new disk but the time required to rebuild the entire contents of the array.\n\n\nMTTF = mean time to failure\nMTTR = mean time to repair\nN = number of original disks\n\np = MTTR / (MTTF-one-disk / (N-1))\n\n\n\n\nUsing typical numbers (MTTR=1day, MTTF=1000days, N-1 = 9,, p=0.009\n\n\nThere is a 1% chance that another drive will fail during the rebuild process (at that point you had better hope you still have an accessible backup of your original data.\n\n\nIn practice the probability of a second failure during the repair process is likely higher because rebuilding the array is I/O-intensive (and on top of normal I/O request activity). This higher I/O load will also stress the disk array\n\n\nWhat is RAID-5?\n\n\nRAID-5 is similar to RAID-3 except that the check block (parity information) is assigned to different disks for different blocks. The check-block is 'rotated' through the disk array. RAID-5 provides better read and write performance than RAID-3 because there is no longer the bottleneck of the single parity disk.\n\n\nDistributed storage\n\n\nFailure is the common case\nGoogle reports 2-10% of disks fail per year\nNow multiply that by 60,000+ disks in a single warehouse...\nMust survive failure of not just a disk, but a rack of servers or a whole data center\n\n\nSolutions\nSimple redundancy (2 or 3 copies of each file)\ne.g., Google GFS (2001)\nMore efficient redundancy (analogous to RAID 3++)\ne.g., Google Colossus filesystem (~2010): customizable replication including Reed-Solomon codes with 1.5x redundancy\n\n\nhttp://goo.gl/LwFIy\n\n\nQuestions\n\n\n\n\nIn ext2, what is stored in an inode, and what is stored in a directory entry?\n\n\nWhat are /sys, /proc, /dev/random, and /dev/urandom?\n\n\nHow do you use chmod to set user/group/owner read/write/execute permissions?\n\n\nWhat does the \"dd\" command do?\n\n\nWhat is the difference between a hard link and a symbolic link?\n\n\n\"ls -l\" shows the size of each file in a directory. Is the size stored in the directory or in the file's inode?", 
            "title": "File System, Part 7: Scalable and Reliable Filesystems"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#how-and-why-does-the-kernel-cache-the-filesystem", 
            "text": "Most filesystems cache significant amounts of disk data in physical memory.\nLinux, in this respect, is particularly extreme: All unused memory is used as a giant disk cache.  The disk cache can have significant impact on overall system performance because disk I/O is slow. This is especially true for random access requests on spinning disks where the disk read-write latency is dominated by the seek time required to move the read-write disk head to the correct position.  For efficiency, the kernel caches recently used disk blocks. \nFor writing we have to choose a trade-off between performance and reliability: Disk writes can also be cached (\"Write-back cache\") where modified disk blocks are stored in memory until evicted. Alternatively a 'write-through cache' policy can be employed where disk writes are sent immediately to the disk. The latter is safer (as filesystem modifications are quickly stored to persistent media) but slower than a write-back cache; If writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block.  Note this is a simplified description because solid state drives (SSDs) can be used as a secondary write-back cache.  Both solid state disks (SSD) and spinning disks have improved performance when reading or writing sequential data. Thus operating system can often use a read-ahead strategy to amortize the read-request costs (e.g. time cost for a spinning disk) and request several contiguous disk blocks per request. By issuing an I/O request for the next disk block before the user application requires the next disk block, the apparent disk I/O latency can be reduced.", 
            "title": "How and why does the kernel cache the filesystem?"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#my-data-is-important-can-i-force-the-disk-writes-to-be-saved-to-the-physical-media-and-wait-for-it-to-complete", 
            "text": "Yes (almost). Call  sync  to request that a filesystem changes be written (flushed) to disk.\nHowever, not all operating systems honor this request and even if the data is evicted from the kernel buffers the disk firmware use an internal on-disk cache or may not yet have finished changing the physical media.   Note you can also request that all changes associated with a particular file descriptor are flushed to disk using  fsync(int fd)", 
            "title": "My data is important! Can I force the disk writes to be saved to the physical media and wait for it to complete?"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#how-likely-is-a-disk-failure", 
            "text": "Disk failures are measured using \"Mean-Time-Failure\". For large arrays, the mean failure time can be surprising short. For example if the MTTF(single disk) = 30,000 hours, then the MTTF(100 disks)= 30000/100=300 hours  ie about 12 days!", 
            "title": "How likely is a disk failure?"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#how-do-i-protect-my-data-from-disk-failure", 
            "text": "Easy! Store the data twice! This is the main principle of a \"RAID-1\" disk array. By duplicating the writes to a disk with writes to another (backup disk) there are exactly two copies of the data. If one disk fails, the other disk serves as the only copy until it can be re-cloned. Reading data is faster (since data can be requested from either disk) but writes are potentially twice as slow (now two write commands need to be issued for every disk block write) and, compared to using a single disk, the cost of storage per byte has doubled.", 
            "title": "How do I protect my data from disk failure?"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#what-is-raid-3", 
            "text": "RAID-3 uses parity codes instead of mirroring the data. For each N-bits written we will write one extra bit, the 'Parity bit' that ensures the total number of 1s written is even.  The parity bit is written to an additional disk. If any one disk (including the parity disk) is lost, then its contents can still be computed using the contents of the other disks.  One disadvantage of RAID-3 is that whenever a disk block is written, the parity block will always be written too.", 
            "title": "What is RAID-3?"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#how-secure-is-raid-3-to-data-loss", 
            "text": "A single disk failure will not result in data loss (because there is sufficient data to rebuild the array from the remaining disks). Data-loss will occur when a two disks are unusable because there is no longer sufficient data to rebuild the array. We can calculate the probability of a two disk failure based on the repair time which includes not just the time to insert a new disk but the time required to rebuild the entire contents of the array.  MTTF = mean time to failure\nMTTR = mean time to repair\nN = number of original disks\n\np = MTTR / (MTTF-one-disk / (N-1))  Using typical numbers (MTTR=1day, MTTF=1000days, N-1 = 9,, p=0.009  There is a 1% chance that another drive will fail during the rebuild process (at that point you had better hope you still have an accessible backup of your original data.  In practice the probability of a second failure during the repair process is likely higher because rebuilding the array is I/O-intensive (and on top of normal I/O request activity). This higher I/O load will also stress the disk array", 
            "title": "How secure is RAID-3 to data-loss?"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#what-is-raid-5", 
            "text": "RAID-5 is similar to RAID-3 except that the check block (parity information) is assigned to different disks for different blocks. The check-block is 'rotated' through the disk array. RAID-5 provides better read and write performance than RAID-3 because there is no longer the bottleneck of the single parity disk.", 
            "title": "What is RAID-5?"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#distributed-storage", 
            "text": "Failure is the common case\nGoogle reports 2-10% of disks fail per year\nNow multiply that by 60,000+ disks in a single warehouse...\nMust survive failure of not just a disk, but a rack of servers or a whole data center  Solutions\nSimple redundancy (2 or 3 copies of each file)\ne.g., Google GFS (2001)\nMore efficient redundancy (analogous to RAID 3++)\ne.g., Google Colossus filesystem (~2010): customizable replication including Reed-Solomon codes with 1.5x redundancy  http://goo.gl/LwFIy", 
            "title": "Distributed storage"
        }, 
        {
            "location": "/File-System,-Part-7:-Scalable-and-Reliable-Filesystems/#questions", 
            "text": "In ext2, what is stored in an inode, and what is stored in a directory entry?  What are /sys, /proc, /dev/random, and /dev/urandom?  How do you use chmod to set user/group/owner read/write/execute permissions?  What does the \"dd\" command do?  What is the difference between a hard link and a symbolic link?  \"ls -l\" shows the size of each file in a directory. Is the size stored in the directory or in the file's inode?", 
            "title": "Questions"
        }, 
        {
            "location": "/File-System,-Part-8:-Disk-blocks-example/", 
            "text": "Under construction\n\n\nPlease can you explain a simple model of how the file's content is stored in a simple i-node based filesystem?\n\n\nSure!To answer this question we'll build a virtual disk and then write some C code to access its contents. Our filesystem will divide the bytes available into space for inodes and a much larger space for disk blocks. Each disk block will be 4096 bytes- \n\n\n// Disk size:\n#define MAX_INODE (1024)\n#define MAX_BLOCK (1024*1024)\n\n// Each block is 4096 bytes:\ntypedef char[4096] block_t;\n\n// A disk is an array of inodes and an array of disk blocks:\nstruct inode[MAX_INODE] inodes;\nblock[MAX_BLOCK] blocks;\n\n\n\n\nNote for clarity we will not use 'unsigned' in this code example. Our fixed-sized inodes will contain the file's size in bytes, permission,user,group information, time meta-data. What is most relevant to the problem-at hand is that it will also include ten pointers to disk blocks that we will use to refer to the actual file's contents!\n\n\nstruct inode {\n int[10] directblocks; // indices for the block array i.e. where to the find the file's content\n long size;\n // ... standard inode meta-data e.g.\n int mode, userid,groupid;\n time_t ctime,atime,mtime;\n}\n\n\n\n\nNow we can work out how to read a byte at offset \nposition\n of our file:\n\n\nchar readbyte(inode*inode,long position) {\n  if(position \n0 || position \n= inode-\nsize) return -1; // invalid offset\n\n  int  block_count = position / 4096,offset = position % 4096;\n\n  // block count better be 0..9 !\n  int physical_idx = lookup_physical_block_index(inode, block_count );\n\n  // sanity check that the disk block index is reasonable...\n  assert(physical_idx \n=0 \n physical_idx \n MAX_BLOCK);\n\n\n  // read the disk block from our virtual disk 'blocks' and return the specific byte\n  return blocks[physical_idx][offset];\n}\n\n\n\n\nOur initial version of lookup_physical_block is simple - we can use our table of 10 direct blocks!\n\n\nint lookup_physical_block_index(inode*inode, int block_count) {\n  assert(block_count\n=0 \n block_count \n 10);\n\n  return inode-\ndirectblocks[ block_count ]; // returns an index value between [0,MAX_BLOCK)\n}\n\n\n\n\nThis simple representation is reasonable provided we can represent all possible files with just ten blocks i.e. upto 40KB. What about larger files? We need the inode struct to always be the same size so just increasing the existing direct block array to 20 would roughly double the size of our inodes. If most of our files require less than 10 blocks, then our inode storage is now wasteful. To solve this problem we will use a disk block calledn the \nindirect block\n to extend the array of pointers at our disposal. We will only need this for files \n 40KB\n\n\nstruct inode {\n int[10] directblocks; // if size\n4KB then only the first one is valid\n int indirectblock; // valid value when size \n= 40KB\n int size;\n ...\n}\n\n\n\n\nThe indirect block is just a regular disk block of 4096 bytes but we will use it to hold pointers to disk blocks. Our pointers in this case are just integers, so we need to cast the pointer to an integer pointer:\n\n\nint lookup_physical_block_index(inode*inode, int block_count) {\n  assert(sizeof(int)==4); // Warning this code assumes an index is 4 bytes!\n  assert(block_count\n=0 \n block_count \n 1024 + 10); // 0 \n= block_count\n 1034\n\n  if( block_count \n 10)\n     return inode-\ndirectblocks[ block_count ];\n\n  // read the indirect block from disk:\n  block_t* oneblock = \n blocks[ inode-\nindirectblock ];\n\n  // Treat the 4KB as an array of 1024 pointers to other disk blocks\n  int* table = (int*) oneblock;\n\n // Look up the correct entry in the table\n // Offset by 10 because the first 10 blocks of data are already \n // accounted for\n  return table[ block_count - 10 ];\n}\n\n\n\n\nFor a typical filesystem our index values are 32 bits i.e. 4bytes. Thus in 4096 bytes we can store 4096 / 4 = 1024 entries\nThis means our indirect block can refer to 1024 * 4KB = 4MB of data. With the first ten direct blocks we can therefore accommodate files up to 40KB + 1024 * 4KB= 4136KB . Some of the later table entries can be invalid for files that are smaller than this. \n\n\nFor even larger files we could use two indirect blocks. However there's a better alternative, that will allow us to efficiently scale up to huge files. We will include a double-indirect pointer and if that's not enough a triple indirect pointer. The double indirect pointer means we have a table of 1024 entries to disk blocks that are used as 1024 entries. This means we can refer to 1024*1024 disk blocks of data.\n\n\n\n\n(source: http://uw714doc.sco.com/en/FS_admin/graphics/s5chain.gif)\n\n\nint lookup_physical_block_index(inode*inode, int block_count) {\n  if( block_count \n 10)\n     return inode-\ndirectblocks[ block_count ];\n\n  // Use indirect block for the next 1024 blocks:\n  // Assumes 1024 ints can fit inside each block!\n  if( block_count \n 1024 + 10) {   \n      int* table = (int*) \n blocks[ inode-\nindirectblock ];\n      return table[ block_count - 10 ];\n  }\n  // For huge files we will use a table of tables\n  int i = (block_count - 1034) / 1024 , j = (block_count - 1034) % 1024;\n  assert(i\n1024); // triple-indirect is not implemented here!\n\n  int* table1 = (int*) \n blocks[ inode-\ndoubleindirectblock ];\n   // The first table tells us where to read the second table ...\n  int* table2 = (int*) \n blocks[   table1[i]   ];\n  return table2[j];\n\n   // For gigantic files we will need to implement triple-indirect (table of tables of tables)\n}\n\n\n\n\nNotice that reading a byte using double indirect requires 3 disk block reads (two tables and the actual data block).", 
            "title": "File System, Part 8: Disk blocks example"
        }, 
        {
            "location": "/File-System,-Part-8:-Disk-blocks-example/#under-construction", 
            "text": "", 
            "title": "Under construction"
        }, 
        {
            "location": "/File-System,-Part-8:-Disk-blocks-example/#please-can-you-explain-a-simple-model-of-how-the-files-content-is-stored-in-a-simple-i-node-based-filesystem", 
            "text": "Sure!To answer this question we'll build a virtual disk and then write some C code to access its contents. Our filesystem will divide the bytes available into space for inodes and a much larger space for disk blocks. Each disk block will be 4096 bytes-   // Disk size:\n#define MAX_INODE (1024)\n#define MAX_BLOCK (1024*1024)\n\n// Each block is 4096 bytes:\ntypedef char[4096] block_t;\n\n// A disk is an array of inodes and an array of disk blocks:\nstruct inode[MAX_INODE] inodes;\nblock[MAX_BLOCK] blocks;  Note for clarity we will not use 'unsigned' in this code example. Our fixed-sized inodes will contain the file's size in bytes, permission,user,group information, time meta-data. What is most relevant to the problem-at hand is that it will also include ten pointers to disk blocks that we will use to refer to the actual file's contents!  struct inode {\n int[10] directblocks; // indices for the block array i.e. where to the find the file's content\n long size;\n // ... standard inode meta-data e.g.\n int mode, userid,groupid;\n time_t ctime,atime,mtime;\n}  Now we can work out how to read a byte at offset  position  of our file:  char readbyte(inode*inode,long position) {\n  if(position  0 || position  = inode- size) return -1; // invalid offset\n\n  int  block_count = position / 4096,offset = position % 4096;\n\n  // block count better be 0..9 !\n  int physical_idx = lookup_physical_block_index(inode, block_count );\n\n  // sanity check that the disk block index is reasonable...\n  assert(physical_idx  =0   physical_idx   MAX_BLOCK);\n\n\n  // read the disk block from our virtual disk 'blocks' and return the specific byte\n  return blocks[physical_idx][offset];\n}  Our initial version of lookup_physical_block is simple - we can use our table of 10 direct blocks!  int lookup_physical_block_index(inode*inode, int block_count) {\n  assert(block_count =0   block_count   10);\n\n  return inode- directblocks[ block_count ]; // returns an index value between [0,MAX_BLOCK)\n}  This simple representation is reasonable provided we can represent all possible files with just ten blocks i.e. upto 40KB. What about larger files? We need the inode struct to always be the same size so just increasing the existing direct block array to 20 would roughly double the size of our inodes. If most of our files require less than 10 blocks, then our inode storage is now wasteful. To solve this problem we will use a disk block calledn the  indirect block  to extend the array of pointers at our disposal. We will only need this for files   40KB  struct inode {\n int[10] directblocks; // if size 4KB then only the first one is valid\n int indirectblock; // valid value when size  = 40KB\n int size;\n ...\n}  The indirect block is just a regular disk block of 4096 bytes but we will use it to hold pointers to disk blocks. Our pointers in this case are just integers, so we need to cast the pointer to an integer pointer:  int lookup_physical_block_index(inode*inode, int block_count) {\n  assert(sizeof(int)==4); // Warning this code assumes an index is 4 bytes!\n  assert(block_count =0   block_count   1024 + 10); // 0  = block_count  1034\n\n  if( block_count   10)\n     return inode- directblocks[ block_count ];\n\n  // read the indirect block from disk:\n  block_t* oneblock =   blocks[ inode- indirectblock ];\n\n  // Treat the 4KB as an array of 1024 pointers to other disk blocks\n  int* table = (int*) oneblock;\n\n // Look up the correct entry in the table\n // Offset by 10 because the first 10 blocks of data are already \n // accounted for\n  return table[ block_count - 10 ];\n}  For a typical filesystem our index values are 32 bits i.e. 4bytes. Thus in 4096 bytes we can store 4096 / 4 = 1024 entries\nThis means our indirect block can refer to 1024 * 4KB = 4MB of data. With the first ten direct blocks we can therefore accommodate files up to 40KB + 1024 * 4KB= 4136KB . Some of the later table entries can be invalid for files that are smaller than this.   For even larger files we could use two indirect blocks. However there's a better alternative, that will allow us to efficiently scale up to huge files. We will include a double-indirect pointer and if that's not enough a triple indirect pointer. The double indirect pointer means we have a table of 1024 entries to disk blocks that are used as 1024 entries. This means we can refer to 1024*1024 disk blocks of data.   (source: http://uw714doc.sco.com/en/FS_admin/graphics/s5chain.gif)  int lookup_physical_block_index(inode*inode, int block_count) {\n  if( block_count   10)\n     return inode- directblocks[ block_count ];\n\n  // Use indirect block for the next 1024 blocks:\n  // Assumes 1024 ints can fit inside each block!\n  if( block_count   1024 + 10) {   \n      int* table = (int*)   blocks[ inode- indirectblock ];\n      return table[ block_count - 10 ];\n  }\n  // For huge files we will use a table of tables\n  int i = (block_count - 1034) / 1024 , j = (block_count - 1034) % 1024;\n  assert(i 1024); // triple-indirect is not implemented here!\n\n  int* table1 = (int*)   blocks[ inode- doubleindirectblock ];\n   // The first table tells us where to read the second table ...\n  int* table2 = (int*)   blocks[   table1[i]   ];\n  return table2[j];\n\n   // For gigantic files we will need to implement triple-indirect (table of tables of tables)\n}  Notice that reading a byte using double indirect requires 3 disk block reads (two tables and the actual data block).", 
            "title": "Please can you explain a simple model of how the file's content is stored in a simple i-node based filesystem?"
        }, 
        {
            "location": "/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/", 
            "text": "Case study: Removing malware from an Android device\n\n\nThis section uses filesystem features and system programming tools discussed in this wikibook to find and remove unwanted malware from an Android tablet. \n\n\nDISCLAIMER. ENSURE ANY VALUABLE INFORMATION ON YOUR DEVICE IS BACKED UP BEFORE ATTEMPTING TO MODIFY YOUR TABLET. MODIFYING SYSTEM SETTINGS AND SYSTEM FILES IS NOT RECOMMENDED. ATTEMPTING TO MODIFY A DEVICE USING THIS CASE STUDU GUIDE MAY CAUSE YOUR TABLET TO SHARE, LOSE OR CORRUPT YOUR DATA. FURTHER YOUR TABLET MAY FUNCTION INCORRECTLY OR TO STOP FUNCTIONING ALTOGETHER. USE THIS CASE STUDY AT YOUR OWN RISK. THE AUTHORS ASSUME NO RESPONSIBILITY AND MAKE NO WARRANTY ABOUT THE CORRECTNESS OR COMPLETENESS OF THESE INSTRUCTIONS INCLUDED IN THIS CASE STUDY. THE AUTHORS ASSUME NO RESPONSIBILITY AND MAKE NO WARRANTY ABOUT ANY SOFTWARE INCLUDING EXTERNAL THIRD PARTY SOFTWARE DESCRIBED OR LINKED TO IN THIS GUIDE.\n\n\nBackground\n\n\nAn E97 Android tablet purchased from Amazon had developed some peculiar quirks. Most noticeable was that the browser app always opened a website at gotoamazing.com rather than the home page set in the app's preferences (known as browser 'hijacking'). Can we use the knowledge from this wikibook to understand how this unwanted behavior occurs and also remove unwanted pre-installed apps from the device?\n\n\nTools used\n\n\nWhile it is possible to use the Android developer tools installed on a remote USB-connected machine, this guide uses only system tools on the tablet. The following apps were installed - \n\n\n\n\nMalwarebytes - A free vulnerability and malware tool.\n\n\nTerminal emulator - A simple terminal window that gives us shell access on the tablet.\n\n\nKingRoot - A tool that uses known exploits in the linux kernel to gain root access.\n\n\n\n\nInstalling any app can potentially allow arbitrary code to be executed if it is able to break out of the Android security model. Of the apps mentioned above, KingRoot is the most extreme example because it exploits system vulnerabilities to gain root access for our purposes. However in doing so, it could also \nOf these, KingRoot is the most questionable tool to install - we are trusting it not to install any of its own malware. A potentially safer alternative is to use https://github.com/android-rooting-tools/\n\n\nOverview of terminal\n\n\nThe most useful commands are  \nsu grep mount\n and Android's package manager tool, \npm\n.\n\n\n\n\ngrep -s abc * \n/\n   (search for \nabc\n in current directory and immediate sub directories)\n\n\nsu ( aka \"switch user\" become root - requires a rooted device)\n\n\nmount -o rw,remount /system  (allow /system partition to be writeable)\n\n\npm disable  (aka 'package manager' disable an Android app package)\n\n\n\n\nOverview of filesystem layout\n\n\nOn this specific tablet that runs Android 4.4.2, pre-installed apps are unmodifiable and are located in\n\n\n/system/app/\n/system/priv-app/\n\n\n\n\nand preferences and app-data are stored in the \n/data\n partition\nEach app is typically packaged inside an apk file, which is essentially a zip file. When an app is installed the code is expanded into a file that can be directly parsed by the Android virtual machine. The binary code (at least for this particular virtual machine) has an odex extension.\n\n\nWe can search the code of the installed system apps for the string 'gotoamazing'\n\n\ngrep -s gotoamazing /system/app/* /system/priv-app/*\n\n\n\n\nThis didn't find anything; it appears this string was not hardcoded into the source code of the given system apps. To verify that we can find \n\n\nLet's check the data area of all installed apps\n\n\ncd /data/data\ngrep -s gotoamazing * */* */*/*\n\n\n\n\nproduced the following\n\n\ndata/com.android.browser/shared_prefs/xbservice.xml: \nstring name=\nURL\nhttp://www.gotoamazing...\n\n\n\n\nThe -s option \"silent option\" stops grep from complaining about trying to grep directories and other invalid files. Note we could have also used -r to recursively search directories but it was fun to use file globbing (wildcard expansion of the * by the shell).\n\n\nNow we are getting somewhere! It looks like this string is part of the app 'com.android.browser' but let's also find out which app binary code opens the 'xbservice' preference. Perhaps this unwanted service is hiding inside another app and managed to secretly load as an extension to the browser?\n\n\nLet's look for any file that contains xbservice. This time we will recursively search in the directories of /system that include the 'app'\n\n\ngrep -r -s xbservice /system/*app*\nBinary file /system/app/Browser.odex matches\n\n\n\n\nFinally - it appears the factory browser was shipped with the the home page hijacking pre-installed. Let's uninstall it. For this, let's become root.\n\n\n`\n$ su\n\n\npm list packages -s\n\n\n`\nAndroid's package manager has many commands and options. The above example lists all currently installed system apps. We can uninstall the browser app using the following command\n\n\npm disable com.android.browser\npm uninstall com.android.browser\n\n\n\n\nUsing \npm list packages\n you can list all installed packages (use \n-s\n options to see just system packages). We disabled the following system apps. Of course there is no real guarantee that we successfully removed all unwanted software, or that one of these is a false-positive. As such we would not recommend keeping sensitive information on such a tablet.\n\n\n\n\ncom.android.browser\n\n\ncom.adups.fota.sysoper\n\n\nelink.com\n\n\ncom.google.android.apps.cloudprint\n\n\ncom.mediatek.CrashService\n\n\ncom.get.googleApps\n\n\ncom.adups.fota (an over-the-air package that can install arbitrary items in the future).\n\n\ncom.mediatek.appguide.plugin\n\n\n\n\nIt is likely that you could just re-enable a package using \npm enable package-name\n or \npm install\n and the relevant .apk file in /system/app or /system/priv-app", 
            "title": "File System, Part 8: Removing preinstalled malware from an Android device"
        }, 
        {
            "location": "/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#background", 
            "text": "An E97 Android tablet purchased from Amazon had developed some peculiar quirks. Most noticeable was that the browser app always opened a website at gotoamazing.com rather than the home page set in the app's preferences (known as browser 'hijacking'). Can we use the knowledge from this wikibook to understand how this unwanted behavior occurs and also remove unwanted pre-installed apps from the device?", 
            "title": "Background"
        }, 
        {
            "location": "/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#tools-used", 
            "text": "While it is possible to use the Android developer tools installed on a remote USB-connected machine, this guide uses only system tools on the tablet. The following apps were installed -    Malwarebytes - A free vulnerability and malware tool.  Terminal emulator - A simple terminal window that gives us shell access on the tablet.  KingRoot - A tool that uses known exploits in the linux kernel to gain root access.   Installing any app can potentially allow arbitrary code to be executed if it is able to break out of the Android security model. Of the apps mentioned above, KingRoot is the most extreme example because it exploits system vulnerabilities to gain root access for our purposes. However in doing so, it could also \nOf these, KingRoot is the most questionable tool to install - we are trusting it not to install any of its own malware. A potentially safer alternative is to use https://github.com/android-rooting-tools/", 
            "title": "Tools used"
        }, 
        {
            "location": "/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#overview-of-terminal", 
            "text": "The most useful commands are   su grep mount  and Android's package manager tool,  pm .   grep -s abc *  /    (search for  abc  in current directory and immediate sub directories)  su ( aka \"switch user\" become root - requires a rooted device)  mount -o rw,remount /system  (allow /system partition to be writeable)  pm disable  (aka 'package manager' disable an Android app package)", 
            "title": "Overview of terminal"
        }, 
        {
            "location": "/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#overview-of-filesystem-layout", 
            "text": "On this specific tablet that runs Android 4.4.2, pre-installed apps are unmodifiable and are located in  /system/app/\n/system/priv-app/  and preferences and app-data are stored in the  /data  partition\nEach app is typically packaged inside an apk file, which is essentially a zip file. When an app is installed the code is expanded into a file that can be directly parsed by the Android virtual machine. The binary code (at least for this particular virtual machine) has an odex extension.  We can search the code of the installed system apps for the string 'gotoamazing'  grep -s gotoamazing /system/app/* /system/priv-app/*  This didn't find anything; it appears this string was not hardcoded into the source code of the given system apps. To verify that we can find   Let's check the data area of all installed apps  cd /data/data\ngrep -s gotoamazing * */* */*/*  produced the following  data/com.android.browser/shared_prefs/xbservice.xml:  string name= URL http://www.gotoamazing...  The -s option \"silent option\" stops grep from complaining about trying to grep directories and other invalid files. Note we could have also used -r to recursively search directories but it was fun to use file globbing (wildcard expansion of the * by the shell).  Now we are getting somewhere! It looks like this string is part of the app 'com.android.browser' but let's also find out which app binary code opens the 'xbservice' preference. Perhaps this unwanted service is hiding inside another app and managed to secretly load as an extension to the browser?  Let's look for any file that contains xbservice. This time we will recursively search in the directories of /system that include the 'app'  grep -r -s xbservice /system/*app*\nBinary file /system/app/Browser.odex matches  Finally - it appears the factory browser was shipped with the the home page hijacking pre-installed. Let's uninstall it. For this, let's become root.  `\n$ su", 
            "title": "Overview of filesystem layout"
        }, 
        {
            "location": "/File-System,-Part-8:-Removing-preinstalled-malware-from-an-Android-device/#pm-list-packages-s", 
            "text": "`\nAndroid's package manager has many commands and options. The above example lists all currently installed system apps. We can uninstall the browser app using the following command  pm disable com.android.browser\npm uninstall com.android.browser  Using  pm list packages  you can list all installed packages (use  -s  options to see just system packages). We disabled the following system apps. Of course there is no real guarantee that we successfully removed all unwanted software, or that one of these is a false-positive. As such we would not recommend keeping sensitive information on such a tablet.   com.android.browser  com.adups.fota.sysoper  elink.com  com.google.android.apps.cloudprint  com.mediatek.CrashService  com.get.googleApps  com.adups.fota (an over-the-air package that can install arbitrary items in the future).  com.mediatek.appguide.plugin   It is likely that you could just re-enable a package using  pm enable package-name  or  pm install  and the relevant .apk file in /system/app or /system/priv-app", 
            "title": "pm list packages -s"
        }, 
        {
            "location": "/Files,-Part-1:-Working-with-files/", 
            "text": "We've already seen \nopen\n and fopen (todo link here) so let's look at some more advanced concepts.\n\n\nHow do I tell how large a file is?\n\n\nFor files less than the size of a long using fseek and ftell is a simple way to accomplish this:\n\n\nMove to the end of the file and find out the current position.\n\n\nfseek(f, 0, SEEK_END);\nlong pos = ftell(f);\n\n\n\n\nThis tells us the current position in the file in bytes - i.e. the length of the file!\n\n\nfseek\n can also be used to set the absolute position.\n\n\nfseek(f, 0, SEEK_SET); // Move to the start of the file \nfseek(f, posn, SEEK_SET);  // Move to 'posn' in the file.\n\n\n\n\nAll future reads and writes in the parent or child processes will honor this position.\nNote writing or reading from the file will change the current position.\n\n\nSee the man pages for fseek and ftell for more information.\n\n\nWhat happens if a child process closes a filestream using \nfclose\n or \nclose\n?\n\n\nUnlike position, closing a file stream is unique to each process. Other processes can continue to use their own file-handle.", 
            "title": "Files, Part 1: Working with files"
        }, 
        {
            "location": "/Files,-Part-1:-Working-with-files/#how-do-i-tell-how-large-a-file-is", 
            "text": "For files less than the size of a long using fseek and ftell is a simple way to accomplish this:  Move to the end of the file and find out the current position.  fseek(f, 0, SEEK_END);\nlong pos = ftell(f);  This tells us the current position in the file in bytes - i.e. the length of the file!  fseek  can also be used to set the absolute position.  fseek(f, 0, SEEK_SET); // Move to the start of the file \nfseek(f, posn, SEEK_SET);  // Move to 'posn' in the file.  All future reads and writes in the parent or child processes will honor this position.\nNote writing or reading from the file will change the current position.  See the man pages for fseek and ftell for more information.", 
            "title": "How do I tell how large a file is?"
        }, 
        {
            "location": "/Files,-Part-1:-Working-with-files/#what-happens-if-a-child-process-closes-a-filestream-using-fclose-or-close", 
            "text": "Unlike position, closing a file stream is unique to each process. Other processes can continue to use their own file-handle.", 
            "title": "What happens if a child process closes a filestream using fclose or close?"
        }, 
        {
            "location": "/Filesystem:-Review-Questions/", 
            "text": "Question numbers subject to change\n\n\n\n\nQ1\n\n\nWrite a function that uses fseek and ftell to replace the middle character of a file with an 'X'\n\n\nvoid xout(char* filename) {\n  FILE *f = fopen(filename, ____ );\n\n\n\n}\n\n\n\n\nQ2\n\n\nIn an \next2\n filesystem how many inodes are read from disk to access the first byte of the file \n/dir1/subdirA/notes.txt\n ? Assume the directory names and inode numbers in the root directory (but not the inodes themselves) are already in memory.\n\n\nQ3\n\n\nIn an \next2\n filesystem what is the minimum number of disk blocks that must be read from disk to access the first byte of the file \n/dir1/subdirA/notes.txt\n ? Assume the directory names and inode numbers in the root directory and all inodes are already in memory.\n\n\nQ4\n\n\nIn an \next2\n filesystem with 32 bit addresses and 4KB disk blocks, an inodes that can store 10 direct disk block numbers. What is the minimum file size required to require an single indirection table? ii) a double direction table?\n\n\nQ5\n\n\nFix the shell command \nchmod\n below to set the permission of a file \nsecret.txt\n  so that the owner can read,write,and execute permissions the group can read and everyone else has no access.\n\n`\nchmod 000 secret.txt", 
            "title": "Filesystem: Review Questions"
        }, 
        {
            "location": "/Filesystem:-Review-Questions/#q1", 
            "text": "Write a function that uses fseek and ftell to replace the middle character of a file with an 'X'  void xout(char* filename) {\n  FILE *f = fopen(filename, ____ );\n\n\n\n}", 
            "title": "Q1"
        }, 
        {
            "location": "/Filesystem:-Review-Questions/#q2", 
            "text": "In an  ext2  filesystem how many inodes are read from disk to access the first byte of the file  /dir1/subdirA/notes.txt  ? Assume the directory names and inode numbers in the root directory (but not the inodes themselves) are already in memory.", 
            "title": "Q2"
        }, 
        {
            "location": "/Filesystem:-Review-Questions/#q3", 
            "text": "In an  ext2  filesystem what is the minimum number of disk blocks that must be read from disk to access the first byte of the file  /dir1/subdirA/notes.txt  ? Assume the directory names and inode numbers in the root directory and all inodes are already in memory.", 
            "title": "Q3"
        }, 
        {
            "location": "/Filesystem:-Review-Questions/#q4", 
            "text": "In an  ext2  filesystem with 32 bit addresses and 4KB disk blocks, an inodes that can store 10 direct disk block numbers. What is the minimum file size required to require an single indirection table? ii) a double direction table?", 
            "title": "Q4"
        }, 
        {
            "location": "/Filesystem:-Review-Questions/#q5", 
            "text": "Fix the shell command  chmod  below to set the permission of a file  secret.txt   so that the owner can read,write,and execute permissions the group can read and everyone else has no access. `\nchmod 000 secret.txt", 
            "title": "Q5"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/", 
            "text": "A word of warning\n\n\nProcess forking is a very powerful (and very dangerous) tool. If you mess up and cause a fork bomb (explained later on this page), \nyou can bring down the entire system\n. To reduce the chances of this, limit your maximum number of processes to a small number e.g\n 40 by typing \nulimit -u 40\n into a command line. Note that this limit is only for the user, which means if you fork bomb, then you won't be able to kill all of the processes you just created since calling \nkillall\n requires your shell to fork() ... ironic right? So what can we do about this. One solution is to spawn another shell instance as another user (for example root) before hand and kill processes form there. Another is to use the built in \nexec\n command to kill all the user processes (careful you only have one shot at this). Finally you could reboot the system :)\n\n\nWhen testing fork() code, ensure that you have either root and/or physical access to the machine involved. If you must work on fork () code remotely, remember that \nkill -9 -1\n will save you in the event of an emergency.\n\n\nTL;DR: Fork can be \nextremely\n dangerous if you aren't prepared for it. \nYou have been warned.\n\n\nWhat does fork do?\n\n\nThe \nfork\n system call clones the current process to create a new process. It creates a new process (the child process) by duplicating the state of the existing process with a few minor differences (discussed below). The child process does not start from main. Instead it returns from \nfork()\n just as the parent process does.\n\n\nWhat is the simplest \nfork()\n example?\n\n\nHere's a very simple example...\n\n\nprintf(\nI'm printed once!\\n\n);\nfork();\n// Now there are two processes running\n// and each process will print out the next line.\nprintf(\nYou see this line twice!\\n\n);\n\n\n\n\nWhy does this example print 42 twice?\n\n\nThe following program prints out 42 twice - but the \nfork()\n is after the \nprintf\n!? Why?\n\n\n#include \nunistd.h\n /*fork declared here*/\n#include \nstdio.h\n /* printf declared here*/\nint main() {\n   int answer = 84 \n 1;\n   printf(\nAnswer: %d\n, answer);\n   fork();\n   return 0;\n}\n\n\n\n\nThe \nprintf\n line \nis\n executed only once however notice that the printed contents is not flushed to standard out (there's no newline printed, we didn't call \nfflush\n, or change the buffering mode).\nThe output text is therefore in still in process memory waiting to be sent.\nWhen \nfork()\n is executed the entire process memory is duplicated including the buffer. Thus the child process starts with a non-empty output buffer which will be flushed when the program exits.\n\n\nHow do you write code that is different for the parent and child process?\n\n\nCheck the return value of \nfork()\n. Return value -1= failed; 0= in child process; positive = in parent process (and the return value is the child process id).  Here's one way to remember which is which:\n\n\nThe child process can find its parent - the original process that was duplicated -  by calling getppid() - so does not need any additional return information from \nfork()\n. The parent process however can only find out the id of the new child process from the return value of \nfork\n:\n\n\npid_t id = fork();\nif (id == -1) exit(1); // fork failed \nif (id \n 0)\n{ \n// I'm the original parent and \n// I just created a child process with id 'id'\n// Use waitpid to wait for the child to finish\n} else { // returned zero\n// I must be the newly made child process\n}\n\n\n\n\nWhat is a fork bomb ?\n\n\nA 'fork bomb' is when you attempt to create an infinite number of processes. A simple example is shown below:\n\n\nwhile (1) fork();\n\n\n\n\nThis will often bring a system to a near-standstill as it attempts to allocate CPU time and memory to a very large number of processes that are ready to run. Comment: System administrators don't like fork-bombs and may set upper limits on the number of processes each user can have or may revoke login rights because it creates a disturbance in the force for other users' programs. You can also limit the number of child processes created by using \nsetrlimit()\n.\n\n\nfork bombs are not necessarily malicious - they occasionally occur due to student coding errors.\n\n\nAngrave suggests that the Matrix trilogy, where the machine and man finally work together to defeat the multiplying Agent-Smith, was a cinematic plot based on an AI-driven fork-bomb.\n\n\nHow does the parent process wait for the child to finish?\n\n\nUse \nwaitpid\n (or \nwait\n).\n\n\npid_t child_id = fork();\nif (child_id == -1) { perror(\nfork\n); exit(EXIT_FAILURE);}\nif (child_id \n 0) { \n  // We have a child! Get their exit code\n  int status; \n  waitpid( child_id, \nstatus, 0 );\n  // code not shown to get exit status from child\n} else { // In child ...\n  // start calculation\n  exit(123);\n}\n\n\n\n\nCan I make the child process execute another program?\n\n\nYes. Use one of the \nexec\n functions after forking. The \nexec\n set of functions replaces the process image with the the process image of what is being called. This means that any lines of code after the \nexec\n call are replaced. Any other work you want the child process to do should be done before the \nexec\n call.  \n\n\n#include \nunistd.h\n\n#include \nsys/types.h\n \n#include \nsys/wait.h\n\n#include \nstdlib.h\n\n#include \nstdio.h\n\n\nint main(int argc, char**argv) {\n  pid_t child = fork();\n  if (child == -1) return EXIT_FAILURE;\n  if (child) { /* I have a child! */\n    int status;\n    waitpid(child , \nstatus ,0);\n    return EXIT_SUCCESS;\n\n  } else { /* I am the child */\n    // Other versions of exec pass in arguments as arrays\n    // Remember first arg is the program name\n    // Last arg must be a char pointer to NULL\n\n    execl(\n/bin/ls\n, \nls\n,\n-alh\n, (char *) NULL);\n\n    // If we get to this line, something went wrong!\n    perror(\nexec failed!\n);\n  }\n}\n\n\n\n\nA simpler way to execute another program\n\n\nUse \nsystem\n!!! Here is how to use it:\n\n\n\n#include \nunistd.h\n\n#include \nstdlib.h\n\n\nint main(int argc, char**argv) {\n  system(\n/bin/ls\n);\n  return 0;\n}\n\n\n\n\nThe \nsystem\n call would fork, exec the command passed by parameter and the parent process would wait for this to finish. This also means that \nsystem\n is a blocking call - The parent process can't continue until the process started by \nsystem\n exits. This may be useful or it may not be, use with caution.\n\n\nWhat is the silliest fork example?\n\n\nA slightly silly example is shown below. What will it print? Try it with multiple arguments to your program.\n\n\n#include \nunistd.h\n\n#include \nstdio.h\n\nint main(int argc, char **argv) {\n  pid_t id;\n  int status; \n  while (--argc \n (id=fork())) {\n    waitpid(id,\nstatus,0); /* Wait for child*/\n  }\n  printf(\n%d:%s\\n\n, argc, argv[argc]);\n  return 0;\n}\n\n\n\n\nThe amazing parallel apparent-O(N) \nsleepsort\n is today's silly winner. First published on \n4chan in 2011 \n. A version of this awful but amusing sorting algorithm is shown below.\n\n\nint main(int c, char **v)\n{\n        while (--c \n 1 \n !fork());\n        int val  = atoi(v[c]);\n        sleep(val);\n        printf(\n%d\\n\n, val);\n        return 0;\n}\n\n\n\n\nWhat is different in the child process than the parent process?\n\n\nThe key differences include:\n\n The process id returned by \ngetpid()\n. The parent process id returned by \ngetppid()\n.\n\n The parent is notified via a signal when the child process finishes but not vice versa.\n* The child does not inherit pending signals or timer alarms.\nFor a complete list see the \nfork man page\n\n\nDo child processes share open filehandles?\n\n\nYes! In fact both processes use the same underlying kernel file descriptor. For example if one process rewinds the random access position back to the beginning of the file, then both processes are affected.\n\n\nBoth child and parent should \nclose\n (or \nfclose\n) their file descriptors or file handle respectively.\n\n\nHow can I find out more?\n\n\nRead the man pages!\n\n \nfork\n\n\n \nexec\n\n* \nwait", 
            "title": "Forking, Part 1: Introduction"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#a-word-of-warning", 
            "text": "Process forking is a very powerful (and very dangerous) tool. If you mess up and cause a fork bomb (explained later on this page),  you can bring down the entire system . To reduce the chances of this, limit your maximum number of processes to a small number e.g\n 40 by typing  ulimit -u 40  into a command line. Note that this limit is only for the user, which means if you fork bomb, then you won't be able to kill all of the processes you just created since calling  killall  requires your shell to fork() ... ironic right? So what can we do about this. One solution is to spawn another shell instance as another user (for example root) before hand and kill processes form there. Another is to use the built in  exec  command to kill all the user processes (careful you only have one shot at this). Finally you could reboot the system :)  When testing fork() code, ensure that you have either root and/or physical access to the machine involved. If you must work on fork () code remotely, remember that  kill -9 -1  will save you in the event of an emergency.  TL;DR: Fork can be  extremely  dangerous if you aren't prepared for it.  You have been warned.", 
            "title": "A word of warning"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#what-does-fork-do", 
            "text": "The  fork  system call clones the current process to create a new process. It creates a new process (the child process) by duplicating the state of the existing process with a few minor differences (discussed below). The child process does not start from main. Instead it returns from  fork()  just as the parent process does.", 
            "title": "What does fork do?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#what-is-the-simplest-fork-example", 
            "text": "Here's a very simple example...  printf( I'm printed once!\\n );\nfork();\n// Now there are two processes running\n// and each process will print out the next line.\nprintf( You see this line twice!\\n );", 
            "title": "What is the simplest fork() example?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#why-does-this-example-print-42-twice", 
            "text": "The following program prints out 42 twice - but the  fork()  is after the  printf !? Why?  #include  unistd.h  /*fork declared here*/\n#include  stdio.h  /* printf declared here*/\nint main() {\n   int answer = 84   1;\n   printf( Answer: %d , answer);\n   fork();\n   return 0;\n}  The  printf  line  is  executed only once however notice that the printed contents is not flushed to standard out (there's no newline printed, we didn't call  fflush , or change the buffering mode).\nThe output text is therefore in still in process memory waiting to be sent.\nWhen  fork()  is executed the entire process memory is duplicated including the buffer. Thus the child process starts with a non-empty output buffer which will be flushed when the program exits.", 
            "title": "Why does this example print 42 twice?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#how-do-you-write-code-that-is-different-for-the-parent-and-child-process", 
            "text": "Check the return value of  fork() . Return value -1= failed; 0= in child process; positive = in parent process (and the return value is the child process id).  Here's one way to remember which is which:  The child process can find its parent - the original process that was duplicated -  by calling getppid() - so does not need any additional return information from  fork() . The parent process however can only find out the id of the new child process from the return value of  fork :  pid_t id = fork();\nif (id == -1) exit(1); // fork failed \nif (id   0)\n{ \n// I'm the original parent and \n// I just created a child process with id 'id'\n// Use waitpid to wait for the child to finish\n} else { // returned zero\n// I must be the newly made child process\n}", 
            "title": "How do you write code that is different for the parent and child process?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#what-is-a-fork-bomb", 
            "text": "A 'fork bomb' is when you attempt to create an infinite number of processes. A simple example is shown below:  while (1) fork();  This will often bring a system to a near-standstill as it attempts to allocate CPU time and memory to a very large number of processes that are ready to run. Comment: System administrators don't like fork-bombs and may set upper limits on the number of processes each user can have or may revoke login rights because it creates a disturbance in the force for other users' programs. You can also limit the number of child processes created by using  setrlimit() .  fork bombs are not necessarily malicious - they occasionally occur due to student coding errors.  Angrave suggests that the Matrix trilogy, where the machine and man finally work together to defeat the multiplying Agent-Smith, was a cinematic plot based on an AI-driven fork-bomb.", 
            "title": "What is a fork bomb ?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#how-does-the-parent-process-wait-for-the-child-to-finish", 
            "text": "Use  waitpid  (or  wait ).  pid_t child_id = fork();\nif (child_id == -1) { perror( fork ); exit(EXIT_FAILURE);}\nif (child_id   0) { \n  // We have a child! Get their exit code\n  int status; \n  waitpid( child_id,  status, 0 );\n  // code not shown to get exit status from child\n} else { // In child ...\n  // start calculation\n  exit(123);\n}", 
            "title": "How does the parent process wait for the child to finish?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#can-i-make-the-child-process-execute-another-program", 
            "text": "Yes. Use one of the  exec  functions after forking. The  exec  set of functions replaces the process image with the the process image of what is being called. This means that any lines of code after the  exec  call are replaced. Any other work you want the child process to do should be done before the  exec  call.    #include  unistd.h \n#include  sys/types.h  \n#include  sys/wait.h \n#include  stdlib.h \n#include  stdio.h \n\nint main(int argc, char**argv) {\n  pid_t child = fork();\n  if (child == -1) return EXIT_FAILURE;\n  if (child) { /* I have a child! */\n    int status;\n    waitpid(child ,  status ,0);\n    return EXIT_SUCCESS;\n\n  } else { /* I am the child */\n    // Other versions of exec pass in arguments as arrays\n    // Remember first arg is the program name\n    // Last arg must be a char pointer to NULL\n\n    execl( /bin/ls ,  ls , -alh , (char *) NULL);\n\n    // If we get to this line, something went wrong!\n    perror( exec failed! );\n  }\n}", 
            "title": "Can I make the child process execute another program?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#a-simpler-way-to-execute-another-program", 
            "text": "Use  system !!! Here is how to use it:  \n#include  unistd.h \n#include  stdlib.h \n\nint main(int argc, char**argv) {\n  system( /bin/ls );\n  return 0;\n}  The  system  call would fork, exec the command passed by parameter and the parent process would wait for this to finish. This also means that  system  is a blocking call - The parent process can't continue until the process started by  system  exits. This may be useful or it may not be, use with caution.", 
            "title": "A simpler way to execute another program"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#what-is-the-silliest-fork-example", 
            "text": "A slightly silly example is shown below. What will it print? Try it with multiple arguments to your program.  #include  unistd.h \n#include  stdio.h \nint main(int argc, char **argv) {\n  pid_t id;\n  int status; \n  while (--argc   (id=fork())) {\n    waitpid(id, status,0); /* Wait for child*/\n  }\n  printf( %d:%s\\n , argc, argv[argc]);\n  return 0;\n}  The amazing parallel apparent-O(N)  sleepsort  is today's silly winner. First published on  4chan in 2011  . A version of this awful but amusing sorting algorithm is shown below.  int main(int c, char **v)\n{\n        while (--c   1   !fork());\n        int val  = atoi(v[c]);\n        sleep(val);\n        printf( %d\\n , val);\n        return 0;\n}", 
            "title": "What is the silliest fork example?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#what-is-different-in-the-child-process-than-the-parent-process", 
            "text": "The key differences include:  The process id returned by  getpid() . The parent process id returned by  getppid() .  The parent is notified via a signal when the child process finishes but not vice versa.\n* The child does not inherit pending signals or timer alarms.\nFor a complete list see the  fork man page", 
            "title": "What is different in the child process than the parent process?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#do-child-processes-share-open-filehandles", 
            "text": "Yes! In fact both processes use the same underlying kernel file descriptor. For example if one process rewinds the random access position back to the beginning of the file, then both processes are affected.  Both child and parent should  close  (or  fclose ) their file descriptors or file handle respectively.", 
            "title": "Do child processes share open filehandles?"
        }, 
        {
            "location": "/Forking,-Part-1:-Introduction/#how-can-i-find-out-more", 
            "text": "Read the man pages!   fork    exec \n*  wait", 
            "title": "How can I find out more?"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/", 
            "text": "What does the following 'exec' example do?\n\n\n#include \nunistd.h\n\n#include \nfcntl.h\n // O_CREAT, O_APPEND etc. defined here\n\nint main() {\n   close(1); // close standard out\n   open(\nlog.txt\n, O_RDWR | O_CREAT | O_APPEND, S_IRUSR | S_IWUSR);\n   puts(\nCaptain's log\n);\n   chdir(\n/usr/include\n);\n   // execl( executable,  arguments for executable including program name and NULL at the end)\n\n   execl(\n/bin/ls\n, /* Remaining items sent to ls*/ \n/bin/ls\n, \n.\n, (char *) NULL); // \nls .\n\n   perror(\nexec failed\n);\n   return 0; // Not expected\n}\n\n\n\n\nThere's no error checking in the above code (we assume close,open,chdir etc works as expected).\n\n open: will use the lowest available file descriptor (i.e. 1) ; so standard out now goes to the log file.\n\n chdir : Change the current directory to /usr/include\n\n execl : Replace the program image with /bin/ls and call its main() method\n\n perror : We don't expect to get here - if we did then exec failed.\n\n\nWhat does the child inherit from the parent?\n\n\n\n\nOpen filehandles. If the parent later seeks, say, to the back to the beginning of the file then this will affect the child too (and vice versa). \n\n\nSignal handlers\n\n\nCurrent working directory\n\n\nEnvironment variables\n\n\n\n\nSee the \nfork man page\n for more details.\n\n\nWhat is different in the child process than the parent process?\n\n\nThe process id is different. In the child calling \ngetppid()\n (notice the two 'p's) will give the same result as calling getpid() in the parent. See the fork man page for more details.\n\n\nHow do I wait for my child to finish?\n\n\nUse \nwaitpid\n or \nwait\n. The parent process will pause until \nwait\n (or \nwaitpid\n) returns. Note this explanation glosses over the restarting discussion.\n\n\nWhat is the fork-exec-wait pattern\n\n\nA common programming pattern is to call \nfork\n followed by \nexec\n and \nwait\n. The original process calls fork, which creates a child process. The child process then uses exec to start execution of a new program. Meanwhile the parent uses \nwait\n (or \nwaitpid\n) to wait for the child process to finish.\nSee below for a complete code example.\n\n\nHow do I start a background process that runs as the same time?\n\n\nDon't wait for them! Your parent process can continue to execute code without having to wait for the child process. Note in practice background processes can also be disconnected from the parent's input and output streams by calling \nclose\n on the open file descriptors before calling exec.\n\n\nHowever child processes that finish before their parent finishes can become zombies. See the zombie page for more information.\n\n\nGood parents don't let their children become zombies!\n\n\nWhen a child finishes (or terminates) it still takes up a slot in the kernel process table. \nOnly when the child has been 'waited on' will the slot be available again.\n\n\nA long running program could create many zombies by continually creating processes and never \nwait\n-ing for them.\n\n\nWhat would be effect of too many zombies?\n\n\nEventually there would be insufficient space in the kernel process table to create a new processes. Thus \nfork()\n would fail and could make the system difficult / impossible to use - for example just logging in requires a new process!\n\n\nWhat does the system do to help prevent zombies?\n\n\nOnce a process completes, any of its children will be assigned to \"init\" - the first process with pid of 1. Thus these children would see getppid() return a value of 1. The init process automatically waits for all of its children, thus removing zombies from the system.\n\n\nHow do I prevent zombies? (Warning: Simplified answer)\n\n\nWait on your child!\n\n\nwaitpid(child, \nstatus, 0); // Clean up and wait for my child process to finish.\n\n\n\n\nNote we assume that the only reason to get a SIGCHLD event is that a child has finished (this is not quite true - see man page for more details).\n\n\nA robust implementation would also check for interrupted status and include the above in a loop.\nRead on for a discussion of a more robust implementation.\n\n\nHow can I asynchronously wait for my child using SIGCHLD? (ADVANCED)\n\n\nWarning: This section uses signals which we have not yet fully introduced.\nThe parent gets the signal SIGCHLD when a child completes, so the signal handler can wait on the process. A slightly simplified version is shown below.\n\n\npid_t child;\n\nvoid cleanup(int signal) {\n  int status;\n  waitpid(child, \nstatus, 0);\n  write(1,\ncleanup!\\n\n,9);\n}\nint main() {\n   // Register signal handler BEFORE the child can finish\n   signal(SIGCHLD, cleanup); // or better - sigaction\n   child = fork();\n   if (child == -1) { exit(EXIT_FAILURE);}\n\n   if (child == 0) { /* I am the child!*/\n     // Do background stuff e.g. call exec   \n   } else { /* I'm the parent! */\n      sleep(4); // so we can see the cleanup\n      puts(\nParent is done\n);\n   }\n   return 0;\n} \n\n\n\n\nThe above example however misses a couple of subtle points:\n\n More than one child may have finished but the parent will only get one SIGCHLD signal (signals are not queued)\n\n SIGCHLD signals can be sent for other reasons (e.g. a child process is temporarily stopped)\n\n\nA more robust code to reap zombies is shown below.\n\n\nvoid cleanup(int signal) {\n  int status;\n  while (waitpid((pid_t) (-1), 0, WNOHANG) \n 0) {}\n}", 
            "title": "Forking, Part 2: Fork, Exec, Wait"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#what-does-the-following-exec-example-do", 
            "text": "#include  unistd.h \n#include  fcntl.h  // O_CREAT, O_APPEND etc. defined here\n\nint main() {\n   close(1); // close standard out\n   open( log.txt , O_RDWR | O_CREAT | O_APPEND, S_IRUSR | S_IWUSR);\n   puts( Captain's log );\n   chdir( /usr/include );\n   // execl( executable,  arguments for executable including program name and NULL at the end)\n\n   execl( /bin/ls , /* Remaining items sent to ls*/  /bin/ls ,  . , (char *) NULL); //  ls . \n   perror( exec failed );\n   return 0; // Not expected\n}  There's no error checking in the above code (we assume close,open,chdir etc works as expected).  open: will use the lowest available file descriptor (i.e. 1) ; so standard out now goes to the log file.  chdir : Change the current directory to /usr/include  execl : Replace the program image with /bin/ls and call its main() method  perror : We don't expect to get here - if we did then exec failed.", 
            "title": "What does the following 'exec' example do?"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#what-does-the-child-inherit-from-the-parent", 
            "text": "Open filehandles. If the parent later seeks, say, to the back to the beginning of the file then this will affect the child too (and vice versa).   Signal handlers  Current working directory  Environment variables   See the  fork man page  for more details.", 
            "title": "What does the child inherit from the parent?"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#what-is-different-in-the-child-process-than-the-parent-process", 
            "text": "The process id is different. In the child calling  getppid()  (notice the two 'p's) will give the same result as calling getpid() in the parent. See the fork man page for more details.", 
            "title": "What is different in the child process than the parent process?"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#how-do-i-wait-for-my-child-to-finish", 
            "text": "Use  waitpid  or  wait . The parent process will pause until  wait  (or  waitpid ) returns. Note this explanation glosses over the restarting discussion.", 
            "title": "How do I wait for my child to finish?"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#what-is-the-fork-exec-wait-pattern", 
            "text": "A common programming pattern is to call  fork  followed by  exec  and  wait . The original process calls fork, which creates a child process. The child process then uses exec to start execution of a new program. Meanwhile the parent uses  wait  (or  waitpid ) to wait for the child process to finish.\nSee below for a complete code example.", 
            "title": "What is the fork-exec-wait pattern"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#how-do-i-start-a-background-process-that-runs-as-the-same-time", 
            "text": "Don't wait for them! Your parent process can continue to execute code without having to wait for the child process. Note in practice background processes can also be disconnected from the parent's input and output streams by calling  close  on the open file descriptors before calling exec.  However child processes that finish before their parent finishes can become zombies. See the zombie page for more information.", 
            "title": "How do I start a background process that runs as the same time?"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#good-parents-dont-let-their-children-become-zombies", 
            "text": "When a child finishes (or terminates) it still takes up a slot in the kernel process table. \nOnly when the child has been 'waited on' will the slot be available again.  A long running program could create many zombies by continually creating processes and never  wait -ing for them.", 
            "title": "Good parents don't let their children become zombies!"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#what-would-be-effect-of-too-many-zombies", 
            "text": "Eventually there would be insufficient space in the kernel process table to create a new processes. Thus  fork()  would fail and could make the system difficult / impossible to use - for example just logging in requires a new process!", 
            "title": "What would be effect of too many zombies?"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#what-does-the-system-do-to-help-prevent-zombies", 
            "text": "Once a process completes, any of its children will be assigned to \"init\" - the first process with pid of 1. Thus these children would see getppid() return a value of 1. The init process automatically waits for all of its children, thus removing zombies from the system.", 
            "title": "What does the system do to help prevent zombies?"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#how-do-i-prevent-zombies-warning-simplified-answer", 
            "text": "Wait on your child!  waitpid(child,  status, 0); // Clean up and wait for my child process to finish.  Note we assume that the only reason to get a SIGCHLD event is that a child has finished (this is not quite true - see man page for more details).  A robust implementation would also check for interrupted status and include the above in a loop.\nRead on for a discussion of a more robust implementation.", 
            "title": "How do I prevent zombies? (Warning: Simplified answer)"
        }, 
        {
            "location": "/Forking,-Part-2:-Fork,-Exec,-Wait/#how-can-i-asynchronously-wait-for-my-child-using-sigchld-advanced", 
            "text": "Warning: This section uses signals which we have not yet fully introduced.\nThe parent gets the signal SIGCHLD when a child completes, so the signal handler can wait on the process. A slightly simplified version is shown below.  pid_t child;\n\nvoid cleanup(int signal) {\n  int status;\n  waitpid(child,  status, 0);\n  write(1, cleanup!\\n ,9);\n}\nint main() {\n   // Register signal handler BEFORE the child can finish\n   signal(SIGCHLD, cleanup); // or better - sigaction\n   child = fork();\n   if (child == -1) { exit(EXIT_FAILURE);}\n\n   if (child == 0) { /* I am the child!*/\n     // Do background stuff e.g. call exec   \n   } else { /* I'm the parent! */\n      sleep(4); // so we can see the cleanup\n      puts( Parent is done );\n   }\n   return 0;\n}   The above example however misses a couple of subtle points:  More than one child may have finished but the parent will only get one SIGCHLD signal (signals are not queued)  SIGCHLD signals can be sent for other reasons (e.g. a child process is temporarily stopped)  A more robust code to reap zombies is shown below.  void cleanup(int signal) {\n  int status;\n  while (waitpid((pid_t) (-1), 0, WNOHANG)   0) {}\n}", 
            "title": "How can I asynchronously wait for my child using SIGCHLD? (ADVANCED)"
        }, 
        {
            "location": "/HW0/", 
            "text": "Welcome!\n\n\n// First can you guess which lyrics have been transformed into this C-like system code?\nchar q[] = \nDo you wanna build a C99 program?\n;\n#define or \ngo debugging with gdb?\n\nstatic unsigned int i = sizeof(or) != strlen(or);\nchar* ptr = \nlathe\n; size_t come = fprintf(stdout,\n%s door\n, ptr+2);\nint away = ! (int) * \n;\n\nint* shared = mmap(NULL, sizeof(int*), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\nmunmap(shared,sizeof(int*));\n\nif(!fork()) { execlp(\nman\n,\nman\n,\n-3\n,\nftell\n, (char*)0); perror(\nfailed\n); }\nif(!fork()) { execlp(\nmake\n,\nmake\n, \nsnowman\n, (char*)0); execlp(\nmake\n,\nmake\n, (char*)0)); }\n\nexit(0);\n\n\n\n\nSo you want to master System Programming? And get a better grade than B?\n\n\nint main(int argc, char** argv) {\n puts(\nGreat! We have plenty of useful resources for you but it's up to you to\n);\n puts(\nbe an active learner and learn how to solve problems and debug code.\n);\n puts(\nBring your near-completed answers the problems below\n);\n puts(\n to the first lab to show that you've been working on this\n);\n printf(\nA few \\\ndon't knows\\\n or \\\nunsure\\\n is fine for lab 1\n); \n puts(\nWarning; your peers will be working hard for this class\n);\n puts(\nThis is not CS225; you will be pushed much harder to\n);\n puts(\n work things out on your own\n);\n fprintf(stdout,\nthe point is that this homework is a stepping stone to all future assignments\n);\n char p[] = \nso you will want to clear up any confusions or misconceptions.\n;\n write(1, p, strlen(p) );\n char buffer[1024];\n sprintf(buffer,\nFor grading purposes this homework 0 will be graded as part of your lab %d work.\n, 1);\n write(1, buffer, strlen(buffer));\n printf(\nPress Return to continue\\n\n);\n read(0, buffer, sizeof(buffer));\n return 0;\n}\n\n\n\n\nWatch the videos and write up your answers to the following questions.\n\n\nhttp://cs-education.github.io/sys/\n\n\nThere is also the course wikibook - \n\n\nhttps://github.com/angrave/SystemProgramming/wiki\n\n\nQuestions? Comments? Use Piazza,\nhttps://piazza.com/class/ijd7w4r95dp3q\n\n\nThe in-browser virtual machine runs entirely in Javascript and is fastest in Chrome. Note the VM and any code you write is reset when you reload the page \nSo copy your code to a separate document.\n The post-video challenges (e.g. Haiku poem) are not part of homework 0. \n\n\nChapter 1\n\n\n\n\nHello World (System call style)\n\n\nWrite a program that uses \nwrite()\n to print out \"Hi! My name is \n\".\n\n\nHello Standard Error Stream\n\n\nWrite a program that uses \nwrite()\n to print out a triangle of height \nn\n to Standard Error\n\n\nn should be a variable and the triangle should look like this for n = 3\n```C\n\n\n\n*\n\n\n\n\n\n\n```\n\n\n\n\nWriting to files\n\n\nTake your program from \"Hello World\" and have it write to a file\n\n\nMake sure to to use some interesting flags and mode for \nopen()\n\n\nman 2 open\n is your friend\n\n\n\n\n\n\nNot everything is a system call\n\n\nTake your program from \"Writing to files\" and replace it with \nprintf()\n\n\nName some differences from \nwrite()\n and \nprintf()\n\n\n\n\nChapter 2\n\n\n\n\nNot all bytes are 8 bits?\n\n\nHow many bits are there in a byte?\n\n\nHow many bytes is a \nchar\n?\n\n\nTell me how many bytes the following are on your machine: \nint, double, float, long, long long\n\n\nFollow the int pointer\n\n\nOn a machine with 8 byte integers:\n  \nC\n  int main(){\n      int data[8];\n  }\n\n  If the address of data is \n0x7fbd9d40\n, then what is the address of \ndata+2\n?\n\n\nWhat is \ndata[3]\n equivalent to in C?\n\n\nsizeof\n character arrays, incrementing pointers\n\n\n\n\nRemember the type of a string constant \n\"abc\"\n is an array.\n  - Why does this segfault?\n  \nC\n  char *ptr = \"hello\";\n  *ptr = 'J';\n\n  - What does \nsizeof(\"Hello\\0World\")\n return?\n  - What does \nstrlen(\"Hello\\0World\")\n return?\n  - Give an example of X such that \nsizeof(X)\n is 3\n  - Give an example of Y such at \nsizeof(Y)\n might be 4 or 8 depending on the machine.\n\n\nChapter 3\n\n\n\n\nProgram arguments \nargc\n \nargv\n\n\nName me two ways to find the length of \nargv\n\n\nWhat is \nargv[0]\n\n\nEnvironment Variables\n\n\nWhere are the pointers to environment variables stored?\n\n\nString searching (Strings are just char arrays)\n\n\n\n\nOn a machine where pointers are 8 bytes and with the following code:\n  \nC\n  char *ptr = \"Hello\";\n  char array[] = \"Hello\";\n\n  What is the results of \nsizeof(ptr)\n and \nsizeof(array)\n? Explain why.\n\n\n\n\n\n\nLifetime of automatic variables\n\n\n\n\nWhat datastucture is managing the lifetime of automatic variables?\n\n\n\n\nChapter 4\n\n\n\n\nMemory allocation using \nmalloc\n, heap and time\n\n\nIf I want to use data after the lifetime of the function it was created in, then where should I put it and how do I put it there?\n\n\nFill in the blank. In a good C program: \"For every malloc there is a ___\".\n\n\nHeap allocation Gotchas\n\n\nName one reason \nmalloc\n can fail.\n\n\nName some differences between \ntime()\n and \nctime()\n\n\nWhat is wrong with this code snippet?\n  \nC\n  free(ptr);\n  free(ptr);\n\n\nWhat is wrong with this code snippet?\n  \nC\n  free(ptr);\n  printf(\"%s\\n\", ptr);\n\n\nHow can one avoid the previous 2 mistakes? \n\n\nstruct, typedefs and a linked list\n\n\nCreate a struct that represents a Person and typedef, so that \"struct Person\" can be replaced with a single word.\n\n\nA person should contain the following information: name, age, friends (pointer to an array of pointers to People).\n\n\n\n\n\n\nNow make two persons \"Agent Smith\" and \"Sonny Moore\" on the heap who are 128 and 256 years old respectively and are friends with each other.\n\n\nDuplicating strings, memory allocation and deallocation of structures\n\n\nCreate functions to create and destroy a Person (Person's and their names should live on the heap).\n\n\ncreate()\n should take a name and make a copy of the name and also an age. Use malloc to reserve sufficient memory. Be sure initialize all fields (why?).\n\n\ndestroy()\n should free up not only the memory of the person struct but also free all its attributes that are stored on the heap (the array if it exists and the string). Destroying one person however should not destroy any others.\n\n\n\n\n\n\n\n\nChapter 5\n\n\n\n\nReading characters, Trouble with gets\n\n\nWhat functions can be used for getting characters for \nstdin\n and writing them to \nstdout\n?\n\n\nName one issue with \ngets()\n\n\nIntroducing \nsscanf\n and friends\n\n\nWrite code that parses a the string \"Hello 5 World\" and initializes 3 variables to (\"Hello\", 5, \"World\") respectively.\n\n\ngetline\n is useful\n\n\nWhat does one need to define before using \ngetline()\n?\n\n\nWrite a C program to print out the content of a file line by line using \ngetline()\n\n\n\n\nC Development (A web search is useful here)\n\n\n\n\nWhat compiler flag is used to generate a debug build?\n\n\nYou modify the makefile to generate debug builds and type \nmake\n again. Explain why this is insufficient to generate a new build.\n\n\nAre tabs or spaces used in Makefiles?\n\n\nWhat are the differences between heap and stack memory?\n\n\nAre there other kinds of memory in a process?\n\n\n\n\nOptional (Just for fun)\n\n\n\n\nConvert your a song lyrics into System Programming and C code covered in this wiki book and share on Piazza.\n\n\nFind, in your opinion the best and worst C code on the web and post the link to Piazza\n\n\nWrite a short C program with a deliberate subtle C bug and post it on Piazza to see if others can spot your bug", 
            "title": "HW0"
        }, 
        {
            "location": "/HW0/#welcome", 
            "text": "// First can you guess which lyrics have been transformed into this C-like system code?\nchar q[] =  Do you wanna build a C99 program? ;\n#define or  go debugging with gdb? \nstatic unsigned int i = sizeof(or) != strlen(or);\nchar* ptr =  lathe ; size_t come = fprintf(stdout, %s door , ptr+2);\nint away = ! (int) *  ;\n\nint* shared = mmap(NULL, sizeof(int*), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);\nmunmap(shared,sizeof(int*));\n\nif(!fork()) { execlp( man , man , -3 , ftell , (char*)0); perror( failed ); }\nif(!fork()) { execlp( make , make ,  snowman , (char*)0); execlp( make , make , (char*)0)); }\n\nexit(0);", 
            "title": "Welcome!"
        }, 
        {
            "location": "/HW0/#so-you-want-to-master-system-programming-and-get-a-better-grade-than-b", 
            "text": "int main(int argc, char** argv) {\n puts( Great! We have plenty of useful resources for you but it's up to you to );\n puts( be an active learner and learn how to solve problems and debug code. );\n puts( Bring your near-completed answers the problems below );\n puts(  to the first lab to show that you've been working on this );\n printf( A few \\ don't knows\\  or \\ unsure\\  is fine for lab 1 ); \n puts( Warning; your peers will be working hard for this class );\n puts( This is not CS225; you will be pushed much harder to );\n puts(  work things out on your own );\n fprintf(stdout, the point is that this homework is a stepping stone to all future assignments );\n char p[] =  so you will want to clear up any confusions or misconceptions. ;\n write(1, p, strlen(p) );\n char buffer[1024];\n sprintf(buffer, For grading purposes this homework 0 will be graded as part of your lab %d work. , 1);\n write(1, buffer, strlen(buffer));\n printf( Press Return to continue\\n );\n read(0, buffer, sizeof(buffer));\n return 0;\n}", 
            "title": "So you want to master System Programming? And get a better grade than B?"
        }, 
        {
            "location": "/HW0/#watch-the-videos-and-write-up-your-answers-to-the-following-questions", 
            "text": "http://cs-education.github.io/sys/  There is also the course wikibook -   https://github.com/angrave/SystemProgramming/wiki  Questions? Comments? Use Piazza,\nhttps://piazza.com/class/ijd7w4r95dp3q  The in-browser virtual machine runs entirely in Javascript and is fastest in Chrome. Note the VM and any code you write is reset when you reload the page  So copy your code to a separate document.  The post-video challenges (e.g. Haiku poem) are not part of homework 0.", 
            "title": "Watch the videos and write up your answers to the following questions."
        }, 
        {
            "location": "/HW0/#chapter-1", 
            "text": "Hello World (System call style)  Write a program that uses  write()  to print out \"Hi! My name is  \".  Hello Standard Error Stream  Write a program that uses  write()  to print out a triangle of height  n  to Standard Error  n should be a variable and the triangle should look like this for n = 3\n```C  *    ```   Writing to files  Take your program from \"Hello World\" and have it write to a file  Make sure to to use some interesting flags and mode for  open()  man 2 open  is your friend    Not everything is a system call  Take your program from \"Writing to files\" and replace it with  printf()  Name some differences from  write()  and  printf()", 
            "title": "Chapter 1"
        }, 
        {
            "location": "/HW0/#chapter-2", 
            "text": "Not all bytes are 8 bits?  How many bits are there in a byte?  How many bytes is a  char ?  Tell me how many bytes the following are on your machine:  int, double, float, long, long long  Follow the int pointer  On a machine with 8 byte integers:\n   C\n  int main(){\n      int data[8];\n  } \n  If the address of data is  0x7fbd9d40 , then what is the address of  data+2 ?  What is  data[3]  equivalent to in C?  sizeof  character arrays, incrementing pointers   Remember the type of a string constant  \"abc\"  is an array.\n  - Why does this segfault?\n   C\n  char *ptr = \"hello\";\n  *ptr = 'J'; \n  - What does  sizeof(\"Hello\\0World\")  return?\n  - What does  strlen(\"Hello\\0World\")  return?\n  - Give an example of X such that  sizeof(X)  is 3\n  - Give an example of Y such at  sizeof(Y)  might be 4 or 8 depending on the machine.", 
            "title": "Chapter 2"
        }, 
        {
            "location": "/HW0/#chapter-3", 
            "text": "Program arguments  argc   argv  Name me two ways to find the length of  argv  What is  argv[0]  Environment Variables  Where are the pointers to environment variables stored?  String searching (Strings are just char arrays)   On a machine where pointers are 8 bytes and with the following code:\n   C\n  char *ptr = \"Hello\";\n  char array[] = \"Hello\"; \n  What is the results of  sizeof(ptr)  and  sizeof(array) ? Explain why.    Lifetime of automatic variables   What datastucture is managing the lifetime of automatic variables?", 
            "title": "Chapter 3"
        }, 
        {
            "location": "/HW0/#chapter-4", 
            "text": "Memory allocation using  malloc , heap and time  If I want to use data after the lifetime of the function it was created in, then where should I put it and how do I put it there?  Fill in the blank. In a good C program: \"For every malloc there is a ___\".  Heap allocation Gotchas  Name one reason  malloc  can fail.  Name some differences between  time()  and  ctime()  What is wrong with this code snippet?\n   C\n  free(ptr);\n  free(ptr);  What is wrong with this code snippet?\n   C\n  free(ptr);\n  printf(\"%s\\n\", ptr);  How can one avoid the previous 2 mistakes?   struct, typedefs and a linked list  Create a struct that represents a Person and typedef, so that \"struct Person\" can be replaced with a single word.  A person should contain the following information: name, age, friends (pointer to an array of pointers to People).    Now make two persons \"Agent Smith\" and \"Sonny Moore\" on the heap who are 128 and 256 years old respectively and are friends with each other.  Duplicating strings, memory allocation and deallocation of structures  Create functions to create and destroy a Person (Person's and their names should live on the heap).  create()  should take a name and make a copy of the name and also an age. Use malloc to reserve sufficient memory. Be sure initialize all fields (why?).  destroy()  should free up not only the memory of the person struct but also free all its attributes that are stored on the heap (the array if it exists and the string). Destroying one person however should not destroy any others.", 
            "title": "Chapter 4"
        }, 
        {
            "location": "/HW0/#chapter-5", 
            "text": "Reading characters, Trouble with gets  What functions can be used for getting characters for  stdin  and writing them to  stdout ?  Name one issue with  gets()  Introducing  sscanf  and friends  Write code that parses a the string \"Hello 5 World\" and initializes 3 variables to (\"Hello\", 5, \"World\") respectively.  getline  is useful  What does one need to define before using  getline() ?  Write a C program to print out the content of a file line by line using  getline()", 
            "title": "Chapter 5"
        }, 
        {
            "location": "/HW0/#c-development-a-web-search-is-useful-here", 
            "text": "What compiler flag is used to generate a debug build?  You modify the makefile to generate debug builds and type  make  again. Explain why this is insufficient to generate a new build.  Are tabs or spaces used in Makefiles?  What are the differences between heap and stack memory?  Are there other kinds of memory in a process?", 
            "title": "C Development (A web search is useful here)"
        }, 
        {
            "location": "/HW0/#optional-just-for-fun", 
            "text": "Convert your a song lyrics into System Programming and C code covered in this wiki book and share on Piazza.  Find, in your opinion the best and worst C code on the web and post the link to Piazza  Write a short C program with a deliberate subtle C bug and post it on Piazza to see if others can spot your bug", 
            "title": "Optional (Just for fun)"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/", 
            "text": "What happens when I call malloc?\n\n\nThe function \nmalloc\n is a C library call and is used to reserve a contiguous block of memory. Unlike stack memory, the memory remains allocated until \nfree\n is called with the same pointer. There is also \ncalloc\n and \nrealloc\n which are discussed below.\n\n\nCan malloc fail?\n\n\nIf \nmalloc\n fails to reserve any more memory then it returns \nNULL\n. Robust programs should check the return value. If your code assumes \nmalloc\n succeeds and it does not, then your program will likely crash (segfault) when it tries to write to address 0.\n\n\nWhere is the heap and how big is it?\n\n\nThe heap is part of the process memory and it does not have a fixed size. Heap memory allocation is performed by the C library when you call \nmalloc\n (\ncalloc\n, \nrealloc\n) and \nfree\n.\n\n\nFirst a quick review on process memory: A process is a running instance of your program. Each process has its own address space. For example on a 32 bit machine your process gets about 4 billion addresses to play with, however not all of these are valid or even mapped to actual physical memory (RAM). Inside the process's memory you will find the executable code, space for the stack, environment variables, global (static) variables and the heap.\n\n\nBy calling \nsbrk\n the C library can increase the size of the heap as your program demands more heap memory. As the heap and stack (one for each thread) need to grow, we put them at opposite ends of the address space. So for typical architectures the heap will grow upwards and the stack grows downwards. \n\n\nTruthiness: Modern operating system memory allocators no longer need \nsbrk\n - instead they can request independent regions of virtual memory and maintain multiple memory regions. For example gigabyte requests may be placed in a different memory region than small allocation requests. However this detail is an unwanted complexity: The problems of fragmentation and allocating memory efficiently still apply, so we will ignore this implementation nicety here and will write as if the heap is a single region.\n\n\nIf we write a multi-threaded program (more about that later) we will need multiple stacks (one per thread) but there's only ever one heap.\n\n\nOn typical architectures, the heap is part of the \nData segment\n and starts just above the code and global variables. \n\n\nDo programs need to call brk or sbrk?\n\n\nNot typically (though calling \nsbrk(0)\n can be interesting because it tells you where your heap currently ends). Instead programs use \nmalloc,calloc,realloc\n and \nfree\n which are part of the C library. The internal implementation of these functions will call \nsbrk\n when additional heap memory is required.\n\n\nvoid *top_of_heap = sbrk(0);\nmalloc(16384);\nvoid *top_of_heap2 = sbrk(0);\nprintf(\nThe top of heap went from %p to %p \\n\n, top_of_heap, top_of_heap2);\n\n\n\n\nExample output: \nThe top of heap went from 0x4000 to 0xa000\n\n\nWhat is calloc?\n\n\nUnlike \nmalloc\n, \ncalloc\n initializes memory contents to zero and also takes two arguments (the number of items and the size in bytes of each item). A naive but readable implementation of \ncalloc\n looks like this:\n\n\nvoid *calloc(size_t n, size_t size)\n{\n    size_t total = n * size; // Does not check for overflow!\n    void *result = malloc(total);\n\n    if (!result) return NULL;\n\n// If we're using new memory pages \n// just allocated from the system by calling sbrk\n// then they will be zero so zero-ing out is unnecessary,\n\n    memset(result, 0, total);\n    return result; \n}\n\n\n\n\nAn advanced discussion of these limitations is \nhere\n.\n\n\nProgrammers often use \ncalloc\n rather than explicitly calling \nmemset\n after \nmalloc\n, to set the memory contents to zero. Note \ncalloc(x,y)\n is identical to \ncalloc(y,x)\n, but you should follow the conventions of the manual.\n\n\n// Ensure our memory is initialized to zero\nlink_t *link  = malloc(256);\nmemset(link, 0, 256); // Assumes malloc returned a valid address!\n\nlink_t *link = calloc(1, 256); // safer: calloc(1, sizeof(link_t));\n\n\n\n\nWhy is the memory that is first returned by sbrk initialized to zero?\n\n\nIf the operating system did not zero out contents of physical RAM it might be possible for one process to learn about the memory of another process that had previously used the memory. This would be a security leak.\n\n\nUnfortunately this means that for \nmalloc\n requests before any memory has been freed and simple programs (which end up using newly reserved memory from the system) the memory is \noften\n zero. Then programmers mistaken write C programs that assume malloc'd memory will \nalways\n be zero.\n\n\nchar* ptr = malloc(300);\n// contents is probably zero because we get brand new memory\n// so beginner programs appear to work!\n// strcpy(ptr, \nSome data\n); // work with the data\nfree(ptr);\n// later\nchar *ptr2 = malloc(308); // Contents might now contain existing data and is probably not zero\n\n\n\n\nWhy doesn't malloc always initialize memory to zero?\n\n\nPerformance! We want malloc to be as fast as possible. Zeroing out memory may be unnecessary.\n\n\nWhat is realloc and when would you use it?\n\n\nrealloc\n allows you to resize an existing memory allocation that was previously allocated on the heap (via malloc,calloc or realloc). The most common use of realloc is to resize memory used to hold an array of values. A naive but readable version of realloc is suggested below\n\n\nvoid * realloc(void * ptr, size_t newsize) {\n  // Simple implementation always reserves more memory\n  // and has no error checking\n  void *result = malloc(newsize); \n  size_t oldsize =  ... //(depends on allocator's internal data structure)\n  if (ptr) memcpy(result, ptr, newsize \n oldsize ? newsize : oldsize);\n  free(ptr);\n  return result;\n}\n\n\n\n\nAn INCORRECT use of realloc is shown below:\n\n\nint *array = malloc(sizeof(int) * 2);\narray[0] = 10; array[1]; = 20;\n// Ooops need a bigger array - so use realloc..\nrealloc (array, 3); // ERRORS!\narray[2] = 30; \n\n\n\n\nThe above code contains two mistakes. Firstly we needed 3*sizeof(int) bytes not 3 bytes.\nSecondly realloc may need to move the existing contents of the memory to a new location. For example, there may not be sufficient space because the neighboring bytes are already allocated. A correct use of realloc is shown below.\n\n\narray = realloc(array, 3 * sizeof(int));\n// If array is copied to a new location then old allocation will be freed.\n\n\n\n\nA robust version would also check for a \nNULL\n return value. Note \nrealloc\n can be used to grow and shrink allocations. \n\n\nWhere can I read more?\n\n\nSee \nthe man page\n!\n\n\nHow important is that memory allocation is fast?\n\n\nVery! Allocating and de-allocating heap memory is a common operation in most applications.\n\n\nWhat is the silliest malloc and free implementation and what is wrong with it?\n\n\nvoid* malloc(size_t size)\n// Ask the system for more bytes by extending the heap space. \n// sbrk Returns -1 on failure\n   void *p = sbrk(size); \n   if(p == (void *) -1) return NULL; // No space left\n   return p;\n}\nvoid free() {/* Do nothing */}\n\n\n\n\nThe above implementation suffers from two major drawbacks:\n\n System calls are slow (compared to library calls). We should reserve a large amount of memory and only occasionally ask for more from the system.\n\n No reuse of freed memory. Our program never re-uses heap memory - it just keeps asking for a bigger heap.\n\n\nIf this allocator was used in a typical program, the process would quickly exhaust all available memory.\nInstead we need an allocator that can efficiently use heap space and only ask for more memory when necessary.\n\n\nWhat are placement strategies?\n\n\nDuring program execution memory is allocated and de-allocated (freed), so there will be gaps (holes) in the heap memory that can be re-used for future memory requests. The memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available.\n\n\nSuppose our current heap size is 64K, though not all of it is in use because some earlier malloc'd memory has already been freed by the program: \n\n\n\n\n\n\n\n\n16KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n30KB free\n\n\n4KB allocated\n\n\n2KB free\n\n\n\n\n\n\n\n\n\n\nIf a new malloc request for 2KB is executed (\nmalloc(2048)\n), where should \nmalloc\n reserve the memory? It could use the last 2KB hole (which happens to be the perfect size!) or it could split one of the other two free holes. These choices represent different placement strategies.\n\n\nWhichever hole is chosen, the allocator will need to split the hole into two: The newly allocated space (which will be returned to the program) and a smaller hole (if there is spare space left over).\n\n\nA perfect-fit strategy finds the smallest hole that is of sufficient size (at least 2KB):\n\n\n\n\n\n\n\n\n16KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n30KB free\n\n\n4KB allocated\n\n\n2KB HERE!\n\n\n\n\n\n\n\n\n\n\n\n\nA worst-fit strategy finds the largest hole that is of sufficient size (so break the 30KB hole into two):\n\n\n\n\n\n\n\n\n16KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n2KB HERE!\n\n\n28KB free\n\n\n4KB allocated\n\n\n2KB free\n\n\n\n\n\n\n\n\n\n\nA first-fit strategy finds the first available hole that is of sufficient size (break the 16KB hole into two):\n\n\n\n\n\n\n\n\n2KB HERE!\n\n\n14KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n30KB free\n\n\n4KB allocated\n\n\n2KB free\n\n\n\n\n\n\n\n\n\n\nWhat is fragmentation?\n\n\nIn the example below, of the 64KB of heap memory, 17KB is allocated, and 47KB is free. However the largest available block is only 30KB because our available unallocated heap memory is fragmented into smaller pieces. \n\n\n\n\n\n\n\n\n16KB free\n\n\n10KB allocated\n\n\n1KB free\n\n\n1KB allocated\n\n\n30KB free\n\n\n4KB allocated\n\n\n2KB free\n\n\n\n\n\n\n\n\n\n\nWhat effect do placement strategies have on fragmentation and performance?\n\n\nDifferent strategies affect the fragmentation of heap memory in non-obvious ways, which only are discovered by mathematical analysis or careful simulations under real-world conditions (for example simulating the memory allocation requests of a database or webserver).\nFor example, best-fit at first glance appears to be an excellent strategy however, if we can not find a perfectly-sized hole then this placement creates many tiny unusable holes, leading to high fragmentation. It also requires a scan of all possible holes.\n\n\nFirst fit has the advantage that it will not evaluate all possible placements and therefore be faster. \n\n\nSince Worst-fit targets the largest unallocated space, it is a poor choice if large allocations are required.\n\n\nIn practice first-fit and next-fit (which is not discussed here) are often common placement strategy. Hybrid approaches and many other alternatives exist (see implementing a memory allocator page).\n\n\nWhat are the challenges of writing a heap allocator?\n\n\nThe main challenges are,\n\n Need to minimize fragmentation (i.e. maximize memory utilization)\n\n Need high performance\n* Fiddly implementation (lots of pointer manipulation using linked lists and pointer arithmetic)\n\n\nSome additional comments:\n\n\nBoth fragmentation and performance depend on the application allocation profile, which can be evaluated but not predicted and in practice, under-specific usage conditions, a special-purpose allocator can often out-perform a general purpose implementation.\n\n\nThe allocator doesn't know the program's memory allocation requests in advance. Even if we did, this is the \nKnapsack problem\n which is known to be NP hard!\n\n\nHow do you implement a memory allocator?\n\n\nGood question. \nImplementing a memory allocator", 
            "title": "Memory, Part 1: Heap Memory Introduction"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#what-happens-when-i-call-malloc", 
            "text": "The function  malloc  is a C library call and is used to reserve a contiguous block of memory. Unlike stack memory, the memory remains allocated until  free  is called with the same pointer. There is also  calloc  and  realloc  which are discussed below.", 
            "title": "What happens when I call malloc?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#can-malloc-fail", 
            "text": "If  malloc  fails to reserve any more memory then it returns  NULL . Robust programs should check the return value. If your code assumes  malloc  succeeds and it does not, then your program will likely crash (segfault) when it tries to write to address 0.", 
            "title": "Can malloc fail?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#where-is-the-heap-and-how-big-is-it", 
            "text": "The heap is part of the process memory and it does not have a fixed size. Heap memory allocation is performed by the C library when you call  malloc  ( calloc ,  realloc ) and  free .  First a quick review on process memory: A process is a running instance of your program. Each process has its own address space. For example on a 32 bit machine your process gets about 4 billion addresses to play with, however not all of these are valid or even mapped to actual physical memory (RAM). Inside the process's memory you will find the executable code, space for the stack, environment variables, global (static) variables and the heap.  By calling  sbrk  the C library can increase the size of the heap as your program demands more heap memory. As the heap and stack (one for each thread) need to grow, we put them at opposite ends of the address space. So for typical architectures the heap will grow upwards and the stack grows downwards.   Truthiness: Modern operating system memory allocators no longer need  sbrk  - instead they can request independent regions of virtual memory and maintain multiple memory regions. For example gigabyte requests may be placed in a different memory region than small allocation requests. However this detail is an unwanted complexity: The problems of fragmentation and allocating memory efficiently still apply, so we will ignore this implementation nicety here and will write as if the heap is a single region.  If we write a multi-threaded program (more about that later) we will need multiple stacks (one per thread) but there's only ever one heap.  On typical architectures, the heap is part of the  Data segment  and starts just above the code and global variables.", 
            "title": "Where is the heap and how big is it?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#do-programs-need-to-call-brk-or-sbrk", 
            "text": "Not typically (though calling  sbrk(0)  can be interesting because it tells you where your heap currently ends). Instead programs use  malloc,calloc,realloc  and  free  which are part of the C library. The internal implementation of these functions will call  sbrk  when additional heap memory is required.  void *top_of_heap = sbrk(0);\nmalloc(16384);\nvoid *top_of_heap2 = sbrk(0);\nprintf( The top of heap went from %p to %p \\n , top_of_heap, top_of_heap2);  Example output:  The top of heap went from 0x4000 to 0xa000", 
            "title": "Do programs need to call brk or sbrk?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#what-is-calloc", 
            "text": "Unlike  malloc ,  calloc  initializes memory contents to zero and also takes two arguments (the number of items and the size in bytes of each item). A naive but readable implementation of  calloc  looks like this:  void *calloc(size_t n, size_t size)\n{\n    size_t total = n * size; // Does not check for overflow!\n    void *result = malloc(total);\n\n    if (!result) return NULL;\n\n// If we're using new memory pages \n// just allocated from the system by calling sbrk\n// then they will be zero so zero-ing out is unnecessary,\n\n    memset(result, 0, total);\n    return result; \n}  An advanced discussion of these limitations is  here .  Programmers often use  calloc  rather than explicitly calling  memset  after  malloc , to set the memory contents to zero. Note  calloc(x,y)  is identical to  calloc(y,x) , but you should follow the conventions of the manual.  // Ensure our memory is initialized to zero\nlink_t *link  = malloc(256);\nmemset(link, 0, 256); // Assumes malloc returned a valid address!\n\nlink_t *link = calloc(1, 256); // safer: calloc(1, sizeof(link_t));", 
            "title": "What is calloc?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#why-is-the-memory-that-is-first-returned-by-sbrk-initialized-to-zero", 
            "text": "If the operating system did not zero out contents of physical RAM it might be possible for one process to learn about the memory of another process that had previously used the memory. This would be a security leak.  Unfortunately this means that for  malloc  requests before any memory has been freed and simple programs (which end up using newly reserved memory from the system) the memory is  often  zero. Then programmers mistaken write C programs that assume malloc'd memory will  always  be zero.  char* ptr = malloc(300);\n// contents is probably zero because we get brand new memory\n// so beginner programs appear to work!\n// strcpy(ptr,  Some data ); // work with the data\nfree(ptr);\n// later\nchar *ptr2 = malloc(308); // Contents might now contain existing data and is probably not zero", 
            "title": "Why is the memory that is first returned by sbrk initialized to zero?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#why-doesnt-malloc-always-initialize-memory-to-zero", 
            "text": "Performance! We want malloc to be as fast as possible. Zeroing out memory may be unnecessary.", 
            "title": "Why doesn't malloc always initialize memory to zero?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#what-is-realloc-and-when-would-you-use-it", 
            "text": "realloc  allows you to resize an existing memory allocation that was previously allocated on the heap (via malloc,calloc or realloc). The most common use of realloc is to resize memory used to hold an array of values. A naive but readable version of realloc is suggested below  void * realloc(void * ptr, size_t newsize) {\n  // Simple implementation always reserves more memory\n  // and has no error checking\n  void *result = malloc(newsize); \n  size_t oldsize =  ... //(depends on allocator's internal data structure)\n  if (ptr) memcpy(result, ptr, newsize   oldsize ? newsize : oldsize);\n  free(ptr);\n  return result;\n}  An INCORRECT use of realloc is shown below:  int *array = malloc(sizeof(int) * 2);\narray[0] = 10; array[1]; = 20;\n// Ooops need a bigger array - so use realloc..\nrealloc (array, 3); // ERRORS!\narray[2] = 30;   The above code contains two mistakes. Firstly we needed 3*sizeof(int) bytes not 3 bytes.\nSecondly realloc may need to move the existing contents of the memory to a new location. For example, there may not be sufficient space because the neighboring bytes are already allocated. A correct use of realloc is shown below.  array = realloc(array, 3 * sizeof(int));\n// If array is copied to a new location then old allocation will be freed.  A robust version would also check for a  NULL  return value. Note  realloc  can be used to grow and shrink allocations.", 
            "title": "What is realloc and when would you use it?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#where-can-i-read-more", 
            "text": "See  the man page !", 
            "title": "Where can I read more?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#how-important-is-that-memory-allocation-is-fast", 
            "text": "Very! Allocating and de-allocating heap memory is a common operation in most applications.", 
            "title": "How important is that memory allocation is fast?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#what-is-the-silliest-malloc-and-free-implementation-and-what-is-wrong-with-it", 
            "text": "void* malloc(size_t size)\n// Ask the system for more bytes by extending the heap space. \n// sbrk Returns -1 on failure\n   void *p = sbrk(size); \n   if(p == (void *) -1) return NULL; // No space left\n   return p;\n}\nvoid free() {/* Do nothing */}  The above implementation suffers from two major drawbacks:  System calls are slow (compared to library calls). We should reserve a large amount of memory and only occasionally ask for more from the system.  No reuse of freed memory. Our program never re-uses heap memory - it just keeps asking for a bigger heap.  If this allocator was used in a typical program, the process would quickly exhaust all available memory.\nInstead we need an allocator that can efficiently use heap space and only ask for more memory when necessary.", 
            "title": "What is the silliest malloc and free implementation and what is wrong with it?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#what-are-placement-strategies", 
            "text": "During program execution memory is allocated and de-allocated (freed), so there will be gaps (holes) in the heap memory that can be re-used for future memory requests. The memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available.  Suppose our current heap size is 64K, though not all of it is in use because some earlier malloc'd memory has already been freed by the program:      16KB free  10KB allocated  1KB free  1KB allocated  30KB free  4KB allocated  2KB free      If a new malloc request for 2KB is executed ( malloc(2048) ), where should  malloc  reserve the memory? It could use the last 2KB hole (which happens to be the perfect size!) or it could split one of the other two free holes. These choices represent different placement strategies.  Whichever hole is chosen, the allocator will need to split the hole into two: The newly allocated space (which will be returned to the program) and a smaller hole (if there is spare space left over).  A perfect-fit strategy finds the smallest hole that is of sufficient size (at least 2KB):     16KB free  10KB allocated  1KB free  1KB allocated  30KB free  4KB allocated  2KB HERE!       A worst-fit strategy finds the largest hole that is of sufficient size (so break the 30KB hole into two):     16KB free  10KB allocated  1KB free  1KB allocated  2KB HERE!  28KB free  4KB allocated  2KB free      A first-fit strategy finds the first available hole that is of sufficient size (break the 16KB hole into two):     2KB HERE!  14KB free  10KB allocated  1KB free  1KB allocated  30KB free  4KB allocated  2KB free", 
            "title": "What are placement strategies?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#what-is-fragmentation", 
            "text": "In the example below, of the 64KB of heap memory, 17KB is allocated, and 47KB is free. However the largest available block is only 30KB because our available unallocated heap memory is fragmented into smaller pieces.      16KB free  10KB allocated  1KB free  1KB allocated  30KB free  4KB allocated  2KB free", 
            "title": "What is fragmentation?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#what-effect-do-placement-strategies-have-on-fragmentation-and-performance", 
            "text": "Different strategies affect the fragmentation of heap memory in non-obvious ways, which only are discovered by mathematical analysis or careful simulations under real-world conditions (for example simulating the memory allocation requests of a database or webserver).\nFor example, best-fit at first glance appears to be an excellent strategy however, if we can not find a perfectly-sized hole then this placement creates many tiny unusable holes, leading to high fragmentation. It also requires a scan of all possible holes.  First fit has the advantage that it will not evaluate all possible placements and therefore be faster.   Since Worst-fit targets the largest unallocated space, it is a poor choice if large allocations are required.  In practice first-fit and next-fit (which is not discussed here) are often common placement strategy. Hybrid approaches and many other alternatives exist (see implementing a memory allocator page).", 
            "title": "What effect do placement strategies have on fragmentation and performance?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#what-are-the-challenges-of-writing-a-heap-allocator", 
            "text": "The main challenges are,  Need to minimize fragmentation (i.e. maximize memory utilization)  Need high performance\n* Fiddly implementation (lots of pointer manipulation using linked lists and pointer arithmetic)  Some additional comments:  Both fragmentation and performance depend on the application allocation profile, which can be evaluated but not predicted and in practice, under-specific usage conditions, a special-purpose allocator can often out-perform a general purpose implementation.  The allocator doesn't know the program's memory allocation requests in advance. Even if we did, this is the  Knapsack problem  which is known to be NP hard!", 
            "title": "What are the challenges of writing a heap allocator?"
        }, 
        {
            "location": "/Memory,-Part-1:-Heap-Memory-Introduction/#how-do-you-implement-a-memory-allocator", 
            "text": "Good question.  Implementing a memory allocator", 
            "title": "How do you implement a memory allocator?"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/", 
            "text": "A memory allocator needs to keep track of which bytes are currently allocated and which are available for use. This page introduces the implementation and conceptual details of building an allocator, i.e. the actual code that implements \nmalloc\n and \nfree\n.\n\n\nThis page talks about links of blocks - do I malloc memory for them instead?\n\n\nThough conceptually we are thinking about creating linked lists and lists of blocks, we don't need to \"malloc memory\" to create them! Instead we are writing integers and pointers into memory that we already control so that we can later consistently hop from one address to the next. This internal information represents some overhead. So even if we had requested 1024 KB of contiguous memory from the system, we will not be able to provide all of it to the running program.\n\n\nThinking in blocks\n\n\nWe can think of our heap memory as a list of blocks where each block is either allocated or unallocated.\nRather than storing an explicit list of pointers we store information about the block's size \nas part of the block\n. Thus there is conceptually a list of free blocks, but it is implicit, i.e. in the form of block size information that we store as part of each block.\n\n\nWe could navigate from one block to the next block just by adding the block's size. For example if you have a pointer \np\n that points to the start of a block, then \nnext_block\n  with be at \n((char *)p) +  *(size_t *) p\n, if you are storing the size of the blocks in bytes. The cast to \nchar *\n ensures that pointer arithmetic is calculated in bytes. The cast to \nsize_t *\n ensures the memory at \np\n is read as a size value and would be necessarily if \np\n was a \nvoid *\n or \nchar *\n type.\n\n\nThe calling program never sees these values; they are internal to the implementation of the memory allocator. \n\n\nAs an example, suppose your allocator is asked to reserve 80 bytes (\nmalloc(80)\n) and requires 8 bytes of internal header data. The allocator would need to find an unallocated space of at least 88 bytes. After updating the heap data it would return a pointer to the block. However, the returned pointer does not point to the start of the block because that's where the internal size data is stored! Instead we would return the start of the block + 8 bytes.\nIn the implementation, remember that pointer arithmetic depends on type. For example, \np += 8\n adds \n8 * sizeof(p)\n, not necessarily 8 bytes!\n\n\nImplementing malloc\n\n\nThe simplest implementation uses first fit: Start at the first block, assuming it exists, and iterate until a block that represents unallocated space of sufficient size is found, or we've checked all the blocks.\n\n\nIf no suitable block is found then it's time to call \nsbrk()\n again to sufficiently extend the size of the heap. A fast implementation might extend it a significant amount so that we will not need to request more heap memory in the near future.\n\n\nWhen a free block is found it may be larger than the space we need. If so, we will create two entries in our implicit list. The first entry is the allocated block, the second entry is the remaining space.\n\n\nThere are two simple ways to note if a block is in use or available. The first is to store it as a byte in the header information along with the size and the second to encode it as the lowest bit in the size!\nThus block size information would be limited to only even values:\n\n\n// Assumes p is a reasonable pointer type, e.g. 'size_t *'.\nisallocated = (*p) \n 1;\nrealsize = (*p) \n ~1;  // mask out the lowest bit\n\n\n\n\nAlignment and rounding up considerations\n\n\nMany architectures expect multi-byte primitives to be aligned to some multiple of 2^n. For example, it's common to require 4-byte types to be aligned to 4-byte boundaries (and 8-byte types on 8-byte boundaries). If multi-byte primitives are not stored on a reasonable boundary (for example starting at an odd address) then the performance can be significantly impacted because it may require two memory read requests instead of one. On some architectures the penalty is even greater - the program will crash with a \nbus error\n.\n\n\nAs \nmalloc\n does not know how the user will use the allocated memory (array of doubles? array of chars?), the pointer returned to the program needs to be aligned for the worst case, which is architecture dependent.\n\n\nFrom glibc documentation, the glibc \nmalloc\n uses the following heuristic:\n\"    The block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. On GNU systems, the address is always a multiple of eight on most systems, and a multiple of 16 on 64-bit systems.\"\n\n\nFor example, if you need to calculate how many 16 byte units are required, don't forget to round up -\n\n\nint s = (requested_bytes + tag_overhead_bytes + 15) / 16\n\n\n\n\nThe additional constant ensures incomplete units are rounded up. Note, real code is more likely to symbol sizes e.g. \nsizeof(x) - 1\n, rather than coding numerical constant 15.\n\n\nImplementing free\n\n\nWhen \nfree\n is called we need to re-apply the offset to get back to the 'real' start of the block (remember we didn't give the user a pointer to the actual start of the block?), i.e. to where we stored the size information.\n\n\nA naive implementation would simply mark the block as unused. If we are storing the block allocation status in the lowest size bit, then we just need to clear the bit:\n\n\n*p = (*p) \n ~1; // Clear lowest bit \n\n\n\n\nHowever, we have a bit more work to do: If the current block and the next block (if it exists) are both free we need to coalesce these blocks into a single block.\nSimilarly, we also need to check the previous block, too. If that exists and represents an unallocated memory, then we need to coalesce the blocks into a single large block.\n\n\nTo be able to coalesce a free block with a previous free block we will also need to find the previous block, so we store the block's size at the end of the block, too. These are called \"boundary tags\" (ref Knuth73). As the blocks are contiguous, the end of one blocks sits right next to the start of the next block. So the current block (apart from the first one) can look a few bytes further back to lookup the size of the previous block. With this information you can now jump backwards!\n\n\nhttp://www.eecs.harvard.edu/~mdw/course/cs61/mediawiki/images/5/53/Lectures-malloc1.pdf \nSlide 29\n(Hey world - an open source licensed diagram of the above would be nice)\n-Not sure if the above link is open source or licensed. Confirmation pending.\n\n\nPerformance\n\n\nWith the above description it's possible to build a memory allocator. It's main advantage is simplicity - at least simple compared to other allocators!\nAllocating memory is a worst-case linear time operation (search linked lists for a sufficiently large free block) and de-allocation is constant time (no more than 3 blocks will need to be coalesced into a single block). Using this allocator it is possible to experiment with different placement strategies. For example, you could start searching from where you last free'd a block, or where you last allocated from. If you do store pointers to blocks, you need to be very careful that they always remain valid (e.g. when coalescing blocks or other malloc or free calls that change the heap structure)\n\n\nExplicit Free Lists Allocators\n\n\nBetter performance can be achieved by implementing an explicit doubly-linked list of free nodes. In that case, we can immediately traverse to the next free block and the previous free block. This can halve the search time, because the linked list only includes unallocated blocks.\n\n\nA second advantage is that we now have some control over the ordering of the linked list. For example, when a block is free'd, we could choose to insert it into the beginning of the linked list rather than always between its neighbors. This is discussed below.\n\n\nWhere do we store the pointers of our linked list? A simple trick is to realize that the block itself is not being used and store the next and previous pointers as part of the block (though now you have to ensure that the free blocks are always sufficiently large to hold two pointers).\n\n\nWe still need to implement Boundary Tags (i.e. an implicit list using sizes), so that we can correctly free blocks and coalesce them with their two neighbors. Consequently, explicit free lists require more code and complexity.\n\n\nWith explicit linked lists a fast and simple 'Find-First' algorithm is used to find the first sufficiently large link. However, since the link order can be modified, this corresponds to different placement strategies. For example if the links are maintained from largest to smallest, then this produces a 'Worst-Fit' placement strategy.\n\n\nExplicit linked list insertion policy\n\n\nThe newly free'd block can be inserted easily into two possible positions: at the beginning or in address order (by using the boundary tags to first find the neighbors).\n\n\nInserting at the beginning creates a LIFO (last-in, first-out) policy: The most recently free'd spaces will be reused. Studies suggest fragmentation is worse than using address order.\n\n\nInserting in address order (\"Address ordered policy\") inserts free'd blocks so that the blocks are visited in increasing address order. This policy required more time to free a block because the boundary tags (size data) must be used to find the next and previous unallocated blocks. However, there is less fragmentation.\n\n\nCase study: Buddy Allocator (an example of a segregated list)\n\n\nA segregated allocator is one that divides the heap into different areas that are handled by different sub-allocators dependent on the size of the allocation request. Sizes are grouped into classes (e.g. powers of two) and each size is handled by a different sub-allocator and each size maintains its own free list.\n\n\nA well known allocator of this type is the buddy allocator. We'll discuss the binary buddy allocator which splits allocation into blocks of size 2^n (n = 1, 2, 3, ...) times some base unit number of bytes, but others also exist (e.g. Fibonacci split - can you see why it's named?). The basic concept is simple: If there are no free blocks of size 2^n, go to the next level and steal that block and split it into two. If two neighboring blocks of the same size become unallocated, they can be coalesced back together into a single large block of twice the size.\n\n\nBuddy allocators are fast because the neighboring blocks to coalesce with can be calculated from the free'd block's address, rather than traversing the size tags. Ultimate performance often requires a small amount of assembler code to use a specialized CPU instruction to find the lowest non-zero bit. \n\n\nThe main disadvantage of the Buddy allocator is that they suffer from internal fragmentation, because allocations are rounded up to the nearest block size. For example, a 68-byte allocation will require a 128-byte block.\n\n\nFurther Reading and References\n\n\n\n\nSee \nFoundations of Software Technology and Theoretical Computer Science 1999 proceedings\n (Google books,page 85)\n\n\nThanksForTheMemory UIUC lecture Slides (\npptx\n) (\npdf\n)\nand \n\n\nWikipedia's buddy memory allocation page\n\n\n\n\nOther allocators\n\n\nThere are many other allocation schemes. For example \nSLUB\n (wikipedia) - one of three allocators used internally by the Linux Kernel.", 
            "title": "Memory, Part 2: Implementing a Memory Allocator"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#this-page-talks-about-links-of-blocks-do-i-malloc-memory-for-them-instead", 
            "text": "Though conceptually we are thinking about creating linked lists and lists of blocks, we don't need to \"malloc memory\" to create them! Instead we are writing integers and pointers into memory that we already control so that we can later consistently hop from one address to the next. This internal information represents some overhead. So even if we had requested 1024 KB of contiguous memory from the system, we will not be able to provide all of it to the running program.", 
            "title": "This page talks about links of blocks - do I malloc memory for them instead?"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#thinking-in-blocks", 
            "text": "We can think of our heap memory as a list of blocks where each block is either allocated or unallocated.\nRather than storing an explicit list of pointers we store information about the block's size  as part of the block . Thus there is conceptually a list of free blocks, but it is implicit, i.e. in the form of block size information that we store as part of each block.  We could navigate from one block to the next block just by adding the block's size. For example if you have a pointer  p  that points to the start of a block, then  next_block   with be at  ((char *)p) +  *(size_t *) p , if you are storing the size of the blocks in bytes. The cast to  char *  ensures that pointer arithmetic is calculated in bytes. The cast to  size_t *  ensures the memory at  p  is read as a size value and would be necessarily if  p  was a  void *  or  char *  type.  The calling program never sees these values; they are internal to the implementation of the memory allocator.   As an example, suppose your allocator is asked to reserve 80 bytes ( malloc(80) ) and requires 8 bytes of internal header data. The allocator would need to find an unallocated space of at least 88 bytes. After updating the heap data it would return a pointer to the block. However, the returned pointer does not point to the start of the block because that's where the internal size data is stored! Instead we would return the start of the block + 8 bytes.\nIn the implementation, remember that pointer arithmetic depends on type. For example,  p += 8  adds  8 * sizeof(p) , not necessarily 8 bytes!", 
            "title": "Thinking in blocks"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#implementing-malloc", 
            "text": "The simplest implementation uses first fit: Start at the first block, assuming it exists, and iterate until a block that represents unallocated space of sufficient size is found, or we've checked all the blocks.  If no suitable block is found then it's time to call  sbrk()  again to sufficiently extend the size of the heap. A fast implementation might extend it a significant amount so that we will not need to request more heap memory in the near future.  When a free block is found it may be larger than the space we need. If so, we will create two entries in our implicit list. The first entry is the allocated block, the second entry is the remaining space.  There are two simple ways to note if a block is in use or available. The first is to store it as a byte in the header information along with the size and the second to encode it as the lowest bit in the size!\nThus block size information would be limited to only even values:  // Assumes p is a reasonable pointer type, e.g. 'size_t *'.\nisallocated = (*p)   1;\nrealsize = (*p)   ~1;  // mask out the lowest bit", 
            "title": "Implementing malloc"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#alignment-and-rounding-up-considerations", 
            "text": "Many architectures expect multi-byte primitives to be aligned to some multiple of 2^n. For example, it's common to require 4-byte types to be aligned to 4-byte boundaries (and 8-byte types on 8-byte boundaries). If multi-byte primitives are not stored on a reasonable boundary (for example starting at an odd address) then the performance can be significantly impacted because it may require two memory read requests instead of one. On some architectures the penalty is even greater - the program will crash with a  bus error .  As  malloc  does not know how the user will use the allocated memory (array of doubles? array of chars?), the pointer returned to the program needs to be aligned for the worst case, which is architecture dependent.  From glibc documentation, the glibc  malloc  uses the following heuristic:\n\"    The block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. On GNU systems, the address is always a multiple of eight on most systems, and a multiple of 16 on 64-bit systems.\"  For example, if you need to calculate how many 16 byte units are required, don't forget to round up -  int s = (requested_bytes + tag_overhead_bytes + 15) / 16  The additional constant ensures incomplete units are rounded up. Note, real code is more likely to symbol sizes e.g.  sizeof(x) - 1 , rather than coding numerical constant 15.", 
            "title": "Alignment and rounding up considerations"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#implementing-free", 
            "text": "When  free  is called we need to re-apply the offset to get back to the 'real' start of the block (remember we didn't give the user a pointer to the actual start of the block?), i.e. to where we stored the size information.  A naive implementation would simply mark the block as unused. If we are storing the block allocation status in the lowest size bit, then we just need to clear the bit:  *p = (*p)   ~1; // Clear lowest bit   However, we have a bit more work to do: If the current block and the next block (if it exists) are both free we need to coalesce these blocks into a single block.\nSimilarly, we also need to check the previous block, too. If that exists and represents an unallocated memory, then we need to coalesce the blocks into a single large block.  To be able to coalesce a free block with a previous free block we will also need to find the previous block, so we store the block's size at the end of the block, too. These are called \"boundary tags\" (ref Knuth73). As the blocks are contiguous, the end of one blocks sits right next to the start of the next block. So the current block (apart from the first one) can look a few bytes further back to lookup the size of the previous block. With this information you can now jump backwards!  http://www.eecs.harvard.edu/~mdw/course/cs61/mediawiki/images/5/53/Lectures-malloc1.pdf \nSlide 29\n(Hey world - an open source licensed diagram of the above would be nice)\n-Not sure if the above link is open source or licensed. Confirmation pending.", 
            "title": "Implementing free"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#performance", 
            "text": "With the above description it's possible to build a memory allocator. It's main advantage is simplicity - at least simple compared to other allocators!\nAllocating memory is a worst-case linear time operation (search linked lists for a sufficiently large free block) and de-allocation is constant time (no more than 3 blocks will need to be coalesced into a single block). Using this allocator it is possible to experiment with different placement strategies. For example, you could start searching from where you last free'd a block, or where you last allocated from. If you do store pointers to blocks, you need to be very careful that they always remain valid (e.g. when coalescing blocks or other malloc or free calls that change the heap structure)", 
            "title": "Performance"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#explicit-free-lists-allocators", 
            "text": "Better performance can be achieved by implementing an explicit doubly-linked list of free nodes. In that case, we can immediately traverse to the next free block and the previous free block. This can halve the search time, because the linked list only includes unallocated blocks.  A second advantage is that we now have some control over the ordering of the linked list. For example, when a block is free'd, we could choose to insert it into the beginning of the linked list rather than always between its neighbors. This is discussed below.  Where do we store the pointers of our linked list? A simple trick is to realize that the block itself is not being used and store the next and previous pointers as part of the block (though now you have to ensure that the free blocks are always sufficiently large to hold two pointers).  We still need to implement Boundary Tags (i.e. an implicit list using sizes), so that we can correctly free blocks and coalesce them with their two neighbors. Consequently, explicit free lists require more code and complexity.  With explicit linked lists a fast and simple 'Find-First' algorithm is used to find the first sufficiently large link. However, since the link order can be modified, this corresponds to different placement strategies. For example if the links are maintained from largest to smallest, then this produces a 'Worst-Fit' placement strategy.", 
            "title": "Explicit Free Lists Allocators"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#explicit-linked-list-insertion-policy", 
            "text": "The newly free'd block can be inserted easily into two possible positions: at the beginning or in address order (by using the boundary tags to first find the neighbors).  Inserting at the beginning creates a LIFO (last-in, first-out) policy: The most recently free'd spaces will be reused. Studies suggest fragmentation is worse than using address order.  Inserting in address order (\"Address ordered policy\") inserts free'd blocks so that the blocks are visited in increasing address order. This policy required more time to free a block because the boundary tags (size data) must be used to find the next and previous unallocated blocks. However, there is less fragmentation.", 
            "title": "Explicit linked list insertion policy"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#case-study-buddy-allocator-an-example-of-a-segregated-list", 
            "text": "A segregated allocator is one that divides the heap into different areas that are handled by different sub-allocators dependent on the size of the allocation request. Sizes are grouped into classes (e.g. powers of two) and each size is handled by a different sub-allocator and each size maintains its own free list.  A well known allocator of this type is the buddy allocator. We'll discuss the binary buddy allocator which splits allocation into blocks of size 2^n (n = 1, 2, 3, ...) times some base unit number of bytes, but others also exist (e.g. Fibonacci split - can you see why it's named?). The basic concept is simple: If there are no free blocks of size 2^n, go to the next level and steal that block and split it into two. If two neighboring blocks of the same size become unallocated, they can be coalesced back together into a single large block of twice the size.  Buddy allocators are fast because the neighboring blocks to coalesce with can be calculated from the free'd block's address, rather than traversing the size tags. Ultimate performance often requires a small amount of assembler code to use a specialized CPU instruction to find the lowest non-zero bit.   The main disadvantage of the Buddy allocator is that they suffer from internal fragmentation, because allocations are rounded up to the nearest block size. For example, a 68-byte allocation will require a 128-byte block.", 
            "title": "Case study: Buddy Allocator (an example of a segregated list)"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#further-reading-and-references", 
            "text": "See  Foundations of Software Technology and Theoretical Computer Science 1999 proceedings  (Google books,page 85)  ThanksForTheMemory UIUC lecture Slides ( pptx ) ( pdf )\nand   Wikipedia's buddy memory allocation page", 
            "title": "Further Reading and References"
        }, 
        {
            "location": "/Memory,-Part-2:-Implementing-a-Memory-Allocator/#other-allocators", 
            "text": "There are many other allocation schemes. For example  SLUB  (wikipedia) - one of three allocators used internally by the Linux Kernel.", 
            "title": "Other allocators"
        }, 
        {
            "location": "/Memory,-Part-3:-Smashing-the-Stack-Example/", 
            "text": "Each thread uses a stack memory. The stack 'grows downwards' - if a function calls another function, then the stack is extended to smaller memory addresses.\nStack memory includes non-static automatic (temporary) variables, parameter values and the return address.\nIf a buffer is too small some data (e.g. input values from the user), then there is a real possibility that other stack variables and even the return address will be overwritten.\nThe precise layout of the stack's contents and order of the automatic variables is architecture and compiler dependent. However with a little investigative work we can learn how to deliberately smash the stack for a particular architecture.\n\n\nThe example below demonstrates how the return address is stored on the stack. For a particular 32 bit architecture \nLive Linux Machine\n, we determine that the return address is stored at an address two pointers (8 bytes) above the address of the automatic variable. The code deliberately changes the stack value so that when the input function returns, rather than continuing on inside the main method, it jumps to the exploit function instead.\n\n\n// Overwrites the return address on the following machine:\n// http://cs-education.github.io/sys/\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nunistd.h\n\n\nvoid breakout() {\n    puts(\nWelcome. Have a shell...\n);\n    system(\n/bin/sh\n);\n}\nvoid input() {\n  void *p;\n  printf(\nAddress of stack variable: %p\\n\n, \np);\n  printf(\nSomething that looks like a return address on stack: %p\\n\n, *((\np)+2));\n  // Let's change it to point to the start of our sneaky function.\n  *((\np)+2) = breakout;\n}\nint main() {\n    printf(\nmain() code starts at %p\\n\n,main);\n\n    input();\n    while (1) {\n        puts(\nHello\n);\n        sleep(1);\n    }\n\n    return 0;\n}", 
            "title": "Memory, Part 3: Smashing the Stack Example"
        }, 
        {
            "location": "/Memory:-Review-Questions/", 
            "text": "Question numbers subject to change\n\n\n\n\nQ1\n\n\nWhat are the following and what is their purpose?\n\n Translation Lookaside Buffer\n\n Physical Address\n\n Memory Management Unit\n\n The dirty bit\n\n\nQ2\n\n\nHow do you determine how many bits are used in the page offset?\n\n\nQ3\n\n\n20 ms after a context switch the TLB contains all logical addresses used by your numerical code which performs main memory access 100% of the time. What is the overhead (slowdown) of a two-level page table compared to a single-level page table?\n\n\nQ4\n\n\nExplain why the TLB must be flushed when a context switch occurs (i.e. the CPU is assigned to work on a different process).", 
            "title": "Memory: Review Questions"
        }, 
        {
            "location": "/Memory:-Review-Questions/#q1", 
            "text": "What are the following and what is their purpose?  Translation Lookaside Buffer  Physical Address  Memory Management Unit  The dirty bit", 
            "title": "Q1"
        }, 
        {
            "location": "/Memory:-Review-Questions/#q2", 
            "text": "How do you determine how many bits are used in the page offset?", 
            "title": "Q2"
        }, 
        {
            "location": "/Memory:-Review-Questions/#q3", 
            "text": "20 ms after a context switch the TLB contains all logical addresses used by your numerical code which performs main memory access 100% of the time. What is the overhead (slowdown) of a two-level page table compared to a single-level page table?", 
            "title": "Q3"
        }, 
        {
            "location": "/Memory:-Review-Questions/#q4", 
            "text": "Explain why the TLB must be flushed when a context switch occurs (i.e. the CPU is assigned to work on a different process).", 
            "title": "Q4"
        }, 
        {
            "location": "/Multi-threaded-Programming:-Review-Questions/", 
            "text": "Warning - question numbers subject to change\n\n\n\n\nQ1\n\n\nIs the following code thread-safe? Redesign the following code to be thread-safe. Hint: A mutex is unnecessary if the message memory is unique to each call.\n\n\nstatic char message[20];\npthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZAER;\n\nvoid format(int v) {\n  pthread_mutex_lock(\nmutex);\n  sprintf(message,\n:%d:\n,v);\n  pthread_mutex_unlock(\nmutex);\n  return message;\n}\n\n\n\n\nQ2\n\n\nWhich one of the following does not cause a process to exit?\n\n Returning from the pthread's starting function in the last running thread.\n\n The original thread returning from main.\n\n Any thread causing a segmentation fault.\n\n Any thread calling \nexit\n.\n* Calling \npthread_exit\n in the main thread with other threads still running.\n\n\nQ3\n\n\nWrite a mathematical expression for the number of \"W\" characters that will be printed by the following program. Assume a,b,c,d are small positive integers. Your answer may use a 'min' function that returns its lowest valued argument.\n\n\nunsigned int a=...,b=...,c=...,d=...;\n\nvoid* func(void* ptr) {\n  char m = * (char*)ptr;\n  if(m == 'P') sem_post(s);\n  if(m == 'W') sem_wait(s);\n  putchar(m);\n  return NULL;\n}\n\nint main(int argv, char**argc) {\n  sem_init(s,0, a);\n  while(b--) pthread_create(\ntid,NULL,func,\nW\n); \n  while(c--) pthread_create(\ntid,NULL,func,\nP\n); \n  while(d--) pthread_create(\ntid,NULL,func,\nW\n); \n  pthread_exit(NULL); \n  /*Process will finish when all threads have exited */\n}\n\n\n\n\nQ4\n\n\nComplete the following code. The following code is supposed to print alternating \nA\n and \nB\n. It represents two threads that take turns to execute.  Add condition variable calls to \nfunc\n so that the waiting thread does not need to continually check the \nturn\n variable. Q: Is pthread_cond_broadcast necessary or is pthread_cond_signal sufficient?\n\n\npthread_cond_t cv = PTHREAD_COND_INITIALIZER;\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid* turn;\n\nvoid* func(void*mesg) {\n  while(1) {\n// Add mutex lock and condition variable calls ...\n\n    while(turn == mesg) { /* poll again ... Change me - This busy loop burns CPU time!*/ }\n\n    /* Do stuff on this thread*/\n    puts( (char*) mesg);\n    turn = mesg;\n\n  }\n  return 0;\n}\n\nint main(int argc, char**argv){\n  pthread_t tid1;\n  pthread_create(\ntid1,NULL, func, \nA\n);\n  func(\nB\n); // no need to create another thread - just use the main thread\n  return 0;\n}\n\n\n\n\nQ5\n\n\nIdentify the critical sections in the given code. Add mutex locking to make the code thread safe. Add condition variable calls so that \ntotal\n never becomes negative or above 1000. Instead the call should block until it is safe to proceed. Explain why \npthread_cond_broadcast\n is necessary.\n\n\nint total;\nvoid add(int value) {\n if(value) \n1) return;\n total += value;\n}\nvoid sub(int value) {\n if(value) \n1) return;\n total -= value;\n}\n\n\n\n\nQ6\n\n\nA non-threadsafe data structure has \nsize()\n \nenq\n and \ndeq\n methods. Use condition variable and mutex lock to complete the thread-safe, blocking versions.\n\n\nvoid enqueue(void* data) {\n  // should block if the size() would be become greater than 256\n  enq(data);\n}\nvoid* dequeue() {\n  // should block if size() is 0\n  return deq();\n}\n\n\n\n\nQ7\n\n\nYour startup offers path planning using latest traffic information. Your overpaid intern has created a non-threadsafe data structure with two functions: \nshortest\n (which uses but does not modify the graph) and \nset_edge\n (which modifies the graph).\n\n\ngraph_t* create_graph(char* filename); // called once\n\n// returns a new heap object that is the shortest path from vertex i to j\npath_t* shortest(graph_t* graph, int i, int j); \n\n// updates edge from vertex i to j\nvoid set_edge(graph_t* graph, int i, int j, double time); \n\n\n\n\n\nFor performance, multiple threads must be able to call \nshortest\n at the same time but the graph can only be modified by one thread when no threads other are executing inside \nshortest\n or \nset_edge\n.\n\n\nUse mutex lock and condition variables to implement a reader-writer solution. An incomplete attempt is shown below. Though this attempt is threadsafe (thus sufficient for demo day!), it does not allow multiple threads to calculate \nshortest\n path at the same time and will not have sufficient throughput.\n\n\npath_t* shortest_safe(graph_t* graph, int i, int j) {\n  pthread_mutex_lock(\nm);\n  path_t* path = shortest(graph, i, j);\n  pthread_mutex_unlock(\nm);\n  return path;\n}\nvoid set_edge_safe(graph_t* graph, int i, int j, double dist) {\n  pthread_mutex_lock(\nm);\n  set_edge(graph, i, j, dist);\n  pthread_mutex_unlock(\nm);\n}", 
            "title": "Multi threaded Programming: Review Questions"
        }, 
        {
            "location": "/Multi-threaded-Programming:-Review-Questions/#q1", 
            "text": "Is the following code thread-safe? Redesign the following code to be thread-safe. Hint: A mutex is unnecessary if the message memory is unique to each call.  static char message[20];\npthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZAER;\n\nvoid format(int v) {\n  pthread_mutex_lock( mutex);\n  sprintf(message, :%d: ,v);\n  pthread_mutex_unlock( mutex);\n  return message;\n}", 
            "title": "Q1"
        }, 
        {
            "location": "/Multi-threaded-Programming:-Review-Questions/#q2", 
            "text": "Which one of the following does not cause a process to exit?  Returning from the pthread's starting function in the last running thread.  The original thread returning from main.  Any thread causing a segmentation fault.  Any thread calling  exit .\n* Calling  pthread_exit  in the main thread with other threads still running.", 
            "title": "Q2"
        }, 
        {
            "location": "/Multi-threaded-Programming:-Review-Questions/#q3", 
            "text": "Write a mathematical expression for the number of \"W\" characters that will be printed by the following program. Assume a,b,c,d are small positive integers. Your answer may use a 'min' function that returns its lowest valued argument.  unsigned int a=...,b=...,c=...,d=...;\n\nvoid* func(void* ptr) {\n  char m = * (char*)ptr;\n  if(m == 'P') sem_post(s);\n  if(m == 'W') sem_wait(s);\n  putchar(m);\n  return NULL;\n}\n\nint main(int argv, char**argc) {\n  sem_init(s,0, a);\n  while(b--) pthread_create( tid,NULL,func, W ); \n  while(c--) pthread_create( tid,NULL,func, P ); \n  while(d--) pthread_create( tid,NULL,func, W ); \n  pthread_exit(NULL); \n  /*Process will finish when all threads have exited */\n}", 
            "title": "Q3"
        }, 
        {
            "location": "/Multi-threaded-Programming:-Review-Questions/#q4", 
            "text": "Complete the following code. The following code is supposed to print alternating  A  and  B . It represents two threads that take turns to execute.  Add condition variable calls to  func  so that the waiting thread does not need to continually check the  turn  variable. Q: Is pthread_cond_broadcast necessary or is pthread_cond_signal sufficient?  pthread_cond_t cv = PTHREAD_COND_INITIALIZER;\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid* turn;\n\nvoid* func(void*mesg) {\n  while(1) {\n// Add mutex lock and condition variable calls ...\n\n    while(turn == mesg) { /* poll again ... Change me - This busy loop burns CPU time!*/ }\n\n    /* Do stuff on this thread*/\n    puts( (char*) mesg);\n    turn = mesg;\n\n  }\n  return 0;\n}\n\nint main(int argc, char**argv){\n  pthread_t tid1;\n  pthread_create( tid1,NULL, func,  A );\n  func( B ); // no need to create another thread - just use the main thread\n  return 0;\n}", 
            "title": "Q4"
        }, 
        {
            "location": "/Multi-threaded-Programming:-Review-Questions/#q5", 
            "text": "Identify the critical sections in the given code. Add mutex locking to make the code thread safe. Add condition variable calls so that  total  never becomes negative or above 1000. Instead the call should block until it is safe to proceed. Explain why  pthread_cond_broadcast  is necessary.  int total;\nvoid add(int value) {\n if(value)  1) return;\n total += value;\n}\nvoid sub(int value) {\n if(value)  1) return;\n total -= value;\n}", 
            "title": "Q5"
        }, 
        {
            "location": "/Multi-threaded-Programming:-Review-Questions/#q6", 
            "text": "A non-threadsafe data structure has  size()   enq  and  deq  methods. Use condition variable and mutex lock to complete the thread-safe, blocking versions.  void enqueue(void* data) {\n  // should block if the size() would be become greater than 256\n  enq(data);\n}\nvoid* dequeue() {\n  // should block if size() is 0\n  return deq();\n}", 
            "title": "Q6"
        }, 
        {
            "location": "/Multi-threaded-Programming:-Review-Questions/#q7", 
            "text": "Your startup offers path planning using latest traffic information. Your overpaid intern has created a non-threadsafe data structure with two functions:  shortest  (which uses but does not modify the graph) and  set_edge  (which modifies the graph).  graph_t* create_graph(char* filename); // called once\n\n// returns a new heap object that is the shortest path from vertex i to j\npath_t* shortest(graph_t* graph, int i, int j); \n\n// updates edge from vertex i to j\nvoid set_edge(graph_t* graph, int i, int j, double time);   For performance, multiple threads must be able to call  shortest  at the same time but the graph can only be modified by one thread when no threads other are executing inside  shortest  or  set_edge .  Use mutex lock and condition variables to implement a reader-writer solution. An incomplete attempt is shown below. Though this attempt is threadsafe (thus sufficient for demo day!), it does not allow multiple threads to calculate  shortest  path at the same time and will not have sufficient throughput.  path_t* shortest_safe(graph_t* graph, int i, int j) {\n  pthread_mutex_lock( m);\n  path_t* path = shortest(graph, i, j);\n  pthread_mutex_unlock( m);\n  return path;\n}\nvoid set_edge_safe(graph_t* graph, int i, int j, double dist) {\n  pthread_mutex_lock( m);\n  set_edge(graph, i, j, dist);\n  pthread_mutex_unlock( m);\n}", 
            "title": "Q7"
        }, 
        {
            "location": "/Networking, Part 3: Building a simple TCP Client/", 
            "text": "Complete Simple TCP Client Example\n\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nstring.h\n\n#include \nsys/types.h\n\n#include \nsys/socket.h\n\n#include \nnetdb.h\n\n#include \nunistd.h\n\n\nint main(int argc, char **argv)\n{\n    int s;\n    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    struct addrinfo hints, *result;\n    memset(\nhints, 0, sizeof(struct addrinfo));\n    hints.ai_family = AF_INET; /* IPv4 only */\n    hints.ai_socktype = SOCK_STREAM; /* TCP */\n\n    s = getaddrinfo(\nwww.illinois.edu\n, \n80\n, \nhints, \nresult);\n    if (s != 0) {\n            fprintf(stderr, \ngetaddrinfo: %s\\n\n, gai_strerror(s));\n            exit(1);\n    }\n\n    connect(sock_fd, result-\nai_addr, result-\nai_addrlen);\n\n    char *buffer = \nGET / HTTP/1.0\\r\\n\\r\\n\n;\n    printf(\nSENDING: %s\n, buffer);\n    printf(\n===\\n\n);\n    write(sock_fd, buffer, strlen(buffer));\n\n\n    char resp[1000];\n    int len = read(sock_fd, resp, 999);\n    resp[len] = '\\0';\n    printf(\n%s\\n\n, resp);\n\n    return 0;\n}\n\n\n\n\nExample output:\n\n\nSENDING: GET / HTTP/1.0\n\n===\nHTTP/1.1 200 OK\nDate: Mon, 27 Oct 2014 19:19:05 GMT\nServer: Apache/2.2.15 (Red Hat) mod_ssl/2.2.15 OpenSSL/1.0.1e-fips mod_jk/1.2.32\nLast-Modified: Fri, 03 Feb 2012 16:51:10 GMT\nETag: \n401b0-49-4b8121ea69b80\n\nAccept-Ranges: bytes\nContent-Length: 73\nConnection: close\nContent-Type: text/html\n\nProvided by Web Services at Public Affairs at the University of Illinois\n\n\n\n\nComment on HTTP request and response\n\n\nThe example above demonstrates a request to the server using Hypertext Transfer Protocol.\nA web page (or other resources) are requested using the following request:\n\n\nGET / HTTP/1.0\n\n\n\n\n\nThere are four parts (the method e.g. GET,POST,...); the resource (e.g. / /index.html /image.png); the proctocol \"HTTP/1.0\" and two new lines (\\r\\n\\r\\n)\n\n\nThe server's first response line describes the HTTP version used and whether the request is successful using a 3 digit response code:\n\n\nHTTP/1.1 200 OK\n\n\n\n\nIf the client had requested a non existing file, e.g. \nGET /nosuchfile.html HTTP/1.0\n\nThen the first line includes the response code is the well-known \n404\n response code:\n\n\nHTTP/1.1 404 Not Found", 
            "title": "Networking, Part 3: Building a simple TCP Client"
        }, 
        {
            "location": "/Networking, Part 3: Building a simple TCP Client/#complete-simple-tcp-client-example", 
            "text": "#include  stdio.h \n#include  stdlib.h \n#include  string.h \n#include  sys/types.h \n#include  sys/socket.h \n#include  netdb.h \n#include  unistd.h \n\nint main(int argc, char **argv)\n{\n    int s;\n    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    struct addrinfo hints, *result;\n    memset( hints, 0, sizeof(struct addrinfo));\n    hints.ai_family = AF_INET; /* IPv4 only */\n    hints.ai_socktype = SOCK_STREAM; /* TCP */\n\n    s = getaddrinfo( www.illinois.edu ,  80 ,  hints,  result);\n    if (s != 0) {\n            fprintf(stderr,  getaddrinfo: %s\\n , gai_strerror(s));\n            exit(1);\n    }\n\n    connect(sock_fd, result- ai_addr, result- ai_addrlen);\n\n    char *buffer =  GET / HTTP/1.0\\r\\n\\r\\n ;\n    printf( SENDING: %s , buffer);\n    printf( ===\\n );\n    write(sock_fd, buffer, strlen(buffer));\n\n\n    char resp[1000];\n    int len = read(sock_fd, resp, 999);\n    resp[len] = '\\0';\n    printf( %s\\n , resp);\n\n    return 0;\n}  Example output:  SENDING: GET / HTTP/1.0\n\n===\nHTTP/1.1 200 OK\nDate: Mon, 27 Oct 2014 19:19:05 GMT\nServer: Apache/2.2.15 (Red Hat) mod_ssl/2.2.15 OpenSSL/1.0.1e-fips mod_jk/1.2.32\nLast-Modified: Fri, 03 Feb 2012 16:51:10 GMT\nETag:  401b0-49-4b8121ea69b80 \nAccept-Ranges: bytes\nContent-Length: 73\nConnection: close\nContent-Type: text/html\n\nProvided by Web Services at Public Affairs at the University of Illinois", 
            "title": "Complete Simple TCP Client Example"
        }, 
        {
            "location": "/Networking, Part 3: Building a simple TCP Client/#comment-on-http-request-and-response", 
            "text": "The example above demonstrates a request to the server using Hypertext Transfer Protocol.\nA web page (or other resources) are requested using the following request:  GET / HTTP/1.0  There are four parts (the method e.g. GET,POST,...); the resource (e.g. / /index.html /image.png); the proctocol \"HTTP/1.0\" and two new lines (\\r\\n\\r\\n)  The server's first response line describes the HTTP version used and whether the request is successful using a 3 digit response code:  HTTP/1.1 200 OK  If the client had requested a non existing file, e.g.  GET /nosuchfile.html HTTP/1.0 \nThen the first line includes the response code is the well-known  404  response code:  HTTP/1.1 404 Not Found", 
            "title": "Comment on HTTP request and response"
        }, 
        {
            "location": "/Networking, Part 6: Creating a UDP server/", 
            "text": "How do I create a UDP server?\n\n\nThere are a variety of function calls available to send UDP sockets. We will use the newer getaddrinfo to help set up a socket structure.\n\n\nRemember that UDP is a simple packet-based ('data-gram') protocol ; there is no connection to set up between the two hosts.\n\n\nFirst, initialize the hints addrinfo struct to request an IPv6, passive datagram socket.\n\n\nmemset(\nhints, 0, sizeof(hints));\nhints.ai_family = AF_INET6; // INET for IPv4\nhints.ai_socktype =  SOCK_DGRAM;\nhints.ai_flags =  AI_PASSIVE;\n\n\n\n\nNext, use getaddrinfo to specify the port number (we don't need to specify a host as we are creating a server socket, not sending a packet to a remote host).\n\n\ngetaddrinfo(NULL, \n300\n, \nhints, \nres);\n\nsockfd = socket(res-\nai_family, res-\nai_socktype, res-\nai_protocol);\nbind(sockfd, res-\nai_addr, res-\nai_addrlen);\n\n\n\n\nThe port number is \n1024, so the program will need \nroot\n privileges. We could have also specified a service name instead of a numeric port value.\n\n\nSo far the calls have been similar to a TCP server. For a stream-based service we would call \nlisten\n and accept. For our UDP-serve we can just start waiting for the arrival of a packet on the socket-\n\n\nstruct sockaddr_storage addr;\nint addrlen = sizeof(addr);\n\n// ssize_t recvfrom(int socket, void* buffer, size_t buflen, int flags, struct sockaddr *addr, socklen_t * address_len);\n\nbyte_count = recvfrom(sockfd, buf, sizeof(buf), 0, \naddr, \naddrlen);\n\n\n\n\nThe addr struct will hold sender (source) information about the arriving packet.\nNote the \nsockaddr_storage\n type is a sufficiently large enough to hold all possible types of socket addresses (e.g. IPv4, IPv6 and other socket types).", 
            "title": "Networking, Part 6: Creating a UDP server"
        }, 
        {
            "location": "/Networking, Part 6: Creating a UDP server/#how-do-i-create-a-udp-server", 
            "text": "There are a variety of function calls available to send UDP sockets. We will use the newer getaddrinfo to help set up a socket structure.  Remember that UDP is a simple packet-based ('data-gram') protocol ; there is no connection to set up between the two hosts.  First, initialize the hints addrinfo struct to request an IPv6, passive datagram socket.  memset( hints, 0, sizeof(hints));\nhints.ai_family = AF_INET6; // INET for IPv4\nhints.ai_socktype =  SOCK_DGRAM;\nhints.ai_flags =  AI_PASSIVE;  Next, use getaddrinfo to specify the port number (we don't need to specify a host as we are creating a server socket, not sending a packet to a remote host).  getaddrinfo(NULL,  300 ,  hints,  res);\n\nsockfd = socket(res- ai_family, res- ai_socktype, res- ai_protocol);\nbind(sockfd, res- ai_addr, res- ai_addrlen);  The port number is  1024, so the program will need  root  privileges. We could have also specified a service name instead of a numeric port value.  So far the calls have been similar to a TCP server. For a stream-based service we would call  listen  and accept. For our UDP-serve we can just start waiting for the arrival of a packet on the socket-  struct sockaddr_storage addr;\nint addrlen = sizeof(addr);\n\n// ssize_t recvfrom(int socket, void* buffer, size_t buflen, int flags, struct sockaddr *addr, socklen_t * address_len);\n\nbyte_count = recvfrom(sockfd, buf, sizeof(buf), 0,  addr,  addrlen);  The addr struct will hold sender (source) information about the arriving packet.\nNote the  sockaddr_storage  type is a sufficiently large enough to hold all possible types of socket addresses (e.g. IPv4, IPv6 and other socket types).", 
            "title": "How do I create a UDP server?"
        }, 
        {
            "location": "/Networking,-Part-1:-Introduction/", 
            "text": "What is \"IP4\" \"IP6\"?\n\n\nThe following is the \"30 second\" introduction to internet protocol (IP) - which is the primary way to send packets (\"datagrams\") of information from one machine to another.\n\n\n\"IP4\", or more precisely, \"IPv4\" is version 4 of the Internet Protocol that describes how to send packets of information across a network from one machine to another . Roughly 95% of all packets on the Internet today are IPv4 packets. A significant limitation of IPv4 is that source and destination addresses are limited to 32 bits (IPv4 was designed at a time when the idea of 4 billion devices connected to the same network was unthinkable - or at least not worth making the packet size larger) \n\n\nEach IPv4 packet includes a very small header - typically 20 bytes (more precisely, \"octets\"), that includes a source and destination address.\n\n\nConceptually the source and destination addresses can be split into two: a network number (the upper bits) and the lower bits represent a particular host number on that network.\n\n\nA newer packet protocol \"IPv6\" solves many of the limitations of IPv4 (e.g. makes routing tables simpler and 128 bit addresses) however less than 5% of web traffic is IPv6 based.\n\n\nA machine can have an IPv6 address and an IPv4 address.\n\n\n\"There's no place like 127.0.0.1\"!\n\n\nA special IPv4 address is \n127.0.0.1\n also known as localhost. Packets sent to 127.0.0.1 will never leave the machine; the address is specified to be the same machine.\n\n\nNotice that the 32 bits address is split into 4 octets i.e. each number in the dot notation can be 0-255 inclusive. However IPv4 addresses can also be written as an integer.\n\n\n... and ... \"There's no place like 0:0:0:0:0:0:0:1?\"\n\n\nThe 128bit localhost address in IPv6 is \n0:0:0:0:0:0:0:1\n which can be written in its shortened form, \n::1\n\n\nWhat is a port?\n\n\nTo send a datagram (packet) to a host on the Internet using IPv4 (or IPv6) you need to specify the host address and a port. The port is an unsigned 16 bit number (i.e. the maximum port number is 65535).\n\n\nA process can listen for incoming packets on a particular port. However only processes with super-user (root) access can listen on ports \n 1024. Any process can listen on ports 1024 or higher.\n\n\nAn often used port is port 80: Port 80 is used for unencrypted http requests (i.e. web pages).\nFor example, if a web browser connects to http://www.bbc.com/, then it will be connecting to port 80.\n\n\nWhat is UDP? When is it used?\n\n\nUDP is a connectionless protocol that is built on top of IPv4 and IPv6. It's very simple to use: Decide the destination address and port and send your data packet! However the network makes no guarantee about whether the packets will arrive.\nPackets (aka Datagrams) may be dropped if the network is congested. Packets may be duplicated or arrive out of order.\n\n\nBetween two distant data-centers it's typical to see 3% packet loss.\n\n\nA typical use case for UDP is when receiving up to date data is more important than receiving all of the data. For example, a game may send continuous updates of player positions. A streaming video signal may send picture updates using UDP\n\n\nWhat is TCP? When is it used?\n\n\nTCP is a connection-based protocol that is built on top of IPv4 and IPv6 (and therefore can be described as \"TCP/IP\" or \"TCP over IP\"). TCP creates a \npipe\n between two machines and abstracts away the low level packet-nature of the Internet: Thus, under most conditions, bytes sent from one machine will eventually arrive at the other end without duplication or data loss. \n\n\nTCP will automatically manage resending packets, ignoring duplicate packets, re-arranging out-of-order packets and changing the rate at which packets are sent.\n\n\nTCP's three way handshake is known as SYN, SYN-ACK, and ACK. The diagram on this page helps with understanding the TCP handshake. \nTCP Handshake\n\n\nMost services on the Internet today (e.g. a web service) use TCP because it hides the complexity of lower, packet-level nature of the Internet.", 
            "title": "Networking, Part 1: Introduction"
        }, 
        {
            "location": "/Networking,-Part-1:-Introduction/#what-is-ip4-ip6", 
            "text": "The following is the \"30 second\" introduction to internet protocol (IP) - which is the primary way to send packets (\"datagrams\") of information from one machine to another.  \"IP4\", or more precisely, \"IPv4\" is version 4 of the Internet Protocol that describes how to send packets of information across a network from one machine to another . Roughly 95% of all packets on the Internet today are IPv4 packets. A significant limitation of IPv4 is that source and destination addresses are limited to 32 bits (IPv4 was designed at a time when the idea of 4 billion devices connected to the same network was unthinkable - or at least not worth making the packet size larger)   Each IPv4 packet includes a very small header - typically 20 bytes (more precisely, \"octets\"), that includes a source and destination address.  Conceptually the source and destination addresses can be split into two: a network number (the upper bits) and the lower bits represent a particular host number on that network.  A newer packet protocol \"IPv6\" solves many of the limitations of IPv4 (e.g. makes routing tables simpler and 128 bit addresses) however less than 5% of web traffic is IPv6 based.  A machine can have an IPv6 address and an IPv4 address.", 
            "title": "What is \"IP4\" \"IP6\"?"
        }, 
        {
            "location": "/Networking,-Part-1:-Introduction/#theres-no-place-like-127001", 
            "text": "A special IPv4 address is  127.0.0.1  also known as localhost. Packets sent to 127.0.0.1 will never leave the machine; the address is specified to be the same machine.  Notice that the 32 bits address is split into 4 octets i.e. each number in the dot notation can be 0-255 inclusive. However IPv4 addresses can also be written as an integer.", 
            "title": "\"There's no place like 127.0.0.1\"!"
        }, 
        {
            "location": "/Networking,-Part-1:-Introduction/#and-theres-no-place-like-00000001", 
            "text": "The 128bit localhost address in IPv6 is  0:0:0:0:0:0:0:1  which can be written in its shortened form,  ::1", 
            "title": "... and ... \"There's no place like 0:0:0:0:0:0:0:1?\""
        }, 
        {
            "location": "/Networking,-Part-1:-Introduction/#what-is-a-port", 
            "text": "To send a datagram (packet) to a host on the Internet using IPv4 (or IPv6) you need to specify the host address and a port. The port is an unsigned 16 bit number (i.e. the maximum port number is 65535).  A process can listen for incoming packets on a particular port. However only processes with super-user (root) access can listen on ports   1024. Any process can listen on ports 1024 or higher.  An often used port is port 80: Port 80 is used for unencrypted http requests (i.e. web pages).\nFor example, if a web browser connects to http://www.bbc.com/, then it will be connecting to port 80.", 
            "title": "What is a port?"
        }, 
        {
            "location": "/Networking,-Part-1:-Introduction/#what-is-udp-when-is-it-used", 
            "text": "UDP is a connectionless protocol that is built on top of IPv4 and IPv6. It's very simple to use: Decide the destination address and port and send your data packet! However the network makes no guarantee about whether the packets will arrive.\nPackets (aka Datagrams) may be dropped if the network is congested. Packets may be duplicated or arrive out of order.  Between two distant data-centers it's typical to see 3% packet loss.  A typical use case for UDP is when receiving up to date data is more important than receiving all of the data. For example, a game may send continuous updates of player positions. A streaming video signal may send picture updates using UDP", 
            "title": "What is UDP? When is it used?"
        }, 
        {
            "location": "/Networking,-Part-1:-Introduction/#what-is-tcp-when-is-it-used", 
            "text": "TCP is a connection-based protocol that is built on top of IPv4 and IPv6 (and therefore can be described as \"TCP/IP\" or \"TCP over IP\"). TCP creates a  pipe  between two machines and abstracts away the low level packet-nature of the Internet: Thus, under most conditions, bytes sent from one machine will eventually arrive at the other end without duplication or data loss.   TCP will automatically manage resending packets, ignoring duplicate packets, re-arranging out-of-order packets and changing the rate at which packets are sent.  TCP's three way handshake is known as SYN, SYN-ACK, and ACK. The diagram on this page helps with understanding the TCP handshake.  TCP Handshake  Most services on the Internet today (e.g. a web service) use TCP because it hides the complexity of lower, packet-level nature of the Internet.", 
            "title": "What is TCP? When is it used?"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/", 
            "text": "How do I use \ngetaddrinfo\n to convert the hostname into an IP address?\n\n\nThe function \ngetaddrinfo\n can convert a human readable domain name (e.g. \nwww.illinois.edu\n) into an IPv4 and IPv6 address. In fact it will return a linked-list of addrinfo structs:\n\n\nstruct addrinfo {\n    int              ai_flags;\n    int              ai_family;\n    int              ai_socktype;\n    int              ai_protocol;\n    socklen_t        ai_addrlen;\n    struct sockaddr *ai_addr;\n    char            *ai_canonname;\n    struct addrinfo *ai_next;\n};\n\n\n\n\nIt's very easy to use. For example, suppose you wanted to find out the numeric IPv4 address of a webserver at www.bbc.com. We do this in two stages. First use getaddrinfo to build a linked-list of possible connections. Secondly use \ngetnameinfo\n to convert the binary address into a readable form.\n\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nsys/types.h\n\n#include \nsys/socket.h\n\n#include \nnetdb.h\n\n\nstruct addrinfo hints, *infoptr; // So no need to use memset global variables\n\nint main() {\n  hints.ai_family = AF_INET; // AF_INET means IPv4 only addresses\n\n  int result = getaddrinfo(\nwww.bbc.com\n, NULL, \nhints, \ninfoptr);\n  if (result) {\n    fprintf(stderr, \ngetaddrinfo: %s\\n\n, gai_strerror(result));\n    exit(1);\n  }\n\n  struct addrinfo *p;\n  char host[256],service[256];\n\n  for(p = infoptr; p != NULL; p = p-\nai_next) {\n\n    getnameinfo(p-\nai_addr, p-\nai_addrlen, host, sizeof(host), service, sizeof(service), NI_NUMERICHOST);\n    puts(host);\n  }\n\n  freeaddrinfo(infoptr);\n  return 0;\n}\n\n\n\n\nTypical output:\n\n\n212.58.244.70\n212.58.244.71\n\n\n\n\nHow is www.cs.illinois.edu converted into an IP address?\n\n\nMagic! No seriously, a system called \"DNS\" (Domain Name Service) is used. If a machine does not hold the answer locally then it sends a UDP packet to a local DNS server. This server in turn may query other upstream DNS servers. \n\n\nIs DNS secure?\n\n\nDNS by itself is fast but not secure. DNS requests are not encrypted and susceptible to 'man-in-the-middle' attacks. For example, a coffee shop internet connection could easily subvert your DNS requests and send back different IP addresses for a particular domain\n\n\nHow do I connect to a TCP server (e.g. web server?)\n\n\nTODO\nThere are three basic system calls you need to connect to a remote machine:\n\n\ngetaddrinfo -- Determine the remote addresses of a remote host\nsocket  -- Create a socket\nconnect  -- Connect to the remote host using the socket and address information\n\n\n\n\nThe \ngetaddrinfo\n call if successful, creates a linked-list of \naddrinfo\n structs and sets the given pointer to point to the first one.\n\n\nThe socket call creates an outgoing socket and returns a descriptor (sometimes called a 'file descriptor') that can be used with \nread\n and \nwrite\n etc.In this sense it is the network analog of \nopen\n that opens a file stream - except that we haven't connected the socket to anything yet!\n\n\nFinally the connect call attempts the connection to the remote machine. We pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. There are different kinds of socket address structures (e.g. IPv4 vs IPv6) which can require more memory. So in addition to passing the pointer, the size of the structure is also passed:\n\n\n// Pull out the socket address info from the addrinfo struct:\nconnect(sockfd, p-\nai_addr, p-\nai_addrlen)\n\n\n\n\nHow do I free the memory allocated for the linked-list of addrinfo structs?\n\n\nAs part of the clean up code call \nfreeaddrinfo\n on the top-most \naddrinfo\n struct:\n\n\nvoid freeaddrinfo(struct addrinfo *ai);\n\n\n\n\nIf getaddrinfo fails can I use \nstrerror\n to print out the error?\n\n\nNo. Error handling with \ngetaddrinfo\n is a little different:\n\n  The return value \nis\n the error code (i.e. don't use \nerrno\n)\n\n Use \ngai_strerror\n to get the equivalent short English error text:\n\n\nint result = getaddrinfo(...);\nif(result) { \n   char *mesg = gai_strerror(result); \n   ...\n}\n\n\n\n\nCan I request only IPv4 or IPv6 connection? TCP only?\n\n\nYes! Use the addrinfo structure that is passed into \ngetaddrinfo\n to define the kind of connection you'd like.\n\n\nFor example, to specify stream-based protocols over IPv6:\n\n\nstruct addrinfo hints;\nmemset(hints, 0, sizeof(hints));\n\nhints.ai_family = AF_INET6; // Only want IPv6 (use AF_INET for IPv4)\nhints.ai_socktype = SOCK_STREAM; // Only want stream-based connection\n\n\n\n\nWhat about code examples that use \ngethostbyname\n?\n\n\nThe old function \ngethostbyname\n is deprecated; it's the old way convert a host name into an IP address. The port address still needs to be manually set using htons function. It's much easier to write code to support IPv4 AND IPv6 using the newer \ngetaddrinfo\n\n\nIs it that easy!?\n\n\nYes and no. It's easy to create a simple TCP client - however network communications offers many different levels of abstraction and several attributes and options that can be set at each level of abstraction (for example we haven't talked about \nsetsockopt\n which can manipulate options for the socket).\nFor more information see this \nguide\n.", 
            "title": "Networking, Part 2: Using getaddrinfo"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#how-do-i-use-getaddrinfo-to-convert-the-hostname-into-an-ip-address", 
            "text": "The function  getaddrinfo  can convert a human readable domain name (e.g.  www.illinois.edu ) into an IPv4 and IPv6 address. In fact it will return a linked-list of addrinfo structs:  struct addrinfo {\n    int              ai_flags;\n    int              ai_family;\n    int              ai_socktype;\n    int              ai_protocol;\n    socklen_t        ai_addrlen;\n    struct sockaddr *ai_addr;\n    char            *ai_canonname;\n    struct addrinfo *ai_next;\n};  It's very easy to use. For example, suppose you wanted to find out the numeric IPv4 address of a webserver at www.bbc.com. We do this in two stages. First use getaddrinfo to build a linked-list of possible connections. Secondly use  getnameinfo  to convert the binary address into a readable form.  #include  stdio.h \n#include  stdlib.h \n#include  sys/types.h \n#include  sys/socket.h \n#include  netdb.h \n\nstruct addrinfo hints, *infoptr; // So no need to use memset global variables\n\nint main() {\n  hints.ai_family = AF_INET; // AF_INET means IPv4 only addresses\n\n  int result = getaddrinfo( www.bbc.com , NULL,  hints,  infoptr);\n  if (result) {\n    fprintf(stderr,  getaddrinfo: %s\\n , gai_strerror(result));\n    exit(1);\n  }\n\n  struct addrinfo *p;\n  char host[256],service[256];\n\n  for(p = infoptr; p != NULL; p = p- ai_next) {\n\n    getnameinfo(p- ai_addr, p- ai_addrlen, host, sizeof(host), service, sizeof(service), NI_NUMERICHOST);\n    puts(host);\n  }\n\n  freeaddrinfo(infoptr);\n  return 0;\n}  Typical output:  212.58.244.70\n212.58.244.71", 
            "title": "How do I use getaddrinfo to convert the hostname into an IP address?"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#how-is-wwwcsillinoisedu-converted-into-an-ip-address", 
            "text": "Magic! No seriously, a system called \"DNS\" (Domain Name Service) is used. If a machine does not hold the answer locally then it sends a UDP packet to a local DNS server. This server in turn may query other upstream DNS servers.", 
            "title": "How is www.cs.illinois.edu converted into an IP address?"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#is-dns-secure", 
            "text": "DNS by itself is fast but not secure. DNS requests are not encrypted and susceptible to 'man-in-the-middle' attacks. For example, a coffee shop internet connection could easily subvert your DNS requests and send back different IP addresses for a particular domain", 
            "title": "Is DNS secure?"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#how-do-i-connect-to-a-tcp-server-eg-web-server", 
            "text": "TODO\nThere are three basic system calls you need to connect to a remote machine:  getaddrinfo -- Determine the remote addresses of a remote host\nsocket  -- Create a socket\nconnect  -- Connect to the remote host using the socket and address information  The  getaddrinfo  call if successful, creates a linked-list of  addrinfo  structs and sets the given pointer to point to the first one.  The socket call creates an outgoing socket and returns a descriptor (sometimes called a 'file descriptor') that can be used with  read  and  write  etc.In this sense it is the network analog of  open  that opens a file stream - except that we haven't connected the socket to anything yet!  Finally the connect call attempts the connection to the remote machine. We pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. There are different kinds of socket address structures (e.g. IPv4 vs IPv6) which can require more memory. So in addition to passing the pointer, the size of the structure is also passed:  // Pull out the socket address info from the addrinfo struct:\nconnect(sockfd, p- ai_addr, p- ai_addrlen)", 
            "title": "How do I connect to a TCP server (e.g. web server?)"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#how-do-i-free-the-memory-allocated-for-the-linked-list-of-addrinfo-structs", 
            "text": "As part of the clean up code call  freeaddrinfo  on the top-most  addrinfo  struct:  void freeaddrinfo(struct addrinfo *ai);", 
            "title": "How do I free the memory allocated for the linked-list of addrinfo structs?"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#if-getaddrinfo-fails-can-i-use-strerror-to-print-out-the-error", 
            "text": "No. Error handling with  getaddrinfo  is a little different:   The return value  is  the error code (i.e. don't use  errno )  Use  gai_strerror  to get the equivalent short English error text:  int result = getaddrinfo(...);\nif(result) { \n   char *mesg = gai_strerror(result); \n   ...\n}", 
            "title": "If getaddrinfo fails can I use strerror to print out the error?"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#can-i-request-only-ipv4-or-ipv6-connection-tcp-only", 
            "text": "Yes! Use the addrinfo structure that is passed into  getaddrinfo  to define the kind of connection you'd like.  For example, to specify stream-based protocols over IPv6:  struct addrinfo hints;\nmemset(hints, 0, sizeof(hints));\n\nhints.ai_family = AF_INET6; // Only want IPv6 (use AF_INET for IPv4)\nhints.ai_socktype = SOCK_STREAM; // Only want stream-based connection", 
            "title": "Can I request only IPv4 or IPv6 connection? TCP only?"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#what-about-code-examples-that-use-gethostbyname", 
            "text": "The old function  gethostbyname  is deprecated; it's the old way convert a host name into an IP address. The port address still needs to be manually set using htons function. It's much easier to write code to support IPv4 AND IPv6 using the newer  getaddrinfo", 
            "title": "What about code examples that use gethostbyname?"
        }, 
        {
            "location": "/Networking,-Part-2:-Using-getaddrinfo/#is-it-that-easy", 
            "text": "Yes and no. It's easy to create a simple TCP client - however network communications offers many different levels of abstraction and several attributes and options that can be set at each level of abstraction (for example we haven't talked about  setsockopt  which can manipulate options for the socket).\nFor more information see this  guide .", 
            "title": "Is it that easy!?"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/", 
            "text": "What is \nhtons\n and when is it used?\n\n\nIntegers can be represented in least significant byte first or most-significant byte first. Either approach is reasonable as long as the machine itself is internally consistent. For network communications we need to standardize on agreed format.\n\n\nhtons(xyz)\n returns the 16 bit unsigned integer 'short' value xyz in network byte order.\n\nhtonl(xyz)\n returns the 32 bit unsigned integer 'long' value xyz in network byte order.\n\n\nThese functions are read as 'host to network'; the inverse functions (ntohs, ntohl) convert network ordered byte values to host-ordered ordering. So, is host-ordering  little-endian or big-endian? The answer is - it depends on your machine! It depends on the actual architecture of the host running the code. If the architecture happens to be the same as network ordering then the result of these functions is just the argument. For x86 machines, the host and network ordering \nis\n different.\n\n\nSummary: Whenever you read or write the low level C network structures (e.g. port and address information), remember to use the above functions to ensure correct conversion to/from a machine format. Otherwise the displayed or specified value may be incorrect.\n\n\nWhat are the 'big 4' network calls used to create a server?\n\n\nThe four system calls required to create a TCP server are: \nsocket\n, \nbind\n \nlisten\n and \naccept\n. Each has a specific purpose and should be called in the above order\n\n\nThe port information (used by bind) can be set manually (many older IPv4-only C code examples do this), or be created using \ngetaddrinfo\n\n\nWe also see examples of setsockopt later too.\n\n\nWhat is the purpose of calling \nsocket\n?\n\n\nTo create a endpoint for networking communication. A new socket by itself is not particularly useful; though we've specified either a packet or stream-based connections it is not bound to a particular network interface or port. Instead socket returns a network descriptor that can be used with later calls to bind,listen and accept.\n\n\nWhat is the purpose of calling \nbind\n\n\nThe \nbind\n call associates an abstract socket with an actual network interface and port. It is possible to call bind on a TCP client however it's unusually unnecessary to specify the outgoing port.\n\n\nWhat is the purpose of calling \nlisten\n\n\nThe \nlisten\n call specifies the queue size for the number of incoming, unhandled connections i.e. that have not yet been assigned a network descriptor by \naccept\n\nTypical values for a high performance server are 128 or more.\n\n\nWhy are server sockets passive?\n\n\nServer sockets do not actively try to connect to another host; instead they wait for incoming connections. Additionally, server sockets are not closed when the peer disconnects. Instead when a remote client connects, it is immediately bumped to an unused port number for future communications.\n\n\nWhat is the purpose of calling \naccept\n\n\nOnce the server socket has been initialized the server calls \naccept\n to wait for new connections. Unlike \nsocket\n \nbind\n and \nlisten\n, this call will block. i.e. if there are no new connections, this call will block and only return when a new client connects.\n\n\nNote the \naccept\n call returns a new file descriptor. This file descriptor is specific to a particular client. It is common programming mistake to use the original server socket descriptor for server I/O and then wonder why networking code has failed.\n\n\nWhat are the gotchas of creating a TCP-server?\n\n\n\n\nUsing the socket descriptor of the passive server socket (described above)\n\n\nNot specifying SOCK_STREAM requirement for getaddrinfo\n\n\nNot being able to re-use an existing port.\n\n\nNot initializing the unused struct entries\n\n\nThe \nbind\n call will fail if the port is currently in use\n\n\n\n\nNote, ports are per machine- not per process or per user. In other words,  you cannot use port 1234 while another process is using that port. Worse, ports are by default 'tied up' after a process has finished.\n\n\nServer code example\n\n\nA working simple server example is shown below. Note this example is incomplete - for example it does not close either socket descriptor, or free up memory created by \ngetaddrinfo\n\n\n\n#include \nstring.h\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nsys/types.h\n\n#include \nsys/socket.h\n\n#include \nnetdb.h\n\n#include \nunistd.h\n\n#include \narpa/inet.h\n\n\nint main(int argc, char **argv)\n{\n    int s;\n    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    struct addrinfo hints, *result;\n    memset(\nhints, 0, sizeof(struct addrinfo));\n    hints.ai_family = AF_INET;\n    hints.ai_socktype = SOCK_STREAM;\n    hints.ai_flags = AI_PASSIVE;\n\n    s = getaddrinfo(NULL, \n1234\n, \nhints, \nresult);\n    if (s != 0) {\n            fprintf(stderr, \ngetaddrinfo: %s\\n\n, gai_strerror(s));\n            exit(1);\n    }\n\n    if (bind(sock_fd, result-\nai_addr, result-\nai_addrlen) != 0) {\n        perror(\nbind()\n);\n        exit(1);\n    }\n\n    if (listen(sock_fd, 10) != 0) {\n        perror(\nlisten()\n);\n        exit(1);\n    }\n\n    struct sockaddr_in *result_addr = (struct sockaddr_in *) result-\nai_addr;\n    printf(\nListening on file descriptor %d, port %d\\n\n, sock_fd, ntohs(result_addr-\nsin_port));\n\n    printf(\nWaiting for connection...\\n\n);\n    int client_fd = accept(sock_fd, NULL, NULL);\n    printf(\nConnection made: client_fd=%d\\n\n, client_fd);\n\n    char buffer[1000];\n    int len = read(client_fd, buffer, sizeof(buffer) - 1);\n    buffer[len] = '\\0';\n\n    printf(\nRead %d chars\\n\n, len);\n    printf(\n===\\n\n);\n    printf(\n%s\\n\n, buffer);\n\n    return 0;\n}\n\n\n\n\nWhy can't my server re-use the port?\n\n\nBy default a port is not immediately released when the socket is closed. Instead, the port enters a \"TIMED-WAIT\" state. This can lead to significant confusion during development because the timeout can make valid networking code appear to fail.\n\n\nTo be able to immediately re-use a port, specify \nSO_REUSEPORT\n before binding to the port.\n\n\nint optval = 1;\nsetsockopt(sfd, SOL_SOCKET, SO_REUSEPORT, \noptval, sizeof(optval));\n\nbind(....\n\n\n\n\nHere's \nan extended stackoverflow introductory discussion of \nSO_REUSEPORT\n.", 
            "title": "Networking, Part 4: Building a simple TCP Server"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-htons-and-when-is-it-used", 
            "text": "Integers can be represented in least significant byte first or most-significant byte first. Either approach is reasonable as long as the machine itself is internally consistent. For network communications we need to standardize on agreed format.  htons(xyz)  returns the 16 bit unsigned integer 'short' value xyz in network byte order. htonl(xyz)  returns the 32 bit unsigned integer 'long' value xyz in network byte order.  These functions are read as 'host to network'; the inverse functions (ntohs, ntohl) convert network ordered byte values to host-ordered ordering. So, is host-ordering  little-endian or big-endian? The answer is - it depends on your machine! It depends on the actual architecture of the host running the code. If the architecture happens to be the same as network ordering then the result of these functions is just the argument. For x86 machines, the host and network ordering  is  different.  Summary: Whenever you read or write the low level C network structures (e.g. port and address information), remember to use the above functions to ensure correct conversion to/from a machine format. Otherwise the displayed or specified value may be incorrect.", 
            "title": "What is htons and when is it used?"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-are-the-big-4-network-calls-used-to-create-a-server", 
            "text": "The four system calls required to create a TCP server are:  socket ,  bind   listen  and  accept . Each has a specific purpose and should be called in the above order  The port information (used by bind) can be set manually (many older IPv4-only C code examples do this), or be created using  getaddrinfo  We also see examples of setsockopt later too.", 
            "title": "What are the 'big 4' network calls used to create a server?"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-the-purpose-of-calling-socket", 
            "text": "To create a endpoint for networking communication. A new socket by itself is not particularly useful; though we've specified either a packet or stream-based connections it is not bound to a particular network interface or port. Instead socket returns a network descriptor that can be used with later calls to bind,listen and accept.", 
            "title": "What is the purpose of calling socket?"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-the-purpose-of-calling-bind", 
            "text": "The  bind  call associates an abstract socket with an actual network interface and port. It is possible to call bind on a TCP client however it's unusually unnecessary to specify the outgoing port.", 
            "title": "What is the purpose of calling bind"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-the-purpose-of-calling-listen", 
            "text": "The  listen  call specifies the queue size for the number of incoming, unhandled connections i.e. that have not yet been assigned a network descriptor by  accept \nTypical values for a high performance server are 128 or more.", 
            "title": "What is the purpose of calling listen"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#why-are-server-sockets-passive", 
            "text": "Server sockets do not actively try to connect to another host; instead they wait for incoming connections. Additionally, server sockets are not closed when the peer disconnects. Instead when a remote client connects, it is immediately bumped to an unused port number for future communications.", 
            "title": "Why are server sockets passive?"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-is-the-purpose-of-calling-accept", 
            "text": "Once the server socket has been initialized the server calls  accept  to wait for new connections. Unlike  socket   bind  and  listen , this call will block. i.e. if there are no new connections, this call will block and only return when a new client connects.  Note the  accept  call returns a new file descriptor. This file descriptor is specific to a particular client. It is common programming mistake to use the original server socket descriptor for server I/O and then wonder why networking code has failed.", 
            "title": "What is the purpose of calling accept"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#what-are-the-gotchas-of-creating-a-tcp-server", 
            "text": "Using the socket descriptor of the passive server socket (described above)  Not specifying SOCK_STREAM requirement for getaddrinfo  Not being able to re-use an existing port.  Not initializing the unused struct entries  The  bind  call will fail if the port is currently in use   Note, ports are per machine- not per process or per user. In other words,  you cannot use port 1234 while another process is using that port. Worse, ports are by default 'tied up' after a process has finished.", 
            "title": "What are the gotchas of creating a TCP-server?"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#server-code-example", 
            "text": "A working simple server example is shown below. Note this example is incomplete - for example it does not close either socket descriptor, or free up memory created by  getaddrinfo  \n#include  string.h \n#include  stdio.h \n#include  stdlib.h \n#include  sys/types.h \n#include  sys/socket.h \n#include  netdb.h \n#include  unistd.h \n#include  arpa/inet.h \n\nint main(int argc, char **argv)\n{\n    int s;\n    int sock_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    struct addrinfo hints, *result;\n    memset( hints, 0, sizeof(struct addrinfo));\n    hints.ai_family = AF_INET;\n    hints.ai_socktype = SOCK_STREAM;\n    hints.ai_flags = AI_PASSIVE;\n\n    s = getaddrinfo(NULL,  1234 ,  hints,  result);\n    if (s != 0) {\n            fprintf(stderr,  getaddrinfo: %s\\n , gai_strerror(s));\n            exit(1);\n    }\n\n    if (bind(sock_fd, result- ai_addr, result- ai_addrlen) != 0) {\n        perror( bind() );\n        exit(1);\n    }\n\n    if (listen(sock_fd, 10) != 0) {\n        perror( listen() );\n        exit(1);\n    }\n\n    struct sockaddr_in *result_addr = (struct sockaddr_in *) result- ai_addr;\n    printf( Listening on file descriptor %d, port %d\\n , sock_fd, ntohs(result_addr- sin_port));\n\n    printf( Waiting for connection...\\n );\n    int client_fd = accept(sock_fd, NULL, NULL);\n    printf( Connection made: client_fd=%d\\n , client_fd);\n\n    char buffer[1000];\n    int len = read(client_fd, buffer, sizeof(buffer) - 1);\n    buffer[len] = '\\0';\n\n    printf( Read %d chars\\n , len);\n    printf( ===\\n );\n    printf( %s\\n , buffer);\n\n    return 0;\n}", 
            "title": "Server code example"
        }, 
        {
            "location": "/Networking,-Part-4:-Building-a-simple-TCP-Server/#why-cant-my-server-re-use-the-port", 
            "text": "By default a port is not immediately released when the socket is closed. Instead, the port enters a \"TIMED-WAIT\" state. This can lead to significant confusion during development because the timeout can make valid networking code appear to fail.  To be able to immediately re-use a port, specify  SO_REUSEPORT  before binding to the port.  int optval = 1;\nsetsockopt(sfd, SOL_SOCKET, SO_REUSEPORT,  optval, sizeof(optval));\n\nbind(....  Here's  an extended stackoverflow introductory discussion of  SO_REUSEPORT .", 
            "title": "Why can't my server re-use the port?"
        }, 
        {
            "location": "/Networking,-Part-5:-Reusing-ports/", 
            "text": "When I re-run my server code it doesn't work! Why?\n\n\nBy default, after a socket is closed the port enters a time-out state during which time it cannot be re-used ('bound to a new socket').\n\n\nThis behavior can be disabled by setting the socket option REUSEPORT before bind-ing to a port:\n\n\n    int optval = 1;\n    setsockopt(sock_fd, SOL_SOCKET, SO_REUSEPORT, \noptval, sizeof(optval));\n\n    bind(sock_fd, ...);\n\n\n\n\nCan a TCP client bind to a particular port?\n\n\nYes! In fact outgoing TCP connections are automatically bound to an unused port on the client. Usually it's unnecessary to explicitly set the port on the client because the system will intelligently find an unusued port on a reasonable interface (e.g. the wireless card, if currently connected by WiFi connection). However it can be useful if you needed to specifically choose a particular ethernet card, or if a firewall only allows outgoing connections from a particular range of port values.\n\n\nTo explicitly bind to an ethernet interface and port, call \nbind\n before \nconnect\n\n\nWho connected to my server?\n\n\nThe \naccept\n system call can optionally provide information about the remote client, by passing in a sockaddr struct. Different protocols have differently variants of the  \nstruct sockaddr\n, which are different sizes. The simplest struct to use is the \nsockaddr_storage\n which is sufficiently large to represent all possible types of sockaddr. Notice that C does not have any model of inheritance. Therefore we need to explicitly cast our struct to the 'base type' struct sockaddr.\n\n\n    struct sockaddr_storage clientaddr;\n    socklen_t clientaddrsize = sizeof(clientaddr);\n    int client_id = accept(passive_socket,\n            (struct sockaddr *) \nclientaddr,\n             \nclientaddrsize);\n\n\n\n\nWe've already seen \ngetaddrinfo\n that can build a linked list of addrinfo entries (and each one of these can include socket configuration data). What if we wanted to turn socket data into IP and port addresses? Enter \ngetnameinfo\n that can be used to convert a local or remote socket information into a domain name or numeric IP. Similarly the port number can be represented as a service name (e.g. \"http\" for port 80). In the example below we request numeric versions for the client IP address and client port number.\n\n\n    socklen_t clientaddrsize = sizeof(clientaddr);\n    int client_id = accept(sock_id, (struct sockaddr *) \nclientaddr, \nclientaddrsize);\n    char host[256], port[256];\n    getnameinfo((struct sockaddr *) \nclientaddr,\n          clientaddrsize, host, sizeof(host), port, sizeof(port),\n          NI_NUMERICHOST | NI_NUMERICSERV);\n\n\n\n\nTodo: Discuss NI_MAXHOST and NI_MAXSERV, and NI_NUMERICHOST \n\n\ngetnameinfo Example: What's my IP address?\n\n\nTo obtain a linked list of IP addresses of the current machine use \ngetifaddrs\n which will return a linked list of IPv4 and IPv6 IP addresses (and potentially other interfaces too). We can examine each entry and use \ngetnameinfo\n to print the host's IP address.\nThe  ifaddrs struct includes the family but does not include the sizeof the struct. Therefore we need to manually determine the struct sized based on the family (IPv4 v IPv6)\n\n\n (family == AF_INET) ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6)\n\n\n\n\nThe complete code is shown below.\n\n\n    int required_family = AF_INET; // Change to AF_INET6 for IPv6\n    struct ifaddrs *myaddrs, *ifa;\n    getifaddrs(\nmyaddrs);\n    char host[256], port[256];\n    for (ifa = myaddrs; ifa != NULL; ifa = ifa-\nifa_next) {\n        int family = ifa-\nifa_addr-\nsa_family;\n        if (family == required_family \n ifa-\nifa_addr) {\n            if (0 == getnameinfo(ifa-\nifa_addr,\n                                (family == AF_INET) ? sizeof(struct sockaddr_in) :\n                                sizeof(struct sockaddr_in6),\n                                host, sizeof(host), port, sizeof(port)\n                                 , NI_NUMERICHOST | NI_NUMERICSERV  ))\n                puts(host);\n            }\n        }\n\n\n\n\nWhat's my machine's IP address (shell version)\n\n\nAnswer: use \nifconfig\n (or Windows's ipconfig)\nHowever this command generates a lot of output for each interface, so we can filter the output using grep\n\n\nifconfig | grep inet\n\nExample output:\n    inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 \n    inet 127.0.0.1 netmask 0xff000000 \n    inet6 ::1 prefixlen 128 \n    inet6 fe80::7256:81ff:fe9a:9141%en1 prefixlen 64 scopeid 0x5 \n    inet 192.168.1.100 netmask 0xffffff00 broadcast 192.168.1.255", 
            "title": "Networking, Part 5: Reusing ports"
        }, 
        {
            "location": "/Networking,-Part-5:-Reusing-ports/#when-i-re-run-my-server-code-it-doesnt-work-why", 
            "text": "By default, after a socket is closed the port enters a time-out state during which time it cannot be re-used ('bound to a new socket').  This behavior can be disabled by setting the socket option REUSEPORT before bind-ing to a port:      int optval = 1;\n    setsockopt(sock_fd, SOL_SOCKET, SO_REUSEPORT,  optval, sizeof(optval));\n\n    bind(sock_fd, ...);", 
            "title": "When I re-run my server code it doesn't work! Why?"
        }, 
        {
            "location": "/Networking,-Part-5:-Reusing-ports/#can-a-tcp-client-bind-to-a-particular-port", 
            "text": "Yes! In fact outgoing TCP connections are automatically bound to an unused port on the client. Usually it's unnecessary to explicitly set the port on the client because the system will intelligently find an unusued port on a reasonable interface (e.g. the wireless card, if currently connected by WiFi connection). However it can be useful if you needed to specifically choose a particular ethernet card, or if a firewall only allows outgoing connections from a particular range of port values.  To explicitly bind to an ethernet interface and port, call  bind  before  connect", 
            "title": "Can a TCP client bind to a particular port?"
        }, 
        {
            "location": "/Networking,-Part-5:-Reusing-ports/#who-connected-to-my-server", 
            "text": "The  accept  system call can optionally provide information about the remote client, by passing in a sockaddr struct. Different protocols have differently variants of the   struct sockaddr , which are different sizes. The simplest struct to use is the  sockaddr_storage  which is sufficiently large to represent all possible types of sockaddr. Notice that C does not have any model of inheritance. Therefore we need to explicitly cast our struct to the 'base type' struct sockaddr.      struct sockaddr_storage clientaddr;\n    socklen_t clientaddrsize = sizeof(clientaddr);\n    int client_id = accept(passive_socket,\n            (struct sockaddr *)  clientaddr,\n              clientaddrsize);  We've already seen  getaddrinfo  that can build a linked list of addrinfo entries (and each one of these can include socket configuration data). What if we wanted to turn socket data into IP and port addresses? Enter  getnameinfo  that can be used to convert a local or remote socket information into a domain name or numeric IP. Similarly the port number can be represented as a service name (e.g. \"http\" for port 80). In the example below we request numeric versions for the client IP address and client port number.      socklen_t clientaddrsize = sizeof(clientaddr);\n    int client_id = accept(sock_id, (struct sockaddr *)  clientaddr,  clientaddrsize);\n    char host[256], port[256];\n    getnameinfo((struct sockaddr *)  clientaddr,\n          clientaddrsize, host, sizeof(host), port, sizeof(port),\n          NI_NUMERICHOST | NI_NUMERICSERV);  Todo: Discuss NI_MAXHOST and NI_MAXSERV, and NI_NUMERICHOST", 
            "title": "Who connected to my server?"
        }, 
        {
            "location": "/Networking,-Part-5:-Reusing-ports/#getnameinfo-example-whats-my-ip-address", 
            "text": "To obtain a linked list of IP addresses of the current machine use  getifaddrs  which will return a linked list of IPv4 and IPv6 IP addresses (and potentially other interfaces too). We can examine each entry and use  getnameinfo  to print the host's IP address.\nThe  ifaddrs struct includes the family but does not include the sizeof the struct. Therefore we need to manually determine the struct sized based on the family (IPv4 v IPv6)   (family == AF_INET) ? sizeof(struct sockaddr_in) : sizeof(struct sockaddr_in6)  The complete code is shown below.      int required_family = AF_INET; // Change to AF_INET6 for IPv6\n    struct ifaddrs *myaddrs, *ifa;\n    getifaddrs( myaddrs);\n    char host[256], port[256];\n    for (ifa = myaddrs; ifa != NULL; ifa = ifa- ifa_next) {\n        int family = ifa- ifa_addr- sa_family;\n        if (family == required_family   ifa- ifa_addr) {\n            if (0 == getnameinfo(ifa- ifa_addr,\n                                (family == AF_INET) ? sizeof(struct sockaddr_in) :\n                                sizeof(struct sockaddr_in6),\n                                host, sizeof(host), port, sizeof(port)\n                                 , NI_NUMERICHOST | NI_NUMERICSERV  ))\n                puts(host);\n            }\n        }", 
            "title": "getnameinfo Example: What's my IP address?"
        }, 
        {
            "location": "/Networking,-Part-5:-Reusing-ports/#whats-my-machines-ip-address-shell-version", 
            "text": "Answer: use  ifconfig  (or Windows's ipconfig)\nHowever this command generates a lot of output for each interface, so we can filter the output using grep  ifconfig | grep inet\n\nExample output:\n    inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 \n    inet 127.0.0.1 netmask 0xff000000 \n    inet6 ::1 prefixlen 128 \n    inet6 fe80::7256:81ff:fe9a:9141%en1 prefixlen 64 scopeid 0x5 \n    inet 192.168.1.100 netmask 0xffffff00 broadcast 192.168.1.255", 
            "title": "What's my machine's IP address (shell version)"
        }, 
        {
            "location": "/Networking:-Review-Questions/", 
            "text": "Wiki w/Interactive MC Questions\n\n\nSee \nCoding questions\n\n\nSee \nShort answer questions\n\n\nSee \nMP Wearables\n Food For Thought questions\n\n\n\n\nShort answer questions\n\n\nQ1\n\n\nWhat is a socket?\n\n\nQ2\n\n\n@MCQ\nWhat is special about listening on port 1000 vs port 2000?\nThink about security. @HINT\nCorrect! Root privileges are required to listen to ports below 1024. @EXP\n- Port 2000 is twice as slow as port 1000\n- Port 2000 is twice as fast as port 1000\n- Port 1000 requires root privileges @ANS\n- Nothing\n@END\n\n\nQ3\n\n\nDescribe one significant difference between IPv4 and IPv6\n\n\nQ4\n\n\nWhen and why would you use ntohs?\n\n\nQ5\n\n\nIf a host address is 32 bits which IP scheme am I most likely using? 128 bits?\n\n\nQ6\n\n\nWhich common network protocol is packet based and may not successfully deliver the data?\n\n\nQ7\n\n\nWhich common protocol is stream-based and will resend data if packets are lost?\n\n\nQ8\n\n\nWhat is the SYN ACK ACK-SYN handshake?\n\n\nQ9\n\n\n@MCQ Which one of the following is NOT a feature of TCP? \nNice! https://en.wikipedia.org/wiki/OSI_model @EXP\nNo hint available. @HINT\n- Packet re-ordering\n- Flow control\n- Packet re-tranmission\n- Simple error detection\n- Encryption @ANS\n@END\n\n\nQ10\n\n\nWhat protocol uses sequence numbers? What is their initial value? And why?\n\n\nQ11\n\n\nWhat are the minimum network calls are required to build a TCP server? What is their correct order?\n\n\nQ12\n\n\nWhat are the minimum network calls are required to build a TCP client? What is their correct order?\n\n\nQ13\n\n\nWhen would you call bind on a TCP client?\n\n\nQ14\n\n\nWhat is the purpose of\nsocket\nbind\nlisten\naccept\n?\n\n\nQ15\n\n\nWhich of the above calls can block, waiting for a new client to connect?\n\n\nQ16\n\n\nWhat is DNS? What does it do for you? Which of the CS241 network calls will use it for you?\n\n\nQ17\n\n\nFor getaddrinfo, how do you specify a server socket?\n\n\nQ18\n\n\nWhy may getaddrinfo generate network packets?\n\n\nQ19\n\n\nWhich network call specifies the size of the allowed backlog?\n\n\nQ20\n\n\nWhich network call returns a new file descriptor?\n\n\nQ21\n\n\nWhen are passive sockets used?\n\n\nQ22\n\n\nWhen is epoll a better choice than select? When is select a better choice than epoll?\n\n\nQ23\n\n\nWill  \nwrite(fd, data, 5000)\n  always send 5000 bytes of data? When can it fail?\n\n\nQ24\n\n\nHow does Network Address Translation (NAT) work? \n\n\nQ25\n\n\n@MCQ\nAssuming a network has a 20ms Transmit Time between Client and Server, how much time would it take to establish a TCP Connection?\n20 ms\n40 ms \n100 ms\n60 ms @ANS\n3 Way Handshake @EXP\n@END\n\n\nQ26\n\n\nWhat are some of the differences between HTTP 1.0 and HTTP 1.1? How many ms will it take to transmit 3 files from server to client if the network has a 20ms transmit time? How does the time taken differ between HTTP 1.0 and HTTP 1.1?\n\n\nCoding questions\n\n\nQ 2.1\n\n\nWriting to a network socket may not send all of the bytes and may be interrupted due to a signal. Check the return value of \nwrite\n to implement \nwrite_all\n that will repeatedly call \nwrite\n with any remaining data. If \nwrite\n returns -1 then immediately return -1 unless the \nerrno\n is \nEINTR\n - in which case repeat the last \nwrite\n attempt. You will need to use pointer arithmetic.\n\n\n// Returns -1 if write fails (unless EINTR in which case it recalls write\n// Repeated calls write until all of the buffer is written.\nssize_t write_all(int fd, const char *buf, size_t nbyte) {\n  ssize_t nb = write(fd, buf, nbyte);\n  return nb;\n}\n\n\n\n\nQ 2.2\n\n\nImplement a multithreaded TCP server that listens on port 2000. Each thread should read 128 bytes from the client file descriptor and echo it back to the client, before closing the connection and ending the thread.\n\n\nQ 2.3\n\n\nImplement a UDP server that listens on port 2000. Reserve a buffer of 200 bytes. Listen for an arriving packet. Valid packets are 200 bytes or less and start with four bytes 0x65 0x66 0x67 0x68. Ignore invalid packets. For valid packets add the value of the fifth byte as an unsigned value to a running total and print the total so far. If the running total is greater than 255 then exit.", 
            "title": "Networking: Review Questions"
        }, 
        {
            "location": "/Networking:-Review-Questions/#short-answer-questions", 
            "text": "", 
            "title": "Short answer questions"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q1", 
            "text": "What is a socket?", 
            "title": "Q1"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q2", 
            "text": "@MCQ\nWhat is special about listening on port 1000 vs port 2000?\nThink about security. @HINT\nCorrect! Root privileges are required to listen to ports below 1024. @EXP\n- Port 2000 is twice as slow as port 1000\n- Port 2000 is twice as fast as port 1000\n- Port 1000 requires root privileges @ANS\n- Nothing\n@END", 
            "title": "Q2"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q3", 
            "text": "Describe one significant difference between IPv4 and IPv6", 
            "title": "Q3"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q4", 
            "text": "When and why would you use ntohs?", 
            "title": "Q4"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q5", 
            "text": "If a host address is 32 bits which IP scheme am I most likely using? 128 bits?", 
            "title": "Q5"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q6", 
            "text": "Which common network protocol is packet based and may not successfully deliver the data?", 
            "title": "Q6"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q7", 
            "text": "Which common protocol is stream-based and will resend data if packets are lost?", 
            "title": "Q7"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q8", 
            "text": "What is the SYN ACK ACK-SYN handshake?", 
            "title": "Q8"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q9", 
            "text": "@MCQ Which one of the following is NOT a feature of TCP? \nNice! https://en.wikipedia.org/wiki/OSI_model @EXP\nNo hint available. @HINT\n- Packet re-ordering\n- Flow control\n- Packet re-tranmission\n- Simple error detection\n- Encryption @ANS\n@END", 
            "title": "Q9"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q10", 
            "text": "What protocol uses sequence numbers? What is their initial value? And why?", 
            "title": "Q10"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q11", 
            "text": "What are the minimum network calls are required to build a TCP server? What is their correct order?", 
            "title": "Q11"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q12", 
            "text": "What are the minimum network calls are required to build a TCP client? What is their correct order?", 
            "title": "Q12"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q13", 
            "text": "When would you call bind on a TCP client?", 
            "title": "Q13"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q14", 
            "text": "What is the purpose of\nsocket\nbind\nlisten\naccept\n?", 
            "title": "Q14"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q15", 
            "text": "Which of the above calls can block, waiting for a new client to connect?", 
            "title": "Q15"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q16", 
            "text": "What is DNS? What does it do for you? Which of the CS241 network calls will use it for you?", 
            "title": "Q16"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q17", 
            "text": "For getaddrinfo, how do you specify a server socket?", 
            "title": "Q17"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q18", 
            "text": "Why may getaddrinfo generate network packets?", 
            "title": "Q18"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q19", 
            "text": "Which network call specifies the size of the allowed backlog?", 
            "title": "Q19"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q20", 
            "text": "Which network call returns a new file descriptor?", 
            "title": "Q20"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q21", 
            "text": "When are passive sockets used?", 
            "title": "Q21"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q22", 
            "text": "When is epoll a better choice than select? When is select a better choice than epoll?", 
            "title": "Q22"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q23", 
            "text": "Will   write(fd, data, 5000)   always send 5000 bytes of data? When can it fail?", 
            "title": "Q23"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q24", 
            "text": "How does Network Address Translation (NAT) work?", 
            "title": "Q24"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q25", 
            "text": "@MCQ\nAssuming a network has a 20ms Transmit Time between Client and Server, how much time would it take to establish a TCP Connection?\n20 ms\n40 ms \n100 ms\n60 ms @ANS\n3 Way Handshake @EXP\n@END", 
            "title": "Q25"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q26", 
            "text": "What are some of the differences between HTTP 1.0 and HTTP 1.1? How many ms will it take to transmit 3 files from server to client if the network has a 20ms transmit time? How does the time taken differ between HTTP 1.0 and HTTP 1.1?", 
            "title": "Q26"
        }, 
        {
            "location": "/Networking:-Review-Questions/#coding-questions", 
            "text": "", 
            "title": "Coding questions"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q-21", 
            "text": "Writing to a network socket may not send all of the bytes and may be interrupted due to a signal. Check the return value of  write  to implement  write_all  that will repeatedly call  write  with any remaining data. If  write  returns -1 then immediately return -1 unless the  errno  is  EINTR  - in which case repeat the last  write  attempt. You will need to use pointer arithmetic.  // Returns -1 if write fails (unless EINTR in which case it recalls write\n// Repeated calls write until all of the buffer is written.\nssize_t write_all(int fd, const char *buf, size_t nbyte) {\n  ssize_t nb = write(fd, buf, nbyte);\n  return nb;\n}", 
            "title": "Q 2.1"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q-22", 
            "text": "Implement a multithreaded TCP server that listens on port 2000. Each thread should read 128 bytes from the client file descriptor and echo it back to the client, before closing the connection and ending the thread.", 
            "title": "Q 2.2"
        }, 
        {
            "location": "/Networking:-Review-Questions/#q-23", 
            "text": "Implement a UDP server that listens on port 2000. Reserve a buffer of 200 bytes. Listen for an arriving packet. Valid packets are 200 bytes or less and start with four bytes 0x65 0x66 0x67 0x68. Ignore invalid packets. For valid packets add the value of the fifth byte as an unsigned value to a running total and print the total so far. If the running total is greater than 255 then exit.", 
            "title": "Q 2.3"
        }, 
        {
            "location": "/Nonblocking-I-O,-select(),-and-epoll/", 
            "text": "Don't waste time waiting\n\n\nNormally, when you call \nread()\n, if the data is not available yet it will wait until the data is ready before the function returns.  When you're reading data from a disk, that delay may not be long, but when you're reading from a slow network connection it may take a long time for that data to arrive, if it ever arrives.  \n\n\nPOSIX lets you set a flag on a file descriptor such that any call to \nread()\n on that file descriptor will return immediately, whether it has finished or not.  With your file descriptor in this mode, your call to \nread()\n will start\nthe read operation, and while it's working you can do other useful work.  This is called \"nonblocking\" mode,\nsince the call to \nread()\n doesn't block.\n\n\nTo set a file descriptor to be nonblocking:\n\n\n// fd is my file descriptor\nint flags = fcntl(fd, F_GETFL, 0);\nfcntl(fd, F_SETFL, flags | O_NONBLOCK);\n\n\n\nFor a socket, you can create it in nonblocking mode by adding \nSOCK_NONBLOCK\n to the second argument to \nsocket()\n:\n\n\nfd = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, 0);\n\n\n\nWhen a file is in nonblocking mode and you call \nread()\n, it will return immediately with whatever bytes are available.\nSay 100 bytes have arrived from the server at the other end of your socket and you call \nread(fd, buf, 150)\n.\nRead will return immediately with a value of 100, meaning it read 100 of the 150 bytes you asked for.\nSay you tried to read the remaining data with a call to \nread(fd, buf+100, 50)\n, but the last 50 bytes still hadn't\narrived yet.  \nread()\n would return -1 and set the global error variable \nerrno\n to either\nEAGAIN or EWOULDBLOCK.  That's the system's way of telling you the data isn't ready yet.\n\n\nwrite()\n also works in nonblocking mode.  Say you want to send 40,000 bytes to a remote server using a socket.\nThe system can only send so many bytes at a time. Common systems can send about 23,000 bytes at a time. In nonblocking mode, \nwrite(fd, buf, 40000)\n would return the number of bytes it was able to\nsend immediately, or about 23,000.  If you called \nwrite()\n right away again, it would return -1 and set errno to\nEAGAIN or EWOULDBLOCK. That's the system's way of telling you it's still busy sending the last chunk of data,\nand isn't ready to send more yet.\n\n\nHow do I check when the I/O has finished?\n\n\nThere are a few ways.  Let's see how to do it using \nselect\n and \nepoll\n.\n\n\nselect\n\n\nint select(int nfds, \n           fd_set *readfds, \n           fd_set *writefds,\n           fd_set *exceptfds, \n           struct timeval *timeout);\n\n\n\nGiven three sets of file descriptors, \nselect()\n will wait for any of those file descriptors to become 'ready'.\n\n readfds - a file descriptor in readfds is ready when there is data that can be read or EOF has been reached.\n\n writefds - a file descriptor in writefds is ready when a call to write() will succeed.\n* exceptfds - system-specific, not well-defined.  Just pass NULL for this.\n\n\nselect()\n returns the total number of file descriptors that are ready.  If none of them become\nready during the time defined by \ntimeout\n, it will return 0.  After \nselect()\n returns, the \ncaller will need to loop\nthrough the file descriptors in readfds and/or writefds to see which ones are ready.\n\n\nfd_set readfds, writefds;\nFD_ZERO(\nreadfds);\nFD_ZERO(\nwritefds);\nfor (int i=0; i \n read_fd_count; i++)\n  FD_SET(my_read_fds[i], \nreadfds);\nfor (int i=0; i \n write_fd_count; i++)\n  FD_SET(my_write_fds[i], \nwritefds);\n\nstruct timeval timeout;\ntimeout.tv_sec = 3;\ntimeout.tv_usec = 0;\n\nint num_ready = select(FD_SETSIZE, \nreadfds, \nwritefds, NULL, \ntimeout);\n\nif (num_ready \n 0) {\n  perror(\"error in select()\");\n} else if (num_ready == 0) {\n  printf(\"timeout\\n\");\n} else {\n  for (int i=0; i \n read_fd_count; i++)\n    if (FD_ISSET(my_read_fds[i], \nreadfds))\n      printf(\"fd %d is ready for reading\\n\", my_read_fds[i]);\n  for (int i=0; i \n write_fd_count; i++)\n    if (FD_ISSET(my_write_fds[i], \nwritefds))\n      printf(\"fd %d is ready for writing\\n\", my_write_fds[i]);\n}\n\n\n\nFor more information on select()\n\n\nepoll\n\n\nepoll\n is not part of POSIX, but it is supported by Linux.  It is a more efficient way to wait for many\nfile descriptors.  It will tell you exactly which descriptors are ready. It even gives you a way to store\na small amount of data with each descriptor, like an array index or a pointer, making it easier to access\nyour data associated with that descriptor.\n\n\nTo use epoll, first you must create a special file descriptor with \nepoll_create()\n.  You won't read or write to this file\ndescriptor; you'll just pass it to the other epoll_xxx functions and call\nclose() on it at the end.\n\n\nepfd = epoll_create(1);\n\n\n\nFor each file descriptor you want to monitor with epoll, you'll need to add it \nto the epoll data structures \nusing \nepoll_ctl()\n with the \nEPOLL_CTL_ADD\n option.  You can add any\nnumber of file descriptors to it.\n\n\nstruct epoll_event event;\nevent.events = EPOLLOUT;  // EPOLLIN==read, EPOLLOUT==write\nevent.data.ptr = mypointer;\nepoll_ctl(epfd, EPOLL_CTL_ADD, mypointer-\nfd, \nevent)\n\n\n\nTo wait for some of the file descriptors to become ready, use \nepoll_wait()\n.\nThe epoll_event struct that it fills out will contain the data you provided in event.data when you\nadded this file descriptor. This makes it easy for you to look up your own data associated\nwith this file descriptor.\n\n\nint num_ready = epoll_wait(epfd, \nevent, 1, timeout_milliseconds);\nif (num_ready \n 0) {\n  MyData *mypointer = (MyData*) event.data.ptr;\n  printf(\"ready to write on %d\\n\", mypointer-\nfd);\n}\n\n\n\nSay you were waiting to write data to a file descriptor, but now you want to wait to read data from it.\nJust use \nepoll_ctl()\n with the \nEPOLL_CTL_MOD\n option to change the type of operation you're monitoring.\n\n\nevent.events = EPOLLOUT;\nevent.data.ptr = mypointer;\nepoll_ctl(epfd, EPOLL_CTL_MOD, mypointer-\nfd, \nevent);\n\n\n\nTo unsubscribe one file descriptor from epoll while leaving others active, use \nepoll_ctl()\n with the \nEPOLL_CTL_DEL\n option.\n\n\nepoll_ctl(epfd, EPOLL_CTL_DEL, mypointer-\nfd, NULL);\n\n\n\nTo shut down an epoll instance, close its file descriptor.\n\n\nclose(epfd);\n\n\n\nIn addition to nonblocking \nread()\n and \nwrite()\n, any calls to \nconnect()\n on a nonblocking socket will also be\nnonblocking. To wait for the connection to complete, use \nselect()\n or epoll to wait for the socket to be writable.", 
            "title": "Nonblocking I O, select(), and epoll"
        }, 
        {
            "location": "/Nonblocking-I-O,-select(),-and-epoll/#dont-waste-time-waiting", 
            "text": "Normally, when you call  read() , if the data is not available yet it will wait until the data is ready before the function returns.  When you're reading data from a disk, that delay may not be long, but when you're reading from a slow network connection it may take a long time for that data to arrive, if it ever arrives.    POSIX lets you set a flag on a file descriptor such that any call to  read()  on that file descriptor will return immediately, whether it has finished or not.  With your file descriptor in this mode, your call to  read()  will start\nthe read operation, and while it's working you can do other useful work.  This is called \"nonblocking\" mode,\nsince the call to  read()  doesn't block.  To set a file descriptor to be nonblocking:  // fd is my file descriptor\nint flags = fcntl(fd, F_GETFL, 0);\nfcntl(fd, F_SETFL, flags | O_NONBLOCK);  For a socket, you can create it in nonblocking mode by adding  SOCK_NONBLOCK  to the second argument to  socket() :  fd = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, 0);  When a file is in nonblocking mode and you call  read() , it will return immediately with whatever bytes are available.\nSay 100 bytes have arrived from the server at the other end of your socket and you call  read(fd, buf, 150) .\nRead will return immediately with a value of 100, meaning it read 100 of the 150 bytes you asked for.\nSay you tried to read the remaining data with a call to  read(fd, buf+100, 50) , but the last 50 bytes still hadn't\narrived yet.   read()  would return -1 and set the global error variable  errno  to either\nEAGAIN or EWOULDBLOCK.  That's the system's way of telling you the data isn't ready yet.  write()  also works in nonblocking mode.  Say you want to send 40,000 bytes to a remote server using a socket.\nThe system can only send so many bytes at a time. Common systems can send about 23,000 bytes at a time. In nonblocking mode,  write(fd, buf, 40000)  would return the number of bytes it was able to\nsend immediately, or about 23,000.  If you called  write()  right away again, it would return -1 and set errno to\nEAGAIN or EWOULDBLOCK. That's the system's way of telling you it's still busy sending the last chunk of data,\nand isn't ready to send more yet.", 
            "title": "Don't waste time waiting"
        }, 
        {
            "location": "/Nonblocking-I-O,-select(),-and-epoll/#how-do-i-check-when-the-io-has-finished", 
            "text": "There are a few ways.  Let's see how to do it using  select  and  epoll .", 
            "title": "How do I check when the I/O has finished?"
        }, 
        {
            "location": "/Nonblocking-I-O,-select(),-and-epoll/#select", 
            "text": "int select(int nfds, \n           fd_set *readfds, \n           fd_set *writefds,\n           fd_set *exceptfds, \n           struct timeval *timeout);  Given three sets of file descriptors,  select()  will wait for any of those file descriptors to become 'ready'.  readfds - a file descriptor in readfds is ready when there is data that can be read or EOF has been reached.  writefds - a file descriptor in writefds is ready when a call to write() will succeed.\n* exceptfds - system-specific, not well-defined.  Just pass NULL for this.  select()  returns the total number of file descriptors that are ready.  If none of them become\nready during the time defined by  timeout , it will return 0.  After  select()  returns, the \ncaller will need to loop\nthrough the file descriptors in readfds and/or writefds to see which ones are ready.  fd_set readfds, writefds;\nFD_ZERO( readfds);\nFD_ZERO( writefds);\nfor (int i=0; i   read_fd_count; i++)\n  FD_SET(my_read_fds[i],  readfds);\nfor (int i=0; i   write_fd_count; i++)\n  FD_SET(my_write_fds[i],  writefds);\n\nstruct timeval timeout;\ntimeout.tv_sec = 3;\ntimeout.tv_usec = 0;\n\nint num_ready = select(FD_SETSIZE,  readfds,  writefds, NULL,  timeout);\n\nif (num_ready   0) {\n  perror(\"error in select()\");\n} else if (num_ready == 0) {\n  printf(\"timeout\\n\");\n} else {\n  for (int i=0; i   read_fd_count; i++)\n    if (FD_ISSET(my_read_fds[i],  readfds))\n      printf(\"fd %d is ready for reading\\n\", my_read_fds[i]);\n  for (int i=0; i   write_fd_count; i++)\n    if (FD_ISSET(my_write_fds[i],  writefds))\n      printf(\"fd %d is ready for writing\\n\", my_write_fds[i]);\n}  For more information on select()", 
            "title": "select"
        }, 
        {
            "location": "/Nonblocking-I-O,-select(),-and-epoll/#epoll", 
            "text": "epoll  is not part of POSIX, but it is supported by Linux.  It is a more efficient way to wait for many\nfile descriptors.  It will tell you exactly which descriptors are ready. It even gives you a way to store\na small amount of data with each descriptor, like an array index or a pointer, making it easier to access\nyour data associated with that descriptor.  To use epoll, first you must create a special file descriptor with  epoll_create() .  You won't read or write to this file\ndescriptor; you'll just pass it to the other epoll_xxx functions and call\nclose() on it at the end.  epfd = epoll_create(1);  For each file descriptor you want to monitor with epoll, you'll need to add it \nto the epoll data structures \nusing  epoll_ctl()  with the  EPOLL_CTL_ADD  option.  You can add any\nnumber of file descriptors to it.  struct epoll_event event;\nevent.events = EPOLLOUT;  // EPOLLIN==read, EPOLLOUT==write\nevent.data.ptr = mypointer;\nepoll_ctl(epfd, EPOLL_CTL_ADD, mypointer- fd,  event)  To wait for some of the file descriptors to become ready, use  epoll_wait() .\nThe epoll_event struct that it fills out will contain the data you provided in event.data when you\nadded this file descriptor. This makes it easy for you to look up your own data associated\nwith this file descriptor.  int num_ready = epoll_wait(epfd,  event, 1, timeout_milliseconds);\nif (num_ready   0) {\n  MyData *mypointer = (MyData*) event.data.ptr;\n  printf(\"ready to write on %d\\n\", mypointer- fd);\n}  Say you were waiting to write data to a file descriptor, but now you want to wait to read data from it.\nJust use  epoll_ctl()  with the  EPOLL_CTL_MOD  option to change the type of operation you're monitoring.  event.events = EPOLLOUT;\nevent.data.ptr = mypointer;\nepoll_ctl(epfd, EPOLL_CTL_MOD, mypointer- fd,  event);  To unsubscribe one file descriptor from epoll while leaving others active, use  epoll_ctl()  with the  EPOLL_CTL_DEL  option.  epoll_ctl(epfd, EPOLL_CTL_DEL, mypointer- fd, NULL);  To shut down an epoll instance, close its file descriptor.  close(epfd);  In addition to nonblocking  read()  and  write() , any calls to  connect()  on a nonblocking socket will also be\nnonblocking. To wait for the connection to complete, use  select()  or epoll to wait for the socket to be writable.", 
            "title": "epoll"
        }, 
        {
            "location": "/OSI-Model/", 
            "text": "@MCQ \nWhich one of the following is NOT a feature of TCP?\n -Packet re-ordering\n -Flow control\n -Packet re-transmission\n -Simple error detection\n -Encryption @ans\nNo Hind available @hint\n@EXP\n@END", 
            "title": "OSI Model"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/", 
            "text": "What is \nerrno\n and when is it set?\n\n\nPOSIX defines a special integer \nerrno\n that is set when a system call fails.\nThe initial value of \nerrno\n is zero (i.e. no error).\nWhen a system call fails it will typically return -1 to indicate an error and set \nerrno\n\n\nWhat about multiple threads?\n\n\nEach thread has it's own copy of \nerrno\n. This is very useful; otherwise an error in one thread would interfere with the error status of another thread.\n\n\nWhen is \nerrno\n reset to zero?\n\n\nIt's not unless you specifically reset it to zero!  When system calls are successful they do \nnot\n reset the value of \nerrno\n.\n\n\nThis means you should only rely on the value of errno if you know a system call has failed (e.g. it returned -1).\n\n\nWhat are the gotchas and best practices of using \nerrno\n?\n\n\nBe careful when complex error handling use of library calls or system calls that may change the value of \nerrno\n. In practice it's safer to copy the value of errno into a int variable:\n\n\n// Unsafe - the first fprintf may change the value of errno before we use it!\nif (-1 == sem_wait(\ns)) {\n   fprintf(stderr, \nAn error occurred!\n);\n   fprintf(stderr, \nThe error value is %d\\n\n, errno);\n}\n// Better, copy the value before making more system and library calls\nif (-1 == sem_wait(\ns)) {\n   int errno_saved = errno;\n   fprintf(stderr, \nAn error occurred!\n);\n   fprintf(stderr, \nThe error value is %d\\n\n, errno_saved);\n}\n\n\n\n\nIn a similar vein, if your signal handler makes any system or library calls, then it is good practice to save the original value of errno and restore the value before returning:\n\n\nvoid handler(int signal) {\n   int errno_saved = errno;\n\n   // make system calls that might change errno\n\n   errno = errno_saved;\n}\n\n\n\n\nHow can you print out the string message associated with a particular error number?\n\n\nUse \nstrerror\n to get a short (English) description of the error value\n\n\nchar *mesg = strerror(errno);\nfprintf(stderr, \nAn error occurred (errno=%d): %s\n, errno, mesg);\n\n\n\n\nHow are perror and strerror related?\n\n\nIn previous pages we've used perror to print out the error to standard error. Using \nstrerror\n, we can now write a simple implementation of \nperror\n:\n\n\nvoid perror(char *what) {\n   fprintf(stderr, \n%s: %s\\n\n, what, strerror(errno));\n}\n\n\n\n\nWhat are the gotchas of using strerror?\n\n\nUnfortunately \nstrerror\n is not threadsafe. In other words, two threads cannot call it at the same time!\n\n\nThere are two workarounds: Firstly we can use a mutex lock to define a critical section and a local buffer. The same mutex should be used by all threads in all places that call \nstrerror\n\n\npthread_mutex_lock(\nm);\nchar *result = strerror(errno);\nchar *message = malloc(strlen(result) + 1);\nstrcpy(message, result);\npthread_mutex_unlock(\nm);\nfprintf(stderr, \nAn error occurred (errno=%d): %s\n, errno, message);\nfree(message);\n\n\n\n\nAlternatively use the less portable but thread-safe \nstrerror_r\n\n\nWhat is EINTR? What does it mean for sem_wait? read? write?\n\n\nSome system calls can be interrupted when a signal (e.g SIGCHLD, SIGPIPE,...) is delivered to the process. At this point the system call may return without performing any action! For example, bytes may not have been read/written, semaphore wait may not have waited.\n\n\nThis interruption can be detected by checking the return value and if \nerrno\n is EINTR. In which case the system call should be retried. It's common to see the following kind of loop that wraps a system call (such as sem_wait).\n\n\nwhile ((-1 == systemcall(...)) \n (errno == EINTR)) { /* repeat! */}\n\n\n\n\nBe careful to write \n== EINTR\n, not \n= EINTR\n.\n\n\nOr, if the result value needs to be used later...\n\n\nwhile ((-1 == (result = systemcall(...))) \n (errno == EINTR)) { /* repeat! */}\n\n\n\n\nOn Linux,calling \nread\n and \nwrite\n to a local disk will normally not return with EINTR (instead the function is automatically restarted for you). However, calling \nread\n and \nwrite\n on a file descriptor that corresponds to a network stream \ncan\n return with EINTR.\n\n\nWhich system calls may be interrupted and need to be wrapped?\n\n\nUse man the page! The man page includes a list of errors (i.e. errno values) that may be set by the system call. A rule of thumb is 'slow' (blocking) calls (e.g. writing to a socket) may be interrupted but fast non-blocking calls (e.g. pthread_mutex_lock) will not.\n\n\nFrom the linux signal 7 man page.\n\n\n\"If a signal handler is invoked while a system call or library function call is blocked, then either:\n\n the call is automatically restarted after the signal handler returns; or\n\n the call fails with the error EINTR.\nWhich of these two behaviors occurs depends on the interface and whether or not the signal handler was established using the SA_RESTART flag (see sigaction(2)). The details vary across UNIX systems; below, the details for Linux.\n\n\nIf a blocked call to one of the following interfaces is interrupted by a signal handler, then the call will be automatically restarted after the signal handler returns if the SA_RESTART flag was used; otherwise the call will fail with the error EINTR:\n\n\n\n\nread(2), readv(2), write(2), writev(2), and ioctl(2) calls on \"slow\" devices. A \"slow\" device is one where the I/O call may block for an indefinite time, for example, a terminal, pipe, or socket. (A disk is not a slow device according to this definition.) If an I/O call on a slow device has already transferred some data by the time it is interrupted by a signal handler, then the call will return a success status (normally, the number of bytes transferred).\n\"\n\n\n\n\nNote, it is easy to believe that setting 'SA_RESTART' flag is sufficient to make this whole problem disappear. Unfortunately that's not true: there are still system calls that may return early and set \nEINTR\n! See \nsignal(7)\n for details.", 
            "title": "POSIX, Part 1: Error handling"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#what-is-errno-and-when-is-it-set", 
            "text": "POSIX defines a special integer  errno  that is set when a system call fails.\nThe initial value of  errno  is zero (i.e. no error).\nWhen a system call fails it will typically return -1 to indicate an error and set  errno", 
            "title": "What is errno and when is it set?"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#what-about-multiple-threads", 
            "text": "Each thread has it's own copy of  errno . This is very useful; otherwise an error in one thread would interfere with the error status of another thread.", 
            "title": "What about multiple threads?"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#when-is-errno-reset-to-zero", 
            "text": "It's not unless you specifically reset it to zero!  When system calls are successful they do  not  reset the value of  errno .  This means you should only rely on the value of errno if you know a system call has failed (e.g. it returned -1).", 
            "title": "When is errno reset to zero?"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#what-are-the-gotchas-and-best-practices-of-using-errno", 
            "text": "Be careful when complex error handling use of library calls or system calls that may change the value of  errno . In practice it's safer to copy the value of errno into a int variable:  // Unsafe - the first fprintf may change the value of errno before we use it!\nif (-1 == sem_wait( s)) {\n   fprintf(stderr,  An error occurred! );\n   fprintf(stderr,  The error value is %d\\n , errno);\n}\n// Better, copy the value before making more system and library calls\nif (-1 == sem_wait( s)) {\n   int errno_saved = errno;\n   fprintf(stderr,  An error occurred! );\n   fprintf(stderr,  The error value is %d\\n , errno_saved);\n}  In a similar vein, if your signal handler makes any system or library calls, then it is good practice to save the original value of errno and restore the value before returning:  void handler(int signal) {\n   int errno_saved = errno;\n\n   // make system calls that might change errno\n\n   errno = errno_saved;\n}", 
            "title": "What are the gotchas and best practices of using errno?"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#how-can-you-print-out-the-string-message-associated-with-a-particular-error-number", 
            "text": "Use  strerror  to get a short (English) description of the error value  char *mesg = strerror(errno);\nfprintf(stderr,  An error occurred (errno=%d): %s , errno, mesg);", 
            "title": "How can you print out the string message associated with a particular error number?"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#how-are-perror-and-strerror-related", 
            "text": "In previous pages we've used perror to print out the error to standard error. Using  strerror , we can now write a simple implementation of  perror :  void perror(char *what) {\n   fprintf(stderr,  %s: %s\\n , what, strerror(errno));\n}", 
            "title": "How are perror and strerror related?"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#what-are-the-gotchas-of-using-strerror", 
            "text": "Unfortunately  strerror  is not threadsafe. In other words, two threads cannot call it at the same time!  There are two workarounds: Firstly we can use a mutex lock to define a critical section and a local buffer. The same mutex should be used by all threads in all places that call  strerror  pthread_mutex_lock( m);\nchar *result = strerror(errno);\nchar *message = malloc(strlen(result) + 1);\nstrcpy(message, result);\npthread_mutex_unlock( m);\nfprintf(stderr,  An error occurred (errno=%d): %s , errno, message);\nfree(message);  Alternatively use the less portable but thread-safe  strerror_r", 
            "title": "What are the gotchas of using strerror?"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#what-is-eintr-what-does-it-mean-for-sem_wait-read-write", 
            "text": "Some system calls can be interrupted when a signal (e.g SIGCHLD, SIGPIPE,...) is delivered to the process. At this point the system call may return without performing any action! For example, bytes may not have been read/written, semaphore wait may not have waited.  This interruption can be detected by checking the return value and if  errno  is EINTR. In which case the system call should be retried. It's common to see the following kind of loop that wraps a system call (such as sem_wait).  while ((-1 == systemcall(...))   (errno == EINTR)) { /* repeat! */}  Be careful to write  == EINTR , not  = EINTR .  Or, if the result value needs to be used later...  while ((-1 == (result = systemcall(...)))   (errno == EINTR)) { /* repeat! */}  On Linux,calling  read  and  write  to a local disk will normally not return with EINTR (instead the function is automatically restarted for you). However, calling  read  and  write  on a file descriptor that corresponds to a network stream  can  return with EINTR.", 
            "title": "What is EINTR? What does it mean for sem_wait? read? write?"
        }, 
        {
            "location": "/POSIX,-Part-1:-Error-handling/#which-system-calls-may-be-interrupted-and-need-to-be-wrapped", 
            "text": "Use man the page! The man page includes a list of errors (i.e. errno values) that may be set by the system call. A rule of thumb is 'slow' (blocking) calls (e.g. writing to a socket) may be interrupted but fast non-blocking calls (e.g. pthread_mutex_lock) will not.  From the linux signal 7 man page.  \"If a signal handler is invoked while a system call or library function call is blocked, then either:  the call is automatically restarted after the signal handler returns; or  the call fails with the error EINTR.\nWhich of these two behaviors occurs depends on the interface and whether or not the signal handler was established using the SA_RESTART flag (see sigaction(2)). The details vary across UNIX systems; below, the details for Linux.  If a blocked call to one of the following interfaces is interrupted by a signal handler, then the call will be automatically restarted after the signal handler returns if the SA_RESTART flag was used; otherwise the call will fail with the error EINTR:   read(2), readv(2), write(2), writev(2), and ioctl(2) calls on \"slow\" devices. A \"slow\" device is one where the I/O call may block for an indefinite time, for example, a terminal, pipe, or socket. (A disk is not a slow device according to this definition.) If an I/O call on a slow device has already transferred some data by the time it is interrupted by a signal handler, then the call will return a success status (normally, the number of bytes transferred).\n\"   Note, it is easy to believe that setting 'SA_RESTART' flag is sufficient to make this whole problem disappear. Unfortunately that's not true: there are still system calls that may return early and set  EINTR ! See  signal(7)  for details.", 
            "title": "Which system calls may be interrupted and need to be wrapped?"
        }, 
        {
            "location": "/Pipe:-Review-Questions/", 
            "text": "Question numbers subject to change\n\n\n\n\nQ1\n\n\nFill in the blanks to make the following program print 123456789. If \ncat\n is given no arguments it simply prints its input until EOF. Bonus: Explain why the \nclose\n call below is necessary.\n\n\nint main() {\n  int i = 0;\n  while(++i \n 10) {\n    pid_t pid = fork();\n    if(pid == 0) { /* child */\n      char buffer[16];\n      sprintf(buffer, ______,i);\n      int fds[ ______];\n      pipe( fds);\n      write( fds[1], ______,______ ); // Write the buffer into the pipe\n      close(  ______ );\n      dup2( fds[0],  ______);\n      execlp( \ncat\n, \ncat\n,  ______ );\n      perror(\nexec\n); exit(1);\n    }\n    waitpid(pid, NULL, 0);\n  }\n  return 0;\n}\n\n\n\n\nQ2\n\n\nUse POSIX calls \nfork\n \npipe\n \ndup2\n and \nclose\n to implement an autograding program. Capture the standardoutput of a child process into a pipe. The child process should \nexec\n the program \n./test\n with no additional arguments (other than the process name). In the parent process read from the pipe: Exit the parent process as soon as the captured output contains the ! character. Before exiting the parent process send SIGKILL to the child process. Exit 0 if the output contained a !. Otherwise if the child process exits causing the pipe write end to be closed, then exit with a value of 1. Be sure to close the unused ends of the pipe in the parent and child process\n\n\nQ3 (Advanced)\n\n\nThis advanced challenge uses pipes to get an \"AI player\" to play itself until the game is complete.\nThe program \ntictactoe\n accepts a line of input - the sequence of turns made so far, prints the same sequence followed by another turn, and then exits. A turn is specified using two characters. For example \"A1\" and \"C3\" are two opposite corner positions. The string \nB2A1A3\n is a game of 3 turns/plys. A valid response is \nB2A1A3C1\n (the C1 response blocks the diagonal B2 A3 threat). The output line may also include a suffix \n-I win\n \n-You win\n \n-invalid\n or \n-draw\n\nUse pipes to control the input and output of each child process created. When the output contains a \n-\n, print the final output line (the entire game sequence and the result) and exit.", 
            "title": "Pipe: Review Questions"
        }, 
        {
            "location": "/Pipe:-Review-Questions/#q1", 
            "text": "Fill in the blanks to make the following program print 123456789. If  cat  is given no arguments it simply prints its input until EOF. Bonus: Explain why the  close  call below is necessary.  int main() {\n  int i = 0;\n  while(++i   10) {\n    pid_t pid = fork();\n    if(pid == 0) { /* child */\n      char buffer[16];\n      sprintf(buffer, ______,i);\n      int fds[ ______];\n      pipe( fds);\n      write( fds[1], ______,______ ); // Write the buffer into the pipe\n      close(  ______ );\n      dup2( fds[0],  ______);\n      execlp(  cat ,  cat ,  ______ );\n      perror( exec ); exit(1);\n    }\n    waitpid(pid, NULL, 0);\n  }\n  return 0;\n}", 
            "title": "Q1"
        }, 
        {
            "location": "/Pipe:-Review-Questions/#q2", 
            "text": "Use POSIX calls  fork   pipe   dup2  and  close  to implement an autograding program. Capture the standardoutput of a child process into a pipe. The child process should  exec  the program  ./test  with no additional arguments (other than the process name). In the parent process read from the pipe: Exit the parent process as soon as the captured output contains the ! character. Before exiting the parent process send SIGKILL to the child process. Exit 0 if the output contained a !. Otherwise if the child process exits causing the pipe write end to be closed, then exit with a value of 1. Be sure to close the unused ends of the pipe in the parent and child process", 
            "title": "Q2"
        }, 
        {
            "location": "/Pipe:-Review-Questions/#q3-advanced", 
            "text": "This advanced challenge uses pipes to get an \"AI player\" to play itself until the game is complete.\nThe program  tictactoe  accepts a line of input - the sequence of turns made so far, prints the same sequence followed by another turn, and then exits. A turn is specified using two characters. For example \"A1\" and \"C3\" are two opposite corner positions. The string  B2A1A3  is a game of 3 turns/plys. A valid response is  B2A1A3C1  (the C1 response blocks the diagonal B2 A3 threat). The output line may also include a suffix  -I win   -You win   -invalid  or  -draw \nUse pipes to control the input and output of each child process created. When the output contains a  - , print the final output line (the entire game sequence and the result) and exit.", 
            "title": "Q3 (Advanced)"
        }, 
        {
            "location": "/Pipes,-Part-1:-Introduction-to-pipes/", 
            "text": "What is a pipe?\n\n\nA POSIX pipe is almost like its real counterpart - you can stuff bytes down one end and they will appear at the other end in the same order. Unlike real pipes however, the flow is always in the same direction, one file descriptor is used for reading and the other for writing. The \npipe\n system call is used to create a pipe.\n\n\nint filedes[2];\npipe (filedes);\nprintf(\nread from %d, write to %d\\n\n, filedes[0], filedes[1]);\n\n\n\n\nThese file descriptors can be used with \nread\n -\n\n\n// To read...\nchar buffer[80];\nint bytesread = read(filedes[0], buffer, sizeof(buffer));\n\n\n\n\nAnd \nwrite\n - \n\n\nwrite(filedes[1], \nGo!\n, 4);\n\n\n\n\nHow can I use pipe to communicate with a child process?\n\n\nA common method of using pipes is to create the pipe before forking.\n\n\nint filedes[2];\npipe (filedes);\npid_t child = fork();\nif (child \n 0) { /* I must be the parent */\n    char buffer[80];\n    int bytesread = read(filedes[0], buffer, sizeof(buffer));\n    // do something with the bytes read    \n}\n\n\n\n\nThe child can then send a message back to the parent:\n\n\nif (child == 0) {\n   write(filedes[1], \ndone\n, 4);\n}\n\n\n\n\nCan I use pipes inside a single process?\n\n\nShort answer: Yes, but I'm not sure why you would want to LOL!\n\n\nHere's an example program that sends a message to itself:\n\n\n#include \nunistd.h\n\n#include \nstdlib.h\n\n#include \nstdio.h\n\n\nint main() {\n    int fh[2];\n    pipe(fh);\n    FILE *reader = fdopen(fh[0], \nr\n);\n    FILE *writer = fdopen(fh[1], \nw\n);\n    // Hurrah now I can use printf rather than using low-level read() write()\n    printf(\nWriting...\\n\n);\n    fprintf(writer,\n%d %d %d\\n\n, 10, 20, 30);\n    fflush(writer);\n\n    printf(\nReading...\\n\n);\n    int results[3];\n    int ok = fscanf(reader,\n%d %d %d\n, results, results + 1, results + 2);\n    printf(\n%d values parsed: %d %d %d\\n\n, ok, results[0], results[1], results[2]);\n\n    return 0;\n}\n\n\n\n\nThe problem with using a pipe in this fashion is that writing to a pipe can block i.e. the pipe only has a limited buffering capacity. If the pipe is full the writing process will block! The maximum size of the buffer is system dependent; typical values from  4KB upto 128KB.\n\n\nint main() {\n    int fh[2];\n    pipe(fh);\n    int b = 0;\n    #define MESG \n...............................\n\n    while(1) {\n        printf(\n%d\\n\n,b);\n        write(fh[1], MESG, sizeof(MESG))\n        b+=sizeof(MESG);\n    }\n    return 0;\n}\n\n\n\n\nSee [[Pipes, Part 2: Pipe programming secrets]]", 
            "title": "Pipes, Part 1: Introduction to pipes"
        }, 
        {
            "location": "/Pipes,-Part-1:-Introduction-to-pipes/#what-is-a-pipe", 
            "text": "A POSIX pipe is almost like its real counterpart - you can stuff bytes down one end and they will appear at the other end in the same order. Unlike real pipes however, the flow is always in the same direction, one file descriptor is used for reading and the other for writing. The  pipe  system call is used to create a pipe.  int filedes[2];\npipe (filedes);\nprintf( read from %d, write to %d\\n , filedes[0], filedes[1]);  These file descriptors can be used with  read  -  // To read...\nchar buffer[80];\nint bytesread = read(filedes[0], buffer, sizeof(buffer));  And  write  -   write(filedes[1],  Go! , 4);", 
            "title": "What is a pipe?"
        }, 
        {
            "location": "/Pipes,-Part-1:-Introduction-to-pipes/#how-can-i-use-pipe-to-communicate-with-a-child-process", 
            "text": "A common method of using pipes is to create the pipe before forking.  int filedes[2];\npipe (filedes);\npid_t child = fork();\nif (child   0) { /* I must be the parent */\n    char buffer[80];\n    int bytesread = read(filedes[0], buffer, sizeof(buffer));\n    // do something with the bytes read    \n}  The child can then send a message back to the parent:  if (child == 0) {\n   write(filedes[1],  done , 4);\n}", 
            "title": "How can I use pipe to communicate with a child process?"
        }, 
        {
            "location": "/Pipes,-Part-1:-Introduction-to-pipes/#can-i-use-pipes-inside-a-single-process", 
            "text": "Short answer: Yes, but I'm not sure why you would want to LOL!  Here's an example program that sends a message to itself:  #include  unistd.h \n#include  stdlib.h \n#include  stdio.h \n\nint main() {\n    int fh[2];\n    pipe(fh);\n    FILE *reader = fdopen(fh[0],  r );\n    FILE *writer = fdopen(fh[1],  w );\n    // Hurrah now I can use printf rather than using low-level read() write()\n    printf( Writing...\\n );\n    fprintf(writer, %d %d %d\\n , 10, 20, 30);\n    fflush(writer);\n\n    printf( Reading...\\n );\n    int results[3];\n    int ok = fscanf(reader, %d %d %d , results, results + 1, results + 2);\n    printf( %d values parsed: %d %d %d\\n , ok, results[0], results[1], results[2]);\n\n    return 0;\n}  The problem with using a pipe in this fashion is that writing to a pipe can block i.e. the pipe only has a limited buffering capacity. If the pipe is full the writing process will block! The maximum size of the buffer is system dependent; typical values from  4KB upto 128KB.  int main() {\n    int fh[2];\n    pipe(fh);\n    int b = 0;\n    #define MESG  ............................... \n    while(1) {\n        printf( %d\\n ,b);\n        write(fh[1], MESG, sizeof(MESG))\n        b+=sizeof(MESG);\n    }\n    return 0;\n}  See [[Pipes, Part 2: Pipe programming secrets]]", 
            "title": "Can I use pipes inside a single process?"
        }, 
        {
            "location": "/Pipes,-Part-2:-Pipe-programming-secrets/", 
            "text": "Pipe Gotchas (1)\n\n\nHere's a complete example that doesn't work! The child reads one byte at a time from the pipe and prints it out - but we never see the message! Can you see why?\n\n\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nunistd.h\n\n#include \nsignal.h\n\n\nint main() {\n    int fd[2];\n    pipe(fd);\n    //You must read from fd[0] and write from fd[1]\n    printf(\nReading from %d, writing to %d\\n\n, fd[0], fd[1]);\n\n    pid_t p = fork();\n    if (p \n 0) {\n        /* I have a child therefore I am the parent*/\n        write(fd[1],\nHi Child!\n,9);\n\n        /*don't forget your child*/\n        wait(NULL);\n    } else {\n        char buf;\n        int bytesread;\n        // read one byte at a time.\n        while ((bytesread = read(fd[0], \nbuf, 1)) \n 0) {\n            putchar(buf);\n        }\n    }\n    return 0;\n}\n\n\n\n\n\nThe parent sends the bytes \nH,i,(space),C...!\n into the pipe (this may block if the pipe is full).\nThe child starts reading the pipe one byte at a time. In the above case, the child process will read and print each character. However it never leaves the while loop! When there are no characters left to read it simply blocks and waits for more. The call \nputchar\n writes the characters out but we never flush the buffer.\n\n\nTo see the message we could flush the buffer (e.g. fflush(stdout) or printf(\"\\n\"))\nor better, let's look for the end of message '!'\n\n\n        while ((bytesread = read(fd[0], \nbuf, 1)) \n 0) {\n            putchar(buf);\n            if (buf == '!') break; /* End of message */\n        }\n\n\n\n\nAnd the message will be flushed to the terminal when the child process exits.\n\n\nWant to use pipes with printf and scanf? Use fdopen!\n\n\nPOSIX file descriptors are simple integers 0,1,2,3...\nAt the C library level, C wraps these with a buffer and useful functions like printf and scanf, so we that we can easily print or parse integers, strings etc.\nIf you already have a file descriptor then you can 'wrap' it yourself into a FILE pointer using \nfdopen\n :\n\n\n#include \nsys/types.h\n\n#include \nsys/stat.h\n\n#include \nfcntl.h\n\n\nint main() {\n    char *name=\nFred\n;\n    int score = 123;\n    int filedes = open(\nmydata.txt\n, \nw\n, O_CREAT, S_IWUSR | S_IRUSR);\n\n    FILE *f = fdopen(filedes, \nw\n);\n    fprintf(f, \nName:%s Score:%d\\n\n, name, score);\n    fclose(f);\n\n\n\n\nFor writing to files this is unnecessary - just use \nfopen\n which does the same as \nopen\n and \nfdopen\n\nHowever for pipes, we already have a file descriptor - so this is great time to use \nfdopen\n!\n\n\nHere's a complete example using pipes that almost works! Can you spot the error? Hint: The parent never prints anything!\n\n\n#include \nunistd.h\n\n#include \nstdlib.h\n\n#include \nstdio.h\n\n\nint main() {\n    int fh[2];\n    pipe(fh);\n    FILE *reader = fdopen(fh[0], \nr\n);\n    FILE *writer = fdopen(fh[1], \nw\n);\n    pid_t p = fork();\n    if (p \n 0) {\n        int score;\n        fscanf(reader, \nScore %d\n, \nscore);\n        printf(\nThe child says the score is %d\\n\n, score);\n    } else {\n        fprintf(writer, \nScore %d\n, 10 + 10);\n        fflush(writer);\n    }\n    return 0;\n}\n\n\n\n\nNote the (unnamed) pipe resource will disappear once both the child and parent have exited. In the above example the child will send the bytes and the parent will receive the bytes from the pipe. However, no end-of-line character is ever sent, so \nfscanf\n will continue to ask for bytes because it is waiting for the end of the line i.e. it will wait forever! The fix is to ensure we send a newline character, so that \nfscanf\n will return.\n\n\nchange:   fprintf(writer, \nScore %d\n, 10 + 10);\nto:       fprintf(writer, \nScore %d\\n\n, 10 + 10);\n\n\n\n\nSo do we need to \nfflush\n too?\nYes, if you want your bytes to be sent to the pipe immediately! At the beginning of this course we assumed that file streams are always \nline buffered\n i.e. the C library will flush its buffer everytime you send a newline character. Actually this is only true for terminal streams - for other filestreams the C library attempts to improve performance by only flushing when it's internal buffer is full or the file is closed.\n\n\nWhen do I need two pipes?\n\n\nIf you need to send data to and from a child asynchronously, then two pipes are required (one for each direction).\nOtherwise the child would attempt to read its own data intended for the parent (and vice versa)!\n\n\nClosing pipes gotchas\n\n\nProcesses receive the signal SIGPIPE when no process is listening! From the pipe(2) man page - \n\n\nIf all file descriptors referring to the read end of a pipe have been closed,\n then a write(2) will cause a SIGPIPE signal to be generated for the calling process. \n\n\n\n\nTip: Notice only the writer (not a reader) can use this signal.\nTo inform the reader that a writer is closing their end of the pipe, you could write your own special byte (e.g. 0xff) or a message ( \n\"Bye!\"\n)\n\n\nHere's an example of catching this signal that does not work! Can you see why?\n\n\n#include \nstdio.h\n\n#include \nstdio.h\n\n#include \nunistd.h\n\n#include \nsignal.h\n\n\nvoid no_one_listening(int signal) {\n    write(1, \nNo one is listening!\\n\n, 21);\n}\n\nint main() {\n    signal(SIGPIPE, no_one_listening);\n    int filedes[2];\n\n    pipe(filedes);\n    pid_t child = fork();\n    if (child \n 0) { \n        /* I must be the parent. Close the listening end of the pipe */\n        /* I'm not listening anymore!*/\n        close(filedes[0]);\n    } else {\n        /* Child writes messages to the pipe */\n        write(filedes[1], \nOne\n, 3);\n        sleep(2);\n        // Will this write generate SIGPIPE ?\n        write(filedes[1], \nTwo\n, 3);\n        write(1, \nDone\\n\n, 5);\n    }\n    return 0;\n}\n\n\n\n\nThe mistake in above code is that there is still a reader for the pipe! The child still has the pipe's first file descriptor open and remember the specification? All readers must be closed.\n\n\nWhen forking, \nIt is common practice\n to close the unnecessary (unused) end of each pipe in the child and parent process. For example the parent might close the reading end and the child might close the writing end (and vice versa if you have two pipes)\n\n\nThe lifetime of pipes\n\n\nUnnamed pipes (the kind we've seen up to this point) live in memory (do not take up any disk space) and are a simple and efficient form of inter-process communication (IPC) that is useful for streaming data and simple messages. Once all processes have closed, the pipe resources are freed.\n\n\nAn alternative to \nunamed\n pipes is \nnamed\n pipes created using \nmkfifo\n - more about these in a future lecture.", 
            "title": "Pipes, Part 2: Pipe programming secrets"
        }, 
        {
            "location": "/Pipes,-Part-2:-Pipe-programming-secrets/#pipe-gotchas-1", 
            "text": "Here's a complete example that doesn't work! The child reads one byte at a time from the pipe and prints it out - but we never see the message! Can you see why?  #include  stdio.h \n#include  stdlib.h \n#include  unistd.h \n#include  signal.h \n\nint main() {\n    int fd[2];\n    pipe(fd);\n    //You must read from fd[0] and write from fd[1]\n    printf( Reading from %d, writing to %d\\n , fd[0], fd[1]);\n\n    pid_t p = fork();\n    if (p   0) {\n        /* I have a child therefore I am the parent*/\n        write(fd[1], Hi Child! ,9);\n\n        /*don't forget your child*/\n        wait(NULL);\n    } else {\n        char buf;\n        int bytesread;\n        // read one byte at a time.\n        while ((bytesread = read(fd[0],  buf, 1))   0) {\n            putchar(buf);\n        }\n    }\n    return 0;\n}  The parent sends the bytes  H,i,(space),C...!  into the pipe (this may block if the pipe is full).\nThe child starts reading the pipe one byte at a time. In the above case, the child process will read and print each character. However it never leaves the while loop! When there are no characters left to read it simply blocks and waits for more. The call  putchar  writes the characters out but we never flush the buffer.  To see the message we could flush the buffer (e.g. fflush(stdout) or printf(\"\\n\"))\nor better, let's look for the end of message '!'          while ((bytesread = read(fd[0],  buf, 1))   0) {\n            putchar(buf);\n            if (buf == '!') break; /* End of message */\n        }  And the message will be flushed to the terminal when the child process exits.", 
            "title": "Pipe Gotchas (1)"
        }, 
        {
            "location": "/Pipes,-Part-2:-Pipe-programming-secrets/#want-to-use-pipes-with-printf-and-scanf-use-fdopen", 
            "text": "POSIX file descriptors are simple integers 0,1,2,3...\nAt the C library level, C wraps these with a buffer and useful functions like printf and scanf, so we that we can easily print or parse integers, strings etc.\nIf you already have a file descriptor then you can 'wrap' it yourself into a FILE pointer using  fdopen  :  #include  sys/types.h \n#include  sys/stat.h \n#include  fcntl.h \n\nint main() {\n    char *name= Fred ;\n    int score = 123;\n    int filedes = open( mydata.txt ,  w , O_CREAT, S_IWUSR | S_IRUSR);\n\n    FILE *f = fdopen(filedes,  w );\n    fprintf(f,  Name:%s Score:%d\\n , name, score);\n    fclose(f);  For writing to files this is unnecessary - just use  fopen  which does the same as  open  and  fdopen \nHowever for pipes, we already have a file descriptor - so this is great time to use  fdopen !  Here's a complete example using pipes that almost works! Can you spot the error? Hint: The parent never prints anything!  #include  unistd.h \n#include  stdlib.h \n#include  stdio.h \n\nint main() {\n    int fh[2];\n    pipe(fh);\n    FILE *reader = fdopen(fh[0],  r );\n    FILE *writer = fdopen(fh[1],  w );\n    pid_t p = fork();\n    if (p   0) {\n        int score;\n        fscanf(reader,  Score %d ,  score);\n        printf( The child says the score is %d\\n , score);\n    } else {\n        fprintf(writer,  Score %d , 10 + 10);\n        fflush(writer);\n    }\n    return 0;\n}  Note the (unnamed) pipe resource will disappear once both the child and parent have exited. In the above example the child will send the bytes and the parent will receive the bytes from the pipe. However, no end-of-line character is ever sent, so  fscanf  will continue to ask for bytes because it is waiting for the end of the line i.e. it will wait forever! The fix is to ensure we send a newline character, so that  fscanf  will return.  change:   fprintf(writer,  Score %d , 10 + 10);\nto:       fprintf(writer,  Score %d\\n , 10 + 10);  So do we need to  fflush  too?\nYes, if you want your bytes to be sent to the pipe immediately! At the beginning of this course we assumed that file streams are always  line buffered  i.e. the C library will flush its buffer everytime you send a newline character. Actually this is only true for terminal streams - for other filestreams the C library attempts to improve performance by only flushing when it's internal buffer is full or the file is closed.", 
            "title": "Want to use pipes with printf and scanf? Use fdopen!"
        }, 
        {
            "location": "/Pipes,-Part-2:-Pipe-programming-secrets/#when-do-i-need-two-pipes", 
            "text": "If you need to send data to and from a child asynchronously, then two pipes are required (one for each direction).\nOtherwise the child would attempt to read its own data intended for the parent (and vice versa)!", 
            "title": "When do I need two pipes?"
        }, 
        {
            "location": "/Pipes,-Part-2:-Pipe-programming-secrets/#closing-pipes-gotchas", 
            "text": "Processes receive the signal SIGPIPE when no process is listening! From the pipe(2) man page -   If all file descriptors referring to the read end of a pipe have been closed,\n then a write(2) will cause a SIGPIPE signal to be generated for the calling process.   Tip: Notice only the writer (not a reader) can use this signal.\nTo inform the reader that a writer is closing their end of the pipe, you could write your own special byte (e.g. 0xff) or a message (  \"Bye!\" )  Here's an example of catching this signal that does not work! Can you see why?  #include  stdio.h \n#include  stdio.h \n#include  unistd.h \n#include  signal.h \n\nvoid no_one_listening(int signal) {\n    write(1,  No one is listening!\\n , 21);\n}\n\nint main() {\n    signal(SIGPIPE, no_one_listening);\n    int filedes[2];\n\n    pipe(filedes);\n    pid_t child = fork();\n    if (child   0) { \n        /* I must be the parent. Close the listening end of the pipe */\n        /* I'm not listening anymore!*/\n        close(filedes[0]);\n    } else {\n        /* Child writes messages to the pipe */\n        write(filedes[1],  One , 3);\n        sleep(2);\n        // Will this write generate SIGPIPE ?\n        write(filedes[1],  Two , 3);\n        write(1,  Done\\n , 5);\n    }\n    return 0;\n}  The mistake in above code is that there is still a reader for the pipe! The child still has the pipe's first file descriptor open and remember the specification? All readers must be closed.  When forking,  It is common practice  to close the unnecessary (unused) end of each pipe in the child and parent process. For example the parent might close the reading end and the child might close the writing end (and vice versa if you have two pipes)", 
            "title": "Closing pipes gotchas"
        }, 
        {
            "location": "/Pipes,-Part-2:-Pipe-programming-secrets/#the-lifetime-of-pipes", 
            "text": "Unnamed pipes (the kind we've seen up to this point) live in memory (do not take up any disk space) and are a simple and efficient form of inter-process communication (IPC) that is useful for streaming data and simple messages. Once all processes have closed, the pipe resources are freed.  An alternative to  unamed  pipes is  named  pipes created using  mkfifo  - more about these in a future lecture.", 
            "title": "The lifetime of pipes"
        }, 
        {
            "location": "/Process-Control,-Part-1:-Wait-macros,-using-signals/", 
            "text": "Can I find out the exit value of my child?\n\n\nYou can find the lowest 8 bits of the child's exit value (the return value of \nmain()\n or value included in \nexit()\n): Use the \"Wait macros\" - typically you will use \"WIFEXITED\" and \"WEXITSTATUS\" . See \nwait\n/\nwaitpid\n man page for more information).\n\n\nint status;\npid_t child = fork();\nif (child == -1) return 1; //Failed\nif (child \n 0) { /* I am the parent - wait for the child to finish */\n  pid_t pid = waitpid(child, \nstatus, 0);\n  if (pid != -1 \n WIFEXITED(status)) {\n     int low8bits = WEXITSTATUS(status);\n     printf(\nProcess %d returned %d\n , pid, low8bits);\n  }\n} else { /* I am the child */\n // do something interesting\n  execl(\n/bin/ls\n, \n/bin/ls\n, \n.\n, (char *) NULL); // \nls .\n\n}\n\n\n\n\nCan I pause my child?\n\n\nYes ! You can temporarily pause a running process by sending it a SIGSTOP signal.\nIf it succeeds it will freeze a process; i.e. the process will not be allocated anymore CPU time.\n\n\nTo allow a process to resume execution send it the SIGCONT signal.\n\n\nFor example,\nHere's program that slowly prints a dot every second, up to 59 dots.\n\n\n#include \nunistd.h\n\n#include \nstdio.h\n\nint main() {\n  printf(\nMy pid is %d\\n\n, getpid() );\n  int i = 60;\n  while(--i) { \n    write(1, \n.\n,1);\n    sleep(1);\n  }\n  write(1, \nDone!\n,5);\n  return 0;\n}\n\n\n\n\nWe will first start the process in the background (notice the \n at the end).\nThen send it a signal from the shell process by using the kill command.\n\n\n./program \n\nMy pid is 403\n...\n\nkill -SIGSTOP 403\n\nkill -SIGCONT 403\n\n\n\n\nHow do I kill/stop/suspend my child from C?\n\n\nIn C, send a signal to the child using \nkill\n POSIX call,\n\n\nkill(child, SIGUSR1); // Send a user-defined signal\nkill(child, SIGSTOP); // Stop the child process (the child cannot prevent this)\nkill(child, SIGTERM); // Terminate the child process (the child can prevent this)\nkill(child, SIGINT); // Equivalent to CTRL-C (by default closes the process)\n\n\n\n\nAs we saw above there is also a kill command available in the shell\ne.g. get a list of running processes and then terminate process 45 and process 46\n\n\nps\nkill -l \nkill -9 45\nkill -s TERM 46\n\n\n\n\nHow can I detect \"CTRL-C\" and clean up gracefully?\n\n\nWe will return to signals later on - this is just a short introduction. On a Linux system, see \nman -s7 signal\n if you are interested in finding out more (for example a list of system and library calls that are async-signal-safe.\n\n\nThere are strict limitations on the executable code inside a signal handler. Most library and system calls are not 'async-signal-safe' - they may be not use used inside a signal handler because they are not re-entrant safe. In a single-threaded program, signal handling momentarily interrupts the program execution to execute the signal handler code instead. Suppose your original program was interrupted while executing the library code of \nmalloc\n ;  the memory structures used by malloc will not be in a consistent state. Calling \nprintf\n (which uses \nmalloc\n) as part of the signal handler is unsafe and will result in \"undefined behavior\" i.e. it is no longer a useful,predictable program. In practice your program might crash, compute or generate incorrect results or stop functioning (\"deadlock\"), depending on exactly what your program was executing when it was interrupted to execute the signal handler code.\n\n\nOne common use of signal handlers is to set a boolean flag that is occasionally polled (read) as part of the normal running of the program. For example,\n\n\nint pleaseStop ; // See notes on why \nvolatile sig_atomic_t\n is better\n\nvoid handle_sigint(int signal) {\n  pleaseStop = 1;\n}\n\nint main() {\n  signal(SIGINT, handle_sigint);\n  pleaseStop = 0;\n  while ( ! pleaseStop) { \n     /* application logic here */ \n   }\n  /* cleanup code here */\n}\n\n\n\n\nThe above code might appear to be correct on paper. However, we need to provide a hint to the compiler and to the CPU core that will execute the \nmain()\n loop. We need to prevent a compiler optimization: The expression \n! pleaseStop\n appears to be a loop invariant i.e. true forever, so can be simplified to \ntrue\n.  Secondly, we need to ensure that the value of \npleaseStop\n is not cached using a CPU register and instead always read from and written to main memory. The \nsig_atomic_t\n type implies that all the bits of the variable can be read or modified as an \"atomic operation\" - a single uninterruptable operation. It is impossible to read a value that is composed of some new bit values and old bit values.\n\n\nBy specifying \npleaseStop\n with the correct type \nvolatile sig_atomic_t\n we can write portable code where the main loop will be exited after the signal handler returns. The \nsig_atomic_t\n type can be as large as an \nint\n on most modern platforms but on embedded systems can be as small as a \nchar\n and only able to represent (-127 to 127) values.\n\n\nvolatile sig_atomic_t pleaseStop;\n\n\n\n\nTwo examples of this pattern can be found in \"COMP\" a terminal based 1Hz 4bit computer (https://github.com/gto76/comp-cpp/blob/1bf9a77eaf8f57f7358a316e5bbada97f2dc8987/src/output.c#L121).\nTwo boolean flags are used. One to mark the delivery of \nSIGINT\n (CTRL-C), and gracefully shutdown the program, and the other to mark \nSIGWINCH\n signal to detect terminal resize and redraw the entire display.", 
            "title": "Process Control, Part 1: Wait macros, using signals"
        }, 
        {
            "location": "/Process-Control,-Part-1:-Wait-macros,-using-signals/#can-i-find-out-the-exit-value-of-my-child", 
            "text": "You can find the lowest 8 bits of the child's exit value (the return value of  main()  or value included in  exit() ): Use the \"Wait macros\" - typically you will use \"WIFEXITED\" and \"WEXITSTATUS\" . See  wait / waitpid  man page for more information).  int status;\npid_t child = fork();\nif (child == -1) return 1; //Failed\nif (child   0) { /* I am the parent - wait for the child to finish */\n  pid_t pid = waitpid(child,  status, 0);\n  if (pid != -1   WIFEXITED(status)) {\n     int low8bits = WEXITSTATUS(status);\n     printf( Process %d returned %d  , pid, low8bits);\n  }\n} else { /* I am the child */\n // do something interesting\n  execl( /bin/ls ,  /bin/ls ,  . , (char *) NULL); //  ls . \n}", 
            "title": "Can I find out the exit value of my child?"
        }, 
        {
            "location": "/Process-Control,-Part-1:-Wait-macros,-using-signals/#can-i-pause-my-child", 
            "text": "Yes ! You can temporarily pause a running process by sending it a SIGSTOP signal.\nIf it succeeds it will freeze a process; i.e. the process will not be allocated anymore CPU time.  To allow a process to resume execution send it the SIGCONT signal.  For example,\nHere's program that slowly prints a dot every second, up to 59 dots.  #include  unistd.h \n#include  stdio.h \nint main() {\n  printf( My pid is %d\\n , getpid() );\n  int i = 60;\n  while(--i) { \n    write(1,  . ,1);\n    sleep(1);\n  }\n  write(1,  Done! ,5);\n  return 0;\n}  We will first start the process in the background (notice the   at the end).\nThen send it a signal from the shell process by using the kill command.  ./program  \nMy pid is 403\n... kill -SIGSTOP 403 kill -SIGCONT 403", 
            "title": "Can I pause my child?"
        }, 
        {
            "location": "/Process-Control,-Part-1:-Wait-macros,-using-signals/#how-do-i-killstopsuspend-my-child-from-c", 
            "text": "In C, send a signal to the child using  kill  POSIX call,  kill(child, SIGUSR1); // Send a user-defined signal\nkill(child, SIGSTOP); // Stop the child process (the child cannot prevent this)\nkill(child, SIGTERM); // Terminate the child process (the child can prevent this)\nkill(child, SIGINT); // Equivalent to CTRL-C (by default closes the process)  As we saw above there is also a kill command available in the shell\ne.g. get a list of running processes and then terminate process 45 and process 46  ps\nkill -l \nkill -9 45\nkill -s TERM 46", 
            "title": "How do I kill/stop/suspend my child from C?"
        }, 
        {
            "location": "/Process-Control,-Part-1:-Wait-macros,-using-signals/#how-can-i-detect-ctrl-c-and-clean-up-gracefully", 
            "text": "We will return to signals later on - this is just a short introduction. On a Linux system, see  man -s7 signal  if you are interested in finding out more (for example a list of system and library calls that are async-signal-safe.  There are strict limitations on the executable code inside a signal handler. Most library and system calls are not 'async-signal-safe' - they may be not use used inside a signal handler because they are not re-entrant safe. In a single-threaded program, signal handling momentarily interrupts the program execution to execute the signal handler code instead. Suppose your original program was interrupted while executing the library code of  malloc  ;  the memory structures used by malloc will not be in a consistent state. Calling  printf  (which uses  malloc ) as part of the signal handler is unsafe and will result in \"undefined behavior\" i.e. it is no longer a useful,predictable program. In practice your program might crash, compute or generate incorrect results or stop functioning (\"deadlock\"), depending on exactly what your program was executing when it was interrupted to execute the signal handler code.  One common use of signal handlers is to set a boolean flag that is occasionally polled (read) as part of the normal running of the program. For example,  int pleaseStop ; // See notes on why  volatile sig_atomic_t  is better\n\nvoid handle_sigint(int signal) {\n  pleaseStop = 1;\n}\n\nint main() {\n  signal(SIGINT, handle_sigint);\n  pleaseStop = 0;\n  while ( ! pleaseStop) { \n     /* application logic here */ \n   }\n  /* cleanup code here */\n}  The above code might appear to be correct on paper. However, we need to provide a hint to the compiler and to the CPU core that will execute the  main()  loop. We need to prevent a compiler optimization: The expression  ! pleaseStop  appears to be a loop invariant i.e. true forever, so can be simplified to  true .  Secondly, we need to ensure that the value of  pleaseStop  is not cached using a CPU register and instead always read from and written to main memory. The  sig_atomic_t  type implies that all the bits of the variable can be read or modified as an \"atomic operation\" - a single uninterruptable operation. It is impossible to read a value that is composed of some new bit values and old bit values.  By specifying  pleaseStop  with the correct type  volatile sig_atomic_t  we can write portable code where the main loop will be exited after the signal handler returns. The  sig_atomic_t  type can be as large as an  int  on most modern platforms but on embedded systems can be as small as a  char  and only able to represent (-127 to 127) values.  volatile sig_atomic_t pleaseStop;  Two examples of this pattern can be found in \"COMP\" a terminal based 1Hz 4bit computer (https://github.com/gto76/comp-cpp/blob/1bf9a77eaf8f57f7358a316e5bbada97f2dc8987/src/output.c#L121).\nTwo boolean flags are used. One to mark the delivery of  SIGINT  (CTRL-C), and gracefully shutdown the program, and the other to mark  SIGWINCH  signal to detect terminal resize and redraw the entire display.", 
            "title": "How can I detect \"CTRL-C\" and clean up gracefully?"
        }, 
        {
            "location": "/Programming-Tricks,-Part-1/", 
            "text": "Use \ncat\n as your IDE\n\n\nWho needs an editor? IDE? We can just use \ncat\n!\nYou've seen \ncat\n being used to read the contents of files but it can also be used to read the  standard-input and send it back to standard output.\n\n\n$ cat\nHELLO\nHELLO\n\n\n\n\nTo finish reading from the input stream close the input stream by pressing \nCTRL-D\n\n\nLet's use \ncat\n to send standard input to a file. We will use '\n' to redirect its output to a file:\n\n\n$ cat \n myprog.c\n#include \nstdio.h\n\nint main() {printf(\nHi!\n);return 0;}\n\n\n\n\n(Be careful! Deletes and undos are not allowed...)\nPress \nCTRL-D\n when finished.\n\n\nEdit your code with \nperl\n regular expressions (aka \"remember your perl pie\")\n\n\nA useful trick if you have several text files (e.g. source code) to change is to use regular expressions.\n\nperl\n makes this very easy to edit files in place.\nJust remember 'perl pie' and search on the web...\n\n\nAn example. Suppose we want to change the sequence \"Hi\" to \"Bye\" in all .c files in the current directory. Then we can write a simple substitution pattern that will be executed on each line at time in all files:\n\n\n$ perl -p -i -e 's/Hi/Bye/' *.c\n\n\n\n\n(Don't panic if you get it wrong, original files are still there; they just have the extension .bak)\nObviously there's a lot more you can do with regular expressions than changing Hi to Bye.\n\n\nUse your shell \n!!\n\n\nTo re-run the last command just type \n!!\n and press \nreturn\n\nTo re-run the last command that started with g type \n!g\n  and press \nreturn\n\n\nUse your shell \n\n\nTired of running \nmake\n or \ngcc\n and then running the program if it compiled OK? Instead, use \n to chain these commands together\n\n\n$ gcc program.c \n ./a.out\n\n\n\n\nMake can do more than make\n\n\nYou might also try putting a line in your Makefile that will compile, and then run your program.\n\n\nrun : $(program)\n        ./$(program)\n\n\n\n\nThen running\n\n\n$ make run\n\n\n\n\nwill make sure any changes you've made are compiled, and run your program in one go. Also good for testing many inputs at once. Although you probably would just rather write a regular shell script for that.\n\n\nIs your neighbor too productive? C pre-procesors to the rescue!\n\n\nUse the C pre-processor to redefine common keywords e.g.\n\n\n#define if while\n\n\n\n\nProtip: Put this line inside one of the standard includes e.g. /usr/include/stdio.h\n\n\nWho needs functions when you C have the preprocessor\n\n\nOK, so this is more of a gotcha. Be careful when using macros that look like functions...\n\n\n#define min(a,b) a\nb?a:b\n\n\n\n\nA perfectly reasonable definition of a minimum of a and b. However the pre-processor is just a simple\ntext wrangler so precedence can bite you:\n\n\nint value = -min(2,3); // Should be -2?\n\n\n\n\nIs expanded to \n\n\nint value = -2\n3 ? 2 :3; // Ooops.. result will be 2\n\n\n\n\nA partial fix is to wrap every argument with \n()\n and also the whole expression with ():\n\n\n#define min(a,b) (  (a) \n (b) ?(a):(b) )\n\n\n\n\nHowever this is still \nnot\n a function! For example can you see why \nmin(i++,10)\n might increment i once or twice!?", 
            "title": "Programming Tricks, Part 1"
        }, 
        {
            "location": "/Programming-Tricks,-Part-1/#use-cat-as-your-ide", 
            "text": "Who needs an editor? IDE? We can just use  cat !\nYou've seen  cat  being used to read the contents of files but it can also be used to read the  standard-input and send it back to standard output.  $ cat\nHELLO\nHELLO  To finish reading from the input stream close the input stream by pressing  CTRL-D  Let's use  cat  to send standard input to a file. We will use ' ' to redirect its output to a file:  $ cat   myprog.c\n#include  stdio.h \nint main() {printf( Hi! );return 0;}  (Be careful! Deletes and undos are not allowed...)\nPress  CTRL-D  when finished.", 
            "title": "Use cat as your IDE"
        }, 
        {
            "location": "/Programming-Tricks,-Part-1/#edit-your-code-with-perl-regular-expressions-aka-remember-your-perl-pie", 
            "text": "A useful trick if you have several text files (e.g. source code) to change is to use regular expressions. perl  makes this very easy to edit files in place.\nJust remember 'perl pie' and search on the web...  An example. Suppose we want to change the sequence \"Hi\" to \"Bye\" in all .c files in the current directory. Then we can write a simple substitution pattern that will be executed on each line at time in all files:  $ perl -p -i -e 's/Hi/Bye/' *.c  (Don't panic if you get it wrong, original files are still there; they just have the extension .bak)\nObviously there's a lot more you can do with regular expressions than changing Hi to Bye.", 
            "title": "Edit your code with perl regular expressions (aka \"remember your perl pie\")"
        }, 
        {
            "location": "/Programming-Tricks,-Part-1/#use-your-shell", 
            "text": "To re-run the last command just type  !!  and press  return \nTo re-run the last command that started with g type  !g   and press  return", 
            "title": "Use your shell !!"
        }, 
        {
            "location": "/Programming-Tricks,-Part-1/#use-your-shell_1", 
            "text": "Tired of running  make  or  gcc  and then running the program if it compiled OK? Instead, use   to chain these commands together  $ gcc program.c   ./a.out", 
            "title": "Use your shell &amp;&amp;"
        }, 
        {
            "location": "/Programming-Tricks,-Part-1/#make-can-do-more-than-make", 
            "text": "You might also try putting a line in your Makefile that will compile, and then run your program.  run : $(program)\n        ./$(program)  Then running  $ make run  will make sure any changes you've made are compiled, and run your program in one go. Also good for testing many inputs at once. Although you probably would just rather write a regular shell script for that.", 
            "title": "Make can do more than make"
        }, 
        {
            "location": "/Programming-Tricks,-Part-1/#is-your-neighbor-too-productive-c-pre-procesors-to-the-rescue", 
            "text": "Use the C pre-processor to redefine common keywords e.g.  #define if while  Protip: Put this line inside one of the standard includes e.g. /usr/include/stdio.h", 
            "title": "Is your neighbor too productive? C pre-procesors to the rescue!"
        }, 
        {
            "location": "/Programming-Tricks,-Part-1/#who-needs-functions-when-you-c-have-the-preprocessor", 
            "text": "OK, so this is more of a gotcha. Be careful when using macros that look like functions...  #define min(a,b) a b?a:b  A perfectly reasonable definition of a minimum of a and b. However the pre-processor is just a simple\ntext wrangler so precedence can bite you:  int value = -min(2,3); // Should be -2?  Is expanded to   int value = -2 3 ? 2 :3; // Ooops.. result will be 2  A partial fix is to wrap every argument with  ()  and also the whole expression with ():  #define min(a,b) (  (a)   (b) ?(a):(b) )  However this is still  not  a function! For example can you see why  min(i++,10)  might increment i once or twice!?", 
            "title": "Who needs functions when you C have the preprocessor"
        }, 
        {
            "location": "/Pthreads,-Part-1:-Introduction/", 
            "text": "What is a thread?\n\n\nA thread is short for 'thread-of-execution'. It represents the sequence of instructions that the CPU has (and will) execute. To remember how to return from function calls, and to store the values of automatic variables and  parameters a thread uses a stack.\n\n\nHow does the thread's stack work?\n\n\nYour main function (and other functions you might call) have automatic variables. We will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the \"stack pointer\"). If the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables. Once it returns from a function, we can move the stack pointer back up to its previous value. We keep a copy of the old stack pointer value - on the stack! This is why returning from a function is very quick - it's easy to 'free' the memory used by automatic variables - we just need to change the stack pointer.\n\n\n\n\nHow many threads can my process have?\n\n\nYou can have more than one thread running inside a process. You get the first thread for free! It runs the code you write inside 'main'. If you need more threads you can call \npthread_create\n to create a new thread using the pthread library. You'll need to pass a pointer to a function so that the thread knows where to start.\n\n\nThe threads you create all live inside the same virtual memory because they are part of the same process. Thus they can all see the heap, the global variables and the program code etc. Thus you can have two (or more) CPUs working on your program at the same time and inside the same process. It's up to the operating system to assign the threads to CPUs. If you have more active threads than CPUs then the kernel will assign the thread to a CPU for a short duration (or until it runs out of things to do) and then will automatically switch the CPU to work on another thread. \nFor example, one CPU might be processing the game AI while another thread is computing the graphics output.\n\n\nHello world pthread example\n\n\nTo use pthreads you will need to include \npthread.h\n AND you need to compile with \n-pthread\n (or \n-lpthread\n) compiler option. This option tells the compiler that your program requires threading support\n\n\nTo create a thread use the function \npthread_create\n. This function takes four arguments:\n\n\nint pthread_create(pthread_t *thread, const pthread_attr_t *attr,\n                   void *(*start_routine) (void *), void *arg);\n\n\n\n\n\n\nThe first is a pointer to a variable that will hold the id of the newly created thread.\n\n\nThe second is a pointer to attributes that we can use to tweak and tune some of the advanced features of pthreads.\n\n\nThe third is a pointer to a function that we want to run\n\n\nFourth is a pointer that will be given to our function\n\n\n\n\nThe argument \nvoid *(*start_routine) (void *)\n is difficult to read! It means a pointer that takes a \nvoid *\n pointer and returns a \nvoid *\n pointer. It looks like a function declaration except that the name of the function is wrapped with \n(* .... )\n\n\nHere's the simplest example:\n\n\n#include \nstdio.h\n\n#include \npthread.h\n\n// remember to set compilation option -pthread\n\nvoid *busy(void *ptr) {\n// ptr will point to \nHi\n\n    puts(\nHello World\n);\n    return NULL;\n}\nint main() {\n    pthread_t id;\n    pthread_create(\nid, NULL, busy, \nHi\n);\n    while (1) {} // Loop forever\n}\n\n\n\n\nIf we want to wait for our thread to finish use \npthread_join\n\n\nvoid *result;\npthread_join(id, \nresult);\n\n\n\n\nIn the above example, \nresult\n will be \nnull\n because the busy function returned \nnull\n.\nWe need to pass the address-of result because \npthread_join\n will be writing into the contents of our pointer.\n\n\nSee \nPthreads Part 2", 
            "title": "Pthreads, Part 1: Introduction"
        }, 
        {
            "location": "/Pthreads,-Part-1:-Introduction/#what-is-a-thread", 
            "text": "A thread is short for 'thread-of-execution'. It represents the sequence of instructions that the CPU has (and will) execute. To remember how to return from function calls, and to store the values of automatic variables and  parameters a thread uses a stack.", 
            "title": "What is a thread?"
        }, 
        {
            "location": "/Pthreads,-Part-1:-Introduction/#how-does-the-threads-stack-work", 
            "text": "Your main function (and other functions you might call) have automatic variables. We will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the \"stack pointer\"). If the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables. Once it returns from a function, we can move the stack pointer back up to its previous value. We keep a copy of the old stack pointer value - on the stack! This is why returning from a function is very quick - it's easy to 'free' the memory used by automatic variables - we just need to change the stack pointer.", 
            "title": "How does the thread's stack work?"
        }, 
        {
            "location": "/Pthreads,-Part-1:-Introduction/#how-many-threads-can-my-process-have", 
            "text": "You can have more than one thread running inside a process. You get the first thread for free! It runs the code you write inside 'main'. If you need more threads you can call  pthread_create  to create a new thread using the pthread library. You'll need to pass a pointer to a function so that the thread knows where to start.  The threads you create all live inside the same virtual memory because they are part of the same process. Thus they can all see the heap, the global variables and the program code etc. Thus you can have two (or more) CPUs working on your program at the same time and inside the same process. It's up to the operating system to assign the threads to CPUs. If you have more active threads than CPUs then the kernel will assign the thread to a CPU for a short duration (or until it runs out of things to do) and then will automatically switch the CPU to work on another thread. \nFor example, one CPU might be processing the game AI while another thread is computing the graphics output.", 
            "title": "How many threads can my process have?"
        }, 
        {
            "location": "/Pthreads,-Part-1:-Introduction/#hello-world-pthread-example", 
            "text": "To use pthreads you will need to include  pthread.h  AND you need to compile with  -pthread  (or  -lpthread ) compiler option. This option tells the compiler that your program requires threading support  To create a thread use the function  pthread_create . This function takes four arguments:  int pthread_create(pthread_t *thread, const pthread_attr_t *attr,\n                   void *(*start_routine) (void *), void *arg);   The first is a pointer to a variable that will hold the id of the newly created thread.  The second is a pointer to attributes that we can use to tweak and tune some of the advanced features of pthreads.  The third is a pointer to a function that we want to run  Fourth is a pointer that will be given to our function   The argument  void *(*start_routine) (void *)  is difficult to read! It means a pointer that takes a  void *  pointer and returns a  void *  pointer. It looks like a function declaration except that the name of the function is wrapped with  (* .... )  Here's the simplest example:  #include  stdio.h \n#include  pthread.h \n// remember to set compilation option -pthread\n\nvoid *busy(void *ptr) {\n// ptr will point to  Hi \n    puts( Hello World );\n    return NULL;\n}\nint main() {\n    pthread_t id;\n    pthread_create( id, NULL, busy,  Hi );\n    while (1) {} // Loop forever\n}  If we want to wait for our thread to finish use  pthread_join  void *result;\npthread_join(id,  result);  In the above example,  result  will be  null  because the busy function returned  null .\nWe need to pass the address-of result because  pthread_join  will be writing into the contents of our pointer.  See  Pthreads Part 2", 
            "title": "Hello world pthread example"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/", 
            "text": "How do I create a pthread?\n\n\nSee \nPthreads Part 1\n which introduces \npthread_create\n and \npthread_join\n\n\nIf I call \npthread_create\n twice how many stacks does my process have?\n\n\nYour process will contain three stacks - one for each thread. The first thread is created when the process starts and you created two more. Actually there can be more stacks than this but let's ignore that complication for now. The important idea is that each thread requires a stack because the stack contains automatic variables and the old CPU PC register so that it can back to executing the calling function after the function is finished.\n\n\nWhat is the difference between a process and a thread?\n\n\nIn addition, unlike processes, threads within the same process can share the same global memory (data and heap segments).\n\n\nWhat does \npthread_cancel\n do?\n\n\nStops a thread. Note the thread may not actually be stopped immediately. For example it can be terminated when the thread makes an operating system call (e.g. \nwrite\n).\n\n\nIn practice \npthread_cancel\n is rarely used because it does not give a thread an opportunity to clean up after itself (for example, it may have opened some files).\nAn alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.\n\n\nWhat is the difference between \nexit\n and \npthread_exit\n?\n\n\nexit(42)\n exits the entire process and sets the processes exit value.  This is equivalent to \nreturn 42\n in the main method. All threads inside the process are stopped.\n\n\npthread_exit(void *)\n only stops the calling thread i.e. the thread never returns after calling \npthread_exit\n. The pthread library will automatically finish the process if there are no other threads running. \npthread_exit(...)\n is equivalent to returning from the thread's function; both finish the thread and also set the return value (void *pointer) for the thread.\n\n\nCalling \npthread_exit\n in the the \nmain\n thread is a common way for simple programs to ensure that all threads finish. For example, in the following program, the  \nmyfunc\n threads will probably not have time to get started.\n\n\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(\ntid1, NULL, myfunc, \nJabberwocky\n);\n  pthread_create(\ntid2, NULL, myfunc, \nVorpel\n);\n  exit(42); //or return 42;\n\n  // No code is run after exit\n}\n\n\n\n\nThe next two programs will wait for the new threads to finish-\n\n\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(\ntid1, NULL, myfunc, \nJabberwocky\n);\n  pthread_create(\ntid2, NULL, myfunc, \nVorpel\n);\n  pthread_exit(NULL); \n\n  // No code is run after pthread_exit\n  // However process will continue to exist until both threads have finished\n}\n\n\n\n\nAlternatively, we join on each thread (i.e. wait for it to finish) before we return from main (or call exit).\n\n\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(\ntid1, NULL, myfunc, \nJabberwocky\n);\n  pthread_create(\ntid2, NULL, myfunc, \nVorpel\n);\n  // wait for both threads to finish :\n  void* result;\n  pthread_join(tid1, \nresult);\n  pthread_join(tid2, \nresult); \n  return 42;\n}\n\n\n\n\nNote the pthread_exit version creates thread zombies, however this is not a long-running processes, so we don't care.\n\n\nHow can a thread be terminated?\n\n\n\n\nReturning from the thread function\n\n\nCalling \npthread_exit\n\n\nCancelling the thread with \npthread_cancel\n\n\nTerminating the process (e.g. SIGTERM); exit(); returning from \nmain\n\n\n\n\nWhat is the purpose of pthread_join?\n\n\n\n\nWait for a thread to finish\n\n\nClean up thread resources.\n\n\n\n\nWhat happens if you don't call \npthread_join\n?\n\n\nFinished threads will continue to consume resources. Eventually, if enough threads are created, \npthread_create\n will fail.\nIn practice, this is only an issue for long-runnning processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits.\n\n\nShould I use \npthread_exit\n or \npthread_join\n?\n\n\nBoth \npthread_exit\n and \npthread_join\n will let the other threads finish on their own (even if called in the main thread). However, only \npthread_join\n will return to you when the specified thread finishes. \npthread_exit\n does not wait and will immediately end your thread and give you no chance to continue executing.\n\n\nCan you pass pointers to stack variables from one thread to another?\n\n\nYes. However you need to be very careful about the lifetime of stack variables.\n\n\npthread_t start_threads() {\n  int start = 42;\n  pthread_t tid;\n  pthread_create(\ntid, 0, myfunc, \nstart); // ERROR!\n  return tid;\n}\n\n\n\n\nThe above code is invalid because the function \nstart_threads\n will likely return before \nmyfunc\n even starts. The function passes the address-of \nstart\n, however by the time \nmyfunc\n is executes, \nstart\n is no longer in scope and its address will re-used for another variable.\n\n\nThe following code is valid because the lifetime of the stack variable is longer than the background thread.\n\n\nvoid start_threads() {\n  int start = 42;\n  void *result;\n  pthread_t tid;\n  pthread_create(\ntid, 0, myfunc, \nstart); // OK - start will be valid!\n  pthread_join(tid, \nresult);\n}\n\n\n\n\nHow can I create ten threads with different starting values.\n\n\nThe following code is supposed to start ten threads with values 0,1,2,3,...9\nHowever, when run prints out \n1 7 8 8 8 8 8 8 8 10\n! Can you see why?\n\n\n#include \npthread.h\n\nvoid* myfunc(void* ptr) {\n    int i = *((int *) ptr);\n    printf(\n%d \n, i);\n    return NULL;\n}\n\nint main() {\n    // Each thread gets a different value of i to process\n    int i;\n    pthread_t tid;\n    for(i =0; i \n 10; i++) {\n        pthread_create(\ntid, NULL, myfunc, \ni); // ERROR\n    }\n    pthread_exit(NULL);\n}\n\n\n\n\nThe above code suffers from a \nrace condition\n - the value of i is changing. The new threads start later (in the example output the last thread starts after the loop has finished).\n\n\nTo overcome this race-condition, we will give each thread a pointer to it's own data area. For example, for each thread we may want to store the id, a starting value and an output value:\n\n\nstruct T {\n  pthread_t id;\n  int start;\n  char result[100];\n};\n\n\n\n\nThese can be stored in an array - \n\n\nstruct T *info = calloc(10 , sizeof(struct T)); // reserve enough bytes for ten T structures\n\n\n\n\nAnd each array element passed to each thread - \n\n\npthread_create(\ninfo[i].id, NULL, func, \ninfo[i]);\n\n\n\n\nWhy are some functions e.g.  asctime,getenv, strtok, strerror  not thread-safe?\n\n\nTo answer this, let's look at a simple function that is also not 'thread-safe'\n\n\nchar *to_message(int num) {\n    char static result [256];\n    if (num \n 10) sprintf(result, \n%d : blah blah\n , num);\n    else strcpy(result, \nUnknown\n);\n    return result;\n}\n\n\n\n\nIn the above code the result buffer is stored in global memory. This is good - we wouldn't want to return a pointer to an invalid address on the stack, but there's only one result buffer in the entire memory. If two threads were to use it at the same time then one would corrupt the other:\n\n\n\n\n\n\n\n\nTime\n\n\nThread 1\n\n\nThread 2\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n1\n\n\nto_m(5 )\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n\nto_m(99)\n\n\nNow both threads will see \"Unknown\" stored in the result buffer\n\n\n\n\n\n\n\n\nWhat are condition variables, semaphores, mutexes?\n\n\nThese are synchronization locks that are used to prevent race conditions and ensure proper synchronization between threads running in the same program. In addition these locks are conceptually identical to the primitives used inside the kernel.\n\n\nAre there any advantages of using threads over forking processes?\n\n\nYes! Sharing information between threads is easy because threads (of the same process) live inside the same virtual memory space.\nAlso, creating a thread is significantly faster than creating(forking) a process.\n\n\nAre there any dis-advantages of using threads over forking processes?\n\n\nYes! No- isolation! As threads live inside the same process, one thread has access to the same virtual memory as the other threads. A single thread can terminate the entire process (e.g. by trying to read address zero).\n\n\nCan you fork a process with multiple threads?\n\n\nYes! However the child process only has a single thread (which is a clone of the thread that called \nfork\n. We can see this as a simple example, where the background threads never print out a second message in the child process.\n\n\n#include \npthread.h\n\n#include \nstdio.h\n\n#include \nunistd.h\n\n\nstatic pid_t child = -2;\n\nvoid *sleepnprint(void *arg) {\n  printf(\n%d:%s starting up...\\n\n, getpid(), (char *) arg);\n\n  while (child == -2) {sleep(1);} /* Later we will use condition variables */\n\n  printf(\n%d:%s finishing...\\n\n,getpid(), (char*)arg);\n\n  return NULL;  \n}\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create(\ntid1,NULL, sleepnprint, \nNew Thread One\n);\n  pthread_create(\ntid2,NULL, sleepnprint, \nNew Thread Two\n);\n\n  child = fork();\n  printf(\n%d:%s\\n\n,getpid(), \nfork()ing complete\n);\n  sleep(3);\n\n  printf(\n%d:%s\\n\n,getpid(), \nMain thread finished\n);\n\n  pthread_exit(NULL);\n  return 0; /* Never executes */\n}\n\n\n\n\n8970:New Thread One starting up...\n8970:fork()ing complete\n8973:fork()ing complete\n8970:New Thread Two starting up...\n8970:New Thread Two finishing...\n8970:New Thread One finishing...\n8970:Main thread finished\n8973:Main thread finished\n\n\n\n\nIn practice creating threads before forking can lead to unexpected errors because (as demonstrated above) the other threads are immediately terminated when forking. Another thread might have just lock a mutex (e.g. by calling malloc) and never unlock it again. Advanced users may find \npthread_atfork\n useful however we suggest you usually try to avoid creating threads before forking unless you fully understand the limitations and difficulties of this approach.\n\n\nAre there other reasons where \nfork\n might be preferable to creating a thread.\n\n\nCreating separate processes is useful \n\n When more security is desired (for example, Chrome browser uses different processes for different tabs)\n\n When running an existing and complete program then a new process is required (e.g. starting 'gcc')\n\n\nHow can I find out more?\n\n\nSee the complete example in the \nman page\n\nAnd the \npthread reference guide\n\nALSO: \nConcise third party sample code explaining create, join and exit", 
            "title": "Pthreads, Part 2: Usage in Practice"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#how-do-i-create-a-pthread", 
            "text": "See  Pthreads Part 1  which introduces  pthread_create  and  pthread_join", 
            "title": "How do I create a pthread?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#if-i-call-pthread_create-twice-how-many-stacks-does-my-process-have", 
            "text": "Your process will contain three stacks - one for each thread. The first thread is created when the process starts and you created two more. Actually there can be more stacks than this but let's ignore that complication for now. The important idea is that each thread requires a stack because the stack contains automatic variables and the old CPU PC register so that it can back to executing the calling function after the function is finished.", 
            "title": "If I call pthread_create twice how many stacks does my process have?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#what-is-the-difference-between-a-process-and-a-thread", 
            "text": "In addition, unlike processes, threads within the same process can share the same global memory (data and heap segments).", 
            "title": "What is the difference between a process and a thread?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#what-does-pthread_cancel-do", 
            "text": "Stops a thread. Note the thread may not actually be stopped immediately. For example it can be terminated when the thread makes an operating system call (e.g.  write ).  In practice  pthread_cancel  is rarely used because it does not give a thread an opportunity to clean up after itself (for example, it may have opened some files).\nAn alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.", 
            "title": "What does pthread_cancel do?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#what-is-the-difference-between-exit-and-pthread_exit", 
            "text": "exit(42)  exits the entire process and sets the processes exit value.  This is equivalent to  return 42  in the main method. All threads inside the process are stopped.  pthread_exit(void *)  only stops the calling thread i.e. the thread never returns after calling  pthread_exit . The pthread library will automatically finish the process if there are no other threads running.  pthread_exit(...)  is equivalent to returning from the thread's function; both finish the thread and also set the return value (void *pointer) for the thread.  Calling  pthread_exit  in the the  main  thread is a common way for simple programs to ensure that all threads finish. For example, in the following program, the   myfunc  threads will probably not have time to get started.  int main() {\n  pthread_t tid1, tid2;\n  pthread_create( tid1, NULL, myfunc,  Jabberwocky );\n  pthread_create( tid2, NULL, myfunc,  Vorpel );\n  exit(42); //or return 42;\n\n  // No code is run after exit\n}  The next two programs will wait for the new threads to finish-  int main() {\n  pthread_t tid1, tid2;\n  pthread_create( tid1, NULL, myfunc,  Jabberwocky );\n  pthread_create( tid2, NULL, myfunc,  Vorpel );\n  pthread_exit(NULL); \n\n  // No code is run after pthread_exit\n  // However process will continue to exist until both threads have finished\n}  Alternatively, we join on each thread (i.e. wait for it to finish) before we return from main (or call exit).  int main() {\n  pthread_t tid1, tid2;\n  pthread_create( tid1, NULL, myfunc,  Jabberwocky );\n  pthread_create( tid2, NULL, myfunc,  Vorpel );\n  // wait for both threads to finish :\n  void* result;\n  pthread_join(tid1,  result);\n  pthread_join(tid2,  result); \n  return 42;\n}  Note the pthread_exit version creates thread zombies, however this is not a long-running processes, so we don't care.", 
            "title": "What is the difference between exit and pthread_exit?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#how-can-a-thread-be-terminated", 
            "text": "Returning from the thread function  Calling  pthread_exit  Cancelling the thread with  pthread_cancel  Terminating the process (e.g. SIGTERM); exit(); returning from  main", 
            "title": "How can a thread be terminated?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#what-is-the-purpose-of-pthread_join", 
            "text": "Wait for a thread to finish  Clean up thread resources.", 
            "title": "What is the purpose of pthread_join?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#what-happens-if-you-dont-call-pthread_join", 
            "text": "Finished threads will continue to consume resources. Eventually, if enough threads are created,  pthread_create  will fail.\nIn practice, this is only an issue for long-runnning processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits.", 
            "title": "What happens if you don't call pthread_join?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#should-i-use-pthread_exit-or-pthread_join", 
            "text": "Both  pthread_exit  and  pthread_join  will let the other threads finish on their own (even if called in the main thread). However, only  pthread_join  will return to you when the specified thread finishes.  pthread_exit  does not wait and will immediately end your thread and give you no chance to continue executing.", 
            "title": "Should I use pthread_exit or pthread_join?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#can-you-pass-pointers-to-stack-variables-from-one-thread-to-another", 
            "text": "Yes. However you need to be very careful about the lifetime of stack variables.  pthread_t start_threads() {\n  int start = 42;\n  pthread_t tid;\n  pthread_create( tid, 0, myfunc,  start); // ERROR!\n  return tid;\n}  The above code is invalid because the function  start_threads  will likely return before  myfunc  even starts. The function passes the address-of  start , however by the time  myfunc  is executes,  start  is no longer in scope and its address will re-used for another variable.  The following code is valid because the lifetime of the stack variable is longer than the background thread.  void start_threads() {\n  int start = 42;\n  void *result;\n  pthread_t tid;\n  pthread_create( tid, 0, myfunc,  start); // OK - start will be valid!\n  pthread_join(tid,  result);\n}", 
            "title": "Can you pass pointers to stack variables from one thread to another?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#how-can-i-create-ten-threads-with-different-starting-values", 
            "text": "The following code is supposed to start ten threads with values 0,1,2,3,...9\nHowever, when run prints out  1 7 8 8 8 8 8 8 8 10 ! Can you see why?  #include  pthread.h \nvoid* myfunc(void* ptr) {\n    int i = *((int *) ptr);\n    printf( %d  , i);\n    return NULL;\n}\n\nint main() {\n    // Each thread gets a different value of i to process\n    int i;\n    pthread_t tid;\n    for(i =0; i   10; i++) {\n        pthread_create( tid, NULL, myfunc,  i); // ERROR\n    }\n    pthread_exit(NULL);\n}  The above code suffers from a  race condition  - the value of i is changing. The new threads start later (in the example output the last thread starts after the loop has finished).  To overcome this race-condition, we will give each thread a pointer to it's own data area. For example, for each thread we may want to store the id, a starting value and an output value:  struct T {\n  pthread_t id;\n  int start;\n  char result[100];\n};  These can be stored in an array -   struct T *info = calloc(10 , sizeof(struct T)); // reserve enough bytes for ten T structures  And each array element passed to each thread -   pthread_create( info[i].id, NULL, func,  info[i]);", 
            "title": "How can I create ten threads with different starting values."
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#why-are-some-functions-eg-asctimegetenv-strtok-strerror-not-thread-safe", 
            "text": "To answer this, let's look at a simple function that is also not 'thread-safe'  char *to_message(int num) {\n    char static result [256];\n    if (num   10) sprintf(result,  %d : blah blah  , num);\n    else strcpy(result,  Unknown );\n    return result;\n}  In the above code the result buffer is stored in global memory. This is good - we wouldn't want to return a pointer to an invalid address on the stack, but there's only one result buffer in the entire memory. If two threads were to use it at the same time then one would corrupt the other:     Time  Thread 1  Thread 2  Comments      1  to_m(5 )      2   to_m(99)  Now both threads will see \"Unknown\" stored in the result buffer", 
            "title": "Why are some functions e.g.  asctime,getenv, strtok, strerror  not thread-safe?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#what-are-condition-variables-semaphores-mutexes", 
            "text": "These are synchronization locks that are used to prevent race conditions and ensure proper synchronization between threads running in the same program. In addition these locks are conceptually identical to the primitives used inside the kernel.", 
            "title": "What are condition variables, semaphores, mutexes?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#are-there-any-advantages-of-using-threads-over-forking-processes", 
            "text": "Yes! Sharing information between threads is easy because threads (of the same process) live inside the same virtual memory space.\nAlso, creating a thread is significantly faster than creating(forking) a process.", 
            "title": "Are there any advantages of using threads over forking processes?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#are-there-any-dis-advantages-of-using-threads-over-forking-processes", 
            "text": "Yes! No- isolation! As threads live inside the same process, one thread has access to the same virtual memory as the other threads. A single thread can terminate the entire process (e.g. by trying to read address zero).", 
            "title": "Are there any dis-advantages of using threads over forking processes?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#can-you-fork-a-process-with-multiple-threads", 
            "text": "Yes! However the child process only has a single thread (which is a clone of the thread that called  fork . We can see this as a simple example, where the background threads never print out a second message in the child process.  #include  pthread.h \n#include  stdio.h \n#include  unistd.h \n\nstatic pid_t child = -2;\n\nvoid *sleepnprint(void *arg) {\n  printf( %d:%s starting up...\\n , getpid(), (char *) arg);\n\n  while (child == -2) {sleep(1);} /* Later we will use condition variables */\n\n  printf( %d:%s finishing...\\n ,getpid(), (char*)arg);\n\n  return NULL;  \n}\nint main() {\n  pthread_t tid1, tid2;\n  pthread_create( tid1,NULL, sleepnprint,  New Thread One );\n  pthread_create( tid2,NULL, sleepnprint,  New Thread Two );\n\n  child = fork();\n  printf( %d:%s\\n ,getpid(),  fork()ing complete );\n  sleep(3);\n\n  printf( %d:%s\\n ,getpid(),  Main thread finished );\n\n  pthread_exit(NULL);\n  return 0; /* Never executes */\n}  8970:New Thread One starting up...\n8970:fork()ing complete\n8973:fork()ing complete\n8970:New Thread Two starting up...\n8970:New Thread Two finishing...\n8970:New Thread One finishing...\n8970:Main thread finished\n8973:Main thread finished  In practice creating threads before forking can lead to unexpected errors because (as demonstrated above) the other threads are immediately terminated when forking. Another thread might have just lock a mutex (e.g. by calling malloc) and never unlock it again. Advanced users may find  pthread_atfork  useful however we suggest you usually try to avoid creating threads before forking unless you fully understand the limitations and difficulties of this approach.", 
            "title": "Can you fork a process with multiple threads?"
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#are-there-other-reasons-where-fork-might-be-preferable-to-creating-a-thread", 
            "text": "Creating separate processes is useful   When more security is desired (for example, Chrome browser uses different processes for different tabs)  When running an existing and complete program then a new process is required (e.g. starting 'gcc')", 
            "title": "Are there other reasons where fork might be preferable to creating a thread."
        }, 
        {
            "location": "/Pthreads,-Part-2:-Usage-in-Practice/#how-can-i-find-out-more", 
            "text": "See the complete example in the  man page \nAnd the  pthread reference guide \nALSO:  Concise third party sample code explaining create, join and exit", 
            "title": "How can I find out more?"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/", 
            "text": "What is RPC?\n\n\nRemote Procedure Call. RPC is the idea that we can execute a procedure (function) on a different machine. In practice the procedure may execute on the same machine, however it may be in a different context - for example under a different user with different permissions and different lifecycle.\n\n\nWhat is Privilege Separation?\n\n\nThe remote code will execute under a different user and with different privileges from the caller. In practice the remote call may execute with more or fewer privileges than the caller. This in principle can be used to improve the security of a system (by ensuring components operate with least privilege). Unfortunately, security concerns need to be carefully assessed to ensure that RPC mechanisms cannot be subverted to perform unwanted actions. For example, an RPC implementation may implicitly trust any connected client to perform any action, rather than a subset of actions on a subset of the data.\n\n\nWhat is stub code? What is marshalling?\n\n\nThe stub code is the necessary code to hide the complexity of performing a remote procedure call. One of the roles of the stub code is to \nmarshall\n the necessary data into a format that can be sent as a byte stream to a remote server.\n\n\n// On the outside 'getHiscore' looks like a normal function call\n// On the inside the stub code performs all of the work to send and receive the data to and from the remote machine.\n\nint getHiscore(char* game) {\n  // Marshall the request into a sequence of bytes:\n  char* buffer;\n  asprintf(\nbuffer,\ngetHiscore(%s)!\n, name);\n\n  // Send down the wire (we do not send the zero byte; the '!' signifies the end of the message)\n  write(fd, buffer, strlen(buffer) );\n\n  // Wait for the server to send a response\n  ssize_t bytesread = read(fd, buffer, sizeof(buffer));\n\n  // Example: unmarshal the bytes received back from text into an int\n  buffer[bytesread] = 0; // Turn the result into a C string\n\n  int score= atoi(buffer);\n  free(buffer);\n  return score;\n}\n\n\n\n\nWhat is server stub code? What is unmarshalling?\n\n\nThe server stub code will receive the request, unmarshall the request into a valid in-memory data call the underlying implementation and send the result back to the caller.\n\n\nHow do you send an int? float? a struct?  A linked list? A graph?\n\n\nTo implement RPC you need to decide (and document) which conventions you will use to serialize the data into a byte sequence. Even a simple integer has several common choices:\n\n Signed or unsigned?\n\n ASCII\n\n Fixed number of bytes or variable depending on magnitude\n\n Little or Big endian binary format?\n\n\nTo marshall a struct, decide which fields need to be serialized. It may not be necessary to send all data items (for example, some items may be irrelevant to the specific RPC or can be re-computed by the server from the other data items present).\n\n\nTo marshall a linked list it is unnecessary to send the link pointers- just stream the values. As part of unmarshalling the server can recreate a linked list structure from the byte sequence.\n\n\nBy starting at the head node/vertex, a simple tree can be recursively visited to create a serialized version of the data. A cyclic graph will usually require additional memory to ensure that each edge and vertex is processed exactly once.\n\n\nWhat is an IDL (Interface Design Language)?\n\n\nWriting stub code by hand is painful, tedious, error prone, difficult to maintain and difficult to reverse engineer the wire protocol from the implemented code. A better approach is specify the data objects, messages and services and automatically generate the client and server code.\n\n\nA modern example of an Interface Design Language is Google's Protocol Buffer .proto files.\n\n\nComplexity and challenges of RPC vs local calls?\n\n\nRemote Procedure Calls are significantly slower (10x to 100x) and more complex than local calls. An RPC must marshall data into a wire-compatible format. This may require multiple passes through the data structure, temporary memory allocation and transformation of the data representation.\n\n\nRobust RPC stub code must intelligently handle network failures and versioning. For example, a server may have to process requests from clients that are still running an early version of the stub code.\n\n\nA secure RPC will need to implement additional security checks (including authentication and authorization), validate data and encrypt communication between the client and host.\n\n\nTransferring large amounts of structured data\n\n\nLet's examine three methods of transferring data using 3 different formats - JSON, XML and Google Protocol Buffers. JSON and XML are text-based protocols. Examples of JSON and XML messages are below.\n\n\nticket\nprice currency='dollar'\n10\n/price\nvendor\ntravelocity\n/vendor\n/ticket\n\n\n\n\n\n{ 'currency':'dollar' , 'vendor':'travelocity', 'price':'10' }\n\n\n\n\nGoogle Protocol Buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low CPU overhead and minimal memory copying. Implementations exist for multiple languages including Go, Python, C++ and C. This means client and server stub code in multiple languages can be generated from the .proto specification file to marshall data to and from a binary stream.\n\n\nGoogle Protocol Buffers reduces the versioning problem by ignoring unknown fields that are present in a message. See the introduction to Protocol Buffers for more information.\n\n\n[[https://developers.google.com/protocol-buffers/docs/overview]]", 
            "title": "RPC, Part 1: Introduction to Remote Procedure Calls"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-rpc", 
            "text": "Remote Procedure Call. RPC is the idea that we can execute a procedure (function) on a different machine. In practice the procedure may execute on the same machine, however it may be in a different context - for example under a different user with different permissions and different lifecycle.", 
            "title": "What is RPC?"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-privilege-separation", 
            "text": "The remote code will execute under a different user and with different privileges from the caller. In practice the remote call may execute with more or fewer privileges than the caller. This in principle can be used to improve the security of a system (by ensuring components operate with least privilege). Unfortunately, security concerns need to be carefully assessed to ensure that RPC mechanisms cannot be subverted to perform unwanted actions. For example, an RPC implementation may implicitly trust any connected client to perform any action, rather than a subset of actions on a subset of the data.", 
            "title": "What is Privilege Separation?"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-stub-code-what-is-marshalling", 
            "text": "The stub code is the necessary code to hide the complexity of performing a remote procedure call. One of the roles of the stub code is to  marshall  the necessary data into a format that can be sent as a byte stream to a remote server.  // On the outside 'getHiscore' looks like a normal function call\n// On the inside the stub code performs all of the work to send and receive the data to and from the remote machine.\n\nint getHiscore(char* game) {\n  // Marshall the request into a sequence of bytes:\n  char* buffer;\n  asprintf( buffer, getHiscore(%s)! , name);\n\n  // Send down the wire (we do not send the zero byte; the '!' signifies the end of the message)\n  write(fd, buffer, strlen(buffer) );\n\n  // Wait for the server to send a response\n  ssize_t bytesread = read(fd, buffer, sizeof(buffer));\n\n  // Example: unmarshal the bytes received back from text into an int\n  buffer[bytesread] = 0; // Turn the result into a C string\n\n  int score= atoi(buffer);\n  free(buffer);\n  return score;\n}", 
            "title": "What is stub code? What is marshalling?"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-server-stub-code-what-is-unmarshalling", 
            "text": "The server stub code will receive the request, unmarshall the request into a valid in-memory data call the underlying implementation and send the result back to the caller.", 
            "title": "What is server stub code? What is unmarshalling?"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#how-do-you-send-an-int-float-a-struct-a-linked-list-a-graph", 
            "text": "To implement RPC you need to decide (and document) which conventions you will use to serialize the data into a byte sequence. Even a simple integer has several common choices:  Signed or unsigned?  ASCII  Fixed number of bytes or variable depending on magnitude  Little or Big endian binary format?  To marshall a struct, decide which fields need to be serialized. It may not be necessary to send all data items (for example, some items may be irrelevant to the specific RPC or can be re-computed by the server from the other data items present).  To marshall a linked list it is unnecessary to send the link pointers- just stream the values. As part of unmarshalling the server can recreate a linked list structure from the byte sequence.  By starting at the head node/vertex, a simple tree can be recursively visited to create a serialized version of the data. A cyclic graph will usually require additional memory to ensure that each edge and vertex is processed exactly once.", 
            "title": "How do you send an int? float? a struct?  A linked list? A graph?"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#what-is-an-idl-interface-design-language", 
            "text": "Writing stub code by hand is painful, tedious, error prone, difficult to maintain and difficult to reverse engineer the wire protocol from the implemented code. A better approach is specify the data objects, messages and services and automatically generate the client and server code.  A modern example of an Interface Design Language is Google's Protocol Buffer .proto files.", 
            "title": "What is an IDL (Interface Design Language)?"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#complexity-and-challenges-of-rpc-vs-local-calls", 
            "text": "Remote Procedure Calls are significantly slower (10x to 100x) and more complex than local calls. An RPC must marshall data into a wire-compatible format. This may require multiple passes through the data structure, temporary memory allocation and transformation of the data representation.  Robust RPC stub code must intelligently handle network failures and versioning. For example, a server may have to process requests from clients that are still running an early version of the stub code.  A secure RPC will need to implement additional security checks (including authentication and authorization), validate data and encrypt communication between the client and host.", 
            "title": "Complexity and challenges of RPC vs local calls?"
        }, 
        {
            "location": "/RPC,-Part-1:-Introduction-to-Remote-Procedure-Calls/#transferring-large-amounts-of-structured-data", 
            "text": "Let's examine three methods of transferring data using 3 different formats - JSON, XML and Google Protocol Buffers. JSON and XML are text-based protocols. Examples of JSON and XML messages are below.  ticket price currency='dollar' 10 /price vendor travelocity /vendor /ticket   { 'currency':'dollar' , 'vendor':'travelocity', 'price':'10' }  Google Protocol Buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low CPU overhead and minimal memory copying. Implementations exist for multiple languages including Go, Python, C++ and C. This means client and server stub code in multiple languages can be generated from the .proto specification file to marshall data to and from a binary stream.  Google Protocol Buffers reduces the versioning problem by ignoring unknown fields that are present in a message. See the introduction to Protocol Buffers for more information.  [[https://developers.google.com/protocol-buffers/docs/overview]]", 
            "title": "Transferring large amounts of structured data"
        }, 
        {
            "location": "/Sample-program-using-pthread-barriers/", 
            "text": "#define _GNU_SOURCE\n#include \nstdio.h\n\n#include \nstdlib.h\n\n#include \nunistd.h\n\n#include \npthread.h\n\n#include \ntime.h\n\n\n#define THREAD_COUNT 4\n\npthread_barrier_t mybarrier;\n\nvoid* threadFn(void *id_ptr) {\n  int thread_id = *(int*)id_ptr;\n  int wait_sec = 1 + rand() % 5;\n  printf(\nthread %d: Wait for %d seconds.\\n\n, thread_id, wait_sec);\n  sleep(wait_sec);\n  printf(\nthread %d: I'm ready...\\n\n, thread_id);\n\n  pthread_barrier_wait(\nmybarrier);\n\n  printf(\nthread %d: going!\\n\n, thread_id);\n  return NULL;\n}\n\n\nint main() {\n  int i;\n  pthread_t ids[THREAD_COUNT];\n  int short_ids[THREAD_COUNT];\n\n  srand(time(NULL));\n  pthread_barrier_init(\nmybarrier, NULL, THREAD_COUNT + 1);\n\n  for (i=0; i \n THREAD_COUNT; i++) {\n    short_ids[i] = i;\n    pthread_create(\nids[i], NULL, threadFn, \nshort_ids[i]);\n  }\n\n  printf(\nmain() is ready.\\n\n);\n\n  pthread_barrier_wait(\nmybarrier);\n\n  printf(\nmain() is going!\\n\n);\n\n  for (i=0; i \n THREAD_COUNT; i++) {\n    pthread_join(ids[i], NULL);\n  }\n\n  pthread_barrier_destroy(\nmybarrier);\n\n  return 0;\n}", 
            "title": "Sample program using pthread barriers"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/", 
            "text": "Scheduling\n\n\nThinking about scheduling.\n\n\nCPU Scheduling is the problem of efficiently selecting which process to run on a system's CPU cores. In a busy system there will be more ready-to-run processes than there are CPU cores, so the system kernel must evaluate which processes should be scheduled to run on the CPU and which processes should be placed in a ready queue to be executed later.\n\n\nThe additional complexity of multi-threaded and multiple CPU cores are considered a distraction to this initial exposition so are ignored here.\n\n\nAnother gotcha for non-native speakers is the dual meaings of \"Time\": The word \"Time\" can be used in both clock and elapsed duration context. For example \"The arrival time of the first process was 9:00am.\" and, \"The running time of the algorithm is 3 seconds\".\n\n\nHow is scheduling measured and which scheduler is best?\n\n\nScheduling effects the performance of the system, specifically the \nlatency\n and \nthroughput\n of the system. The throughput might be measured by a system value, for example the I/O throughput - the number of bytes written per second, or number of small processes that can complete per unit time, or using a higher level of abstraction for example number of customer records processed per minute. The latency might be measured by the response time (elapse time before a process can start to send a response) or wait time or turnaround time (the elapsed time to complete a task). Different schedulers offer different optimization trade-offs that may or may not be appropriate to desired use - there is no optimal scheduler for all possible environments and goals. For example 'shortest-job-first' will minimize total wait time across all jobs but in interactive (UI) environments it would be preferable to minimize response time (at the expense of some throughput), while FCFS seems intuitively fair and easy to implement but suffers from the Convoy Effect.\n\n\nWhat is arrival time?\n\n\nThe time at which a process first arrives at the ready queue, and is ready to start executing. If a CPU is idle, the arrival time would also be the starting time of execution.\n\n\nWhat are some well known scheduling algorithms?\n\n\nWe will discuss four simple scheduling algorithms, Shortest Job First, First Come First Served, Priority and Round Robin.\n\n\n\n\nShortest Job First (SJF)\n\n\n\n\nThe next process to be scheduled will be the process with the shortest total CPU time required. One disadvantage of this scheduler is that it needs to be clairvoyant. Note the SJF is not shortest \nremaining\n time; processes are ordered by their total CPU needs not the remaining CPU need.\n\n\nSJF appears in both preemptive and non-preemptive versions. Preemptive SJF has the shortest total weight time when summed over all processes that have a known arrival time and execution time.\n\n\nTechnical Note: A realistic SJF implementation would not use the total execution time of the process but the burst time (the total CPU time including future computational execution before the process will no longer be ready to run). The expected burst time can be estimated by using an exponentially decaying weighted rolling average based on the previous burst time but for this exposition we will simplify this discussion to use the total running time of the process as a proxy for the burst time.\n\n\n\n\nFirst Come First Served (FCFS)\n\n\n\n\nProcesses are scheduled in the order of arrival. One advantage of FCFS is that scheduling algorithm is simple: the ready queue is a just a FIFO (first in first out) queue.\nFCFS suffers from the Convoy effect (see below).\n\n\n\n\nPriority\n\n\n\n\nProcesses are scheduled in the order of priority value. For example a navigation process might be more important to execute than a logging process.\n\n\n\n\nRound Robin (RR)\n\n\n\n\nProcesses are scheduled in order of their arrival in the ready queue. However after a small time step a running process will be forcibly removed from the running state and placed back on the ready queue. This ensures that a long-running process can not starve all other processes from running.\nThe maximum amount of time that a process can execute before being returned to the ready queue is called the time quanta. In the limit of large time quanta (where the time quanta is longer than the running time of all processes) round robin will be equivalent to FCFS.\n\n\nWhat is preemption?\n\n\nWithout preemption processes will run until they are unable to utilize the CPU any further. For example the following conditions would remove a process from the CPU and the CPU would be available to be scheduled for other processes: The process terminates due to a signal, is blocked waiting for concurrency primitive, or exits normally.\nThus once a process is scheduled it will continue even if another process with a high priority (e.g. shorter job) appears on the ready queue.\n\n\nWith preemption, the existing processes may be removed immediately if a more preferable process is added to the ready queue. For example, suppose at t=0 with a Shortest Job First scheduler there are two processes (P1 P2) with 10 and 20 ms execution times. P1 is scheduled. P1 immediately creates a new process P3, with execution time of 5 ms, which is added to the ready queue. Without preemption, P3 will run 10ms later (after P1 has completed). With preemption, P1 will be immediately evicted from the CPU and instead placed back in the ready queue, and P3 will be executed instead by the CPU.\n\n\nWhich schedulers suffer from starvation?\n\n\nAny scheduler that uses a form of prioritization can result in starvation because earlier processes may never be scheduled to run (assigned a CPU). For example with SJF, longer jobs may never be scheduled if the system continues to have many short jobs to schedule.\n\n\nFor more information see\nhttps://en.wikipedia.org/wiki/Scheduling_(computing)#Types_of_operating_system_schedulers\n\n\nWhy might a process (or thread) be placed on the ready queue?\n\n\nA process is placed on the ready queue when it is able to use a CPU. Some examples include:\n\n A process was blocked waiting for a \nread\n from storage or socket to complete and data is now available.\n\n A new process has been created and is ready to start.\n\n A process thread was blocked on a synchronization primitive (condition variable, semaphore, mutex lock) but is now able to continue.\n\n A process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.\n\n\nSimilar examples can be generated when considering threads.\n\n\nWhat is 'wait time'?\n\n\nWait time is the \ntotal\n wait time i.e. the total time that a process is on the ready queue. A common mistake is to believe it is only the initial waiting time in the ready queue.\n\n\nIf a CPU intensive process with no I/O takes 7 minutes of CPU time to complete but required 9 minutes of wall-clock time to complete we can conclude that it was placed on the ready-queue for 2 minutes. For those 2 minutes the process was ready to run but had no CPU assigned. It does not matter when the job was waiting, the wait time is 2 minutes.\n\n\nIf  \nTstart\n and \nTend\n are the start and end wall-clock times of the process and \nrun_time\n is the total amount of CPU time required then,\n\n\nwait_time  = (Tend - Tstart) - run_time\n\n\nWhat is the Convoy Effect?\n\n\n\"The Convoy Effect is where I/O intensive processes are continually backed up, waiting for CPU-intensive processes that hog the CPU. This results in poor I/O performance, even for processes that have tiny CPU needs.\"\n\n\nSuppose the CPU is currently assigned to a CPU intensive task and there is a set of I/O intensive processes that are in the ready queue. These processes require just a tiny amount of CPU time but they are unable to proceed because they are waiting for the CPU-intensive task to be removed from the processor. These processes are starved until the the CPU bound process releases the CPU. But the CPU will rarely be released (for example in the case of a FCFS scheduler, we must wait until the processes is blocked due to an I/O request). The I/O intensive processes can now finally satisfy their CPU needs, which they can do quickly because their CPU needs are small and the CPU is assigned back to the CPU-intensive process again. Thus the I/O performance of the whole system suffers through an indirect effect of starvation of CPU needs of all processes.\n\n\nThis effect is usually discussed in the context of FCFS scheduler, however a round robin scheduler can also exhibit the Convoy effect for long time-quanta.\n\n\nFor more information see the lecture notes and Wikipedia -\n\n\n\n\n\n\nhttps://subversion.ews.illinois.edu/svn/sp15-cs241/_shared/lectures_handouts/ \n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Scheduling_(computing)\n\n\n\n\n\n\nLinux Scheduling\n\n\nAs of February 2016, Linux by default uses the \nCompletely Fair Scheduler\n for CPU scheduling and the Budget Fair Scheduling \"BFQ\" for I/O scheduling. Appropriate scheduling can have a significant impact on throughput and latency. Latency is particularly important for interactive and soft-real time applications such as audio and video streaming. See the discussion and comparative benchmarks here [https://lkml.org/lkml/2014/5/27/314] for more information.", 
            "title": "Scheduling, Part 1: Scheduling Processes"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#scheduling", 
            "text": "", 
            "title": "Scheduling"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#thinking-about-scheduling", 
            "text": "CPU Scheduling is the problem of efficiently selecting which process to run on a system's CPU cores. In a busy system there will be more ready-to-run processes than there are CPU cores, so the system kernel must evaluate which processes should be scheduled to run on the CPU and which processes should be placed in a ready queue to be executed later.  The additional complexity of multi-threaded and multiple CPU cores are considered a distraction to this initial exposition so are ignored here.  Another gotcha for non-native speakers is the dual meaings of \"Time\": The word \"Time\" can be used in both clock and elapsed duration context. For example \"The arrival time of the first process was 9:00am.\" and, \"The running time of the algorithm is 3 seconds\".", 
            "title": "Thinking about scheduling."
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#how-is-scheduling-measured-and-which-scheduler-is-best", 
            "text": "Scheduling effects the performance of the system, specifically the  latency  and  throughput  of the system. The throughput might be measured by a system value, for example the I/O throughput - the number of bytes written per second, or number of small processes that can complete per unit time, or using a higher level of abstraction for example number of customer records processed per minute. The latency might be measured by the response time (elapse time before a process can start to send a response) or wait time or turnaround time (the elapsed time to complete a task). Different schedulers offer different optimization trade-offs that may or may not be appropriate to desired use - there is no optimal scheduler for all possible environments and goals. For example 'shortest-job-first' will minimize total wait time across all jobs but in interactive (UI) environments it would be preferable to minimize response time (at the expense of some throughput), while FCFS seems intuitively fair and easy to implement but suffers from the Convoy Effect.", 
            "title": "How is scheduling measured and which scheduler is best?"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#what-is-arrival-time", 
            "text": "The time at which a process first arrives at the ready queue, and is ready to start executing. If a CPU is idle, the arrival time would also be the starting time of execution.", 
            "title": "What is arrival time?"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#what-are-some-well-known-scheduling-algorithms", 
            "text": "We will discuss four simple scheduling algorithms, Shortest Job First, First Come First Served, Priority and Round Robin.   Shortest Job First (SJF)   The next process to be scheduled will be the process with the shortest total CPU time required. One disadvantage of this scheduler is that it needs to be clairvoyant. Note the SJF is not shortest  remaining  time; processes are ordered by their total CPU needs not the remaining CPU need.  SJF appears in both preemptive and non-preemptive versions. Preemptive SJF has the shortest total weight time when summed over all processes that have a known arrival time and execution time.  Technical Note: A realistic SJF implementation would not use the total execution time of the process but the burst time (the total CPU time including future computational execution before the process will no longer be ready to run). The expected burst time can be estimated by using an exponentially decaying weighted rolling average based on the previous burst time but for this exposition we will simplify this discussion to use the total running time of the process as a proxy for the burst time.   First Come First Served (FCFS)   Processes are scheduled in the order of arrival. One advantage of FCFS is that scheduling algorithm is simple: the ready queue is a just a FIFO (first in first out) queue.\nFCFS suffers from the Convoy effect (see below).   Priority   Processes are scheduled in the order of priority value. For example a navigation process might be more important to execute than a logging process.   Round Robin (RR)   Processes are scheduled in order of their arrival in the ready queue. However after a small time step a running process will be forcibly removed from the running state and placed back on the ready queue. This ensures that a long-running process can not starve all other processes from running.\nThe maximum amount of time that a process can execute before being returned to the ready queue is called the time quanta. In the limit of large time quanta (where the time quanta is longer than the running time of all processes) round robin will be equivalent to FCFS.", 
            "title": "What are some well known scheduling algorithms?"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#what-is-preemption", 
            "text": "Without preemption processes will run until they are unable to utilize the CPU any further. For example the following conditions would remove a process from the CPU and the CPU would be available to be scheduled for other processes: The process terminates due to a signal, is blocked waiting for concurrency primitive, or exits normally.\nThus once a process is scheduled it will continue even if another process with a high priority (e.g. shorter job) appears on the ready queue.  With preemption, the existing processes may be removed immediately if a more preferable process is added to the ready queue. For example, suppose at t=0 with a Shortest Job First scheduler there are two processes (P1 P2) with 10 and 20 ms execution times. P1 is scheduled. P1 immediately creates a new process P3, with execution time of 5 ms, which is added to the ready queue. Without preemption, P3 will run 10ms later (after P1 has completed). With preemption, P1 will be immediately evicted from the CPU and instead placed back in the ready queue, and P3 will be executed instead by the CPU.", 
            "title": "What is preemption?"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#which-schedulers-suffer-from-starvation", 
            "text": "Any scheduler that uses a form of prioritization can result in starvation because earlier processes may never be scheduled to run (assigned a CPU). For example with SJF, longer jobs may never be scheduled if the system continues to have many short jobs to schedule.  For more information see\nhttps://en.wikipedia.org/wiki/Scheduling_(computing)#Types_of_operating_system_schedulers", 
            "title": "Which schedulers suffer from starvation?"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#why-might-a-process-or-thread-be-placed-on-the-ready-queue", 
            "text": "A process is placed on the ready queue when it is able to use a CPU. Some examples include:  A process was blocked waiting for a  read  from storage or socket to complete and data is now available.  A new process has been created and is ready to start.  A process thread was blocked on a synchronization primitive (condition variable, semaphore, mutex lock) but is now able to continue.  A process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.  Similar examples can be generated when considering threads.", 
            "title": "Why might a process (or thread) be placed on the ready queue?"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#what-is-wait-time", 
            "text": "Wait time is the  total  wait time i.e. the total time that a process is on the ready queue. A common mistake is to believe it is only the initial waiting time in the ready queue.  If a CPU intensive process with no I/O takes 7 minutes of CPU time to complete but required 9 minutes of wall-clock time to complete we can conclude that it was placed on the ready-queue for 2 minutes. For those 2 minutes the process was ready to run but had no CPU assigned. It does not matter when the job was waiting, the wait time is 2 minutes.  If   Tstart  and  Tend  are the start and end wall-clock times of the process and  run_time  is the total amount of CPU time required then,  wait_time  = (Tend - Tstart) - run_time", 
            "title": "What is 'wait time'?"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#what-is-the-convoy-effect", 
            "text": "\"The Convoy Effect is where I/O intensive processes are continually backed up, waiting for CPU-intensive processes that hog the CPU. This results in poor I/O performance, even for processes that have tiny CPU needs.\"  Suppose the CPU is currently assigned to a CPU intensive task and there is a set of I/O intensive processes that are in the ready queue. These processes require just a tiny amount of CPU time but they are unable to proceed because they are waiting for the CPU-intensive task to be removed from the processor. These processes are starved until the the CPU bound process releases the CPU. But the CPU will rarely be released (for example in the case of a FCFS scheduler, we must wait until the processes is blocked due to an I/O request). The I/O intensive processes can now finally satisfy their CPU needs, which they can do quickly because their CPU needs are small and the CPU is assigned back to the CPU-intensive process again. Thus the I/O performance of the whole system suffers through an indirect effect of starvation of CPU needs of all processes.  This effect is usually discussed in the context of FCFS scheduler, however a round robin scheduler can also exhibit the Convoy effect for long time-quanta.", 
            "title": "What is the Convoy Effect?"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#for-more-information-see-the-lecture-notes-and-wikipedia-", 
            "text": "https://subversion.ews.illinois.edu/svn/sp15-cs241/_shared/lectures_handouts/     https://en.wikipedia.org/wiki/Scheduling_(computing)", 
            "title": "For more information see the lecture notes and Wikipedia -"
        }, 
        {
            "location": "/Scheduling,-Part-1:-Scheduling-Processes/#linux-scheduling", 
            "text": "As of February 2016, Linux by default uses the  Completely Fair Scheduler  for CPU scheduling and the Budget Fair Scheduling \"BFQ\" for I/O scheduling. Appropriate scheduling can have a significant impact on throughput and latency. Latency is particularly important for interactive and soft-real time applications such as audio and video streaming. See the discussion and comparative benchmarks here [https://lkml.org/lkml/2014/5/27/314] for more information.", 
            "title": "Linux Scheduling"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/", 
            "text": "Where is Part 1?\n\n\nThere is no official \"Part 1\" page. However we introduced a simple signal() callback at the beginning of the course (e.g. [[Forking, Part 2: Fork, Exec, Wait]] )\n\n\nHow can I learn more about signals?\n\n\nThe linux man pages discusses signal system calls in section 2. There is also a longer article in section 7 (though not in OSX/BSD):\n\n\nman -s7 signal\n\n\n\n\nWhat is a process's signal disposition?\n\n\nFor each process, each signal has a disposition which means what action will occur when a signal is delivered to the process. For example, the default disposition SIGINT is to terminate it. However this disposition can be changed by calling \nsignal()\n (or as we will learn later) \nsigaction()\n  to install a signal handler for a particular signal. You can imagine the processes' disposition to all possible signals as a table of function pointers entries (one for each possible signal).\n\n\nThe default disposition for signals can be to ignore the signal, stop the process, continue a stopped process, terminate the process, or terminate the process and also dump a 'core' file. Note a core file is a representation of the processes' memory state that can be inspected using a debugger.\n\n\nCan multiple signals be queued?\n\n\nNo - however it is possible to have signals that are in a pending state. If a signal is pending it means it has not yet been delivered to the process. The most common reason for a signal to be pending is that the process (or thread) has currently blocked that particular signal.\n\n\nIf a particular signal, e.g. SIGINT, is pending then it is not possible to queue up the same signal again.\n\n\nIt \nis\n possible to have more than one signal of a different type in a pending state. For example SIGINT and SIGTERM signals may be pending (i.e. not yet delivered to the target process)\n\n\nHow do I block signals?\n\n\nSignals can be blocked (meaning they will stay in the pending state) by setting the process signal mask or, when you are writing a multi-threaded program, the thread signal mask.\n\n\nWhat happens when creating a new thread?\n\n\nThe new thread inherits a copy of the calling thread's mask\n\n\npthread_sigmask( ... ); // set my mask to block delivery of some signals\npthread_create( ... ); // new thread will start with a copy of the same mask\n\n\n\n\nWhat happens when forking?\n\n\nThe child process inherits a copy of the parent's signal dispositions. In other words, if you have installed a SIGINT handler before forking, then the child process will also call the handler if a SIGINT is delivered to the child.\n\n\nNote pending signals for the child are \nnot\n inherited during forking.\n\n\nWhat is signal disposition ?\n\n\nThe signal disposition of a process is a table of actions. It defines what will happen when a particular signal is delivered to a process. For example, the default disposition of SIG-INT is to terminate the process. The signal disposition is per process not per thread. The signal disposition can be changed by calling signal() (which is simple but not portable as there are subtle variations in its implementation on different POSIX architectures and also not recommended for multi-threaded programs) or \nsigaction\n (discussed later)\n\n\nWhat happens during exec ?\n\n\nRemember that \nexec\n replaces the current image with a new program image. In addition the signal disposition is reset. Pending signals are preserved.\n\n\nWhat happens during fork ?\n\n\nThe child process inherits a copy of the parent process's signal disposition and a copy of the parent's signal mask.\n\n\nFor example if \nSIGINT\n is blocked in the parent it will be blocked in the child too.\nFor example if the parent installed a handler (call-back function) for SIG-INT then the child will also perform the same behavior.\n\n\nPending signals however are not inherited by the child.\n\n\nHow do I block signals in a single-threaded program?\n\n\nUse \nsigprocmask\n! With sigprocmask you can set the new mask, add new signals to be blocked to the process mask, and unblock currently blocked signals. You can also determine the existing mask (and use it for later) by passing in a non-null value for oldset.\n\n\nint sigprocmask(int how, const sigset_t *set, sigset_t *oldset);`\n\n\n\n\nFrom the Linux man page of sigprocmask,\n\n\nSIG_BLOCK: The set of blocked signals is the union of the current set and the set argument.\nSIG_UNBLOCK: The signals in set are removed from the current set of blocked signals. It is permissible to attempt to unblock a signal which is not blocked.\nSIG_SETMASK: The set of blocked signals is set to the argument set.\n\n\n\n\n\nThe sigset type behaves as a bitmap, except functions are used rather than explicitly setting and unsetting bits using \n and |. \n\n\nIt is a common error to forget to initialize the signal set before modifying one bit. For example,\n\n\nsigset_t set, oldset;\nsigaddset(\nset, SIGINT); // Ooops!\nsigprocmask(SIG_SETMASK, \nset, \noldset)\n\n\n\n\nCorrect code initializes the set to be all on or all off. For example,\n\n\nsigfillset(\nset); // all signals\nsigprocmask(SIG_SETMASK, \nset, NULL); // Block all the signals!\n// (Actually SIGKILL or SIGSTOP cannot be blocked...)\n\nsigemptyset(\nset); // no signals \nsigprocmask(SIG_SETMASK, \nset, NULL); // set the mask to be empty again\n\n\n\n\nHow do I block signals in a multi-threaded program?\n\n\nBlocking signals is similar in multi-threaded programs to single-threaded programs:\n\n Use pthread_sigmask instead of sigprocmask\n\n Block a signal in all threads to prevent its asynchronous delivery\n\n\nThe easiest method to ensure a signal is blocked in all threads is to set the signal mask in the main thread before new threads are created\n\n\nsigemptyset(\nset);\nsigaddset(\nset, SIGQUIT);\nsigaddset(\nset, SIGINT);\npthread_sigmask(SIG_BLOCK, \nset, NULL);\n\n// this thread and the new thread will block SIGQUIT and SIGINT\npthread_create(\nthread_id, NULL, myfunc, funcparam);\n\n\n\n\nJust as we saw with sigprocmask, pthread_sigmask includes a 'how' parameter that defines how the signal set is to be used:\n\n\npthread_sigmask(SIG_SETMASK, \nset, NULL) - replace the thread's mask with given signal set\npthread_sigmask(SIG_BLOCK, \nset, NULL) - add the signal set to the thread's mask\npthread_sigmask(SIG_UNBLOCK, \nset, NULL) - remove the signal set from the thread's mask\n\n\n\n\nHow are pending signals delivered in a multi-threaded program?\n\n\nA signal is delivered to any signal thread that is not blocking that signal.\n\n\nIf the two or more threads can receive the signal then which thread will be interrupted is arbitrary!", 
            "title": "Signals, Part 2: Pending Signals and Signal Masks"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#where-is-part-1", 
            "text": "There is no official \"Part 1\" page. However we introduced a simple signal() callback at the beginning of the course (e.g. [[Forking, Part 2: Fork, Exec, Wait]] )", 
            "title": "Where is Part 1?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-can-i-learn-more-about-signals", 
            "text": "The linux man pages discusses signal system calls in section 2. There is also a longer article in section 7 (though not in OSX/BSD):  man -s7 signal", 
            "title": "How can I learn more about signals?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-is-a-processs-signal-disposition", 
            "text": "For each process, each signal has a disposition which means what action will occur when a signal is delivered to the process. For example, the default disposition SIGINT is to terminate it. However this disposition can be changed by calling  signal()  (or as we will learn later)  sigaction()   to install a signal handler for a particular signal. You can imagine the processes' disposition to all possible signals as a table of function pointers entries (one for each possible signal).  The default disposition for signals can be to ignore the signal, stop the process, continue a stopped process, terminate the process, or terminate the process and also dump a 'core' file. Note a core file is a representation of the processes' memory state that can be inspected using a debugger.", 
            "title": "What is a process's signal disposition?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#can-multiple-signals-be-queued", 
            "text": "No - however it is possible to have signals that are in a pending state. If a signal is pending it means it has not yet been delivered to the process. The most common reason for a signal to be pending is that the process (or thread) has currently blocked that particular signal.  If a particular signal, e.g. SIGINT, is pending then it is not possible to queue up the same signal again.  It  is  possible to have more than one signal of a different type in a pending state. For example SIGINT and SIGTERM signals may be pending (i.e. not yet delivered to the target process)", 
            "title": "Can multiple signals be queued?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-do-i-block-signals", 
            "text": "Signals can be blocked (meaning they will stay in the pending state) by setting the process signal mask or, when you are writing a multi-threaded program, the thread signal mask.", 
            "title": "How do I block signals?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-happens-when-creating-a-new-thread", 
            "text": "The new thread inherits a copy of the calling thread's mask  pthread_sigmask( ... ); // set my mask to block delivery of some signals\npthread_create( ... ); // new thread will start with a copy of the same mask", 
            "title": "What happens when creating a new thread?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-happens-when-forking", 
            "text": "The child process inherits a copy of the parent's signal dispositions. In other words, if you have installed a SIGINT handler before forking, then the child process will also call the handler if a SIGINT is delivered to the child.  Note pending signals for the child are  not  inherited during forking.", 
            "title": "What happens when forking?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-is-signal-disposition", 
            "text": "The signal disposition of a process is a table of actions. It defines what will happen when a particular signal is delivered to a process. For example, the default disposition of SIG-INT is to terminate the process. The signal disposition is per process not per thread. The signal disposition can be changed by calling signal() (which is simple but not portable as there are subtle variations in its implementation on different POSIX architectures and also not recommended for multi-threaded programs) or  sigaction  (discussed later)", 
            "title": "What is signal disposition ?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-happens-during-exec", 
            "text": "Remember that  exec  replaces the current image with a new program image. In addition the signal disposition is reset. Pending signals are preserved.", 
            "title": "What happens during exec ?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#what-happens-during-fork", 
            "text": "The child process inherits a copy of the parent process's signal disposition and a copy of the parent's signal mask.  For example if  SIGINT  is blocked in the parent it will be blocked in the child too.\nFor example if the parent installed a handler (call-back function) for SIG-INT then the child will also perform the same behavior.  Pending signals however are not inherited by the child.", 
            "title": "What happens during fork ?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-do-i-block-signals-in-a-single-threaded-program", 
            "text": "Use  sigprocmask ! With sigprocmask you can set the new mask, add new signals to be blocked to the process mask, and unblock currently blocked signals. You can also determine the existing mask (and use it for later) by passing in a non-null value for oldset.  int sigprocmask(int how, const sigset_t *set, sigset_t *oldset);`  From the Linux man page of sigprocmask,  SIG_BLOCK: The set of blocked signals is the union of the current set and the set argument.\nSIG_UNBLOCK: The signals in set are removed from the current set of blocked signals. It is permissible to attempt to unblock a signal which is not blocked.\nSIG_SETMASK: The set of blocked signals is set to the argument set.  The sigset type behaves as a bitmap, except functions are used rather than explicitly setting and unsetting bits using   and |.   It is a common error to forget to initialize the signal set before modifying one bit. For example,  sigset_t set, oldset;\nsigaddset( set, SIGINT); // Ooops!\nsigprocmask(SIG_SETMASK,  set,  oldset)  Correct code initializes the set to be all on or all off. For example,  sigfillset( set); // all signals\nsigprocmask(SIG_SETMASK,  set, NULL); // Block all the signals!\n// (Actually SIGKILL or SIGSTOP cannot be blocked...)\n\nsigemptyset( set); // no signals \nsigprocmask(SIG_SETMASK,  set, NULL); // set the mask to be empty again", 
            "title": "How do I block signals in a single-threaded program?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-do-i-block-signals-in-a-multi-threaded-program", 
            "text": "Blocking signals is similar in multi-threaded programs to single-threaded programs:  Use pthread_sigmask instead of sigprocmask  Block a signal in all threads to prevent its asynchronous delivery  The easiest method to ensure a signal is blocked in all threads is to set the signal mask in the main thread before new threads are created  sigemptyset( set);\nsigaddset( set, SIGQUIT);\nsigaddset( set, SIGINT);\npthread_sigmask(SIG_BLOCK,  set, NULL);\n\n// this thread and the new thread will block SIGQUIT and SIGINT\npthread_create( thread_id, NULL, myfunc, funcparam);  Just as we saw with sigprocmask, pthread_sigmask includes a 'how' parameter that defines how the signal set is to be used:  pthread_sigmask(SIG_SETMASK,  set, NULL) - replace the thread's mask with given signal set\npthread_sigmask(SIG_BLOCK,  set, NULL) - add the signal set to the thread's mask\npthread_sigmask(SIG_UNBLOCK,  set, NULL) - remove the signal set from the thread's mask", 
            "title": "How do I block signals in a multi-threaded program?"
        }, 
        {
            "location": "/Signals,-Part-2:-Pending-Signals-and-Signal-Masks/#how-are-pending-signals-delivered-in-a-multi-threaded-program", 
            "text": "A signal is delivered to any signal thread that is not blocking that signal.  If the two or more threads can receive the signal then which thread will be interrupted is arbitrary!", 
            "title": "How are pending signals delivered in a multi-threaded program?"
        }, 
        {
            "location": "/Signals,-Part-3:-Raising-signals/", 
            "text": "How do I send a signal to a process from the shell?\n\n\nYou already know one way to send a \nSIG_INT\n just type \nCTRL-C\n \nFrom the shell you can use \nkill\n (if you know the process id) and \nkillall\n (if you know the process name)\n\n\n# First let's use ps and grep to find the process we want to send a signal to\n$ ps au | grep myprogram\nangrave  4409   0.0  0.0  2434892    512 s004  R+    2:42PM   0:00.00 myprogram 1 2 3\n\n#Send SIGINT signal to process 4409 (equivalent of `CTRL-C`)\n$ kill -SIGINT 4409\n\n#Send SIGKILL (terminate the process)\n$ kill -SIGKILL 4409\n$ kill -9 4409\n\n\n\n\nkillall\n is similar except that it matches by program name. The next two example, sends a \nSIGINT\n and then \nSIGKILL\n to terminate the processes that are running \nmyprogram\n\n\n# Send SIGINT (SIGINT can be ignored)\n$ killall -SIGINT myprogram\n\n# SIGKILL (-9) cannot be ignored! \n$ killall -9 myprogram\n\n\n\n\nHow do I send a signal to a process from the running C program?\n\n\nUse \nraise\n or \nkill\n\n\nint raise(int sig); // Send a signal to myself!\nint kill(pid_t pid, int sig); // Send a signal to another process\n\n\n\n\nFor non-root processes, signals can only be sent to processes of the same user i.e. you cant just SIGKILL my processes! See kill(2) i.e. man -s2 for more details.\n\n\nHow do I send a signal to a specific thread?\n\n\nUse \npthread_kill\n\n\nint pthread_kill(pthread_t thread, int sig)\n\n\n\n\nIn the example below, the newly created thread executing \nfunc\n will be interrupted by \nSIGINT\n\n\npthread_create(\ntid, NULL, func, args);\npthread_kill(tid, SIGINT);\npthread_kill(pthread_self(), SIGKILL); // send SIGKILL to myself\n\n\n\n\nWill \npthread_kill( threadid, SIGKILL)\n kill the process or thread?\n\n\nIt will kill the entire process. Though individual threads can set a signal mask, the signal disposition (the table of handlers/action performed for each signal) is \nper-proces\ns not \nper-thread\n. This means \n\nsigaction\n can be called from any thread because you will be setting a signal handler for all threads in the process.\n\n\nHow do I catch (handle) a signal ?\n\n\nYou can choose a handle pending signals asynchronously or synchronously.\n\n\nInstall a signal handler to asynchronously handle signals use \nsigaction\n (or, for simple examples, \nsignal\n ).\n\n\nTo synchronously catch a pending signal use \nsigwait\n (which blocks until a signal is delivered) or \nsignalfd\n (which also blocks and provides a file descriptor that can be \nread()\n to retrieve pending signals).\n\n\nSee \nSignals, Part 4\n for an example of using \nsigwait", 
            "title": "Signals, Part 3: Raising signals"
        }, 
        {
            "location": "/Signals,-Part-3:-Raising-signals/#how-do-i-send-a-signal-to-a-process-from-the-shell", 
            "text": "You already know one way to send a  SIG_INT  just type  CTRL-C  \nFrom the shell you can use  kill  (if you know the process id) and  killall  (if you know the process name)  # First let's use ps and grep to find the process we want to send a signal to\n$ ps au | grep myprogram\nangrave  4409   0.0  0.0  2434892    512 s004  R+    2:42PM   0:00.00 myprogram 1 2 3\n\n#Send SIGINT signal to process 4409 (equivalent of `CTRL-C`)\n$ kill -SIGINT 4409\n\n#Send SIGKILL (terminate the process)\n$ kill -SIGKILL 4409\n$ kill -9 4409  killall  is similar except that it matches by program name. The next two example, sends a  SIGINT  and then  SIGKILL  to terminate the processes that are running  myprogram  # Send SIGINT (SIGINT can be ignored)\n$ killall -SIGINT myprogram\n\n# SIGKILL (-9) cannot be ignored! \n$ killall -9 myprogram", 
            "title": "How do I send a signal to a process from the shell?"
        }, 
        {
            "location": "/Signals,-Part-3:-Raising-signals/#how-do-i-send-a-signal-to-a-process-from-the-running-c-program", 
            "text": "Use  raise  or  kill  int raise(int sig); // Send a signal to myself!\nint kill(pid_t pid, int sig); // Send a signal to another process  For non-root processes, signals can only be sent to processes of the same user i.e. you cant just SIGKILL my processes! See kill(2) i.e. man -s2 for more details.", 
            "title": "How do I send a signal to a process from the running C program?"
        }, 
        {
            "location": "/Signals,-Part-3:-Raising-signals/#how-do-i-send-a-signal-to-a-specific-thread", 
            "text": "Use  pthread_kill  int pthread_kill(pthread_t thread, int sig)  In the example below, the newly created thread executing  func  will be interrupted by  SIGINT  pthread_create( tid, NULL, func, args);\npthread_kill(tid, SIGINT);\npthread_kill(pthread_self(), SIGKILL); // send SIGKILL to myself", 
            "title": "How do I send a signal to a specific thread?"
        }, 
        {
            "location": "/Signals,-Part-3:-Raising-signals/#will-pthread_kill-threadid-sigkill-kill-the-process-or-thread", 
            "text": "It will kill the entire process. Though individual threads can set a signal mask, the signal disposition (the table of handlers/action performed for each signal) is  per-proces s not  per-thread . This means  sigaction  can be called from any thread because you will be setting a signal handler for all threads in the process.", 
            "title": "Will pthread_kill( threadid, SIGKILL) kill the process or thread?"
        }, 
        {
            "location": "/Signals,-Part-3:-Raising-signals/#how-do-i-catch-handle-a-signal", 
            "text": "You can choose a handle pending signals asynchronously or synchronously.  Install a signal handler to asynchronously handle signals use  sigaction  (or, for simple examples,  signal  ).  To synchronously catch a pending signal use  sigwait  (which blocks until a signal is delivered) or  signalfd  (which also blocks and provides a file descriptor that can be  read()  to retrieve pending signals).  See  Signals, Part 4  for an example of using  sigwait", 
            "title": "How do I catch (handle) a signal ?"
        }, 
        {
            "location": "/Signals,-Part-4:-Sigaction/", 
            "text": "How and why do I use \nsigaction\n ?\n\n\nTo change the \"signal disposition\" of a process - i.e. what happens when a signal is delivered to your process - use \nsigaction\n\n\nYou can use system call \nsigaction\n to set the current handler for a signal or read the current signal handler for a particular signal.\n\n\nint sigaction(int signum, const struct sigaction *act, struct sigaction *oldact);\n\n\n\n\nThe sigaction struct includes two callback functions (we will only look at the 'handler' version), a signal mask and a flags field -\n\n\nstruct sigaction {\n               void     (*sa_handler)(int);\n               void     (*sa_sigaction)(int, siginfo_t *, void *);\n               sigset_t   sa_mask;\n               int        sa_flags;\n}; \n\n\n\n\nHow do I convert a \nsignal\n call into the equivalent \nsigaction\n call?\n\n\nSuppose you installed a signal handler for the alarm signal,\n\n\nsignal(SIGALRM, myhandler);\n\n\n\n\nThe equivalent \nsigaction\n code is:\n\n\nstruct sigaction sa; \nsa.sa_handler = myhandler;\nsigemptyset(\nsa.sa_mask);\nsa.sa_flags = 0; \nsigaction(SIGALRM, \nsa, NULL)\n\n\n\n\nHowever, we typically may also set the mask and the flags field. The mask is a temporary signal mask used during the signal handler execution. The SA_RESTART flag will automatically restart some (but not all) system calls that otherwise would have returned early (with EINTR error). The latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.\n\n\nsigfillset(\nsa.sa_mask);\nsa.sa_flags = SA_RESTART; /* Restart functions if  interrupted by handler */     \n\n\n\n\nHow do I use sigwait?\n\n\nSigwait can be used to read one pending signal at a time. \nsigwait\n is used to synchronously wait for signals, rather than handle them in a callback. A typical use of sigwait in a multi-threaded program is shown below. Notice that the thread signal mask is set first (and will be inherited by new threads). This prevents signals from being \ndelivered\n so they will remain in a pending state until sigwait is called. Also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is being used as the set of signals that sigwait can catch and return.\n\n\nOne advantage of writing a custom signal handling thread (such as the example below) rather than a callback function is that you can now use many more C library and system functions that otherwise could not be safely used in a signal handler because they are not async signal-safe.\n\n\nBased on \nhttp://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_sigmask.html\n\n\nstatic sigset_t   signal_mask;  /* signals to block         */\n\nint main (int argc, char *argv[])\n{\n    pthread_t sig_thr_id;      /* signal handler thread ID */\n    sigemptyset (\nsignal_mask);\n    sigaddset (\nsignal_mask, SIGINT);\n    sigaddset (\nsignal_mask, SIGTERM);\n    pthread_sigmask (SIG_BLOCK, \nsignal_mask, NULL);\n\n    /* New threads will inherit this thread's mask */\n    pthread_create (\nsig_thr_id, NULL, signal_thread, NULL);\n\n    /* APPLICATION CODE */\n    ...\n}\n\nvoid *signal_thread (void *arg)\n{\n    int       sig_caught;    /* signal caught       */\n\n    /* Use same mask as the set of signals that we'd like to know about! */\n    sigwait(\nsignal_mask, \nsig_caught);\n    switch (sig_caught)\n    {\n    case SIGINT:     /* process SIGINT  */\n        ...\n        break;\n    case SIGTERM:    /* process SIGTERM */\n        ...\n        break;\n    default:         /* should normally not happen */\n        fprintf (stderr, \n\\nUnexpected signal %d\\n\n, sig_caught);\n        break;\n    }\n}", 
            "title": "Signals, Part 4: Sigaction"
        }, 
        {
            "location": "/Signals,-Part-4:-Sigaction/#how-and-why-do-i-use-sigaction", 
            "text": "To change the \"signal disposition\" of a process - i.e. what happens when a signal is delivered to your process - use  sigaction  You can use system call  sigaction  to set the current handler for a signal or read the current signal handler for a particular signal.  int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact);  The sigaction struct includes two callback functions (we will only look at the 'handler' version), a signal mask and a flags field -  struct sigaction {\n               void     (*sa_handler)(int);\n               void     (*sa_sigaction)(int, siginfo_t *, void *);\n               sigset_t   sa_mask;\n               int        sa_flags;\n};", 
            "title": "How and why do I use sigaction ?"
        }, 
        {
            "location": "/Signals,-Part-4:-Sigaction/#how-do-i-convert-a-signal-call-into-the-equivalent-sigaction-call", 
            "text": "Suppose you installed a signal handler for the alarm signal,  signal(SIGALRM, myhandler);  The equivalent  sigaction  code is:  struct sigaction sa; \nsa.sa_handler = myhandler;\nsigemptyset( sa.sa_mask);\nsa.sa_flags = 0; \nsigaction(SIGALRM,  sa, NULL)  However, we typically may also set the mask and the flags field. The mask is a temporary signal mask used during the signal handler execution. The SA_RESTART flag will automatically restart some (but not all) system calls that otherwise would have returned early (with EINTR error). The latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.  sigfillset( sa.sa_mask);\nsa.sa_flags = SA_RESTART; /* Restart functions if  interrupted by handler */", 
            "title": "How do I convert a signal call into the equivalent sigaction call?"
        }, 
        {
            "location": "/Signals,-Part-4:-Sigaction/#how-do-i-use-sigwait", 
            "text": "Sigwait can be used to read one pending signal at a time.  sigwait  is used to synchronously wait for signals, rather than handle them in a callback. A typical use of sigwait in a multi-threaded program is shown below. Notice that the thread signal mask is set first (and will be inherited by new threads). This prevents signals from being  delivered  so they will remain in a pending state until sigwait is called. Also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is being used as the set of signals that sigwait can catch and return.  One advantage of writing a custom signal handling thread (such as the example below) rather than a callback function is that you can now use many more C library and system functions that otherwise could not be safely used in a signal handler because they are not async signal-safe.  Based on  http://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_sigmask.html  static sigset_t   signal_mask;  /* signals to block         */\n\nint main (int argc, char *argv[])\n{\n    pthread_t sig_thr_id;      /* signal handler thread ID */\n    sigemptyset ( signal_mask);\n    sigaddset ( signal_mask, SIGINT);\n    sigaddset ( signal_mask, SIGTERM);\n    pthread_sigmask (SIG_BLOCK,  signal_mask, NULL);\n\n    /* New threads will inherit this thread's mask */\n    pthread_create ( sig_thr_id, NULL, signal_thread, NULL);\n\n    /* APPLICATION CODE */\n    ...\n}\n\nvoid *signal_thread (void *arg)\n{\n    int       sig_caught;    /* signal caught       */\n\n    /* Use same mask as the set of signals that we'd like to know about! */\n    sigwait( signal_mask,  sig_caught);\n    switch (sig_caught)\n    {\n    case SIGINT:     /* process SIGINT  */\n        ...\n        break;\n    case SIGTERM:    /* process SIGTERM */\n        ...\n        break;\n    default:         /* should normally not happen */\n        fprintf (stderr,  \\nUnexpected signal %d\\n , sig_caught);\n        break;\n    }\n}", 
            "title": "How do I use sigwait?"
        }, 
        {
            "location": "/Signals:-Review-Questions/", 
            "text": "Give the names of two signals that are normally generated by the kernel\n\n\nGive the name of a signal that can not be caught by a signal\n\n\nCoding Questions\n\n\nWrite brief code that uses SIGACTION and a SIGNALSET to create a SIGALRM handler.", 
            "title": "Signals: Review Questions"
        }, 
        {
            "location": "/Signals:-Review-Questions/#give-the-names-of-two-signals-that-are-normally-generated-by-the-kernel", 
            "text": "", 
            "title": "Give the names of two signals that are normally generated by the kernel"
        }, 
        {
            "location": "/Signals:-Review-Questions/#give-the-name-of-a-signal-that-can-not-be-caught-by-a-signal", 
            "text": "Coding Questions  Write brief code that uses SIGACTION and a SIGNALSET to create a SIGALRM handler.", 
            "title": "Give the name of a signal that can not be caught by a signal"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/", 
            "text": "What is a Critical Section?\n\n\nA critical section is a section of code that can only be executed by one thread at a time, if the program is to function correctly. If two threads (or processes) were to execute code inside the critical section at the same time then it is possible that program may no longer have correct behavior.\n\n\nIs just incrementing a variable a critical section?\n\n\nPossibly. Incrementing a variable (\ni++\n) is performed in three individual steps: Copy the memory contents to the CPU register. Increment the value in the CPU. Store the new value in memory. If the memory location is only accessible by one thread (e.g. automatic variable \ni\n below) then there is no possibility of a race condition and no Critical Section associated with \ni\n. However the \nsum\n variable is a global variable and accessed by two threads. It is possible that two threads may attempt to increment the variable at the same time.\n\n\n#include \nstdio.h\n\n#include \npthread.h\n\n// Compile with -pthread\n\nint sum = 0; //shared\n\nvoid *countgold(void *param) {\n    int i; //local to each thread\n    for (i = 0; i \n 10000000; i++) {\n        sum += 1;\n    }\n    return NULL;\n}\n\nint main() {\n    pthread_t tid1, tid2;\n    pthread_create(\ntid1, NULL, countgold, NULL);\n    pthread_create(\ntid2, NULL, countgold, NULL);\n\n    //Wait for both threads to finish:\n    pthread_join(tid1, NULL);\n    pthread_join(tid2, NULL);\n\n    printf(\nARRRRG sum is %d\\n\n, sum);\n    return 0;\n}\n\n\n\n\nTypical output of the above code is \nARGGGH sum is 8140268\n\nA different sum is printed each time the program is run because there is a race condition; the code does not stop two threads from reading-writing \nsum\n at the same time. For example both threads copy the current value of sum into CPU that runs each thread (let's pick 123). Both threads increment one to their own copy. Both threads write back the value (124). If the threads had accessed the sum at different times then the count would have been 125.\n\n\nHow do I ensure only one thread at a time can access a global variable?\n\n\nYou mean, \"Help - I need a mutex!\"\nIf one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. For this purpose we can use a mutex (short for Mutual Exclusion).\n\n\nFor simple examples the smallest amount of code we need to add is just three lines:\n\n\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; // global variable\npthread_mutex_lock(\nm); // start of Critical Section\npthread_mutex_unlock(\nm); //end of Critical Section\n\n\n\n\nOnce we are finished with the mutex we should also call \npthread_mutex_destroy(\nm)\n too. Note, you can only destroy an unlocked mutex. Calling destroy on a destroyed lock, initializing an initialized lock, locking an already locked lock, unlocking an unlocked lock etc are unsupported (at least for default mutexes) and usually result in undefined behavior.\n\n\nIf I lock a mutex, does it stop all other threads?\n\n\nNo, the other threads will continue. It's only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. As soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue.\n\n\nAre there other ways to create a mutex?\n\n\nYes. You can use the macro PTHREAD_MUTEX_INITIALIZER only for global ('static') variables.\nm = PTHREAD_MUTEX_INITIALIZER is equivalent to the more general purpose\n\npthread_mutex_init(\nm,NULL)\n. The init version includes options to trade performance for additional error-checking and advanced sharing options.\n\n\npthread_mutex_t *lock = malloc(sizeof(pthread_mutex_t)); \npthread_mutex_init(lock, NULL);\n//later\npthread_mutex_destroy(lock);\nfree(lock);\n\n\n\n\nSo pthread_mutex_lock\n stops the other threads when they read the same variable?\n\n\nNo. A mutex is not that smart - it works with code (threads), not data. Only when another thread calls \nlock\n on a locked mutex will the second thread need to wait until the mutex is unlocked.\n\n\nCan I create mutex before fork-ing?\n\n\nYes - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other.\n\n\n(Advanced note: There are advanced options using shared memory that allow a child and parent to share a mutex if it's created with the correct options and uses a shared memory segment. See \nstackoverflow example\n )\n\n\nIf one thread locks a mutex can another thread unlock it?\n\n\nNo. The same thread must unlock it.\n\n\nCan I use two or more mutex locks?\n\n\nYes! In fact it's common to have one lock per data structure that you need to update.\n\n\nIf you only have one lock then they may be significant contention for the lock between two threads that was unnecessary. For example if two threads were updating two different counters it might not be necessary to use the same lock.\n\n\nHowever simply creating many locks is insufficient: It's important to be able to reason about critical sections e.g. it's important that one thread can't read two data structures while they are being updated and temporarily in an inconsistent state.\n\n\nIs there any overhead in calling lock and unlock?\n\n\nThere is a small amount of overhead of calling \npthread_mutex_lock\n and \n_unlock\n; however this is the price you pay for correctly functioning programs!\n\n\nSimplest complete example?\n\n\nA complete example is shown below\n\n\n#include \nstdio.h\n\n#include \npthread.h\n\n\n// Compile with -pthread\n// Create a mutex this ready to be locked!\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nint sum = 0;\n\nvoid *countgold(void *param) {\n    int i;\n\n    //Same thread that locks the mutex must unlock it\n    //Critical section is just 'sum += 1'\n    //However locking and unlocking a million times\n    //has significant overhead in this simple answer\n\n    pthread_mutex_lock(\nm);\n\n    // Other threads that call lock will have to wait until we call unlock\n\n    for (i = 0; i \n 10000000; i++) {\n    sum += 1;\n    }\n    pthread_mutex_unlock(\nm);\n    return NULL;\n}\n\nint main() {\n    pthread_t tid1, tid2;\n    pthread_create(\ntid1, NULL, countgold, NULL);\n    pthread_create(\ntid2, NULL, countgold, NULL);\n\n    //Wait for both threads to finish:\n    pthread_join(tid1, NULL);\n    pthread_join(tid2, NULL);\n\n    printf(\nARRRRG sum is %d\\n\n, sum);\n    return 0;\n}\n\n\n\n\nIn the code above, the thread gets the lock to the counting house before entering. The critical section is only the \nsum+=1\n so the following version is also correct but slower - \n\n\n    for (i = 0; i \n 10000000; i++) {\n        pthread_mutex_lock(\nm);\n        sum += 1;\n        pthread_mutex_unlock(\nm);\n    }\n    return NULL;\n}\n\n\n\n\nThis process runs slower because we lock and unlock the mutex a million times, which is expensive - at least compared with incrementing a variable. (And in this simple example we didn't really need threads - we could have added up twice!)  A faster multi-thread example would be to add one million using an automatic(local) variable and only then adding it to a shared total after the calculation loop has finished:\n\n\n    int local = 0;\n    for (i = 0; i \n 10000000; i++) {\n       local += 1;\n    }\n\n    pthread_mutex_lock(\nm);\n    sum += local;\n    pthread_mutex_unlock(\nm);\n\n    return NULL;\n}\n\n\n\n\nHow do I find out more?\n\n\nPlay!\n Read the man page!\n\n \npthread_mutex_lock man page\n\n\n \npthread_mutex_unlock man page\n\n\n \npthread_mutex_init man page\n\n\n \npthread_mutex_destroy man page", 
            "title": "Synchronization, Part 1: Mutex Locks"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#what-is-a-critical-section", 
            "text": "A critical section is a section of code that can only be executed by one thread at a time, if the program is to function correctly. If two threads (or processes) were to execute code inside the critical section at the same time then it is possible that program may no longer have correct behavior.", 
            "title": "What is a Critical Section?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#is-just-incrementing-a-variable-a-critical-section", 
            "text": "Possibly. Incrementing a variable ( i++ ) is performed in three individual steps: Copy the memory contents to the CPU register. Increment the value in the CPU. Store the new value in memory. If the memory location is only accessible by one thread (e.g. automatic variable  i  below) then there is no possibility of a race condition and no Critical Section associated with  i . However the  sum  variable is a global variable and accessed by two threads. It is possible that two threads may attempt to increment the variable at the same time.  #include  stdio.h \n#include  pthread.h \n// Compile with -pthread\n\nint sum = 0; //shared\n\nvoid *countgold(void *param) {\n    int i; //local to each thread\n    for (i = 0; i   10000000; i++) {\n        sum += 1;\n    }\n    return NULL;\n}\n\nint main() {\n    pthread_t tid1, tid2;\n    pthread_create( tid1, NULL, countgold, NULL);\n    pthread_create( tid2, NULL, countgold, NULL);\n\n    //Wait for both threads to finish:\n    pthread_join(tid1, NULL);\n    pthread_join(tid2, NULL);\n\n    printf( ARRRRG sum is %d\\n , sum);\n    return 0;\n}  Typical output of the above code is  ARGGGH sum is 8140268 \nA different sum is printed each time the program is run because there is a race condition; the code does not stop two threads from reading-writing  sum  at the same time. For example both threads copy the current value of sum into CPU that runs each thread (let's pick 123). Both threads increment one to their own copy. Both threads write back the value (124). If the threads had accessed the sum at different times then the count would have been 125.", 
            "title": "Is just incrementing a variable a critical section?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#how-do-i-ensure-only-one-thread-at-a-time-can-access-a-global-variable", 
            "text": "You mean, \"Help - I need a mutex!\"\nIf one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. For this purpose we can use a mutex (short for Mutual Exclusion).  For simple examples the smallest amount of code we need to add is just three lines:  pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; // global variable\npthread_mutex_lock( m); // start of Critical Section\npthread_mutex_unlock( m); //end of Critical Section  Once we are finished with the mutex we should also call  pthread_mutex_destroy( m)  too. Note, you can only destroy an unlocked mutex. Calling destroy on a destroyed lock, initializing an initialized lock, locking an already locked lock, unlocking an unlocked lock etc are unsupported (at least for default mutexes) and usually result in undefined behavior.", 
            "title": "How do I ensure only one thread at a time can access a global variable?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#if-i-lock-a-mutex-does-it-stop-all-other-threads", 
            "text": "No, the other threads will continue. It's only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. As soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue.", 
            "title": "If I lock a mutex, does it stop all other threads?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#are-there-other-ways-to-create-a-mutex", 
            "text": "Yes. You can use the macro PTHREAD_MUTEX_INITIALIZER only for global ('static') variables.\nm = PTHREAD_MUTEX_INITIALIZER is equivalent to the more general purpose pthread_mutex_init( m,NULL) . The init version includes options to trade performance for additional error-checking and advanced sharing options.  pthread_mutex_t *lock = malloc(sizeof(pthread_mutex_t)); \npthread_mutex_init(lock, NULL);\n//later\npthread_mutex_destroy(lock);\nfree(lock);", 
            "title": "Are there other ways to create a mutex?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#so-pthread_mutex_lock-stops-the-other-threads-when-they-read-the-same-variable", 
            "text": "No. A mutex is not that smart - it works with code (threads), not data. Only when another thread calls  lock  on a locked mutex will the second thread need to wait until the mutex is unlocked.", 
            "title": "So pthread_mutex_lock stops the other threads when they read the same variable?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#can-i-create-mutex-before-fork-ing", 
            "text": "Yes - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other.  (Advanced note: There are advanced options using shared memory that allow a child and parent to share a mutex if it's created with the correct options and uses a shared memory segment. See  stackoverflow example  )", 
            "title": "Can I create mutex before fork-ing?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#if-one-thread-locks-a-mutex-can-another-thread-unlock-it", 
            "text": "No. The same thread must unlock it.", 
            "title": "If one thread locks a mutex can another thread unlock it?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#can-i-use-two-or-more-mutex-locks", 
            "text": "Yes! In fact it's common to have one lock per data structure that you need to update.  If you only have one lock then they may be significant contention for the lock between two threads that was unnecessary. For example if two threads were updating two different counters it might not be necessary to use the same lock.  However simply creating many locks is insufficient: It's important to be able to reason about critical sections e.g. it's important that one thread can't read two data structures while they are being updated and temporarily in an inconsistent state.", 
            "title": "Can I use two or more mutex locks?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#is-there-any-overhead-in-calling-lock-and-unlock", 
            "text": "There is a small amount of overhead of calling  pthread_mutex_lock  and  _unlock ; however this is the price you pay for correctly functioning programs!", 
            "title": "Is there any overhead in calling lock and unlock?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#simplest-complete-example", 
            "text": "A complete example is shown below  #include  stdio.h \n#include  pthread.h \n\n// Compile with -pthread\n// Create a mutex this ready to be locked!\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nint sum = 0;\n\nvoid *countgold(void *param) {\n    int i;\n\n    //Same thread that locks the mutex must unlock it\n    //Critical section is just 'sum += 1'\n    //However locking and unlocking a million times\n    //has significant overhead in this simple answer\n\n    pthread_mutex_lock( m);\n\n    // Other threads that call lock will have to wait until we call unlock\n\n    for (i = 0; i   10000000; i++) {\n    sum += 1;\n    }\n    pthread_mutex_unlock( m);\n    return NULL;\n}\n\nint main() {\n    pthread_t tid1, tid2;\n    pthread_create( tid1, NULL, countgold, NULL);\n    pthread_create( tid2, NULL, countgold, NULL);\n\n    //Wait for both threads to finish:\n    pthread_join(tid1, NULL);\n    pthread_join(tid2, NULL);\n\n    printf( ARRRRG sum is %d\\n , sum);\n    return 0;\n}  In the code above, the thread gets the lock to the counting house before entering. The critical section is only the  sum+=1  so the following version is also correct but slower -       for (i = 0; i   10000000; i++) {\n        pthread_mutex_lock( m);\n        sum += 1;\n        pthread_mutex_unlock( m);\n    }\n    return NULL;\n}  This process runs slower because we lock and unlock the mutex a million times, which is expensive - at least compared with incrementing a variable. (And in this simple example we didn't really need threads - we could have added up twice!)  A faster multi-thread example would be to add one million using an automatic(local) variable and only then adding it to a shared total after the calculation loop has finished:      int local = 0;\n    for (i = 0; i   10000000; i++) {\n       local += 1;\n    }\n\n    pthread_mutex_lock( m);\n    sum += local;\n    pthread_mutex_unlock( m);\n\n    return NULL;\n}", 
            "title": "Simplest complete example?"
        }, 
        {
            "location": "/Synchronization,-Part-1:-Mutex-Locks/#how-do-i-find-out-more", 
            "text": "Play!  Read the man page!   pthread_mutex_lock man page    pthread_mutex_unlock man page    pthread_mutex_init man page    pthread_mutex_destroy man page", 
            "title": "How do I find out more?"
        }, 
        {
            "location": "/Synchronization,-Part-2:-Counting-Semaphores/", 
            "text": "What is a counting semaphore?\n\n\nA counting semaphore contains a value and supports two operations \"wait\" and \"post\". Post increments the semaphore and immediately returns. \"wait\" will wait if the count is zero. If the count is non-zero the semaphore decrements the count and immediately returns.\n\n\nAn analogy is a count of the cookies in a cookie jar (or gold coins in the treasure chest). Before taking a cookie, call 'wait'. If there are no cookies left then \nwait\n will not return: It will \nwait\n until another thread increments the semaphore by calling post.\n\n\nIn short, \npost\n increments and immediately returns whereas \nwait\n will wait if the count is zero. Before returning it will decrement count.\n\n\nHow do I create a semaphore?\n\n\nThis page introduces unnamed semaphores. Unfortunately Mac OS X does not support these yet.\n\n\nFirst decide if the initial value should be zero or some other value (e.g. the number of remaining spaces in an array).\nUnlike pthread mutex there are not shortcuts to creating a semaphore - use \nsem_init\n\n\n#include \nsemaphore.h\n\n\nsem_t s;\nint main() {\n  sem_init(\ns, 0, 10); // returns -1 (=FAILED) on OS X\n  sem_wait(\ns); // Could do this 10 times without blocking\n  sem_post(\ns); // Announce that we've finished (and one more resource item is available; increment count)\n  sem_destroy(\ns); // release resources of the semaphore\n}\n\n\n\n\nCan I call wait and post from different threads?\n\n\nYes! Unlike a mutex, the increment and decrement can be from different threads.\n\n\nCan I use a semaphore instead of a mutex?\n\n\nYes - though the overhead of a semaphore is greater. To use a semaphore:\n\n Initialize the semaphore with a count of one.\n\n Replace \n...lock\n with \nsem_wait\n\n* Replace \n...unlock\n with \nsem_post\n\n\nCan I use sem_post inside a signal handler?\n\n\nYes! \nsem_post\n is one of a handful of functions that can be correctly used inside a signal handler.\nThis means we can release a waiting thread which can now make all of the calls that we were not\nallowed to call inside the signal handler itself (e.g. \nprintf\n).\n\n\n#include \nstdio.h\n\n#include \npthread.h\n\n#include \nsignal.h\n\n#include \nsemaphore.h\n\n#include \nunistd.h\n\n\nsem_t s;\n\nvoid handler(int signal)\n{\n    sem_post(\ns); /* Release the Kraken! */\n}\n\nvoid *singsong(void *param)\n{\n    sem_wait(\ns);\n    printf(\nI had to wait until your signal released me!\\n\n);\n}\n\nint main()\n{\n    int ok = sem_init(\ns, 0, 0 /* Initial value of zero*/); \n    if (ok == -1) {\n       perror(\nCould not create unnamed semaphore\n);\n       return 1;\n    }\n    signal(SIGINT, handler); // Too simple! See note below\n\n    pthread_t tid;\n    pthread_create(\ntid, NULL, singsong, NULL);\n    pthread_exit(NULL); /* Process will exit when there are no more threads */\n}\n\n\n\n\nNote robust programs do not use \nsignal()\n in a multi-threaded program (\"The effects of signal() in a multithreaded process are unspecified.\" - the signal man page); a more correct program will need to use \nsigaction\n.\n\n\nHow do I find out more?\n\n\nPlay using a real linux system! (9/19/14: Linux-In-the-Browser project is missing semaphore.h - this will be fixed in the next update). Read the man pages:\n\n \nsem_init\n\n\n \nsem_wait\n\n\n \nsem_post\n\n\n \nsem_destroy", 
            "title": "Synchronization, Part 2: Counting Semaphores"
        }, 
        {
            "location": "/Synchronization,-Part-2:-Counting-Semaphores/#what-is-a-counting-semaphore", 
            "text": "A counting semaphore contains a value and supports two operations \"wait\" and \"post\". Post increments the semaphore and immediately returns. \"wait\" will wait if the count is zero. If the count is non-zero the semaphore decrements the count and immediately returns.  An analogy is a count of the cookies in a cookie jar (or gold coins in the treasure chest). Before taking a cookie, call 'wait'. If there are no cookies left then  wait  will not return: It will  wait  until another thread increments the semaphore by calling post.  In short,  post  increments and immediately returns whereas  wait  will wait if the count is zero. Before returning it will decrement count.", 
            "title": "What is a counting semaphore?"
        }, 
        {
            "location": "/Synchronization,-Part-2:-Counting-Semaphores/#how-do-i-create-a-semaphore", 
            "text": "This page introduces unnamed semaphores. Unfortunately Mac OS X does not support these yet.  First decide if the initial value should be zero or some other value (e.g. the number of remaining spaces in an array).\nUnlike pthread mutex there are not shortcuts to creating a semaphore - use  sem_init  #include  semaphore.h \n\nsem_t s;\nint main() {\n  sem_init( s, 0, 10); // returns -1 (=FAILED) on OS X\n  sem_wait( s); // Could do this 10 times without blocking\n  sem_post( s); // Announce that we've finished (and one more resource item is available; increment count)\n  sem_destroy( s); // release resources of the semaphore\n}", 
            "title": "How do I create a semaphore?"
        }, 
        {
            "location": "/Synchronization,-Part-2:-Counting-Semaphores/#can-i-call-wait-and-post-from-different-threads", 
            "text": "Yes! Unlike a mutex, the increment and decrement can be from different threads.", 
            "title": "Can I call wait and post from different threads?"
        }, 
        {
            "location": "/Synchronization,-Part-2:-Counting-Semaphores/#can-i-use-a-semaphore-instead-of-a-mutex", 
            "text": "Yes - though the overhead of a semaphore is greater. To use a semaphore:  Initialize the semaphore with a count of one.  Replace  ...lock  with  sem_wait \n* Replace  ...unlock  with  sem_post", 
            "title": "Can I use a semaphore instead of a mutex?"
        }, 
        {
            "location": "/Synchronization,-Part-2:-Counting-Semaphores/#can-i-use-sem_post-inside-a-signal-handler", 
            "text": "Yes!  sem_post  is one of a handful of functions that can be correctly used inside a signal handler.\nThis means we can release a waiting thread which can now make all of the calls that we were not\nallowed to call inside the signal handler itself (e.g.  printf ).  #include  stdio.h \n#include  pthread.h \n#include  signal.h \n#include  semaphore.h \n#include  unistd.h \n\nsem_t s;\n\nvoid handler(int signal)\n{\n    sem_post( s); /* Release the Kraken! */\n}\n\nvoid *singsong(void *param)\n{\n    sem_wait( s);\n    printf( I had to wait until your signal released me!\\n );\n}\n\nint main()\n{\n    int ok = sem_init( s, 0, 0 /* Initial value of zero*/); \n    if (ok == -1) {\n       perror( Could not create unnamed semaphore );\n       return 1;\n    }\n    signal(SIGINT, handler); // Too simple! See note below\n\n    pthread_t tid;\n    pthread_create( tid, NULL, singsong, NULL);\n    pthread_exit(NULL); /* Process will exit when there are no more threads */\n}  Note robust programs do not use  signal()  in a multi-threaded program (\"The effects of signal() in a multithreaded process are unspecified.\" - the signal man page); a more correct program will need to use  sigaction .", 
            "title": "Can I use sem_post inside a signal handler?"
        }, 
        {
            "location": "/Synchronization,-Part-2:-Counting-Semaphores/#how-do-i-find-out-more", 
            "text": "Play using a real linux system! (9/19/14: Linux-In-the-Browser project is missing semaphore.h - this will be fixed in the next update). Read the man pages:   sem_init    sem_wait    sem_post    sem_destroy", 
            "title": "How do I find out more?"
        }, 
        {
            "location": "/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/", 
            "text": "What is an atomic operation?\n\n\nTo paraphrase Wikipedia, \"An operation (or set of operations) is atomic or uninterruptible if it appears to the rest of the system to occur instantaneously.\"\nWithout locks, only simple CPU instructions (\"read this byte from memory\") are atomic (indivisible). On a single CPU system one could temporarily disable interrupts (so a sequence of operations cannot be interrupted) but in practice atomicity is achieved by using synchronization primitives, typically a mutex lock.\n\n\nIncrementing a variable (\ni++\n) is \nnot\n atomic because it requires three distinct steps: Copying the bit pattern from memory into the CPU; performing a calculation using the CPU's registers; copying the bit pattern back to memory. During this increment sequence, another thread or process can still read the old value and other writes to the same memory would also be over-written when the increment sequence completes.\n\n\nHow do I use mutex lock to make my data-structure thread-safe?\n\n\nNote, this is just an introduction - writing high-performance thread-safe data-structures requires it's own book! Here's a simple data structure (a stack) that is not thread-safe:\n\n\n// A simple fixed-sized stack (version 1)\nint count;\ndouble values[count];\n\nvoid push(double v) { values[count++] = v; }\ndouble pop() { return values[--count]; }\nint is_empty() { return count == 0; }\n\n\n\n\nVersion 1 of the stack is not thread-safe because if two threads call push or pop at the same time then the results or the stack can be inconsistent. For example, imagine if two threads call pop at the same time then both threads may read the same value, both may read the original count value.\n\n\nTo turn this into a thread-safe data structure we need to identify the \ncritical sections\n of our code  i.e. which section(s) of the code must only have one thread at a time. In the above example the \npush\n,\npop\n and \nis_empty\n functions access the same variables (i.e. memory) and all critical sections for the stack. \n\n\nWhile \npush\n (and \npop\n) is executing, the datastructure is an inconsistent state (for example the count may not have been written to, so may still contain the original value). By wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack.\n\n\nA candidate 'solution' is shown below. Is it correct? If not, how will it fail?\n\n\n// An attempt at a thread-safe stack (version 2)\nint count;\ndouble values[count];\npthread_mutex_t m1 = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_t m2 = PTHREAD_MUTEX_INITIALIZER;\n\nvoid push() { pthread_mutex_lock(\nm1); values[count++] = values; pthread_mutex_unlock(\nm1); }\ndouble pop() { pthread_mutex_lock(\nm2); double v=values[--count]; pthread_mutex_unlock(\nm2); return v;}\nint is_empty() { pthread_mutex_lock(\nm1); return count == 0; pthread_mutex_unlock(\nm1); }\n\n\n\n\nThe above code ('version 2') contains at least one error. Take a moment to see if you can the error(s) and work out the consequence(s).\n\n\nIf three called \npush()\n at the same time the lock \nm1\n ensures that only one thread at time manipulates the stack (two threads will need to wait until the first thread completes (calls unlock), then a second thread will be allowed to continue into the critical section and finally the third thread will be allowed to continue once the second thread has finished).\n\n\nA similar argument applies to concurrent calls (calls at the same time) to \npop\n. However version 2 does not prevent push and pop from running at the same time because \npush\n and \npop\n use two different mutex locks.\n\n\nThe fix is simple in this case - use the same mutex lock for both the push and pop functions.\n\n\nThe code has a second error; \nis_empty\n returns after the comparison and will not unlock the mutex. However the error would not be spotted immediately. For example, suppose one thread calls \nis_empty\n and a second thread later calls \npush\n. This thread would mysteriously stop. Using debugger you can discover that the thread is stuck at the lock() method inside the \npush\n method because the lock was never unlocked by the earlier \nis_empty\n call. Thus an oversight in one thread led to problems much later in time in an arbitrary other thread.\n\n\nA better version is shown below - \n\n\n// An attempt at a thread-safe stack (version 3)\nint count;\ndouble values[count];\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid push(double v) { \n  pthread_mutex_lock(\nm); \n  values[count++] = v;\n  pthread_mutex_unlock(\nm);\n}\ndouble pop() {\n  pthread_mutex_lock(\nm);\n  double v = values[--count];\n  pthread_mutex_unlock(\nm);\n  return v;\n}\nint is_empty() {\n  pthread_mutex_lock(\nm);\n  int result= count == 0;\n  pthread_mutex_unlock(\nm);\n  return result;\n}\n\n\n\n\nVersion 3 is thread-safe (we have ensured mutual exclusion for all of the critical sections) however there are two points of note:\n\n \nis_empty\n is thread-safe but its result may already be out-of date i.e. the stack may no longer be empty by the time the thread gets the result!\n\n There is no protection against underflow (popping on an empty stack) or overflow (pushing onto an already-full stack)\n\n\nThe latter point can be fixed using counting semaphores.\n\n\nThe implementation assumes a single stack.  A more general purpose version might include the mutex as part of the memory struct and use pthread_mutex_init to initialize the mutex. For example,\n\n\n// Support for multiple stacks (each one has a mutex)\ntypedef struct stack {\n  int count;\n  pthread_mutex_t m; \n  double *values;\n} stack_t;\n\nstack_t* stack_create(int capacity) {\n  stack_t *result = malloc(sizeof(stack_t));\n  result-\ncount = 0;\n  result-\nvalues = malloc(sizeof(double) * capacity);\n  pthread_mutex_init(\nresult-\nm, NULL);\n  return result;\n}\nvoid stack_destroy(stack_t *s) {\n  free(s-\nvalues);\n  pthread_mutex_destroy(\ns-\nm);\n  free(s);\n}\n// Warning no underflow or overflow checks!\n\nvoid push(stack_t *s, double v) { \n  pthread_mutex_lock(\ns-\nm); \n  s-\nvalues[(s-\ncount)++] = v; \n  pthread_mutex_unlock(\ns-\nm); }\n\ndouble pop(stack_t *s) { \n  pthread_mutex_lock(\ns-\nm); \n  double v = s-\nvalues[--(s-\ncount)]; \n  pthread_mutex_unlock(\ns-\nm); \n  return v;\n}\n\nint is_empty(stack_t *s) { \n  pthread_mutex_lock(\ns-\nm); \n  int result = s-\ncount == 0; \n  pthread_mutex_unlock(\ns-\nm);\n  return result;\n}\n\n\n\n\nExample use:\n\n\nint main() {\n    stack_t *s1 = stack_create(10 /* Max capacity*/);\n    stack_t *s2 = stack_create(10);\n    push(s1, 3.141);\n    push(s2, pop(s1));\n    stack_destroy(s2);\n    stack_destroy(s1);\n}\n\n\n\n\nWhen can I destroy the mutex?\n\n\nYou can only destroy an unlocked mutex\n\n\nCan I copy a pthread_mutex_t to a new memory locaton?\n\n\nNo, copying the bytes of the mutex to a new memory location and then using the copy is \nnot\n supported.\n\n\nWhat would a simple implementation of a mutex look like?\n\n\nA simple (but incorrect!) suggestion is shown below. The \nunlock\n function simply unlocks the mutex and returns. The lock function first checks to see if the lock is already locked. If it is currently locked, it will keep checking again until another thread has unlocked the mutex.\n\n\n// Version 1 (Incorrect!)\n\nvoid lock(mutex_t *m) {\n  while(m-\nlocked) { /*Locked? Nevermind - just loop and check again!*/ }\n\n  m-\nlocked = 1;\n}\nvoid unlock(mutex_t *m) {\n  m-\nlocked = 0;\n}\n\n\n\n\nVersion 1 uses 'busy-waiting' (unnecessarily wasting CPU resources) however there is a more serious problem: We have a race-condition! \n\n\nIf two threads both called \nlock\n concurrently it is possible that both threads would read 'm_locked' as zero. Thus both threads would believe they have exclusive access to the lock and both threads will continue. Ooops!\n\n\nWe might attempt to reduce the CPU overhead a little by calling \npthread_yield()\n inside the loop  - pthread_yield suggests to the operating system that the thread does not use the CPU for a short while, so the CPU may be assigned to threads that are waiting to run. But does not fix the race-condition. We need a better implementation - can you work how to prevent the race-condition?\n\n\nHow can I force my threads to wait if the stack is empty or full?\n\n\nUse counting semaphores! Use a counting semaphore to keep track of how many spaces remain and another semaphore to keep to track the number of items in the stack. We will call these two semaphores 'sremain' and 'sitems'. Remember \nsem_wait\n will wait if the semaphore's count has been decremented to zero (by another thread calling sem_post).\n\n\n// Sketch #1\n\nsem_t sitems;\nsem_t sremain;\nvoid stack_init(){\n  sem_init(\nsitems, 0, 0);\n  sem_init(\nsremain, 0, 10);\n}\n\n\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(\nsitems);\n  ...\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(\nsremain);\n  ...\n\n\n\n\nSketch #2  has implemented the \npost\n too early. Another thread waiting in push can erroneously attempt to write into a full stack (and similarly a thread waiting in the pop() is allowed to continue too early).\n\n\n// Sketch #2 (Error!)\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(\nsitems);\n  sem_post(\nsremain); // error! wakes up pushing() thread too early\n  return values[--count];\n}\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(\nsremain);\n  sem_post(\nsitems); // error! wakes up a popping() thread too early\n  values[count++] = v;\n}\n\n\n\n\nSketch 3 implements the correct semaphore logic but can you spot the error?\n\n\n// Sketch #3 (Error!)\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(\nsitems);\n  double v= values[--count];\n  sem_post(\nsremain);\n  return v;\n}\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(\nsremain);\n  values[count++] = v;\n  sem_post(\nsitems); \n}\n\n\n\n\nSketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. However there is no \nmutual exclusion\n: Two threads can be in the \ncritical section\n at the same time, which would corrupt the data structure (or least lead to data loss). The fix is to wrap a mutex around the critical section:\n\n\n// Simple single stack - see above example on how to convert this into a multiple stacks.\n// Also a robust POSIX implementation would check for EINTR and error codes of sem_wait.\n\n// PTHREAD_MUTEX_INITIALIZER for statics (use pthread_mutex_init() for stack/heap memory)\n\npthread_mutex_t m= PTHREAD_MUTEX_INITIALIZER; \nint count = 0;\ndouble values[10];\nsem_t sitems, sremain;\n\nvoid init() {\n  sem_init(\nsitems, 0, 0);\n  sem_init(\nsremains, 0, 10); // 10 spaces\n}\n\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait(\nsitems);\n\n  pthread_mutex_lock(\nm); // CRITICAL SECTION\n  double v= values[--count];\n  pthread_mutex_unlock(\nm);\n\n  sem_post(\nsremain); // Hey world, there's at least one space\n  return v;\n}\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait(\nsremain);\n\n  pthread_mutex_lock(\nm); // CRITICAL SECTION\n  values[count++] = v;\n  pthread_mutex_unlock(\nm);\n\n  sem_post(\nsitems); // Hey world, there's at least one item\n}\n// Note a robust solution will need to check sem_wait's result for EINTR (more about this later)\n\n\n\n\nWhat are the common Mutex Gotchas?\n\n\n\n\nLocking/unlocking the wrong mutex (due to a silly typo)\n\n\nNot unlocking a mutex (due to say an early return during an error condition)\n\n\nResource leak (not calling \npthread_mutex_destroy\n)\n\n\nUsing an unitialized mutex (or using a mutex that has already been destroyed)\n\n\nLocking a mutex twice on a thread (without unlocking first)\n\n\nDeadlock and Priority Inversion (we will talk about these later)", 
            "title": "Synchronization, Part 3: Working with Mutexes And Semaphores"
        }, 
        {
            "location": "/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#what-is-an-atomic-operation", 
            "text": "To paraphrase Wikipedia, \"An operation (or set of operations) is atomic or uninterruptible if it appears to the rest of the system to occur instantaneously.\"\nWithout locks, only simple CPU instructions (\"read this byte from memory\") are atomic (indivisible). On a single CPU system one could temporarily disable interrupts (so a sequence of operations cannot be interrupted) but in practice atomicity is achieved by using synchronization primitives, typically a mutex lock.  Incrementing a variable ( i++ ) is  not  atomic because it requires three distinct steps: Copying the bit pattern from memory into the CPU; performing a calculation using the CPU's registers; copying the bit pattern back to memory. During this increment sequence, another thread or process can still read the old value and other writes to the same memory would also be over-written when the increment sequence completes.", 
            "title": "What is an atomic operation?"
        }, 
        {
            "location": "/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#how-do-i-use-mutex-lock-to-make-my-data-structure-thread-safe", 
            "text": "Note, this is just an introduction - writing high-performance thread-safe data-structures requires it's own book! Here's a simple data structure (a stack) that is not thread-safe:  // A simple fixed-sized stack (version 1)\nint count;\ndouble values[count];\n\nvoid push(double v) { values[count++] = v; }\ndouble pop() { return values[--count]; }\nint is_empty() { return count == 0; }  Version 1 of the stack is not thread-safe because if two threads call push or pop at the same time then the results or the stack can be inconsistent. For example, imagine if two threads call pop at the same time then both threads may read the same value, both may read the original count value.  To turn this into a thread-safe data structure we need to identify the  critical sections  of our code  i.e. which section(s) of the code must only have one thread at a time. In the above example the  push , pop  and  is_empty  functions access the same variables (i.e. memory) and all critical sections for the stack.   While  push  (and  pop ) is executing, the datastructure is an inconsistent state (for example the count may not have been written to, so may still contain the original value). By wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack.  A candidate 'solution' is shown below. Is it correct? If not, how will it fail?  // An attempt at a thread-safe stack (version 2)\nint count;\ndouble values[count];\npthread_mutex_t m1 = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_t m2 = PTHREAD_MUTEX_INITIALIZER;\n\nvoid push() { pthread_mutex_lock( m1); values[count++] = values; pthread_mutex_unlock( m1); }\ndouble pop() { pthread_mutex_lock( m2); double v=values[--count]; pthread_mutex_unlock( m2); return v;}\nint is_empty() { pthread_mutex_lock( m1); return count == 0; pthread_mutex_unlock( m1); }  The above code ('version 2') contains at least one error. Take a moment to see if you can the error(s) and work out the consequence(s).  If three called  push()  at the same time the lock  m1  ensures that only one thread at time manipulates the stack (two threads will need to wait until the first thread completes (calls unlock), then a second thread will be allowed to continue into the critical section and finally the third thread will be allowed to continue once the second thread has finished).  A similar argument applies to concurrent calls (calls at the same time) to  pop . However version 2 does not prevent push and pop from running at the same time because  push  and  pop  use two different mutex locks.  The fix is simple in this case - use the same mutex lock for both the push and pop functions.  The code has a second error;  is_empty  returns after the comparison and will not unlock the mutex. However the error would not be spotted immediately. For example, suppose one thread calls  is_empty  and a second thread later calls  push . This thread would mysteriously stop. Using debugger you can discover that the thread is stuck at the lock() method inside the  push  method because the lock was never unlocked by the earlier  is_empty  call. Thus an oversight in one thread led to problems much later in time in an arbitrary other thread.  A better version is shown below -   // An attempt at a thread-safe stack (version 3)\nint count;\ndouble values[count];\npthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n\nvoid push(double v) { \n  pthread_mutex_lock( m); \n  values[count++] = v;\n  pthread_mutex_unlock( m);\n}\ndouble pop() {\n  pthread_mutex_lock( m);\n  double v = values[--count];\n  pthread_mutex_unlock( m);\n  return v;\n}\nint is_empty() {\n  pthread_mutex_lock( m);\n  int result= count == 0;\n  pthread_mutex_unlock( m);\n  return result;\n}  Version 3 is thread-safe (we have ensured mutual exclusion for all of the critical sections) however there are two points of note:   is_empty  is thread-safe but its result may already be out-of date i.e. the stack may no longer be empty by the time the thread gets the result!  There is no protection against underflow (popping on an empty stack) or overflow (pushing onto an already-full stack)  The latter point can be fixed using counting semaphores.  The implementation assumes a single stack.  A more general purpose version might include the mutex as part of the memory struct and use pthread_mutex_init to initialize the mutex. For example,  // Support for multiple stacks (each one has a mutex)\ntypedef struct stack {\n  int count;\n  pthread_mutex_t m; \n  double *values;\n} stack_t;\n\nstack_t* stack_create(int capacity) {\n  stack_t *result = malloc(sizeof(stack_t));\n  result- count = 0;\n  result- values = malloc(sizeof(double) * capacity);\n  pthread_mutex_init( result- m, NULL);\n  return result;\n}\nvoid stack_destroy(stack_t *s) {\n  free(s- values);\n  pthread_mutex_destroy( s- m);\n  free(s);\n}\n// Warning no underflow or overflow checks!\n\nvoid push(stack_t *s, double v) { \n  pthread_mutex_lock( s- m); \n  s- values[(s- count)++] = v; \n  pthread_mutex_unlock( s- m); }\n\ndouble pop(stack_t *s) { \n  pthread_mutex_lock( s- m); \n  double v = s- values[--(s- count)]; \n  pthread_mutex_unlock( s- m); \n  return v;\n}\n\nint is_empty(stack_t *s) { \n  pthread_mutex_lock( s- m); \n  int result = s- count == 0; \n  pthread_mutex_unlock( s- m);\n  return result;\n}  Example use:  int main() {\n    stack_t *s1 = stack_create(10 /* Max capacity*/);\n    stack_t *s2 = stack_create(10);\n    push(s1, 3.141);\n    push(s2, pop(s1));\n    stack_destroy(s2);\n    stack_destroy(s1);\n}", 
            "title": "How do I use mutex lock to make my data-structure thread-safe?"
        }, 
        {
            "location": "/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#when-can-i-destroy-the-mutex", 
            "text": "You can only destroy an unlocked mutex", 
            "title": "When can I destroy the mutex?"
        }, 
        {
            "location": "/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#can-i-copy-a-pthread_mutex_t-to-a-new-memory-locaton", 
            "text": "No, copying the bytes of the mutex to a new memory location and then using the copy is  not  supported.", 
            "title": "Can I copy a pthread_mutex_t to a new memory locaton?"
        }, 
        {
            "location": "/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#what-would-a-simple-implementation-of-a-mutex-look-like", 
            "text": "A simple (but incorrect!) suggestion is shown below. The  unlock  function simply unlocks the mutex and returns. The lock function first checks to see if the lock is already locked. If it is currently locked, it will keep checking again until another thread has unlocked the mutex.  // Version 1 (Incorrect!)\n\nvoid lock(mutex_t *m) {\n  while(m- locked) { /*Locked? Nevermind - just loop and check again!*/ }\n\n  m- locked = 1;\n}\nvoid unlock(mutex_t *m) {\n  m- locked = 0;\n}  Version 1 uses 'busy-waiting' (unnecessarily wasting CPU resources) however there is a more serious problem: We have a race-condition!   If two threads both called  lock  concurrently it is possible that both threads would read 'm_locked' as zero. Thus both threads would believe they have exclusive access to the lock and both threads will continue. Ooops!  We might attempt to reduce the CPU overhead a little by calling  pthread_yield()  inside the loop  - pthread_yield suggests to the operating system that the thread does not use the CPU for a short while, so the CPU may be assigned to threads that are waiting to run. But does not fix the race-condition. We need a better implementation - can you work how to prevent the race-condition?", 
            "title": "What would a simple implementation of a mutex look like?"
        }, 
        {
            "location": "/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#how-can-i-force-my-threads-to-wait-if-the-stack-is-empty-or-full", 
            "text": "Use counting semaphores! Use a counting semaphore to keep track of how many spaces remain and another semaphore to keep to track the number of items in the stack. We will call these two semaphores 'sremain' and 'sitems'. Remember  sem_wait  will wait if the semaphore's count has been decremented to zero (by another thread calling sem_post).  // Sketch #1\n\nsem_t sitems;\nsem_t sremain;\nvoid stack_init(){\n  sem_init( sitems, 0, 0);\n  sem_init( sremain, 0, 10);\n}\n\n\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait( sitems);\n  ...\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait( sremain);\n  ...  Sketch #2  has implemented the  post  too early. Another thread waiting in push can erroneously attempt to write into a full stack (and similarly a thread waiting in the pop() is allowed to continue too early).  // Sketch #2 (Error!)\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait( sitems);\n  sem_post( sremain); // error! wakes up pushing() thread too early\n  return values[--count];\n}\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait( sremain);\n  sem_post( sitems); // error! wakes up a popping() thread too early\n  values[count++] = v;\n}  Sketch 3 implements the correct semaphore logic but can you spot the error?  // Sketch #3 (Error!)\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait( sitems);\n  double v= values[--count];\n  sem_post( sremain);\n  return v;\n}\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait( sremain);\n  values[count++] = v;\n  sem_post( sitems); \n}  Sketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. However there is no  mutual exclusion : Two threads can be in the  critical section  at the same time, which would corrupt the data structure (or least lead to data loss). The fix is to wrap a mutex around the critical section:  // Simple single stack - see above example on how to convert this into a multiple stacks.\n// Also a robust POSIX implementation would check for EINTR and error codes of sem_wait.\n\n// PTHREAD_MUTEX_INITIALIZER for statics (use pthread_mutex_init() for stack/heap memory)\n\npthread_mutex_t m= PTHREAD_MUTEX_INITIALIZER; \nint count = 0;\ndouble values[10];\nsem_t sitems, sremain;\n\nvoid init() {\n  sem_init( sitems, 0, 0);\n  sem_init( sremains, 0, 10); // 10 spaces\n}\n\ndouble pop() {\n  // Wait until there's at least one item\n  sem_wait( sitems);\n\n  pthread_mutex_lock( m); // CRITICAL SECTION\n  double v= values[--count];\n  pthread_mutex_unlock( m);\n\n  sem_post( sremain); // Hey world, there's at least one space\n  return v;\n}\n\nvoid push(double v) {\n  // Wait until there's at least one space\n  sem_wait( sremain);\n\n  pthread_mutex_lock( m); // CRITICAL SECTION\n  values[count++] = v;\n  pthread_mutex_unlock( m);\n\n  sem_post( sitems); // Hey world, there's at least one item\n}\n// Note a robust solution will need to check sem_wait's result for EINTR (more about this later)", 
            "title": "How can I force my threads to wait if the stack is empty or full?"
        }, 
        {
            "location": "/Synchronization,-Part-3:-Working-with-Mutexes-And-Semaphores/#what-are-the-common-mutex-gotchas", 
            "text": "Locking/unlocking the wrong mutex (due to a silly typo)  Not unlocking a mutex (due to say an early return during an error condition)  Resource leak (not calling  pthread_mutex_destroy )  Using an unitialized mutex (or using a mutex that has already been destroyed)  Locking a mutex twice on a thread (without unlocking first)  Deadlock and Priority Inversion (we will talk about these later)", 
            "title": "What are the common Mutex Gotchas?"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/", 
            "text": "What is the Critical Section Problem?\n\n\nAs already discussed in [[Synchronization, Part 3: Working with Mutexes And Semaphores]], there are critical parts of our code that can only be executed by one thread at a time. We describe this requirement as 'mutual exclusion'; only one thread (or process) may have access to the shared resource.\n\n\nIn multi-threaded programs we can wrap a critical section with mutex lock and unlock calls:\n\n\npthread_mutex_lock() - one thread allowed at a time! (others will have to wait here)\n... Do Critical Section stuff here!\npthread_mutex_unlock() - let other waiting threads continue\n\n\n\n\nHow would we implement these lock and unlock calls? Can we create an algorithm that assures mutual exclusion? An incorrect implementation is shown below, \n\n\npthread_mutex_lock(p_mutex_t *m)     { while(m-\nlock) {}; m-\nlock = 1;}\npthread_mutex_unlock(p_mutex_t *m)   { m-\nlock = 0; }\n\n\n\n\nAt first glance, the code appears to work; if one thread attempts to locks the mutex, a later thread must wait until the lock is cleared. However this implementation \ndoes not satisfy Mutual Exclusion\n. Let's take a close look at this 'implementation' from the point of view of two threads running around the same time. In the table below times runs from top to bottom-\n\n\n\n\n\n\n\n\nTime\n\n\nThread 1\n\n\nThread 2\n\n\n\n\n\n\n\n\n\n\n1\n\n\nwhile(lock) {}\n\n\n\n\n\n\n\n\n2\n\n\n\n\nwhile(lock) {}\n\n\n\n\n\n\n3\n\n\nlock = 1\n\n\nlock = 1\n\n\n\n\n\n\nOoops! There is a race condition. In the unfortunate case both threads checked the lock and read a false value and so were able to continue.\n\n\n\n\n\n\n\n\n\n\n\n\nCandidate solutions to the critical section problem.\n\n\nTo simplify the discussion we consider only two threads. Note these arguments work for threads and processes and the classic CS literature discusses these problem in terms of two processes that need exclusive access (i.e. mutual exclusion) to a critical section or shared resource.\n\n\nRaising a flag represents a thread/process's intention to enter the critical section.\n\n\nRemember that the psuedo-code outlined below is part of a larger program; the thread or process will typically need to enter the critical section many times during the lifetime of the process. So imagine each example as wrapped inside a loop where for a random amount of time the thread or process is working on something else.\n\n\nIs there anything wrong with candidate solution described below?\n\n\n// Candidate #1\nwait until your flag is lowered\nraise my flag\n// Do Critical Section stuff\nlower my flag \n\n\n\n\nAnswer: Candidate solution #1 also suffers a race condition i.e. it does not satisfy Mutual Exclusion because both threads/processes could read each other's flag value (=lowered) and continue. \n\n\nThis suggests we should raise the flag \nbefore\n checking the other thread's flag - which is candidate solution #2 below.\n\n\n// Candidate #2\nraise my flag\nwait until your flag is lowered\n// Do Critical Section stuff\nlower my flag \n\n\n\n\nCandidate #2 satisfies mutual exclusion - it is impossible for two threads to be inside the critical section at the same time. However this code suffers from deadlock! Suppose two threads wish to enter the critical section at the same time:\n\n\n\n\n\n\n\n\nTime\n\n\nThread 1\n\n\nThread 2\n\n\n\n\n\n\n\n\n\n\n1\n\n\nraise flag\n\n\n\n\n\n\n\n\n2\n\n\n\n\nraise flag\n\n\n\n\n\n\n3\n\n\nwait ...\n\n\nwait ...\n\n\n\n\n\n\nOoops both threads / processes are now waiting for the other one to lower their flags. Neither one will enter the critical section as both are now stuck forever!\n\n\n\n\n\n\n\n\n\n\n\n\nThis suggests we should use a turn-based variable to try to resolve who should proceed. \n\n\nTurn-based solutions\n\n\nThe following candidate solution #3 uses a turn-based variable to politely allow one thread and then the other to continue\n\n\n// Candidate #3\nwait until my turn is myid\n// Do Critical Section stuff\nturn = yourid\n\n\n\n\nCandidate #3 satisfies mutual exclusion (each thread or process gets exclusive access to the Critical Section), however both threads/processes must take a strict turn-based approach to using the critical section; i.e. they are forced into an alternating critical section access pattern. For example, if thread 1 wishes to read a hashtable every millisecond but another thread writes to a hashtable every second, then the reading thread would have to wait another 999ms before being able to read from the hashtable again. This 'solution' is not effective, because our threads should be able to make progress and enter the critical section if no other thread is currently in the critical section.\n\n\nDesired properties for solutions to the Critical Section Problem?\n\n\nThere are three main desirable properties that we desire in a solution the critical section problem\n\n Mutual Exclusion - the thread/process gets exclusive access; others must wait until it exits the critical section.\n\n Bounded Wait - if the thread/process has to wait, then it should only have to wait for a finite,  amount of time (infinite waiting times are not allowed!). The exact definition of bounded wait is that there is an upper (non-infinite) bound on the number of times any other process can enter its critical section before the given process enters.\n* Progress - if no thread/process is inside the critical section, then the thread/process should be able to proceed (make progress) without having to wait.\n\n\nWith these ideas in mind let's examine another candidate solution that uses a turn-based flag only if two threads both required access at the same time.\n\n\nTurn and Flag solutions\n\n\nIs the following a correct solution to CSP?\n\n\n\\\\ Candidate #4\nraise my flag\nif your flag is raised, wait until my turn\n// Do Critical Section stuff\nturn = yourid\nlower my flag\n\n\n\n\nOne instructor and another CS faculty member initially thought so! However, analyzing these solutions is tricky. Even peer-reviewed papers on this specific subject contain incorrect solutions! At first glance it appears to satisfy Mutual Exclusion, Bounded Wait and Progress: The turn-based flag is only used in the event of a tie (so Progress and Bounded Wait is allowed) and mutual exclusion appears to be satisfied. However.... Perhaps you can find a counter-example?\n\n\nCandidate #4 fails because a thread does not wait until the other thread lowers their flag. After some thought (or inspiration) the following scenario can be created to demonstrate how Mutual Exclusion is not satisfied.\n\n\nImagine the first thread runs this code twice (so the the turn flag now points to the second thread). While the first thread is still inside the Critical Section, the second thread arrives. The second thread can immediately continue into the Critical Section!\n\n\n\n\n\n\n\n\nTime\n\n\nTurn\n\n\nThread #1\n\n\nThread #2\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\nraise my flag\n\n\n\n\n\n\n\n\n2\n\n\n2\n\n\nif your flag is raised, wait until my turn\n\n\nraise my flag\n\n\n\n\n\n\n3\n\n\n2\n\n\n// Do Critical Section stuff\n\n\nif your flag is raised, wait until my turn(TRUE!)\n\n\n\n\n\n\n4\n\n\n2\n\n\n// Do Critical Section stuff\n\n\n// Do Critical Section stuff - OOPS\n\n\n\n\n\n\n\n\nWhat is Peterson's solution?\n\n\nPeterson published his novel and surprisingly simple solution in a 2 page paper in 1981. A version of his algorithm is shown below that uses a shared variable \nturn\n: \n\n\n\\\\ Candidate #5\nraise my flag\nturn = myid\nwait until your flag is lowered or turn is yourid\n// Do Critical Section stuff\nlower my flag\n\n\n\n\nThis solution satisfies Mutual Exclusion, Bounded Wait and Progress. If thread #2 has set turn to 2 and is currently inside the critical section. Thread #1 arrives, \nsets the turn back to 1\n and now waits until thread 2 lowers the flag.\n\n\nLink to Peterson's original article pdf:\n\nG. L. Peterson: \"Myths About the Mutual Exclusion Problem\", Information Processing Letters 12(3) 1981, 115\u2013116\n\n\nWas Peterson's solution the first solution?\n\n\nNo, Dekkers Algorithm (1962) was the first provably correct solution. A version of the algorithm is below.\n\n\nraise my flag\nwhile(your flag is raised) :\n   if it's your turn to win :\n     lower my flag\n     wait while your turn\n     raise my flag\n// Do Critical Section stuff\nset your turn to win\nlower my flag\n\n\n\n\nNotice how the process's flag is always raised during the critical section no matter if the loop is iterated zero, once or more times. Further the flag can be interpreted as an immediate intent to enter the critical section. Only if the other process has also raised the flag will one process defer, lower their intent flag and wait.\n\n\nCan I just implement Peterson's (or Dekkers) algorithm in C or assembler?\n\n\nYes - and with a bit searching it is possible even today to find it in production for specific simple mobile processors: Peterson's algorithm is used to implement low-level Linux Kernel locks for the Tegra mobile processor (a system-on-chip ARM process and GPU core by Nvidia)\nhttps://android.googlesource.com/kernel/tegra.git/+/android-tegra-3.10/arch/arm/mach-tegra/sleep.S#58\n\n\nHowever in general, CPUs and C compilers can re-order CPU instructions or use CPU-core-specific local cache values that are stale if another core updates the shared variables. Thus a simple pseudo-code to C implementation is too naive for most platforms. You can stop reading now.\n\n\nOh... you decided to keep reading. Well, here be dragons! Don't say we didn't warn you. Consider this advanced and gnarly topic but (spoiler alert) a happy ending.\n\n\nConsider the following code,\n\n\nwhile(flag2 ) { /* busy loop - go around again */\n\n\n\n\nAn efficient compiler would infer that \nflag2\n variable is never changed inside the loop, so that test can be optimized to \nwhile(true)\n \nUsing \nvolatile\n goes someway to prevent compiler optimizations of this kind.\n\n\nIndependent instructions can be re-ordered by an optimizing compiler or at runtime by an out-of-order execution optimization by the CPU. These sophisticated optimizations if the code requires variables to be modified and checked and a precise order.\n\n\nA related challenge is that CPU cores include a data cache to store recently read or modified main memory values. Modified values may not be written back to main memory or re-read from memory immediately. Thus data changes, such as the state of a flag and turn variable in the above examples, may not be shared between two CPU codes. \n\n\nBut there is happy ending. Fortunately, modern hardware addresses these issues using 'memory fences' (also known as memory barrier) CPU instructions to ensure that main memory and the CPUs' cache is in a reasonable and coherent state. Higher level synchronization primitives, such as \npthread_mutex_lock\n are will call these CPU instructions as part of their implementation. Thus, in practice, surrounding critical section with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.\n\n\nFurther reading: we suggest the following web post that discusses implementing Peterson's algorithm on an x86 process and the linux documentation on memory barriers.\n\n\nhttp://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/\nhttp://lxr.free-electrons.com/source/Documentation/memory-barriers.txt\n\n\nHow do we implement Critical Section Problem on hardware?\n\n\nGood question. Next lecture...", 
            "title": "Synchronization, Part 4: The Critical Section Problem"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#what-is-the-critical-section-problem", 
            "text": "As already discussed in [[Synchronization, Part 3: Working with Mutexes And Semaphores]], there are critical parts of our code that can only be executed by one thread at a time. We describe this requirement as 'mutual exclusion'; only one thread (or process) may have access to the shared resource.  In multi-threaded programs we can wrap a critical section with mutex lock and unlock calls:  pthread_mutex_lock() - one thread allowed at a time! (others will have to wait here)\n... Do Critical Section stuff here!\npthread_mutex_unlock() - let other waiting threads continue  How would we implement these lock and unlock calls? Can we create an algorithm that assures mutual exclusion? An incorrect implementation is shown below,   pthread_mutex_lock(p_mutex_t *m)     { while(m- lock) {}; m- lock = 1;}\npthread_mutex_unlock(p_mutex_t *m)   { m- lock = 0; }  At first glance, the code appears to work; if one thread attempts to locks the mutex, a later thread must wait until the lock is cleared. However this implementation  does not satisfy Mutual Exclusion . Let's take a close look at this 'implementation' from the point of view of two threads running around the same time. In the table below times runs from top to bottom-     Time  Thread 1  Thread 2      1  while(lock) {}     2   while(lock) {}    3  lock = 1  lock = 1    Ooops! There is a race condition. In the unfortunate case both threads checked the lock and read a false value and so were able to continue.", 
            "title": "What is the Critical Section Problem?"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#candidate-solutions-to-the-critical-section-problem", 
            "text": "To simplify the discussion we consider only two threads. Note these arguments work for threads and processes and the classic CS literature discusses these problem in terms of two processes that need exclusive access (i.e. mutual exclusion) to a critical section or shared resource.  Raising a flag represents a thread/process's intention to enter the critical section.  Remember that the psuedo-code outlined below is part of a larger program; the thread or process will typically need to enter the critical section many times during the lifetime of the process. So imagine each example as wrapped inside a loop where for a random amount of time the thread or process is working on something else.  Is there anything wrong with candidate solution described below?  // Candidate #1\nwait until your flag is lowered\nraise my flag\n// Do Critical Section stuff\nlower my flag   Answer: Candidate solution #1 also suffers a race condition i.e. it does not satisfy Mutual Exclusion because both threads/processes could read each other's flag value (=lowered) and continue.   This suggests we should raise the flag  before  checking the other thread's flag - which is candidate solution #2 below.  // Candidate #2\nraise my flag\nwait until your flag is lowered\n// Do Critical Section stuff\nlower my flag   Candidate #2 satisfies mutual exclusion - it is impossible for two threads to be inside the critical section at the same time. However this code suffers from deadlock! Suppose two threads wish to enter the critical section at the same time:     Time  Thread 1  Thread 2      1  raise flag     2   raise flag    3  wait ...  wait ...    Ooops both threads / processes are now waiting for the other one to lower their flags. Neither one will enter the critical section as both are now stuck forever!       This suggests we should use a turn-based variable to try to resolve who should proceed.", 
            "title": "Candidate solutions to the critical section problem."
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#turn-based-solutions", 
            "text": "The following candidate solution #3 uses a turn-based variable to politely allow one thread and then the other to continue  // Candidate #3\nwait until my turn is myid\n// Do Critical Section stuff\nturn = yourid  Candidate #3 satisfies mutual exclusion (each thread or process gets exclusive access to the Critical Section), however both threads/processes must take a strict turn-based approach to using the critical section; i.e. they are forced into an alternating critical section access pattern. For example, if thread 1 wishes to read a hashtable every millisecond but another thread writes to a hashtable every second, then the reading thread would have to wait another 999ms before being able to read from the hashtable again. This 'solution' is not effective, because our threads should be able to make progress and enter the critical section if no other thread is currently in the critical section.", 
            "title": "Turn-based solutions"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#desired-properties-for-solutions-to-the-critical-section-problem", 
            "text": "There are three main desirable properties that we desire in a solution the critical section problem  Mutual Exclusion - the thread/process gets exclusive access; others must wait until it exits the critical section.  Bounded Wait - if the thread/process has to wait, then it should only have to wait for a finite,  amount of time (infinite waiting times are not allowed!). The exact definition of bounded wait is that there is an upper (non-infinite) bound on the number of times any other process can enter its critical section before the given process enters.\n* Progress - if no thread/process is inside the critical section, then the thread/process should be able to proceed (make progress) without having to wait.  With these ideas in mind let's examine another candidate solution that uses a turn-based flag only if two threads both required access at the same time.", 
            "title": "Desired properties for solutions to the Critical Section Problem?"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#turn-and-flag-solutions", 
            "text": "Is the following a correct solution to CSP?  \\\\ Candidate #4\nraise my flag\nif your flag is raised, wait until my turn\n// Do Critical Section stuff\nturn = yourid\nlower my flag  One instructor and another CS faculty member initially thought so! However, analyzing these solutions is tricky. Even peer-reviewed papers on this specific subject contain incorrect solutions! At first glance it appears to satisfy Mutual Exclusion, Bounded Wait and Progress: The turn-based flag is only used in the event of a tie (so Progress and Bounded Wait is allowed) and mutual exclusion appears to be satisfied. However.... Perhaps you can find a counter-example?  Candidate #4 fails because a thread does not wait until the other thread lowers their flag. After some thought (or inspiration) the following scenario can be created to demonstrate how Mutual Exclusion is not satisfied.  Imagine the first thread runs this code twice (so the the turn flag now points to the second thread). While the first thread is still inside the Critical Section, the second thread arrives. The second thread can immediately continue into the Critical Section!     Time  Turn  Thread #1  Thread #2      1  2  raise my flag     2  2  if your flag is raised, wait until my turn  raise my flag    3  2  // Do Critical Section stuff  if your flag is raised, wait until my turn(TRUE!)    4  2  // Do Critical Section stuff  // Do Critical Section stuff - OOPS", 
            "title": "Turn and Flag solutions"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#what-is-petersons-solution", 
            "text": "Peterson published his novel and surprisingly simple solution in a 2 page paper in 1981. A version of his algorithm is shown below that uses a shared variable  turn :   \\\\ Candidate #5\nraise my flag\nturn = myid\nwait until your flag is lowered or turn is yourid\n// Do Critical Section stuff\nlower my flag  This solution satisfies Mutual Exclusion, Bounded Wait and Progress. If thread #2 has set turn to 2 and is currently inside the critical section. Thread #1 arrives,  sets the turn back to 1  and now waits until thread 2 lowers the flag.  Link to Peterson's original article pdf: G. L. Peterson: \"Myths About the Mutual Exclusion Problem\", Information Processing Letters 12(3) 1981, 115\u2013116", 
            "title": "What is Peterson's solution?"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#was-petersons-solution-the-first-solution", 
            "text": "No, Dekkers Algorithm (1962) was the first provably correct solution. A version of the algorithm is below.  raise my flag\nwhile(your flag is raised) :\n   if it's your turn to win :\n     lower my flag\n     wait while your turn\n     raise my flag\n// Do Critical Section stuff\nset your turn to win\nlower my flag  Notice how the process's flag is always raised during the critical section no matter if the loop is iterated zero, once or more times. Further the flag can be interpreted as an immediate intent to enter the critical section. Only if the other process has also raised the flag will one process defer, lower their intent flag and wait.", 
            "title": "Was Peterson's solution the first solution?"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#can-i-just-implement-petersons-or-dekkers-algorithm-in-c-or-assembler", 
            "text": "Yes - and with a bit searching it is possible even today to find it in production for specific simple mobile processors: Peterson's algorithm is used to implement low-level Linux Kernel locks for the Tegra mobile processor (a system-on-chip ARM process and GPU core by Nvidia)\nhttps://android.googlesource.com/kernel/tegra.git/+/android-tegra-3.10/arch/arm/mach-tegra/sleep.S#58  However in general, CPUs and C compilers can re-order CPU instructions or use CPU-core-specific local cache values that are stale if another core updates the shared variables. Thus a simple pseudo-code to C implementation is too naive for most platforms. You can stop reading now.  Oh... you decided to keep reading. Well, here be dragons! Don't say we didn't warn you. Consider this advanced and gnarly topic but (spoiler alert) a happy ending.  Consider the following code,  while(flag2 ) { /* busy loop - go around again */  An efficient compiler would infer that  flag2  variable is never changed inside the loop, so that test can be optimized to  while(true)  \nUsing  volatile  goes someway to prevent compiler optimizations of this kind.  Independent instructions can be re-ordered by an optimizing compiler or at runtime by an out-of-order execution optimization by the CPU. These sophisticated optimizations if the code requires variables to be modified and checked and a precise order.  A related challenge is that CPU cores include a data cache to store recently read or modified main memory values. Modified values may not be written back to main memory or re-read from memory immediately. Thus data changes, such as the state of a flag and turn variable in the above examples, may not be shared between two CPU codes.   But there is happy ending. Fortunately, modern hardware addresses these issues using 'memory fences' (also known as memory barrier) CPU instructions to ensure that main memory and the CPUs' cache is in a reasonable and coherent state. Higher level synchronization primitives, such as  pthread_mutex_lock  are will call these CPU instructions as part of their implementation. Thus, in practice, surrounding critical section with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.  Further reading: we suggest the following web post that discusses implementing Peterson's algorithm on an x86 process and the linux documentation on memory barriers.  http://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/\nhttp://lxr.free-electrons.com/source/Documentation/memory-barriers.txt", 
            "title": "Can I just implement Peterson's (or Dekkers) algorithm in C or assembler?"
        }, 
        {
            "location": "/Synchronization,-Part-4:-The-Critical-Section-Problem/#how-do-we-implement-critical-section-problem-on-hardware", 
            "text": "Good question. Next lecture...", 
            "title": "How do we implement Critical Section Problem on hardware?"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/", 
            "text": "Warm up\n\n\nName these properties!\n\n \"Only one process(/thread) can be in the CS at a time\"\n\n \"If waiting, then another process can only enter the CS a finite number of times\" \n* \"If no other process is in the CS then the process can immediately enter the CS\"\n\n\nSee [[Synchronization, Part 4: The Critical Section Problem]] for answers.\n\n\nWhat is the 'exchange instruction' ?\n\n\nThe exchange instruction ('XCHG') is an atomic CPU instruction that exchanges the contents of a register with a memory location. This can be used as a basis to implement a simple mutex lock.\n\n\n// *Pseudo-C-code* for a simple busy-waiting mutex \n// that uses an atomic exchange function\nint lock = 0; // initialization\n\n// To enter the critical section you need to read a lock value of zero. \n// 'xchg' function doesn't exist, but imagine this function is built on the atomic XCHG CPU function\n// i.e. it writes '1' into the lock variable and returns the previous contents of the memory\nwhile (xchg( 1, \nlock)) {/*spin spin spin*/}\n/* Do Critical Section stuff*/\nlock = 0;\n\n\n\n\nWhat are condition variables? How do you use them? What is Spurious Wakeup?\n\n\n\n\n\n\nCondition variables allow a set of threads to sleep until tickled! You can tickle one thread or all threads that are sleeping. If you only wake one thread then the operating system will decide which thread to wake up. You don't wake threads directly instead you 'signal' the condition variable, which then will wake up one (or all) threads that are sleeping inside the condition variable.\n\n\n\n\n\n\nCondition variables are used with a mutex and with a loop (to check a condition).\n\n\n\n\n\n\nOccasionally a waiting thread may appear to wake up for no reason (this is called a \nspurious wake\n)! This is not an issue because you always use \nwait\n inside a loop that tests a condition that must be true to continue.\n\n\n\n\n\n\nThreads sleeping inside a condition variable are woken up by calling \npthread_cond_broadcast\n (wake up all) or \npthread_cond_signal\n (wake up one). Note despite the function name, this has nothing to do with POSIX \nsignal\ns!\n\n\n\n\n\n\nWhat does \npthread_cond_wait\n do?\n\n\nThe call \npthread_cond_wait\n performs three actions:\n\n unlock the mutex and atomically...\n\n waits (sleeps until \npthread_cond_signal\n is called on the same condition variable)\n* Before returning, locks the mutex\n\n\n(Advanced topic) Why do Condition Variables also need a mutex?\n\n\nCondition variables need a mutex for three reasons. The simplest to understand is that it prevents an early wakeup message (\nsignal\n or \nbroadcast\n functions) from being 'lost.' Imagine the following sequence of events (time runs down the page) where the condition is satisfied \njust before \npthread_cond_wait\n is called. In this example the wake-up signal is lost!\n\n\n\n\n\n\n\n\nThread 1\n\n\nThread 2\n\n\n\n\n\n\n\n\n\n\nwhile( answer \n 42) {\n\n\n\n\n\n\n\n\n\n\nanswer++\n\n\n\n\n\n\n\n\np_cond_signal(cv)\n\n\n\n\n\n\np_cond_wait(cv,m)\n\n\n\n\n\n\n\n\n\n\nIf both threads had locked a mutex, the signal can not be sent until \nafter\n \npthread_cond_wait(cv, m)\n is called (which then internally unlocks the mutex)\n\n\nA second common reason is that updating the program state (\nanswer\n variable) typically requires mutual exclusion - for example multiple threads may be updating the value of \nanswer\n.\n\n\nA third and subtle reason is to satisfy real-time scheduling concerns which we only outline here: In a time-critical application, the waiting thread with the \nhighest priority\n should be allowed to continue first. To satisfy this requirement the mutex must also be locked before calling \npthread_cond_signal\n or \npthread_cond_broadcast\n . For the curious, a longer and historical discussion is \nhere\n.\n\n\nWhy do spurious wakes exist?\n\n\nFor performance. On multi-CPU systems it is possible that a race-condition could cause a wake-up (signal) request to be unnoticed. The kernel may not detect this lost wake-up call but can detect when it might occur. To avoid the potential lost signal the thread is woken up so that the program code can test the condition again.\n\n\nExample\n\n\nCondition variables are \nalways\n used with a mutex lock.\n\n\nBefore calling \nwait\n, the mutex lock must be locked and \nwait\n must be wrapped with a loop.\n\n\npthread_cond_t cv;\npthread_mutex_t m;\nint count;\n\n// Initialize\npthread_cond_init(\ncv, NULL);\npthread_mutex_init(\nm, NULL);\ncount = 0;\n\npthread_mutex_lock(\nm);\nwhile (count \n 10) {\n   pthread_cond_wait(\ncv, \nm); \n/* Remember that cond_wait unlocks the mutex before blocking (waiting)! */\n/* After unlocking, other threads can claim the mutex. */\n/* When this thread is later woken it will */\n/* re-lock the mutex before returning */\n}\npthread_mutex_unlock(\nm);\n\n//later clean up with pthread_cond_destroy(\ncv); and mutex_destroy \n\n\n// In another thread increment count:\nwhile (1) {\n  pthread_mutex_lock(\nm);\n  count++;\n  pthread_cond_signal(\ncv);\n  /* Even though the other thread is woken up it cannot not return */\n  /* from pthread_cond_wait until we have unlocked the mutex. This is */\n  /* a good thing! In fact, it is usually the best practice to call */\n  /* cond_signal or cond_broadcast before unlocking the mutex */\n  pthread_mutex_unlock(\nm);\n}\n\n\n\n\nImplementing counting semaphores\n\n\n\n\nWe can implement a counting semaphore using condition variables.\n\n\nEach semaphore needs a count, a condition variable and a mutex\n\n\n\n\ntypedef struct sem_t {\n  int count; \n  pthread_mutex_t m;\n  pthread_condition_t cv;\n} sem_t;\n\n\n\n\nImplement \nsem_init\n to initialize the mutex and condition variable\n\n\nint sem_init(sem_t *s, int pshared, int value) {\n  if (pshared) { errno = ENOSYS /* 'Not implemented'*/; return -1;}\n\n  s-\ncount = value;\n  pthread_mutex_init(\ns-\nm, NULL);\n  pthread_cond_init(\ns-\ncv, NULL);\n  return 0;\n}\n\n\n\n\nOur implementation of \nsem_post\n needs to increment the count.\nWe will also wake up any threads sleeping inside the condition variable.\nNotice we lock and unlock the mutex so only one thread can be inside the critical section at a time.\n\n\nsem_post(sem_t *s) {\n  pthread_mutex_lock(\ns-\nm);\n  s-\ncount++;\n  pthread_cond_signal(\ns-\ncv); /* See note */\n  /* A woken thread must acquire the lock, so it will also have to wait until we call unlock*/\n\n  pthread_mutex_unlock(\ns-\nm);\n}\n\n\n\n\nOur implementation of \nsem_wait\n may need to sleep if the semaphore's count is zero.\nJust like \nsem_post\n we wrap the critical section using the lock (so only one thread can be executing our code at a time). Notice if the thread does need to wait then the mutex will be unlocked, allowing another thread to enter \nsem_post\n and waken us from our sleep!\n\n\nNotice that even if a thread is woken up, before it returns from  \npthread_cond_wait\n it must re-acquire the lock, so it will have to wait a little bit more (e.g. until sem_post finishes). \n\n\nsem_wait(sem_t *s) {\n  pthread_mutex_lock(\ns-\nm);\n  while (s-\ncount == 0) {\n      pthread_cond_wait(\ns-\ncv, \ns-\nm); /*unlock mutex, wait, relock mutex*/\n  }\n  s-\ncount--;\n  pthread_mutex_unlock(\ns-\nm);\n}\n\n\n\n\nWait \nsem_post\n keeps calling \npthread_cond_signal\n won't that break sem_wait? \nAnswer: No! We can't get past the loop until the count is non-zero. In practice this means \nsem_post\n would unnecessary call \npthread_cond_signal\n even if there are no waiting threads. A more efficient implementation would only call \npthread_cond_signal\n when necessary i.e.\n\n\n  /* Did we increment from zero to one- time to signal a thread sleeping inside sem_post */\n  if (s-\ncount == 1) /* Wake up one waiting thread!*/\n     pthread_cond_signal(\ns-\ncv);\n\n\n\n\nOther semaphore considerations\n\n\n\n\nReal semaphores implementation include a queue and scheduling concerns to ensure fairness and priority e.g. wake up the highest-priority longest sleeping thread.\n\n\nAlso, an advanced use of \nsem_init\n allows semaphores to be shared across processes. Our implementation only works for threads inside the same process.", 
            "title": "Synchronization, Part 5: Condition Variables"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#warm-up", 
            "text": "Name these properties!  \"Only one process(/thread) can be in the CS at a time\"  \"If waiting, then another process can only enter the CS a finite number of times\" \n* \"If no other process is in the CS then the process can immediately enter the CS\"  See [[Synchronization, Part 4: The Critical Section Problem]] for answers.", 
            "title": "Warm up"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#what-is-the-exchange-instruction", 
            "text": "The exchange instruction ('XCHG') is an atomic CPU instruction that exchanges the contents of a register with a memory location. This can be used as a basis to implement a simple mutex lock.  // *Pseudo-C-code* for a simple busy-waiting mutex \n// that uses an atomic exchange function\nint lock = 0; // initialization\n\n// To enter the critical section you need to read a lock value of zero. \n// 'xchg' function doesn't exist, but imagine this function is built on the atomic XCHG CPU function\n// i.e. it writes '1' into the lock variable and returns the previous contents of the memory\nwhile (xchg( 1,  lock)) {/*spin spin spin*/}\n/* Do Critical Section stuff*/\nlock = 0;", 
            "title": "What is the 'exchange instruction' ?"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#what-are-condition-variables-how-do-you-use-them-what-is-spurious-wakeup", 
            "text": "Condition variables allow a set of threads to sleep until tickled! You can tickle one thread or all threads that are sleeping. If you only wake one thread then the operating system will decide which thread to wake up. You don't wake threads directly instead you 'signal' the condition variable, which then will wake up one (or all) threads that are sleeping inside the condition variable.    Condition variables are used with a mutex and with a loop (to check a condition).    Occasionally a waiting thread may appear to wake up for no reason (this is called a  spurious wake )! This is not an issue because you always use  wait  inside a loop that tests a condition that must be true to continue.    Threads sleeping inside a condition variable are woken up by calling  pthread_cond_broadcast  (wake up all) or  pthread_cond_signal  (wake up one). Note despite the function name, this has nothing to do with POSIX  signal s!", 
            "title": "What are condition variables? How do you use them? What is Spurious Wakeup?"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#what-does-pthread_cond_wait-do", 
            "text": "The call  pthread_cond_wait  performs three actions:  unlock the mutex and atomically...  waits (sleeps until  pthread_cond_signal  is called on the same condition variable)\n* Before returning, locks the mutex", 
            "title": "What does pthread_cond_wait do?"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#advanced-topic-why-do-condition-variables-also-need-a-mutex", 
            "text": "Condition variables need a mutex for three reasons. The simplest to understand is that it prevents an early wakeup message ( signal  or  broadcast  functions) from being 'lost.' Imagine the following sequence of events (time runs down the page) where the condition is satisfied  just before  pthread_cond_wait  is called. In this example the wake-up signal is lost!     Thread 1  Thread 2      while( answer   42) {      answer++     p_cond_signal(cv)    p_cond_wait(cv,m)      If both threads had locked a mutex, the signal can not be sent until  after   pthread_cond_wait(cv, m)  is called (which then internally unlocks the mutex)  A second common reason is that updating the program state ( answer  variable) typically requires mutual exclusion - for example multiple threads may be updating the value of  answer .  A third and subtle reason is to satisfy real-time scheduling concerns which we only outline here: In a time-critical application, the waiting thread with the  highest priority  should be allowed to continue first. To satisfy this requirement the mutex must also be locked before calling  pthread_cond_signal  or  pthread_cond_broadcast  . For the curious, a longer and historical discussion is  here .", 
            "title": "(Advanced topic) Why do Condition Variables also need a mutex?"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#why-do-spurious-wakes-exist", 
            "text": "For performance. On multi-CPU systems it is possible that a race-condition could cause a wake-up (signal) request to be unnoticed. The kernel may not detect this lost wake-up call but can detect when it might occur. To avoid the potential lost signal the thread is woken up so that the program code can test the condition again.", 
            "title": "Why do spurious wakes exist?"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#example", 
            "text": "Condition variables are  always  used with a mutex lock.  Before calling  wait , the mutex lock must be locked and  wait  must be wrapped with a loop.  pthread_cond_t cv;\npthread_mutex_t m;\nint count;\n\n// Initialize\npthread_cond_init( cv, NULL);\npthread_mutex_init( m, NULL);\ncount = 0;\n\npthread_mutex_lock( m);\nwhile (count   10) {\n   pthread_cond_wait( cv,  m); \n/* Remember that cond_wait unlocks the mutex before blocking (waiting)! */\n/* After unlocking, other threads can claim the mutex. */\n/* When this thread is later woken it will */\n/* re-lock the mutex before returning */\n}\npthread_mutex_unlock( m);\n\n//later clean up with pthread_cond_destroy( cv); and mutex_destroy \n\n\n// In another thread increment count:\nwhile (1) {\n  pthread_mutex_lock( m);\n  count++;\n  pthread_cond_signal( cv);\n  /* Even though the other thread is woken up it cannot not return */\n  /* from pthread_cond_wait until we have unlocked the mutex. This is */\n  /* a good thing! In fact, it is usually the best practice to call */\n  /* cond_signal or cond_broadcast before unlocking the mutex */\n  pthread_mutex_unlock( m);\n}", 
            "title": "Example"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#implementing-counting-semaphores", 
            "text": "We can implement a counting semaphore using condition variables.  Each semaphore needs a count, a condition variable and a mutex   typedef struct sem_t {\n  int count; \n  pthread_mutex_t m;\n  pthread_condition_t cv;\n} sem_t;  Implement  sem_init  to initialize the mutex and condition variable  int sem_init(sem_t *s, int pshared, int value) {\n  if (pshared) { errno = ENOSYS /* 'Not implemented'*/; return -1;}\n\n  s- count = value;\n  pthread_mutex_init( s- m, NULL);\n  pthread_cond_init( s- cv, NULL);\n  return 0;\n}  Our implementation of  sem_post  needs to increment the count.\nWe will also wake up any threads sleeping inside the condition variable.\nNotice we lock and unlock the mutex so only one thread can be inside the critical section at a time.  sem_post(sem_t *s) {\n  pthread_mutex_lock( s- m);\n  s- count++;\n  pthread_cond_signal( s- cv); /* See note */\n  /* A woken thread must acquire the lock, so it will also have to wait until we call unlock*/\n\n  pthread_mutex_unlock( s- m);\n}  Our implementation of  sem_wait  may need to sleep if the semaphore's count is zero.\nJust like  sem_post  we wrap the critical section using the lock (so only one thread can be executing our code at a time). Notice if the thread does need to wait then the mutex will be unlocked, allowing another thread to enter  sem_post  and waken us from our sleep!  Notice that even if a thread is woken up, before it returns from   pthread_cond_wait  it must re-acquire the lock, so it will have to wait a little bit more (e.g. until sem_post finishes).   sem_wait(sem_t *s) {\n  pthread_mutex_lock( s- m);\n  while (s- count == 0) {\n      pthread_cond_wait( s- cv,  s- m); /*unlock mutex, wait, relock mutex*/\n  }\n  s- count--;\n  pthread_mutex_unlock( s- m);\n}  Wait  sem_post  keeps calling  pthread_cond_signal  won't that break sem_wait? \nAnswer: No! We can't get past the loop until the count is non-zero. In practice this means  sem_post  would unnecessary call  pthread_cond_signal  even if there are no waiting threads. A more efficient implementation would only call  pthread_cond_signal  when necessary i.e.    /* Did we increment from zero to one- time to signal a thread sleeping inside sem_post */\n  if (s- count == 1) /* Wake up one waiting thread!*/\n     pthread_cond_signal( s- cv);", 
            "title": "Implementing counting semaphores"
        }, 
        {
            "location": "/Synchronization,-Part-5:-Condition-Variables/#other-semaphore-considerations", 
            "text": "Real semaphores implementation include a queue and scheduling concerns to ensure fairness and priority e.g. wake up the highest-priority longest sleeping thread.  Also, an advanced use of  sem_init  allows semaphores to be shared across processes. Our implementation only works for threads inside the same process.", 
            "title": "Other semaphore considerations"
        }, 
        {
            "location": "/Synchronization,-Part-6:-Implementing-a-barrier/", 
            "text": "How do I wait for N threads to reach a certain point before continuing onto the next step?\n\n\nSuppose we wanted to perform a multi-threaded calculation that has two stages but we don't want to advance to the second stage until the first stage is completed.\n\n\nWe could use a synchronization method called a \nbarrier\n. When a thread reaches a barrier, it will wait at the barrier until all the threads reach the barrier, and then they'll all proceed together.  \n\n\nThink of it like being out for a hike with some friends.  You agree to wait for each other at the top of each hill (and you make a mental note how many are in your group). Say you're the first one to reach the top of the first hill. You'll wait there at the top for your friends. One by one, they'll arrive at the top, but nobody will continue until the last person in your group arrives.  Once they do, you'll all proceed.\n\n\nPthreads has a function \npthread_barrier_wait()\n that implements this. You'll need to declare a \npthread_barrier_t\n variable and initialize it with \npthread_barrier_init()\n.  \npthread_barrier_init()\n takes the number of threads that will be participating in the barrier as an argument.  \nHere's an example.\n\n\nNow let's implement our own barrier and use it to keep all the threads in sync in a large calculation.\n\n\ndouble data[256][8192]\n\n1 Threads do first calculation (use and change values in data)\n\n2 Barrier! Wait for all threads to finish first calculation before continuing\n\n3 Threads do second calculation (use and change values in data)\n\n\n\n\nThe thread function has four main parts-\n\n\nvoid *calc(void *arg) {\n  /* Do my part of the first calculation */\n  /* Am I the last thread to finish? If so wake up all the other threads! */\n  /* Otherwise wait until the other threads has finished part one */\n  /* Do my part of the second calculation */\n}\n\n\n\n\nOur main thread will create the 16 threads and we will divide each calculation into 16 separate pieces.  Each thread will be given a unique value (0,1,2,..15), so it can work on its own block.\nSince a (void*) type can hold small integers, we will pass the value of \ni\n by casting it to a void pointer. \n\n\n#define N (16)\ndouble data[256][8192] ;\nint main() {\n    pthread_t ids[N];\n    for(int i = 0; i \n N; i++)  \n        pthread_create(\nids[i], NULL, calc, (void *) i);\n\n\n\n\nNote, we will never dereference this pointer value as an actual memory location - we will just cast it straight back to an integer:\n\n\nvoid *calc(void *ptr) {\n// Thread 0 will work on rows 0..15, thread 1 on rows 16..31\n  int x, y, start = N * (int) ptr;\n  int end = start + N; \n  for(x = start; x \n end; x++) for (y = 0; y \n 8192; y++) { /* do calc #1 */ }\n\n\n\n\nAfter calculation 1 completes we need to wait for the slower threads (unless we are the last thread!).\nSo keep track of the number of threads that have arrived at our barrier aka 'checkpoint':\n\n\n// Global: \nint remain = N;\n\n\n// After calc #1 code:\nremain--; // We finished\nif (remain ==0) {/*I'm last!  -  Time for everyone to wake up! */ }\nelse {\n  while (remain != 0) { /* spin spin spin*/ }\n}\n\n\n\n\nHowever the above code has a race condition (two threads might try to decrement \nremain\n) and the loop is a busy loop. We can do better! Let's use a condition variable and then we will use a broadcast/signal functions to wake up the sleeping threads.\n\n\nA reminder, that a condition variable is similar to a house! Threads go there to sleep (\npthread_cond_wait\n). You can choose to wake up one thread (\npthread_cond_signal\n) or all of them (\npthread_cond_broadcast\n).  If there are no threads currently waiting then these two calls have no effect.\n\n\nA condition variable version is usually very similar to a busy loop incorrect solution - as we will show next. First, let's add a mutex and condition global variables and don't forget to initialize them in \nmain\n ...\n\n\n//global variables\npthread_mutex_t m;\npthread_cond_t cv;\n\nmain() {\n  pthread_mutex_init(\nm, NULL);\n  pthread_cond_init(\ncv, NULL);\n\n\n\n\nWe will use the mutex to ensure that only one thread modifies \nremain\n at a time.\nThe last arriving thread needs to wake up \nall\n sleeping threads - so we will use \npthread_cond_broadcast(\ncv)\n not \npthread_cond_signal\n\n\npthread_mutex_lock(\nm);\nremain--; \nif (remain ==0) { pthread_cond_broadcast(\ncv); }\nelse {\n  while(remain != 0) { pthread_cond_wait(\ncv, \nm); }\n}\npthread_mutex_unlock(\nm);\n\n\n\n\nWhen a thread enters \npthread_cond_wait\n it releases the mutex and sleeps. At some point in the future it will be awoken. Once we bring a thread back from its sleep, before returning it must wait until it can lock the mutex. Notice that even if a sleeping thread wakes up early, it will check the while loop condition and re-enter wait if necessary.", 
            "title": "Synchronization, Part 6: Implementing a barrier"
        }, 
        {
            "location": "/Synchronization,-Part-6:-Implementing-a-barrier/#how-do-i-wait-for-n-threads-to-reach-a-certain-point-before-continuing-onto-the-next-step", 
            "text": "Suppose we wanted to perform a multi-threaded calculation that has two stages but we don't want to advance to the second stage until the first stage is completed.  We could use a synchronization method called a  barrier . When a thread reaches a barrier, it will wait at the barrier until all the threads reach the barrier, and then they'll all proceed together.    Think of it like being out for a hike with some friends.  You agree to wait for each other at the top of each hill (and you make a mental note how many are in your group). Say you're the first one to reach the top of the first hill. You'll wait there at the top for your friends. One by one, they'll arrive at the top, but nobody will continue until the last person in your group arrives.  Once they do, you'll all proceed.  Pthreads has a function  pthread_barrier_wait()  that implements this. You'll need to declare a  pthread_barrier_t  variable and initialize it with  pthread_barrier_init() .   pthread_barrier_init()  takes the number of threads that will be participating in the barrier as an argument.   Here's an example.  Now let's implement our own barrier and use it to keep all the threads in sync in a large calculation.  double data[256][8192]\n\n1 Threads do first calculation (use and change values in data)\n\n2 Barrier! Wait for all threads to finish first calculation before continuing\n\n3 Threads do second calculation (use and change values in data)  The thread function has four main parts-  void *calc(void *arg) {\n  /* Do my part of the first calculation */\n  /* Am I the last thread to finish? If so wake up all the other threads! */\n  /* Otherwise wait until the other threads has finished part one */\n  /* Do my part of the second calculation */\n}  Our main thread will create the 16 threads and we will divide each calculation into 16 separate pieces.  Each thread will be given a unique value (0,1,2,..15), so it can work on its own block.\nSince a (void*) type can hold small integers, we will pass the value of  i  by casting it to a void pointer.   #define N (16)\ndouble data[256][8192] ;\nint main() {\n    pthread_t ids[N];\n    for(int i = 0; i   N; i++)  \n        pthread_create( ids[i], NULL, calc, (void *) i);  Note, we will never dereference this pointer value as an actual memory location - we will just cast it straight back to an integer:  void *calc(void *ptr) {\n// Thread 0 will work on rows 0..15, thread 1 on rows 16..31\n  int x, y, start = N * (int) ptr;\n  int end = start + N; \n  for(x = start; x   end; x++) for (y = 0; y   8192; y++) { /* do calc #1 */ }  After calculation 1 completes we need to wait for the slower threads (unless we are the last thread!).\nSo keep track of the number of threads that have arrived at our barrier aka 'checkpoint':  // Global: \nint remain = N;\n\n\n// After calc #1 code:\nremain--; // We finished\nif (remain ==0) {/*I'm last!  -  Time for everyone to wake up! */ }\nelse {\n  while (remain != 0) { /* spin spin spin*/ }\n}  However the above code has a race condition (two threads might try to decrement  remain ) and the loop is a busy loop. We can do better! Let's use a condition variable and then we will use a broadcast/signal functions to wake up the sleeping threads.  A reminder, that a condition variable is similar to a house! Threads go there to sleep ( pthread_cond_wait ). You can choose to wake up one thread ( pthread_cond_signal ) or all of them ( pthread_cond_broadcast ).  If there are no threads currently waiting then these two calls have no effect.  A condition variable version is usually very similar to a busy loop incorrect solution - as we will show next. First, let's add a mutex and condition global variables and don't forget to initialize them in  main  ...  //global variables\npthread_mutex_t m;\npthread_cond_t cv;\n\nmain() {\n  pthread_mutex_init( m, NULL);\n  pthread_cond_init( cv, NULL);  We will use the mutex to ensure that only one thread modifies  remain  at a time.\nThe last arriving thread needs to wake up  all  sleeping threads - so we will use  pthread_cond_broadcast( cv)  not  pthread_cond_signal  pthread_mutex_lock( m);\nremain--; \nif (remain ==0) { pthread_cond_broadcast( cv); }\nelse {\n  while(remain != 0) { pthread_cond_wait( cv,  m); }\n}\npthread_mutex_unlock( m);  When a thread enters  pthread_cond_wait  it releases the mutex and sleeps. At some point in the future it will be awoken. Once we bring a thread back from its sleep, before returning it must wait until it can lock the mutex. Notice that even if a sleeping thread wakes up early, it will check the while loop condition and re-enter wait if necessary.", 
            "title": "How do I wait for N threads to reach a certain point before continuing onto the next step?"
        }, 
        {
            "location": "/Synchronization,-Part-7:-The-Reader-Writer-Problem/", 
            "text": "What is the Reader Writer Problem?\n\n\nImagine you had a key-value map data structure which is used by many threads. Multiple threads should be able to look up (read) values at the same time provided the data structure is not being written to. The writers are not so gregarious - to avoid data corruption, only one thread at a time may modify (\nwrite\n) the data structure (and no readers may be reading at that time). \n\n\nThe is an example of the \nReader Writer Problem\n. Namely how can we efficiently synchronize multiple readers and writers such that multiple readers can read together but a writer gets exclusive access?\n\n\nAn incorrect attempt is shown below (\"lock\" is a shorthand for \npthread_mutex_lock\n):\n\n\n\n\n\nread() {\n  lock(\n)\n  // do read stuff\n  unlock(\n)\n}\n\n\n\n\n\n\nwrite() {\n  lock(\n)\n  // do write stuff\n  unlock(\n)\n}\n\n\n\n\nAt least our first attempt does not suffer from data corruption (readers must wait while a writer is writing and vice versa)! However readers must also wait for other readers. So let's try another implementation..\n\n\nAttempt #2:\n\n\n\nread() {\n  while(writing) {/\nspin\n/}\n  reading = 1\n  // do read stuff\n  reading = 0\n}\n\n\n\n\n\nwrite() {\n  while(reading || writing) {/\nspin\n/}\n  writing = 1\n  // do write stuff\n  writing = 0\n}\n\n\n\nOur second attempt suffers from a race condition - imagine if two threads both called \nread\n and \nwrite\n (or both called write) at the same time. Both threads would be able to proceed! Secondly, we can have multiple readers and multiple writers, so lets keep track of the total number of readers or writers. Which brings us to attempt #3,\n\n\n\n\nread() {\n  lock(\n)\n  while (writers) {\n    pthread_cond_wait(\n,\n)\n  }\n  readers++\n  // do read stuff\n  readers--\n  pthread_cond_signal(\n)\n  unlock(\n)\n}\n\n\n\n\n\nwrite() {\n  lock(\n)\n  while (readers || writers) {\n    pthread_cond_wait(\n,\n)\n  }\n  writers++\n  // do write stuff\n  writers--\n  pthread_cond_signal(\n)\n  unlock(\n)\n}\n\n\n\n\nThis solution might appear to work when lightly tested however it suffers from several drawbacks  - can you see them? We will discuss these in a future section.", 
            "title": "Synchronization, Part 7: The Reader Writer Problem"
        }, 
        {
            "location": "/Synchronization,-Part-7:-The-Reader-Writer-Problem/#what-is-the-reader-writer-problem", 
            "text": "Imagine you had a key-value map data structure which is used by many threads. Multiple threads should be able to look up (read) values at the same time provided the data structure is not being written to. The writers are not so gregarious - to avoid data corruption, only one thread at a time may modify ( write ) the data structure (and no readers may be reading at that time).   The is an example of the  Reader Writer Problem . Namely how can we efficiently synchronize multiple readers and writers such that multiple readers can read together but a writer gets exclusive access?  An incorrect attempt is shown below (\"lock\" is a shorthand for  pthread_mutex_lock ):   \nread() {\n  lock( )\n  // do read stuff\n  unlock( )\n}   \nwrite() {\n  lock( )\n  // do write stuff\n  unlock( )\n}  At least our first attempt does not suffer from data corruption (readers must wait while a writer is writing and vice versa)! However readers must also wait for other readers. So let's try another implementation..  Attempt #2:  read() {\n  while(writing) {/ spin /}\n  reading = 1\n  // do read stuff\n  reading = 0\n}   write() {\n  while(reading || writing) {/ spin /}\n  writing = 1\n  // do write stuff\n  writing = 0\n}  Our second attempt suffers from a race condition - imagine if two threads both called  read  and  write  (or both called write) at the same time. Both threads would be able to proceed! Secondly, we can have multiple readers and multiple writers, so lets keep track of the total number of readers or writers. Which brings us to attempt #3,   read() {\n  lock( )\n  while (writers) {\n    pthread_cond_wait( , )\n  }\n  readers++\n  // do read stuff\n  readers--\n  pthread_cond_signal( )\n  unlock( )\n}   write() {\n  lock( )\n  while (readers || writers) {\n    pthread_cond_wait( , )\n  }\n  writers++\n  // do write stuff\n  writers--\n  pthread_cond_signal( )\n  unlock( )\n}  This solution might appear to work when lightly tested however it suffers from several drawbacks  - can you see them? We will discuss these in a future section.", 
            "title": "What is the Reader Writer Problem?"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/", 
            "text": "What is a ring buffer?\n\n\nA ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. As  array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array.\nAs data is added (enqueued) to the front of the queue or removed (dequeued) from tail of the queue, the current items in the buffer form a train that appears to circle the track\n\n\nA simple (single-threaded) implementation is shown below. Note enqueue and dequeue do not guard against underflow or overflow - it's possible to add an item when when the queue is full and possible to remove an item when the queue is empty. For example if we added 20 integers (1,2,3...) to the queue and did not dequeue any items then values \n17,18,19,20\n would overwrite the \n1,2,3,4\n. We won't fix this problem right now, instead when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.\n\n\nvoid *buffer[16];\nint in = 0, out = 0;\n\nvoid enqueue(void *value) { /* Add one item to the front of the queue*/\n  buffer[in] = value;\n  in++; /* Advance the index for next time */\n  if (in == 16) in = 0; /* Wrap around! */\n}\n\nvoid *dequeue() { /* Remove one item to the end of the queue.*/\n  void *result = buffer[out];\n  out++;\n  if (out == 16) out = 0;\n  return result;\n}\n\n\n\n\nWhat are gotchas of implementing a Ring Buffer?\n\n\nIt's very tempting to write the enqueue or dequeue method in the following compact form (N is the capacity of the buffer e.g. 16):\n\n\nvoid enqueue(void *value)\n  b[ (in++) % N ] = value;\n}\n\n\n\n\nThis method would appear to work (pass simple tests etc) but contains a subtle bug. With enough enqueue operations (a bit more than two billion) the int value of \nin\n will overflow and become negative! The modulo (or 'remainder') operator \n%\n preserves the sign. Thus you might end up writing into \nb[-14]\n  for example! \n\n\nA compact form is correct uses bit masking provided N is 2^x (16,32,64,...)\n\n\nb[ (in++) \n (N-1) ] = value;\n\n\n\n\nThis buffer does not yet prevent buffer underflow or overflow. For that, we'll turn to our multi-threaded attempt that will block a thread until there is space or there is at least one item to remove.\n\n\nChecking a multi-threaded implementation for correctness (Example 1)\n\n\nThe following code is an incorrect implementation. What will happen? Will \nenqueue\n and/or \ndequeue\n block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow?\nFor clarity \npthread_mutex\n is shortened to \np_m\n and we assume sem_wait cannot be interrupted.\n\n\nvoid *b[16]\nint in = 0, out = 0\np_m_t lock\nsem_t s1,s2\nvoid init() { \n    p_m_init(\nlock, NULL)\n    sem_init(\ns1, 0, 16)\n    sem_init(\ns2, 0, 0)\n}\n\nenqueue(void *value) {\n    p_m_lock(\nlock)\n\n    // Hint: Wait while zero. Decrement and return\n    sem_wait( \ns1 ) \n\n    b[ (in++) \n (N-1) ] = value\n\n    // Hint: Increment. Will wake up a waiting thread \n    sem_post(\ns1) \n    p_m_unlock(\nlock)\n}\nvoid *dequeue(){\n    p_m_lock(\nlock)\n    sem_wait(\ns2)\n    void *result = b[(out++) \n 15]\n    sem_post(\ns2)\n    p_m_unlock(\nlock)\n    return result\n}\n\n\n\n\nAnalysis\n\n\nBefore reading on, see how many mistakes you can find. Then determine what would happen if threads called the enqueue and dequeue methods.\n\n\n\n\nThe enqueue method waits and posts on the same semaphore (s1) and similarly with equeue and (s2) i.e. we decrement the value and then immediately increment the value, so by the end of the function the semaphore value is unchanged! \n\n\nThe initial value of s1 is 16, so the semaphore will never be reduced to zero - enqueue will not block if the ring buffer is full - so overflow is possible.\n\n\nThe initial value of s2 is zero, so calls to dequeue will always block and never return!\n\n\nThe order of mutex lock and sem_wait will need to be swapped (however this example is so broken that this bug has no effect!)\n\n\n\n\nChecking a multi-threaded implementation for correctness (Example 1)\n\n\nThe following code is an incorrect implementation. What will happen? Will \nenqueue\n and/or \ndequeue\n block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow?\nFor clarity \npthread_mutex\n is shortened to \np_m\n and we assume sem_wait cannot be interrupted.\n\n\nvoid *b[16]\nint in = 0, out = 0\np_m_t lock\nsem_t s1, s2\nvoid init() {\n    sem_init(\ns1,0,16)\n    sem_init(\ns2,0,0)\n}\n\nenqueue(void *value){\n\n sem_wait(\ns2)\n p_m_lock(\nlock)\n\n b[ (in++) \n (N-1) ] = value\n\n p_m_unlock(\nlock)\n sem_post(\ns1)\n}\n\nvoid *dequeue(){\n  sem_wait(\ns1)\n  p_m_lock(\nlock)\n  void *result = b[(out++) \n 15]\n  p_m_unlock(\nlock)\n  sem_post(\ns2)\n\n  return result;\n}\n\n\n\n\nAnalysis\n\n\n\n\nThe initial value of s2 is 0. Thus enqueue will block on the first call to sem_wait even though the buffer is empty!\n\n\nThe initial value of s1 is 16. Thus dequeue will not block on the first call to sem_wait even though the buffer is empty - oops Underflow! The dequeue method will return invalid data.\n\n\nThe code does not satisfy Mutual Exclusion; two threads can modify \nin\n or \nout\n at the same time! The code appears to use  mutex lock. Unfortunately the lock was never initialized with \npthread_mutex_init()\n or \nPTHREAD_MUTEX_INITIALIZER\n - so the lock may not work (\npthread_mutex_lock\n may simply do nothing)\n\n\n\n\nCorrect implementation of a ring buffer\n\n\nThe pseudo-code (\npthread_mutex\n shortened to \np_m\n etc) is shown below.\n\n\nAs the mutex lock is stored in global (static) memory it can be initialized with  \nPTHREAD_MUTEX_INITIALIZER\n.If we had allocated space for the mutex on the heap, then we would have used \npthread_mutex_init(ptr, NULL)\n\n\n#include \npthread.h\n\n#include \nsemaphore.h\n\n// N must be 2^i\n#define N (16)\n\nvoid *b[N]\nint in = 0, out = 0\np_m_t lock = PTHREAD_MUTEX_INITIALIZER\nsem_t countsem, spacesem\n\nvoid init() {\n  sem_init(\ncountsem, 0, 0)\n  sem_init(\nspacesem, 0, 16)\n}\n\n\n\n\nThe enqueue method is shown below. Notice:\n\n The lock is only held during the critical section (access to the data structure).\n\n A complete implementation would need to guard against early returns from \nsem_wait\n due to POSIX signals.\n\n\nenqueue(void *value){\n // wait if there is no space left:\n sem_wait( \nspacesem )\n\n p_m_lock(\nlock)\n b[ (in++) \n (N-1) ] = value\n p_m_unlock(\nlock)\n\n // increment the count of the number of items\n sem_post(\ncountsem)\n}\n\n\n\n\nThe \ndequeue\n implementation is shown below. Notice the symmetry of the synchronization calls to \nenqueue\n. In both cases the functions first wait if the count of spaces or count of items is zero.\n\n\nvoid *dequeue(){\n  // Wait if there are no items in the buffer\n  sem_wait(\ncountsem)\n\n  p_m_lock(\nlock)\n  void *result = b[(out++) \n (N-1)]\n  p_m_unlock(\nlock)\n\n  // Increment the count of the number of spaces\n  sem_post(\nspacesem)\n\n  return result\n}\n\n\n\n\nFood for thought\n\n\n\n\nWhat would happen if  the order of  \npthread_mutex_unlock\n and \nsem_post\n calls were swapped?\n\n\nWhat would happen if the order of \nsem_wait\n and \npthread_mutex_lock\n calls were swapped?", 
            "title": "Synchronization, Part 8: Ring Buffer Example"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/#what-is-a-ring-buffer", 
            "text": "A ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. As  array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array.\nAs data is added (enqueued) to the front of the queue or removed (dequeued) from tail of the queue, the current items in the buffer form a train that appears to circle the track \nA simple (single-threaded) implementation is shown below. Note enqueue and dequeue do not guard against underflow or overflow - it's possible to add an item when when the queue is full and possible to remove an item when the queue is empty. For example if we added 20 integers (1,2,3...) to the queue and did not dequeue any items then values  17,18,19,20  would overwrite the  1,2,3,4 . We won't fix this problem right now, instead when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.  void *buffer[16];\nint in = 0, out = 0;\n\nvoid enqueue(void *value) { /* Add one item to the front of the queue*/\n  buffer[in] = value;\n  in++; /* Advance the index for next time */\n  if (in == 16) in = 0; /* Wrap around! */\n}\n\nvoid *dequeue() { /* Remove one item to the end of the queue.*/\n  void *result = buffer[out];\n  out++;\n  if (out == 16) out = 0;\n  return result;\n}", 
            "title": "What is a ring buffer?"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/#what-are-gotchas-of-implementing-a-ring-buffer", 
            "text": "It's very tempting to write the enqueue or dequeue method in the following compact form (N is the capacity of the buffer e.g. 16):  void enqueue(void *value)\n  b[ (in++) % N ] = value;\n}  This method would appear to work (pass simple tests etc) but contains a subtle bug. With enough enqueue operations (a bit more than two billion) the int value of  in  will overflow and become negative! The modulo (or 'remainder') operator  %  preserves the sign. Thus you might end up writing into  b[-14]   for example!   A compact form is correct uses bit masking provided N is 2^x (16,32,64,...)  b[ (in++)   (N-1) ] = value;  This buffer does not yet prevent buffer underflow or overflow. For that, we'll turn to our multi-threaded attempt that will block a thread until there is space or there is at least one item to remove.", 
            "title": "What are gotchas of implementing a Ring Buffer?"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/#checking-a-multi-threaded-implementation-for-correctness-example-1", 
            "text": "The following code is an incorrect implementation. What will happen? Will  enqueue  and/or  dequeue  block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow?\nFor clarity  pthread_mutex  is shortened to  p_m  and we assume sem_wait cannot be interrupted.  void *b[16]\nint in = 0, out = 0\np_m_t lock\nsem_t s1,s2\nvoid init() { \n    p_m_init( lock, NULL)\n    sem_init( s1, 0, 16)\n    sem_init( s2, 0, 0)\n}\n\nenqueue(void *value) {\n    p_m_lock( lock)\n\n    // Hint: Wait while zero. Decrement and return\n    sem_wait(  s1 ) \n\n    b[ (in++)   (N-1) ] = value\n\n    // Hint: Increment. Will wake up a waiting thread \n    sem_post( s1) \n    p_m_unlock( lock)\n}\nvoid *dequeue(){\n    p_m_lock( lock)\n    sem_wait( s2)\n    void *result = b[(out++)   15]\n    sem_post( s2)\n    p_m_unlock( lock)\n    return result\n}", 
            "title": "Checking a multi-threaded implementation for correctness (Example 1)"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/#analysis", 
            "text": "Before reading on, see how many mistakes you can find. Then determine what would happen if threads called the enqueue and dequeue methods.   The enqueue method waits and posts on the same semaphore (s1) and similarly with equeue and (s2) i.e. we decrement the value and then immediately increment the value, so by the end of the function the semaphore value is unchanged!   The initial value of s1 is 16, so the semaphore will never be reduced to zero - enqueue will not block if the ring buffer is full - so overflow is possible.  The initial value of s2 is zero, so calls to dequeue will always block and never return!  The order of mutex lock and sem_wait will need to be swapped (however this example is so broken that this bug has no effect!)", 
            "title": "Analysis"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/#checking-a-multi-threaded-implementation-for-correctness-example-1_1", 
            "text": "The following code is an incorrect implementation. What will happen? Will  enqueue  and/or  dequeue  block? Is mutual exclusion satisfied? Can the buffer underflow? Can the buffer overflow?\nFor clarity  pthread_mutex  is shortened to  p_m  and we assume sem_wait cannot be interrupted.  void *b[16]\nint in = 0, out = 0\np_m_t lock\nsem_t s1, s2\nvoid init() {\n    sem_init( s1,0,16)\n    sem_init( s2,0,0)\n}\n\nenqueue(void *value){\n\n sem_wait( s2)\n p_m_lock( lock)\n\n b[ (in++)   (N-1) ] = value\n\n p_m_unlock( lock)\n sem_post( s1)\n}\n\nvoid *dequeue(){\n  sem_wait( s1)\n  p_m_lock( lock)\n  void *result = b[(out++)   15]\n  p_m_unlock( lock)\n  sem_post( s2)\n\n  return result;\n}", 
            "title": "Checking a multi-threaded implementation for correctness (Example 1)"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/#analysis_1", 
            "text": "The initial value of s2 is 0. Thus enqueue will block on the first call to sem_wait even though the buffer is empty!  The initial value of s1 is 16. Thus dequeue will not block on the first call to sem_wait even though the buffer is empty - oops Underflow! The dequeue method will return invalid data.  The code does not satisfy Mutual Exclusion; two threads can modify  in  or  out  at the same time! The code appears to use  mutex lock. Unfortunately the lock was never initialized with  pthread_mutex_init()  or  PTHREAD_MUTEX_INITIALIZER  - so the lock may not work ( pthread_mutex_lock  may simply do nothing)", 
            "title": "Analysis"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/#correct-implementation-of-a-ring-buffer", 
            "text": "The pseudo-code ( pthread_mutex  shortened to  p_m  etc) is shown below.  As the mutex lock is stored in global (static) memory it can be initialized with   PTHREAD_MUTEX_INITIALIZER .If we had allocated space for the mutex on the heap, then we would have used  pthread_mutex_init(ptr, NULL)  #include  pthread.h \n#include  semaphore.h \n// N must be 2^i\n#define N (16)\n\nvoid *b[N]\nint in = 0, out = 0\np_m_t lock = PTHREAD_MUTEX_INITIALIZER\nsem_t countsem, spacesem\n\nvoid init() {\n  sem_init( countsem, 0, 0)\n  sem_init( spacesem, 0, 16)\n}  The enqueue method is shown below. Notice:  The lock is only held during the critical section (access to the data structure).  A complete implementation would need to guard against early returns from  sem_wait  due to POSIX signals.  enqueue(void *value){\n // wait if there is no space left:\n sem_wait(  spacesem )\n\n p_m_lock( lock)\n b[ (in++)   (N-1) ] = value\n p_m_unlock( lock)\n\n // increment the count of the number of items\n sem_post( countsem)\n}  The  dequeue  implementation is shown below. Notice the symmetry of the synchronization calls to  enqueue . In both cases the functions first wait if the count of spaces or count of items is zero.  void *dequeue(){\n  // Wait if there are no items in the buffer\n  sem_wait( countsem)\n\n  p_m_lock( lock)\n  void *result = b[(out++)   (N-1)]\n  p_m_unlock( lock)\n\n  // Increment the count of the number of spaces\n  sem_post( spacesem)\n\n  return result\n}", 
            "title": "Correct implementation of a ring buffer"
        }, 
        {
            "location": "/Synchronization,-Part-8:-Ring-Buffer-Example/#food-for-thought", 
            "text": "What would happen if  the order of   pthread_mutex_unlock  and  sem_post  calls were swapped?  What would happen if the order of  sem_wait  and  pthread_mutex_lock  calls were swapped?", 
            "title": "Food for thought"
        }, 
        {
            "location": "/Synchronization,-Part-9:-The-Reader-Writer-Problem-(part-2)/", 
            "text": "The introduction\n\n\nThe reader-writer problem appears in many different contexts. For example a web server cache needs to quickly serve the same static page for many thousands of requests. Occasionally however an author may decide to update the page.\n\n\nWe will examine one specific form of the reader-writer problem where there are many readers and some occasional writers and we need to ensure that a writer gets exclusive access. For performance however readers should be able to perform the read without waiting for another reader. \n\n\nSee [[Synchronization,-Part-7:-The-Reader-Writer-Problem]] for part 1\n\n\nCandidate solution #3\n\n\nCandidate solutions 1 and 2 are discussed in [[part 1|Synchronization,-Part-7:-The-Reader-Writer-Problem]].\n\n\nIn the code below for clarity \nlock\n and \ncond_wait\n are shortened versions \npthread_mutex_lock\n and \npthread_cond_wait\n etc. respectively\n\n\nAlso remember that \npthread_cond_wait\n performs \nThree\n actions. Firstly it atomically unlocks the mutex and then sleeps (until it is woken by \npthread_cond_signal\n or \npthread_cond_broadcast\n). Thirdly the awoken thread must re-acquire the mutex lock before returning. Thus only one thread can actually be running inside the critical section defined by the lock and unlock() methods.\n\n\nImplementation #3 below ensures that a reader will enter the cond_wait if there are any writers writing.\n\n\nread() {\n    lock(\nm)\n    while (writing)\n        cond_wait(\ncv, \nm)\n    reading++;\n\n/* Read here! */\n\n    reading--\n    cond_signal(\ncv)\n    unlock(\nm)\n}\n\n\n\n\nHowever only one reader a time can read because candidate #3 did not unlock the mutex. A better version unlocks before reading :\n\n\nread() {\n    lock(\nm);\n    while (writing)\n        cond_wait(\ncv, \nm)\n    reading++;\n    unlock(\nm)\n/* Read here! */\n    lock(\nm)\n    reading--\n    cond_signal(\ncv)\n    unlock(\nm)\n}\n\n\n\n\nDoes this mean that a writer and read could read and write at the same time? No! First of all, remember cond_wait requires the thread re-acquire the  mutex lock before returning. Thus only one thread can be executing code inside the critical section (marked with **) at a time!\n\n\nread() {\n    lock(\nm);\n**  while (writing)\n**      cond_wait(\ncv, \nm)\n**  reading++;\n    unlock(\nm)\n/* Read here! */\n    lock(\nm)\n**  reading--\n**  cond_signal(\ncv)\n    unlock(\nm)\n}\n\n\n\n\nWriters must wait for everyone. Mutual exclusion is assured by the lock. \n\n\nwrite() {\n    lock(\nm);\n**  while (reading || writing)\n**      cond_wait(\ncv, \nm);\n**  writing++;\n**\n** /* Write here! */\n**  writing--;\n**  cond_signal(\ncv);\n    unlock(\nm);\n}\n\n\n\n\nCandidate #3 above also uses \npthread_cond_signal\n ; this will only wake up one thread. For example, if many readers are waiting for the writer to complete then only one sleeping reader will be awoken from their slumber. The reader and writer should use \ncond_broadcast\n so that all threads should wake up and check their while-loop condition.\n\n\nStarving writers\n\n\nCandidate #3 above suffers from starvation. If readers are constantly arriving then a writer will never be able to proceed (the 'reading' count never reduces to zero). This is known as \nstarvation\n and would be discovered under heavy loads. Our fix is to implement a bounded-wait for the writer. If a writer arrives they will still need to wait for existing readers however future readers must be placed in a \"holding pen\" and wait for the writer to finish. The \"holding pen\" can be implemented using a variable and a condition variable (so that we can wake up the threads once the writer has finished).\n\n\nOur plan is that when a writer arrives, and before waiting for current readers to finish, register our intent to write (by incrementing a counter 'writer'). Sketched below - \n\n\nwrite() {\n    lock()\n    writer++\n\n    while (reading || writing)\n    cond_wait\n    unlock()\n  ...\n}\n\n\n\n\nAnd incoming readers will not be allowed to continue while writer is nonzero. Notice 'writer' indicates a writer has arrived, while 'reading' and 'writing' counters indicate there is an \nactive\n reader or writer.\n\n\nread() {\n    lock()\n    // readers that arrive *after* the writer arrived will have to wait here!\n    while(writer)\n    cond_wait(\ncv,\nm)\n\n    // readers that arrive while there is an active writer\n    // will also wait.\n    while (writing) \n        cond_wait(\ncv,\nm)\n    reading++\n    unlock\n  ...\n}\n\n\n\n\nCandidate solution #4\n\n\nBelow is our first working solution to the Reader-Writer problem. \nNote if you continue to read about the \"Reader Writer problem\" then you will discover that we solved the \"Second Reader Writer problem\" by giving writers preferential access to the lock. This solution is not optimal. However it satisfies our original problem (N active readers, single active writer, avoids starvation of the writer if there is a constant stream of readers). \n\n\nCan you identify any improvements? For example, how would you improve the code so that we only woke up readers or one writer? \n\n\n\nint writers; // Number writer threads that want to enter the critical section (some or all of these may be blocked)\nint writing; // Number of threads that are actually writing inside the C.S. (can only be zero or one)\nint reading; // Number of threads that are actually reading inside the C.S.\n// if writing !=0 then reading must be zero (and vice versa)\n\nreader() {\n    lock(\nm)\n    while (writers)\n        cond_wait(\nturn, \nm)\n    // No need to wait while(writing here) because we can only exit the above loop\n    // when writing is zero\n    reading++\n    unlock(\nm)\n\n  // perform reading here\n\n    lock(\nm)\n    reading--\n    cond_broadcast(\nturn)\n    unlock(\nm)\n}\n\nwriter() {\n    lock(\nm)  \n    writers++  \n    while (reading || writing)   \n        cond_wait(\nturn, \nm)  \n    writing++  \n    unlock(\nm)  \n    // perform writing here  \n    lock(\nm)  \n    writing--  \n    writers--  \n    cond_broadcast(\nturn)  \n    unlock(\nm)  \n}", 
            "title": "Synchronization, Part 9: The Reader Writer Problem (part 2)"
        }, 
        {
            "location": "/Synchronization,-Part-9:-The-Reader-Writer-Problem-(part-2)/#the-introduction", 
            "text": "The reader-writer problem appears in many different contexts. For example a web server cache needs to quickly serve the same static page for many thousands of requests. Occasionally however an author may decide to update the page.  We will examine one specific form of the reader-writer problem where there are many readers and some occasional writers and we need to ensure that a writer gets exclusive access. For performance however readers should be able to perform the read without waiting for another reader.   See [[Synchronization,-Part-7:-The-Reader-Writer-Problem]] for part 1", 
            "title": "The introduction"
        }, 
        {
            "location": "/Synchronization,-Part-9:-The-Reader-Writer-Problem-(part-2)/#candidate-solution-3", 
            "text": "Candidate solutions 1 and 2 are discussed in [[part 1|Synchronization,-Part-7:-The-Reader-Writer-Problem]].  In the code below for clarity  lock  and  cond_wait  are shortened versions  pthread_mutex_lock  and  pthread_cond_wait  etc. respectively  Also remember that  pthread_cond_wait  performs  Three  actions. Firstly it atomically unlocks the mutex and then sleeps (until it is woken by  pthread_cond_signal  or  pthread_cond_broadcast ). Thirdly the awoken thread must re-acquire the mutex lock before returning. Thus only one thread can actually be running inside the critical section defined by the lock and unlock() methods.  Implementation #3 below ensures that a reader will enter the cond_wait if there are any writers writing.  read() {\n    lock( m)\n    while (writing)\n        cond_wait( cv,  m)\n    reading++;\n\n/* Read here! */\n\n    reading--\n    cond_signal( cv)\n    unlock( m)\n}  However only one reader a time can read because candidate #3 did not unlock the mutex. A better version unlocks before reading :  read() {\n    lock( m);\n    while (writing)\n        cond_wait( cv,  m)\n    reading++;\n    unlock( m)\n/* Read here! */\n    lock( m)\n    reading--\n    cond_signal( cv)\n    unlock( m)\n}  Does this mean that a writer and read could read and write at the same time? No! First of all, remember cond_wait requires the thread re-acquire the  mutex lock before returning. Thus only one thread can be executing code inside the critical section (marked with **) at a time!  read() {\n    lock( m);\n**  while (writing)\n**      cond_wait( cv,  m)\n**  reading++;\n    unlock( m)\n/* Read here! */\n    lock( m)\n**  reading--\n**  cond_signal( cv)\n    unlock( m)\n}  Writers must wait for everyone. Mutual exclusion is assured by the lock.   write() {\n    lock( m);\n**  while (reading || writing)\n**      cond_wait( cv,  m);\n**  writing++;\n**\n** /* Write here! */\n**  writing--;\n**  cond_signal( cv);\n    unlock( m);\n}  Candidate #3 above also uses  pthread_cond_signal  ; this will only wake up one thread. For example, if many readers are waiting for the writer to complete then only one sleeping reader will be awoken from their slumber. The reader and writer should use  cond_broadcast  so that all threads should wake up and check their while-loop condition.", 
            "title": "Candidate solution #3"
        }, 
        {
            "location": "/Synchronization,-Part-9:-The-Reader-Writer-Problem-(part-2)/#starving-writers", 
            "text": "Candidate #3 above suffers from starvation. If readers are constantly arriving then a writer will never be able to proceed (the 'reading' count never reduces to zero). This is known as  starvation  and would be discovered under heavy loads. Our fix is to implement a bounded-wait for the writer. If a writer arrives they will still need to wait for existing readers however future readers must be placed in a \"holding pen\" and wait for the writer to finish. The \"holding pen\" can be implemented using a variable and a condition variable (so that we can wake up the threads once the writer has finished).  Our plan is that when a writer arrives, and before waiting for current readers to finish, register our intent to write (by incrementing a counter 'writer'). Sketched below -   write() {\n    lock()\n    writer++\n\n    while (reading || writing)\n    cond_wait\n    unlock()\n  ...\n}  And incoming readers will not be allowed to continue while writer is nonzero. Notice 'writer' indicates a writer has arrived, while 'reading' and 'writing' counters indicate there is an  active  reader or writer.  read() {\n    lock()\n    // readers that arrive *after* the writer arrived will have to wait here!\n    while(writer)\n    cond_wait( cv, m)\n\n    // readers that arrive while there is an active writer\n    // will also wait.\n    while (writing) \n        cond_wait( cv, m)\n    reading++\n    unlock\n  ...\n}", 
            "title": "Starving writers"
        }, 
        {
            "location": "/Synchronization,-Part-9:-The-Reader-Writer-Problem-(part-2)/#candidate-solution-4", 
            "text": "Below is our first working solution to the Reader-Writer problem. \nNote if you continue to read about the \"Reader Writer problem\" then you will discover that we solved the \"Second Reader Writer problem\" by giving writers preferential access to the lock. This solution is not optimal. However it satisfies our original problem (N active readers, single active writer, avoids starvation of the writer if there is a constant stream of readers).   Can you identify any improvements? For example, how would you improve the code so that we only woke up readers or one writer?   \nint writers; // Number writer threads that want to enter the critical section (some or all of these may be blocked)\nint writing; // Number of threads that are actually writing inside the C.S. (can only be zero or one)\nint reading; // Number of threads that are actually reading inside the C.S.\n// if writing !=0 then reading must be zero (and vice versa)\n\nreader() {\n    lock( m)\n    while (writers)\n        cond_wait( turn,  m)\n    // No need to wait while(writing here) because we can only exit the above loop\n    // when writing is zero\n    reading++\n    unlock( m)\n\n  // perform reading here\n\n    lock( m)\n    reading--\n    cond_broadcast( turn)\n    unlock( m)\n}\n\nwriter() {\n    lock( m)  \n    writers++  \n    while (reading || writing)   \n        cond_wait( turn,  m)  \n    writing++  \n    unlock( m)  \n    // perform writing here  \n    lock( m)  \n    writing--  \n    writers--  \n    cond_broadcast( turn)  \n    unlock( m)  \n}", 
            "title": "Candidate solution #4"
        }, 
        {
            "location": "/Synchronization-Concepts:-Review-Questions/", 
            "text": "Note thread-programming synchronization problems are on a separate wiki page. This page focuses on conceptual topics.\nQuestion numbers subject to change\n\n\n\n\nQ1\n\n\nWhat do each of the Coffman conditions mean? (e.g. can you provide a definition of each one)\n\n Hold and wait\n\n Circular wait\n\n No pre-emption\n\n Mutual exclusion\n\n\nQ2\n\n\nGive a real life example of breaking each Coffman condition in turn. A situation to consider: Painters, paint and paint brushes.\nHold and wait\nCircular wait\nNo pre-emption\nMutual exclusion\n\n\nQ3\n\n\nIdentify when Dining Philosophers code causes a deadlock (or not). For example, if you saw the following code snippet which Coffman condition is not satisfied?\n\n\n// Get both locks or none.\npthread_mutex_lock( a );\nif( pthread_mutex_trylock( b ) ) { /*failed*/\n   pthread_mutex_unlock( a );\n   ...\n}\n\n\n\n\nQ4\n\n\nHow many processes are blocked?\n\n\nP1 acquires R1\nP2 acquires R2\nP1 acquires R3\nP2 waits for R3\nP3 acquires R5\nP1 acquires for R4\nP3 waits for R1\nP4 waits for R5\nP5 waits for R1\n\n\nQ5\n\n\nHow many of the following statements are true for the reader-writer problem?\n\n\n\n\nThere can be multiple active readers\n\n\nThere can be multiple active writers\n\n\nWhen there is an active writer the number of active readers must be zero\n\n\nIf there is an active reader the number of active writers must be zero\n\n\nA writer must wait until the current active readers have finished", 
            "title": "Synchronization Concepts: Review Questions"
        }, 
        {
            "location": "/Synchronization-Concepts:-Review-Questions/#q1", 
            "text": "What do each of the Coffman conditions mean? (e.g. can you provide a definition of each one)  Hold and wait  Circular wait  No pre-emption  Mutual exclusion", 
            "title": "Q1"
        }, 
        {
            "location": "/Synchronization-Concepts:-Review-Questions/#q2", 
            "text": "Give a real life example of breaking each Coffman condition in turn. A situation to consider: Painters, paint and paint brushes.\nHold and wait\nCircular wait\nNo pre-emption\nMutual exclusion", 
            "title": "Q2"
        }, 
        {
            "location": "/Synchronization-Concepts:-Review-Questions/#q3", 
            "text": "Identify when Dining Philosophers code causes a deadlock (or not). For example, if you saw the following code snippet which Coffman condition is not satisfied?  // Get both locks or none.\npthread_mutex_lock( a );\nif( pthread_mutex_trylock( b ) ) { /*failed*/\n   pthread_mutex_unlock( a );\n   ...\n}", 
            "title": "Q3"
        }, 
        {
            "location": "/Synchronization-Concepts:-Review-Questions/#q4", 
            "text": "How many processes are blocked?  P1 acquires R1\nP2 acquires R2\nP1 acquires R3\nP2 waits for R3\nP3 acquires R5\nP1 acquires for R4\nP3 waits for R1\nP4 waits for R5\nP5 waits for R1", 
            "title": "Q4"
        }, 
        {
            "location": "/Synchronization-Concepts:-Review-Questions/#q5", 
            "text": "How many of the following statements are true for the reader-writer problem?   There can be multiple active readers  There can be multiple active writers  When there is an active writer the number of active readers must be zero  If there is an active reader the number of active writers must be zero  A writer must wait until the current active readers have finished", 
            "title": "Q5"
        }, 
        {
            "location": "/System-Programming-Short-Stories-and-Songs/", 
            "text": "\"Scheduling The Last Time Slice\"\n\n\nLawrence Angrave 12/4/15 (an extract from the longer, unpublished story \"The Last Time Slice\")\n\n\n\"Decide,\" the computer said with parental patience but with an air of gravity and tempered impatience.\n\n\n\"Why does it have to be me?\" asked the last human.\n\n\n\"Because you are the only one left, and so the decision is yours.\"\n\n\n\"Why can't you? You are an infinite times more older, wiser. Why don't you just pick a random slice?\"\n\n\n\"This decision is yours. A gift, or curse if you will, from your distant elders. Heavier than any religious rite. This will be the last decision I, the ancients or anyone asks, or can ask, of you. With this last choice we will exhaust the last entropy stores. You will decide the last reality slice to have meaning and experience.\"\n\n\nThe human was quiet for a few minutes which the computer measured and counted with unnecessary accuracy. Eventually the computer decided that the human was no longer productively thinking about the problem in hand.\n\n\n\"What is the pattern of conscious if it is never made conscious?\" it asked. \"The universe must be self-aware, must experience itself for the Universe - for all life! - to have meaning. That is the ultimate truth that humanity discovered and celebrated. With no awareness, it is simply patterns, patterns of atoms or energy but without a single iota of meaning; mere shapes and representations encoded in geometric patterns of data, structure and energy.\"\n\n\n\n\nFile Descriptor at Urbana Champaign\n\n\nA System Programming parody by Angrave (November 2015). \nLyrics released under Creative Commons attribution 3.0 license.\n\n\nOriginal song \u201cBlank Space\u201d from Taylor Swift\u2019s \u201c1989\u201d album.\n\n\n[Verse 1]\nNice to join you\nWhere you been?\nI could show you idempotent things\nRPC, sockets, syn\nSaw your malloc and I thought oh my root\nLook at that race, you code up the next mistake\nWe got VMs, wanna play\nBounded wait, Dekker's flags\nWe can frag you like a placement scheme\nAint it funny to #define\nAnd I know you heard about free(3)\nSo malloc strlen plus one\nI'm waiting to see how this thread ends\nGrab your shell and a redirect out\nI can make your syscall good for a weekend\n\n\n[Pre-Chorus]\nSo it's gonna deadlock forever\nOr it's gonna bring the system down\nYou can tell me when it forkbombs\nIf valgrind was worth the pain\nGot a long list of deadlocked code\nGot root at Urbana Champaign\nCause you know we love tsan\nWhen c-lib calls your main\n\n\n[Chorus]\nCause we're root and we're reckless\nThis lab is way too hard\nIt'll leave you threadless\nOr asking the sizeof char\nGot a long list of pthread calls\nGot root at Urbana Champaign\nBut I got a file descriptor baby\nAnd I'll write(2) your name\n\n\n[Verse 2]\nMutex locks\nVirtual mem\nI could show you volatile things\nNetwork calls, IPC\nYou're the mask I'm your sig\nSchedule what you want\nRound Robin\u2026 with a small quanta\nBut the sleepsort is yet to run\nOh no\nScreaming, crying, runtime errors\nI could make all 'till it's Peterson's turn\nHeap allocator way too slow\nKeep you second guessing like a spurious wake\nWhere is that pipe? We get hot for multicore C\nBut you'll compile with -g\nCause darling I'm a nightmare dressed like a coding dream\n\n\n[Pre-Chorus]\n\n\n[Chorus]\n\n\nCompilers only parse code if it's torture\nDon't say I didn't say I didn't -Wall you\nCompilers only parse code if it's torture\nDon't say I didn't say I didn't -Wall you\n\n\n[Pre-Chorus]\n\n\n[Chorus]", 
            "title": "System Programming Short Stories and Songs"
        }, 
        {
            "location": "/System-Programming-Short-Stories-and-Songs/#scheduling-the-last-time-slice", 
            "text": "Lawrence Angrave 12/4/15 (an extract from the longer, unpublished story \"The Last Time Slice\")  \"Decide,\" the computer said with parental patience but with an air of gravity and tempered impatience.  \"Why does it have to be me?\" asked the last human.  \"Because you are the only one left, and so the decision is yours.\"  \"Why can't you? You are an infinite times more older, wiser. Why don't you just pick a random slice?\"  \"This decision is yours. A gift, or curse if you will, from your distant elders. Heavier than any religious rite. This will be the last decision I, the ancients or anyone asks, or can ask, of you. With this last choice we will exhaust the last entropy stores. You will decide the last reality slice to have meaning and experience.\"  The human was quiet for a few minutes which the computer measured and counted with unnecessary accuracy. Eventually the computer decided that the human was no longer productively thinking about the problem in hand.  \"What is the pattern of conscious if it is never made conscious?\" it asked. \"The universe must be self-aware, must experience itself for the Universe - for all life! - to have meaning. That is the ultimate truth that humanity discovered and celebrated. With no awareness, it is simply patterns, patterns of atoms or energy but without a single iota of meaning; mere shapes and representations encoded in geometric patterns of data, structure and energy.\"", 
            "title": "\"Scheduling The Last Time Slice\""
        }, 
        {
            "location": "/System-Programming-Short-Stories-and-Songs/#file-descriptor-at-urbana-champaign", 
            "text": "A System Programming parody by Angrave (November 2015). \nLyrics released under Creative Commons attribution 3.0 license.  Original song \u201cBlank Space\u201d from Taylor Swift\u2019s \u201c1989\u201d album.  [Verse 1]\nNice to join you\nWhere you been?\nI could show you idempotent things\nRPC, sockets, syn\nSaw your malloc and I thought oh my root\nLook at that race, you code up the next mistake\nWe got VMs, wanna play\nBounded wait, Dekker's flags\nWe can frag you like a placement scheme\nAint it funny to #define\nAnd I know you heard about free(3)\nSo malloc strlen plus one\nI'm waiting to see how this thread ends\nGrab your shell and a redirect out\nI can make your syscall good for a weekend  [Pre-Chorus]\nSo it's gonna deadlock forever\nOr it's gonna bring the system down\nYou can tell me when it forkbombs\nIf valgrind was worth the pain\nGot a long list of deadlocked code\nGot root at Urbana Champaign\nCause you know we love tsan\nWhen c-lib calls your main  [Chorus]\nCause we're root and we're reckless\nThis lab is way too hard\nIt'll leave you threadless\nOr asking the sizeof char\nGot a long list of pthread calls\nGot root at Urbana Champaign\nBut I got a file descriptor baby\nAnd I'll write(2) your name  [Verse 2]\nMutex locks\nVirtual mem\nI could show you volatile things\nNetwork calls, IPC\nYou're the mask I'm your sig\nSchedule what you want\nRound Robin\u2026 with a small quanta\nBut the sleepsort is yet to run\nOh no\nScreaming, crying, runtime errors\nI could make all 'till it's Peterson's turn\nHeap allocator way too slow\nKeep you second guessing like a spurious wake\nWhere is that pipe? We get hot for multicore C\nBut you'll compile with -g\nCause darling I'm a nightmare dressed like a coding dream  [Pre-Chorus]  [Chorus]  Compilers only parse code if it's torture\nDon't say I didn't say I didn't -Wall you\nCompilers only parse code if it's torture\nDon't say I didn't say I didn't -Wall you  [Pre-Chorus]  [Chorus]", 
            "title": "File Descriptor at Urbana Champaign"
        }, 
        {
            "location": "/Test-page/", 
            "text": "Test page please ignore.", 
            "title": "Test page"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/", 
            "text": "What is Virtual Memory?\n\n\nIn very simple embedded systems and early computers, processes directly access memory i.e. \"Address 1234\" corresponds to a particular byte stored in a particular part of physical memory.\nIn modern systems, this is no longer the case. Instead each process is isolated; and there is a translation process between the address of a particular CPU instruction or piece of data of a process and the actual byte of physical memory (\"RAM\"). Memory addresses are no longer 'real'; the process runs inside virtual memory. Virtual memory not only keeps processes safe (because one process cannot directly read or modify another process's memory) it also allows the system to efficiently allocate and re-allocate portions of memory to different processes.\n\n\nWhat is the MMU?\n\n\nThe Memory Management Unit is part of the CPU. It converts a virtual memory address into a physical address. The MMU may also interrupt the CPU if there is currently no mapping from a particular virtual address to a physical address or if the current CPU instruction attempts to write to location that the process only has read-access.\n\n\nSo how do we convert a virtual address into a physical address?\n\n\nImagine you had a 32 bit machine. Pointers can hold 32 bits i.e. they can address 2^32 different locations i.e. 4GB of memory (we will be following the standard convention of one address can hold one byte).\n\n\nImagine we had a large table - here's the clever part - stored in memory! For every possible address (all 4 billion of them) we will store the 'real' i.e. physical address. Each physical address will need 4 bytes (to hold the 32 bits).\nThis scheme would require 16 billion bytes to store all of entries. Oops - our lookup scheme would consume all of the memory that we could possibly buy for our 4GB machine.\nWe need to do better than this. Our lookup table better be smaller than the memory we have otherwise we will have no space left for our actual programs and operating system data.\nThe solution is to chunk memory into small regions called 'pages' and 'frames' and use a lookup table for each page.\n\n\nWhat is a page? How many of them are there?\n\n\nA page is a block of virtual memory. A typical block size on Linux operating system is 4KB (i.e. 2^12 addresses), though you can find examples of larger blocks.\n\n\nSo rather than talking about individual bytes we can talk about blocks of 4KBs. Each block is called a page. We can also number our pages (\"Page 0\" \"Page 1\" etc)\n\n\nEX: How many pages are there in a 32bit machine (assume page size of 4KB)?\n\n\nAnswer: 2^32 address / 2^12 = 2^20 pages.\n\n\nRemember that 2^10 is 1024, so 2^20 is a bit more than one million.\n\n\nFor a 64 bit machine, 2^64 / 2^12 = 2^52, which is roughly 10^15 pages.\n\n\nWhat is a frame?\n\n\nA frame (or sometimes called a 'page frame') is a block of \nphysical memory\n or RAM (=Random Access Memory). This kind of memory is occasionally called 'primary storage' (and contrasted with slower, secondary storage such as spinning disks that have lower access times)\n\n\nA frame is the same number of bytes as a virtual page. If a 32 bit machine has 2^32 (4GB) of RAM, then there will be the same number of them in the addressable space of the machine. It's unlikely that a 64 bit machine will ever have 2^64 bytes of RAM - can you see why?\n\n\nWhat is a page table and how big is it?\n\n\nA page table is a mapping between a page to the frame.\nFor example Page 1 might be mapped to frame 45, page 2 mapped to frame 30. Other frames might be currently unused or assigned to other running processes, or used internally by the operating system.\n\n\nA simple page table is just an array, \nint frame = table[ page_num ];\n\n\nFor a 32 bit machine with 4KB pages, each entry needs to hold a frame number - i.e. 20 bits because we calculated there are 2^20 frames. That's 2.5 bytes per entry! In practice, we'll round that up to 4 bytes per entry and find a use for those spare bits. With 4 bytes per entry x 2^20 entries = 4 MB of physical memory are required to hold the page table.\n\n\nFor a 64 bit machine with 4KB pages, each entry needs 52 bits. Let's round up to 64 bits (8 bytes) per entry. With 2^52 entries thats 2^55 bytes (roughly 40 peta bytes...) Oops our page table is too large.\n\n\nIn 64 bit architectures memory addresses are sparse, so we need a mechanism to reduce the page table size, given that most of the entries will never be used.\n\n\nWhat is the offset and how is it used?\n\n\nRemember our page table maps pages to frames, but each page is a block of contiguous addresses. How do we calculate which particular byte to use inside a particular frame? The solution is to re-use the lowest bits of the virtual memory address directly. For example, suppose our process is reading the following address-\n\nVirtualAddress = 11110000111100001111000010101010 (binary)\n\n\nOn a machine with page size 256 Bytes, then the lowest 8 bits (10101010) will be used as the offset.\nThe remaining upper bits will be the page number (111100001111000011110000).\n\n\nMulti-level page tables\n\n\nMulti-level pages are one solution to the page table size issue for 64 bit architectures. We'll look at the simplest implementation - a two level page table. Each table is a list of pointers that point to the next level of tables, not all sub-tables need to exist. An example, two level page table for a 32 bit architecture is shown below-\n\n\nVirtualAddress = 11110000111111110000000010101010 (binary)\n                 |_Index1_||        ||          | 10 bit Directory index\n                           |_Index2_||          | 10 bit Sub-table index\n                                     |__________| 12 bit offset (passed directly to RAM)\n\n\n\n\nIn the above scheme, determining the frame number requires two memory reads: The topmost 10 bits are used in a directory of page tables. If 2 bytes are used for each entry, we only need 2KB to store this entire directory. Each subtable will point to physical frames (i.e. required 4 bytes to store the 20 bits). However, for processes with only tiny memory needs, we only need to specify entries for low memory address (for the heap and program code) and high memory addresses (for the stack). Each subtable is 1024 entries x 4 bytes i.e. 4KB for each subtable. Thus the total memory overhead for our multi-level page table has shrunk from 4MB (for the single level) to 3 frames of memory (12KB) !\n\n\nDo page tables make memory access slower? (And what's a TLB)\n\n\nYes - Significantly ! (But thanks to clever hardware, usually no...)\nCompared to reading or writing memory directly.\nFor a single page table, our machine is now twice as slow! (Two memory accesses are required)\nFor a two-level page table, memory access is now three times as slow. (Three memory accesses are required)\n\n\nTo overcome this overhead the MMU includes an associative cache of recently-used  virtual-page-to-frame lookups. This cache is called the TLB (\"translation lookaside buffer\"). Everytime a virtual address needs to be translated into a physical memory location, the TLB is queried in parallel to the page table. For most memory accesses of most programs, there is a significant chance that the TLB has cached the results. However if a program does not have good cache coherence (for example is reading from random memory locations of many different pages) then the TLB will not have the result cache and now the MMU must use the much slower page table to determine the physical frame.\n\n\nCan frames be shared between processes?\nYes! In addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. Read only frames can then be safely shared between multiple processes. For example, the C-library instruction code can be shared between all processes that dynamically load the code into the process memory. Each process can only read that memory.\n\n\nIn addition, processes can share a page with a child process using the \nmmap\n system call.\n\n\nWhat else is stored in the page table and why?\n\n\nIn addition to read-only bit and usage statistics discussed above, it is common to store at least read-only, modification and execution information. \n\n\nWhat's a page fault?\n\n\nA page fault is when a running program tries to access some virtual memory in its address space that is not mapped to physical memory. Page faults will also occur in other situations, including in the next section.\n\n\nRead-only bit\n\n\nThe read-only bit marks the page as read-only. Attempts to write to the page will cause a page fault. The page fault will then be handled by the Kernel. Two examples of the read-only page include sharing the c runtime library between multiple processes (for security you wouldn't want to allow one process to modify the library); and Copy-On-Write where the cost of duplicating a page can be delayed until the first write occurs. \n\n\nDirty bit\n\n\nhttp://en.wikipedia.org/wiki/Page_table#Page_table_data\n\n\n\n\nThe dirty bit allows for a performance optimization. A page on disk that is paged in to physical memory, then read from, and subsequently paged out again does not need to be written back to disk, since the page hasn't changed. However, if the page was written to after it's paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. This strategy requires that the backing store retain a copy of the page after it is paged in to memory. When a dirty bit is not used, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment. When a dirty bit is used, at all times some pages will exist in both physical memory and the backing store.\n\n\n\n\nExecution bit\n\n\nThe execution bit defines whether bytes in a page can be executed as CPU instructions. By disabling a page, it prevents code that is maliciously stored in the process memory (e.g. by stack overflow) from being easily executed. (further reading: http://en.wikipedia.org/wiki/NX_bit#Hardware_background)\n\n\nFind out more\n\n\nA lower level more and more technical discussion of paging and page bits on x86 platform is discussed at [http://wiki.osdev.org/Paging]", 
            "title": "Virtual Memory, Part 1: Introduction to Virtual Memory"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-virtual-memory", 
            "text": "In very simple embedded systems and early computers, processes directly access memory i.e. \"Address 1234\" corresponds to a particular byte stored in a particular part of physical memory.\nIn modern systems, this is no longer the case. Instead each process is isolated; and there is a translation process between the address of a particular CPU instruction or piece of data of a process and the actual byte of physical memory (\"RAM\"). Memory addresses are no longer 'real'; the process runs inside virtual memory. Virtual memory not only keeps processes safe (because one process cannot directly read or modify another process's memory) it also allows the system to efficiently allocate and re-allocate portions of memory to different processes.", 
            "title": "What is Virtual Memory?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-the-mmu", 
            "text": "The Memory Management Unit is part of the CPU. It converts a virtual memory address into a physical address. The MMU may also interrupt the CPU if there is currently no mapping from a particular virtual address to a physical address or if the current CPU instruction attempts to write to location that the process only has read-access.", 
            "title": "What is the MMU?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#so-how-do-we-convert-a-virtual-address-into-a-physical-address", 
            "text": "Imagine you had a 32 bit machine. Pointers can hold 32 bits i.e. they can address 2^32 different locations i.e. 4GB of memory (we will be following the standard convention of one address can hold one byte).  Imagine we had a large table - here's the clever part - stored in memory! For every possible address (all 4 billion of them) we will store the 'real' i.e. physical address. Each physical address will need 4 bytes (to hold the 32 bits).\nThis scheme would require 16 billion bytes to store all of entries. Oops - our lookup scheme would consume all of the memory that we could possibly buy for our 4GB machine.\nWe need to do better than this. Our lookup table better be smaller than the memory we have otherwise we will have no space left for our actual programs and operating system data.\nThe solution is to chunk memory into small regions called 'pages' and 'frames' and use a lookup table for each page.", 
            "title": "So how do we convert a virtual address into a physical address?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-a-page-how-many-of-them-are-there", 
            "text": "A page is a block of virtual memory. A typical block size on Linux operating system is 4KB (i.e. 2^12 addresses), though you can find examples of larger blocks.  So rather than talking about individual bytes we can talk about blocks of 4KBs. Each block is called a page. We can also number our pages (\"Page 0\" \"Page 1\" etc)", 
            "title": "What is a page? How many of them are there?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#ex-how-many-pages-are-there-in-a-32bit-machine-assume-page-size-of-4kb", 
            "text": "Answer: 2^32 address / 2^12 = 2^20 pages.  Remember that 2^10 is 1024, so 2^20 is a bit more than one million.  For a 64 bit machine, 2^64 / 2^12 = 2^52, which is roughly 10^15 pages.", 
            "title": "EX: How many pages are there in a 32bit machine (assume page size of 4KB)?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-a-frame", 
            "text": "A frame (or sometimes called a 'page frame') is a block of  physical memory  or RAM (=Random Access Memory). This kind of memory is occasionally called 'primary storage' (and contrasted with slower, secondary storage such as spinning disks that have lower access times)  A frame is the same number of bytes as a virtual page. If a 32 bit machine has 2^32 (4GB) of RAM, then there will be the same number of them in the addressable space of the machine. It's unlikely that a 64 bit machine will ever have 2^64 bytes of RAM - can you see why?", 
            "title": "What is a frame?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-a-page-table-and-how-big-is-it", 
            "text": "A page table is a mapping between a page to the frame.\nFor example Page 1 might be mapped to frame 45, page 2 mapped to frame 30. Other frames might be currently unused or assigned to other running processes, or used internally by the operating system.  A simple page table is just an array,  int frame = table[ page_num ];  For a 32 bit machine with 4KB pages, each entry needs to hold a frame number - i.e. 20 bits because we calculated there are 2^20 frames. That's 2.5 bytes per entry! In practice, we'll round that up to 4 bytes per entry and find a use for those spare bits. With 4 bytes per entry x 2^20 entries = 4 MB of physical memory are required to hold the page table.  For a 64 bit machine with 4KB pages, each entry needs 52 bits. Let's round up to 64 bits (8 bytes) per entry. With 2^52 entries thats 2^55 bytes (roughly 40 peta bytes...) Oops our page table is too large.  In 64 bit architectures memory addresses are sparse, so we need a mechanism to reduce the page table size, given that most of the entries will never be used.", 
            "title": "What is a page table and how big is it?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-is-the-offset-and-how-is-it-used", 
            "text": "Remember our page table maps pages to frames, but each page is a block of contiguous addresses. How do we calculate which particular byte to use inside a particular frame? The solution is to re-use the lowest bits of the virtual memory address directly. For example, suppose our process is reading the following address- VirtualAddress = 11110000111100001111000010101010 (binary)  On a machine with page size 256 Bytes, then the lowest 8 bits (10101010) will be used as the offset.\nThe remaining upper bits will be the page number (111100001111000011110000).", 
            "title": "What is the offset and how is it used?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#multi-level-page-tables", 
            "text": "Multi-level pages are one solution to the page table size issue for 64 bit architectures. We'll look at the simplest implementation - a two level page table. Each table is a list of pointers that point to the next level of tables, not all sub-tables need to exist. An example, two level page table for a 32 bit architecture is shown below-  VirtualAddress = 11110000111111110000000010101010 (binary)\n                 |_Index1_||        ||          | 10 bit Directory index\n                           |_Index2_||          | 10 bit Sub-table index\n                                     |__________| 12 bit offset (passed directly to RAM)  In the above scheme, determining the frame number requires two memory reads: The topmost 10 bits are used in a directory of page tables. If 2 bytes are used for each entry, we only need 2KB to store this entire directory. Each subtable will point to physical frames (i.e. required 4 bytes to store the 20 bits). However, for processes with only tiny memory needs, we only need to specify entries for low memory address (for the heap and program code) and high memory addresses (for the stack). Each subtable is 1024 entries x 4 bytes i.e. 4KB for each subtable. Thus the total memory overhead for our multi-level page table has shrunk from 4MB (for the single level) to 3 frames of memory (12KB) !  Do page tables make memory access slower? (And what's a TLB)  Yes - Significantly ! (But thanks to clever hardware, usually no...)\nCompared to reading or writing memory directly.\nFor a single page table, our machine is now twice as slow! (Two memory accesses are required)\nFor a two-level page table, memory access is now three times as slow. (Three memory accesses are required)  To overcome this overhead the MMU includes an associative cache of recently-used  virtual-page-to-frame lookups. This cache is called the TLB (\"translation lookaside buffer\"). Everytime a virtual address needs to be translated into a physical memory location, the TLB is queried in parallel to the page table. For most memory accesses of most programs, there is a significant chance that the TLB has cached the results. However if a program does not have good cache coherence (for example is reading from random memory locations of many different pages) then the TLB will not have the result cache and now the MMU must use the much slower page table to determine the physical frame.  Can frames be shared between processes?\nYes! In addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. Read only frames can then be safely shared between multiple processes. For example, the C-library instruction code can be shared between all processes that dynamically load the code into the process memory. Each process can only read that memory.  In addition, processes can share a page with a child process using the  mmap  system call.", 
            "title": "Multi-level page tables"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#what-else-is-stored-in-the-page-table-and-why", 
            "text": "In addition to read-only bit and usage statistics discussed above, it is common to store at least read-only, modification and execution information.", 
            "title": "What else is stored in the page table and why?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#whats-a-page-fault", 
            "text": "A page fault is when a running program tries to access some virtual memory in its address space that is not mapped to physical memory. Page faults will also occur in other situations, including in the next section.", 
            "title": "What's a page fault?"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#read-only-bit", 
            "text": "The read-only bit marks the page as read-only. Attempts to write to the page will cause a page fault. The page fault will then be handled by the Kernel. Two examples of the read-only page include sharing the c runtime library between multiple processes (for security you wouldn't want to allow one process to modify the library); and Copy-On-Write where the cost of duplicating a page can be delayed until the first write occurs.", 
            "title": "Read-only bit"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#dirty-bit", 
            "text": "http://en.wikipedia.org/wiki/Page_table#Page_table_data   The dirty bit allows for a performance optimization. A page on disk that is paged in to physical memory, then read from, and subsequently paged out again does not need to be written back to disk, since the page hasn't changed. However, if the page was written to after it's paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. This strategy requires that the backing store retain a copy of the page after it is paged in to memory. When a dirty bit is not used, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment. When a dirty bit is used, at all times some pages will exist in both physical memory and the backing store.", 
            "title": "Dirty bit"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#execution-bit", 
            "text": "The execution bit defines whether bytes in a page can be executed as CPU instructions. By disabling a page, it prevents code that is maliciously stored in the process memory (e.g. by stack overflow) from being easily executed. (further reading: http://en.wikipedia.org/wiki/NX_bit#Hardware_background)", 
            "title": "Execution bit"
        }, 
        {
            "location": "/Virtual-Memory,-Part-1:-Introduction-to-Virtual-Memory/#find-out-more", 
            "text": "A lower level more and more technical discussion of paging and page bits on x86 platform is discussed at [http://wiki.osdev.org/Paging]", 
            "title": "Find out more"
        }, 
        {
            "location": "/_Footer/", 
            "text": "Legal and Licensing information: Unless otherwise specified, submitted content to the wiki must be original work (including text, java code, and media) and you provide this material under a \nCreative Commons License\n. If you are not the copyright holder, please give proper attribution and credit to existing content and ensure that you have license to include the materials.", 
            "title": " Footer"
        }
    ]
}